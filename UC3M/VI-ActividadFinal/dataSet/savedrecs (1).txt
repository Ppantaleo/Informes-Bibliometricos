PT	AU	BA	CA	GP	RI	OI	BE	Z2	AU	AA	TI	X1	Y1	Z1	FT	PN	AE	Z3	SO	S1	SE	BS	VL	IS	SI	MA	BP	EP	AR	VN	VH	DI	D2	L1	L2	L3	EA	SU	DT	PD	PY	AB	X4	Y4	Z4	AK	CT	CY	SP	CL	TC	Z8	ZR	ZA	ZB	ZS	Z9	U1	U2	SN	EI	BN	G1	NR	CR	LA	AS	AC	CG	DG	C1	C3	EC	DE	DA	UT	PM	
J	Gu, Albert; Dao, Tri										Mamba: Linear-Time Sequence Modeling with Selective State Spaces								Arxiv											2	2;2024-05-31;https://www.arxiv.org/abs/2312.00752v2| 1;2023-12-01;https://www.arxiv.org/abs/2312.00752v1	arXiv:2312.00752			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 31 2024	2024	Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers’ computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content -based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware -aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end -to -end neural network architecture without attention or even MLP blocks ( Mamba ). Mamba enjoys fast inference (5× × higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million -length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba -3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.																																	2024-06-19	PPRN:86357134		
J	Oquab, Maxime; Darcet, Timothee; Moutakanni, Theo; Vo, Huy V.; Szafraniec, Marc; Khalidov, Vasil; Fernandez, Pierre; Haziza, Daniel; Massa, Francisco; El-Nouby, Alaaeldin; Assran, Mahmoud; Ballas, Nicolas; Galuba, Wojciech; Howes, Russell; Huang, Po-Yao; Li, Shang-Wen; Misra, Ishan; Rabbat, Michael; Sharma, Vasu; Synnaeve, Gabriel; Xu, Hu; Jegou, Herve; Mairal, Julien; Labatut, Patrick; Joulin, Armand; Bojanowski, Piotr				Li, Shangwen/OYE-2845-2025; Mairal, Julien/AAL-5611-2021; Huang, Poyao/IAQ-3889-2023; Rabbat, Michael/G-4582-2012						DINOv2: Learning Robust Visual Features without Supervision								Arxiv											2	2;2024-02-02;https://www.arxiv.org/abs/2304.07193v2| 1;2023-04-14;https://www.arxiv.org/abs/2304.07193v1	arXiv:2304.07193			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.																																	2024-02-19	PPRN:62659142		
J	Liu, Aixin; Feng, Bei; Xue, Bing; Wang, Bingxuan; Wu, Bochao; Lu, Chengda; Zhao, Chenggang; Deng, Chengqi; Zhang, Chenyu; Ruan, Chong; Dai, Damai; Guo, Daya; Yang, Dejian; Chen, Deli; Ji, Dongjie; Li, Erhang; Lin, Fangyun; Dai, Fucong; Luo, Fuli; Hao, Guangbo; Chen, Guanting; Li, Guowei; Zhang, H.; Bao, Han; Xu, Hanwei; Wang, Haocheng; Zhang, Haowei; Ding, Honghui; Xin, Huajian; Gao, Huazuo; Li, Hui; Qu, Hui; Cai, J.L.; Liang, Jian; Guo, Jianzhong; Ni, Jiaqi; Li, Jiashi; Wang, Jiawei; Chen, Jin; Chen, Jingchang; Yuan, Jingyang; Qiu, Junjie; Li, Junlong; Song, Junxiao; Dong, Kai; Hu, Kai; Gao, Kaige; Guan, Kang; Huang, Kexin; Yu, Kuai; Wang, Lean; Zhang, Lecong; Xu, Lei; Xia, Leyi; Zhao, Liang; Wang, Litong; Zhang, Liyue; Li, Meng; Wang, Miaojun; Zhang, Mingchuan; Zhang, Minghua; Tang, Minghui; Li, Mingming; Tian, Ning; Huang, Panpan; Wang, Peiyi; Zhang, Peng; Wang, Qiancheng; Zhu, Qihao; Chen, Qinyu; Du, Qiushi; Chen, R.J.; Jin, R.L.; Ge, Ruiqi; Zhang, Ruisong; Pan, Ruizhe; Wang, Runji; Xu, Runxin; Zhang, Ruoyu; Chen, Ruyi; Li, S.S.; Lu, Shanghao; Zhou, Shangyan; Chen, Shanhuang; Wu, Shaoqing; Ye, Shengfeng; Ye, Shengfeng; Ma, Shirong; Wang, Shiyu; Zhou, Shuang; Yu, Shuiping; Zhou, Shunfeng; Pan, Shuting; Wang, T.; Yun, Tao; Pei, Tian; Sun, Tianyu; Xiao, W.L.; Zeng, Wangding; Zhao, Wanjia; An, Wei; Liu, Wen; Liang, Wenfeng; Gao, Wenjun; Yu, Wenqin; Zhang, Wentao; Li, X.Q.; Jin, Xiangyue; Wang, Xianzu; Bi, Xiao; Liu, Xiaodong; Wang, Xiaohan; Shen, Xiaojin; Chen, Xiaokang; Zhang, Xiaokang; Chen, Xiaosha; Nie, Xiaotao; Sun, Xiaowen; Wang, Xiaoxiang; Cheng, Xin; Liu, Xin; Xie, Xin; Liu, Xingchao; Yu, Xingkai; Song, Xinnan; Shan, Xinxia; Zhou, Xinyi; Yang, Xinyu; Li, Xinyuan; Su, Xuecheng; Lin, Xuheng; Li, Y.K.; Wang, Y.Q.; Wei, Y.X.; Zhu, Y.X.; Zhang, Yang; Xu, Yanhong; Xu, Yanhong; Huang, Yanping; Li, Yao; Zhao, Yao; Sun, Yaofeng; Li, Yaohui; Wang, Yaohui; Yu, Yi; Zheng, Yi; Zhang, Yichao; Shi, Yifan; Xiong, Yiliang; He, Ying; Tang, Ying; Piao, Yishi; Wang, Yisong; Tan, Yixuan; Ma, Yiyang; Liu, Yiyuan; Guo, Yongqiang; Wu, Yu; Ou, Yuan; Zhu, Yuchen; Wang, Yuduan; Gong, Yue; Zou, Yuheng; He, Yujia; Zha, Yukun; Xiong, Yunfan; Ma, Yunxian; Yan, Yuting; Luo, Yuxiang; You, Yuxiang; Liu, Yuxuan; Zhou, Yuyang; Wu, Z.F.; Ren, Z.Z.; Ren, Zehui; Sha, Zhangli; Fu, Zhe; Xu, Zhean; Huang, Zhen; Zhang, Zhen; Xie, Zhenda; Zhang, Zhengyan; Hao, Zhewen; Gou, Zhibin; Ma, Zhicheng; Yan, Zhigang; Shao, Zhihong; Xu, Zhipeng; Wu, Zhiyu; Zhang, Zhongyu; Li, Zhuoshu; Gu, Zihui; Zhu, Zijia; Liu, Zijun; Li, Zilin; Xie, Ziwei; Song, Ziyang; Gao, Ziyi; Pan, Zizheng				Yuan, Jingyang/MCX-3868-2025; gao, wenjun/KIA-6581-2024; Zhu, Qihao/JWO-8071-2024; Zhou, Yuyang/U-7520-2018; Liu, Yuxuan/CAI-9269-2022; Sun, Xiaowen/M-6278-2014; Fu, Zhe/LGY-0539-2024; Zhou, Shuang/KMY-7627-2024; Gong, Yue/MGT-4407-2025; SHAN, XIN/KLD-0029-2024; Gao, Ziyi/JRW-0542-2023; Lu, Chengda/HCH-2781-2022; TANG, YINGZHOU/JDW-0473-2023; Zhao, Liang/GRO-5936-2022; ding, honghui/HNQ-3893-2023; Bi, Xiao/GQG-9410-2022; Xie, Zhenda/O-1198-2013; Li, Junlong/IRZ-0254-2023; Chen, Qinyu/KSL-7335-2024; Dai, Damai/KEJ-3256-2024; Yu, Yi/KTI-6745-2024; Yu, Xingkai/AAO-5118-2020; Chen, Xiaosha/AAV-2753-2020; Hao, Guangbo/D-2429-2012; He, Yujia/KCY-7219-2024; Huang, Pan-Pan/GNH-5177-2022; Yan, Zhigang/IUP-3212-2023; Chen, Deli/LTF-5854-2024; ZHANG, CHENYU/LRC-0575-2024; XUE, Bing/JOZ-6681-2023; HUANG, Yanping/A-2726-2010; Ni, Jiaqi/AEP-9771-2022; Huang, Kexin/IGN-8658-2023; Qiu, Junjie/KCZ-2676-2024; zhengyan, zhang/D-2029-2012; Zhang, Ruoyu/AEL-8074-2022; LIU, YIYUAN/KVB-1577-2024; Zhang, Hewei/LJZ-4067-2024; wang, peiyi/LNR-6224-2024; Xu, Yanhong/HSH-1887-2023; Wang, Shiyu/LQL-0370-2024; Zhang, Mingchuan/I-6902-2018; Zheng, Yiming/AFQ-9684-2022; Guo, Daya/HPG-8192-2023						DeepSeek-V3 Technical Report								Arxiv											1	1;2024-12-27;https://www.arxiv.org/abs/2412.19437v1	arXiv:2412.19437			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 27 2024	2024	We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.  [Graphics]																																	2025-01-24	PPRN:120253656		
J	Shao, Zhihong; Wang, Peiyi; Zhu, Qihao; Xu, Runxin; Song, Junxiao; Bi, Xiao; Zhang, Haowei; Zhang, Mingchuan; Li, Y.K.; Wu, Y.; Guo, Daya				wang, peiyi/LNR-6224-2024; Zhu, Qihao/JWO-8071-2024; Guo, Daya/HPG-8192-2023; Bi, Xiao/GQG-9410-2022; Zhang, Mingchuan/I-6902-2018						DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models								Arxiv											2	2;2024-04-27;https://www.arxiv.org/abs/2402.03300v3| 1;2024-02-06;https://www.arxiv.org/abs/2402.03300v2	arXiv:2402.03300			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 27 2024	2024	Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.																																	2024-05-15	PPRN:87534114		
J	Wang, Peng; Bai, Shuai; Tan, Sinan; Wang, Shijie; Fan, Zhihao; Bai, Jinze; Chen, Keqin; Liu, Xuejing; Wang, Jialin; Ge, Wenbin; Fan, Yang; Dang, Kai; Du, Mengfei; Ren, Xuancheng; Men, Rui; Liu, Dayiheng; Zhou, Chang; Zhou, Jingren; Lin, Junyang				Zhou, Mingyuan/AAE-8717-2021; Men, Rui/ABD-4990-2021; Wang, Pengcheng/JYQ-2527-2024; Chen, Keqin/W-1523-2018						Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution								Arxiv											2	2;2024-10-03;https://www.arxiv.org/abs/2409.12191v2| 1;2024-09-18;https://www.arxiv.org/abs/2409.12191v1	arXiv:2409.12191			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. 																																	2024-10-18	PPRN:92079253		
J	Jiang, Albert Q.; Sablayrolles, Alexandre; Roux, Antoine; Mensch, Arthur; Savary, Blanche; Bamford, Chris; Chaplot, Devendra Singh; de las Casas, Diego; Hanna, Emma Bou; Bressand, Florian; Lengyel, Gianna; Bour, Guillaume; Lample, Guillaume; Lavaud, Lelio Renard; Saulnier, Lucile; Lachaux, Marie-Anne; Stock, Pierre; Subramanian, Sandeep; Yang, Sophia; Antoniak, Szymon; Le Scao, Teven; Gervet, Theophile; Lavril, Thibaut; Wang, Thomas; Lacroix, Timothee; El Sayed, William				Hanna, Emma/OUI-8179-2025						Mixtral of Experts								Arxiv											1	1;2024-01-08;https://www.arxiv.org/abs/2401.04088v1	arXiv:2401.04088			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 08 2024	2024	We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model finetuned to follow instructions, Mixtral 8x7B – Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.																																	2024-05-25	PPRN:87027637		
J	Gao, Yunfan; Xiong, Yun; Gao, Xinyu; Jia, Kangxiang; Pan, Jinliu; Bi, Yuxi; Dai, Yi; Sun, Jiawei; Wang, Meng; Wang, Haofen				Sun, Jiawei/AAD-5528-2022; Gao, Xinyu/GPP-0730-2022; Dai, Yi/AAC-1336-2019						Retrieval-Augmented Generation for Large Language Models: A Survey								Arxiv											5	5;2024-03-27;https://www.arxiv.org/abs/2312.10997v5| 4;2024-01-05;https://www.arxiv.org/abs/2312.10997v4| 3;2024-01-03;https://www.arxiv.org/abs/2312.10997v3| 2;2023-12-29;https://www.arxiv.org/abs/2312.10997v2| 1;2023-12-18;https://www.arxiv.org/abs/2312.10997v1	arXiv:2312.10997			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 27 2024	2024	Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.																																	2024-04-15	PPRN:86686514		
J	Roziere, Baptiste; Gehring, Jonas; Gloeckle, Fabian; Sootla, Sten; Itai, Gat; Tan, Xiaoqing Ellen; Adi, Yossi; Liu, Jingyu; Sauvestre, Romain; Remez, Tal; Rapin, Jeremy; Kozhevnikov, Artyom; Evtimov, Ivan; Bitton, Joanna; Bhatt, Manish; Ferrer, Cristian Canton; Grattafiori, Aaron; Xiong, Wenhan; Defossez, Alexandre; Copet, Jade; Azhar, Faisal; Touvron, Hugo; Martin, Louis; Usunier, Nicolas; Scialom, Thomas; Synnaeve, Gabriel				Touvron, Hugo/AAW-1800-2021; Adi, Yossi/HLG-5748-2023; Bhatt, Manish/O-5347-2019						Code Llama: Open Foundation Models for Code								Arxiv											3	3;2024-01-31;https://www.arxiv.org/abs/2308.12950v3| 2;2023-08-25;https://www.arxiv.org/abs/2308.12950v2| 1;2023-08-24;https://www.arxiv.org/abs/2308.12950v1	arXiv:2308.12950			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 31 2024	2024	We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.																																	2024-05-25	PPRN:83522477		
J	Abdin, Marah; Jacobs, Sam Ade; Awan, Ammar Ahmad; Aneja, Jyoti; Awadallah, Ahmed; Awadalla, Hany; Bach, Nguyen; Bahree, Amit; Bakhtiari, Arash; Behl, Harkirat; Benhaim, Alon; Bilenko, Misha; Bjorck, Johan; Bubeck, Sebastien; Cai, Martin; Mendes, Caio Cesar Teodoro; Chen, Weizhu; Chaudhary, Vishrav; Chopra, Parul; Giorno, Allie Del; Rosa, Gustavo de; Dixon, Matthew; Eldan, Ronen; Iter, Dan; Garg, Amit; Goswami, Abhishek; Gunasekar, Suriya; Haider, Emman; Hao, Junheng; Hewett, Russell J.; Huynh, Jamie; Javaheripi, Mojan; Jin, Xin; Kauffmann, Piero; Karampatziakis, Nikos; Kim, Dongwoo; Khademi, Mahoud; Kurilenko, Lev; Lee, James R.; Lee, Yin Tat; Li, Yuanzhi; Liang, Chen; Liu, Weishung; Lin, Eric; Lin, Zeqi; Madan, Piyush; Mitra, Arindam; Modi, Hardik; Nguyen, Anh; Norick, Brandon; Patra, Barun; Perez-Becker, Daniel; Portet, Thomas; Pryzant, Reid; Qin, Heyang; Radmilac, Marko; Rosset, Corby; Roy, Sambudha; Ruwase, Olatunji; Saarikivi, Olli; Saied, Amin; Salim, Adil; Santacroce, Michael; Shah, Shital; Shang, Ning; Sharma, Hiteshi; Song, Xia; Tanaka, Masahiro; Wang, Xin; Ward, Rachel; Wang, Guanhua; Witte, Philipp; Wyatt, Michael; Xu, Can; Xu, Jiahang; Yadav, Sonali; Yang, Fan; Yang, Ziyi; Yu, Donghan; Zhang, Chengruidong; Zhang, Cyril; Zhang, Jianwen; Zhang, Li Lyna; Zhang, Yi; Zhang, Yue; Zhang, Yunan; Zhou, Xiren				yang, ziyi/JGD-5349-2023; Hao, Junheng/AAJ-8770-2020; Wang, Guanhua/AAF-7033-2021; NGUYEN, ANH/JGM-7116-2023; Shang, Ning/KIB-6928-2024; Witte, Philipp/AAA-1287-2021; Awadallah, Ahmed/MIQ-7219-2025; Benhaim, Alon/KXR-4981-2024; Zhang, JW/G-3488-2010						Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone								Arxiv											1	1;2024-04-23;https://www.arxiv.org/abs/2404.14219v2	arXiv:2404.14219			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 23 2024	2024	We introduce phi -3 -mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi -3 -mini achieves 69% on MMLU and 8.38 on MT -bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled -up version of the one used for phi -2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter -scaling results with a 7B and 14B models trained for 4.8T tokens, called phi -3 -small and phi -3 -medium, both significantly more capable than phi -3 -mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT -bench).																																	2024-05-02	PPRN:88622832		
J	Dong, Qingxiu; Li, Lei; Dai, Damai; Zheng, Ce; Ma, Jingyuan; Li, Rui; Xia, Heming; Xu, Jingjing; Wu, Zhiyong; Chang, Baobao; Sun, Xu; Sui, Zhifang				Liu, Tianyu/HGE-7095-2022; Li, Li/HMO-8564-2023; Xu, Jingjing/ACJ-3010-2022; Li, Lei/LMN-0940-2024; sun, xu/JCN-6491-2023; Ma, Jing-Yuan/GPW-8009-2022; Dai, Damai/KEJ-3256-2024						A Survey on In-context Learning								Arxiv											4	4;2024-10-05;https://www.arxiv.org/abs/2301.00234v6| 3;2024-09-27;https://www.arxiv.org/abs/2301.00234v5| 2;2024-06-18;https://www.arxiv.org/abs/2301.00234v4| 1;2022-12-31;https://www.arxiv.org/abs/2301.00234v1	arXiv:2301.00234			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.																																	2025-08-07	PPRN:35833642		
J	Yang, An; Yang, Baosong; Hui, Binyuan; Zheng, Bo; Yu, Bowen; Zhou, Chang; Li, Chengpeng; Li, Chengyuan; Liu, Dayiheng; Huang, Fei; Dong, Guanting; Wei, Haoran; Lin, Huan; Tang, Jialong; Wang, Jialin; Yang, Jian; Tu, Jianhong; Zhang, Jianwei; Ma, Jianxin; Yang, Jianxin; Xu, Jin; Zhou, Jingren; Bai, Jinze; He, Jinzheng; Lin, Junyang; Dang, Kai; Lu, Keming; Chen, Keqin; Yang, Kexin; Li, Mei; Xue, Mingfeng; Ni, Na; Zhang, Pei; Wang, Peng; Peng, Ru; Men, Rui; Gao, Ruize; Lin, Runji; Wang, Shijie; Bai, Shuai; Tan, Sinan; Zhu, Tianhang; Li, Tianhao; Liu, Tianyu; Ge, Wenbin; Deng, Xiaodong; Zhou, Xiaohuan; Ren, Xingzhang; Zhang, Xinyu; Wei, Xipin; Ren, Xuancheng; Liu, Xuejing; Fan, Yang; Yao, Yang; Zhang, Yichang; Wan, Yu; Chu, Yunfei; Liu, Yuqiong; Cui, Zeyu; Zhang, Zhenru; Guo, Zhifang; Fan, Zhihao				NI, NI/NMJ-9966-2025; Yang, Jianyu/KBD-3342-2024; Wang, Pengcheng/JYQ-2527-2024; Bin/B-6414-2019; Chu, Yunfei/C-8002-2013; Deng, Xiao-dong/M-8286-2019; Yang, Yu/GYJ-1931-2022; Wei, Bing-Yan/T-1552-2019; Bowen, Yu/MFH-7462-2025; Chen, Keqin/W-1523-2018; Zheng, Bo/JDW-6453-2023; Yang, Jianxin/AAU-3843-2021; Yang, Xiyu/KBC-4381-2024; dong, guanting/JGL-9364-2023; Yang, An/NXY-1481-2025; Ma, Jianxin/E-8716-2013; Gao, Ruize/GRO-1078-2022; Li, Chengyuan/AAO-3511-2020; 李, 程鹏/HLP-9229-2023; Men, Rui/ABD-4990-2021; Yang, Kexin/KBQ-5459-2024; Tang, Jialong/JMC-2179-2023; Zhou, Mingyuan/AAE-8717-2021						Qwen2 Technical Report								Arxiv											3	3;2024-09-10;https://www.arxiv.org/abs/2407.10671v4| 2;2024-07-18;https://www.arxiv.org/abs/2407.10671v3| 1;2024-07-16;https://www.arxiv.org/abs/2407.10671v2	arXiv:2407.10671			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 10 2024	2024	This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.   The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.   To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.																																	2024-09-19	PPRN:90851182		
J			Gemma Team								Gemma 2: Improving Open Language Models at a Practical Size								Arxiv											1	1;2024-08-02;https://www.arxiv.org/abs/2408.00118v2	arXiv:2408.00118			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 02 2024	2024	In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× × bigger. We release all our models to the community.																																	2025-03-02	PPRN:91197903		
J	Ravi, Nikhila; Gabeur, Valentin; Hu, Yuan-Ting; Hu, Ronghang; Ryali, Chaitanya; Ma, Tengyu; Khedr, Haitham; Radle, Roman; Rolland, Chloe; Gustafson, Laura; Mintun, Eric; Pan, Junting; Alwala, Kalyan Vasudev; Carion, Nicolas; Wu, Chao-Yuan; Girshick, Ross; Dollar, Piotr; Feichtenhofer, Christoph				Ma, Tengyu/GRX-6381-2022						SAM 2: Segment Anything in Images and Videos								Arxiv											2	2;2024-10-28;https://www.arxiv.org/abs/2408.00714v2| 1;2024-08-01;https://www.arxiv.org/abs/2408.00714v1	arXiv:2408.00714			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 28 2024	2024	We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3× fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6× faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.																																	2024-12-06	PPRN:91201803		
J	Li, Bo; Zhang, Yuanhan; Guo, Dong; Zhang, Renrui; Li, Feng; Zhang, Hao; Zhang, Kaichen; Li, Yanwei; Liu, Ziwei; Li, Chunyuan				Liu, Ziwei/AAG-6939-2021; Zhang, Zhuosheng/AAF-4919-2020						LLaVA-OneVision: Easy Visual Task Transfer								Arxiv											3	3;2024-10-26;https://www.arxiv.org/abs/2408.03326v3| 2;2024-09-14;https://www.arxiv.org/abs/2408.03326v2| 1;2024-08-06;https://www.arxiv.org/abs/2408.03326v1	arXiv:2408.03326			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 14 2024	2024	We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.																																	2024-12-24	PPRN:91260141		
J	Rafailov, Rafael; Sharma, Archit; Mitchell, Eric; Ermon, Stefano; Manning, Christopher D.; Finn, Chelsea				Manning, Christopher/A-1358-2007						Direct Preference Optimization: Your Language Model is Secretly a Reward Model								Arxiv											3	3;2024-07-29;https://www.arxiv.org/abs/2305.18290v3| 2;2023-12-13;https://www.arxiv.org/abs/2305.18290v2| 1;2023-05-29;https://www.arxiv.org/abs/2305.18290v1	arXiv:2305.18290			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 29 2024	2024	While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.																																	2024-08-06	PPRN:72752569		
J	Liu, Haotian; Li, Chunyuan; Li, Yuheng; Lee, Yong Jae				li, yuheng/LPQ-3255-2024; Li, Chunyuan/AAG-1303-2020						Improved Baselines with Visual Instruction Tuning								Arxiv											3	3;2024-05-15;https://www.arxiv.org/abs/2310.03744v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03744v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03744v1	arXiv:2310.03744			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 15 2024	2024	Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.																																	2024-06-01	PPRN:85430672		
J	Fu, Chaoyou; Chen, Peixian; Shen, Yunhang; Qin, Yulei; Zhang, Mengdan; Lin, Xu; Yang, Jinrui; Zheng, Xiawu; Li, Ke; Sun, Xing; Wu, Yunsheng; Ji, Rongrong				Yang, Jinrui/HJP-6768-2023; Zhang, Mengdan/LFV-0431-2024; Shan, Caifeng/W-6178-2019; Shen, Yunhang/ADW-0834-2022; Yang, Jinrui/HGE-7359-2022						MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models								Arxiv											3	3;2024-03-17;https://www.arxiv.org/abs/2306.13394v4| 2;2023-12-06;https://www.arxiv.org/abs/2306.13394v3| 1;2023-06-23;https://www.arxiv.org/abs/2306.13394v1	arXiv:2306.13394			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 17 2024	2024	Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME1. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instructionanswer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https:// github.com/BradyFU/Awesome-MultimodalLarge-Language-Models/tree/Evaluation.																																	2024-04-11	PPRN:73494068		
J	Ye, Qinghao; Xu, Haiyang; Xu, Guohai; Ye, Jiabo; Yan, Ming; Zhou, Yiyang; Wang, Junyang; Hu, Anwen; Shi, Pengcheng; Shi, Yaya; Li, Chenliang; Xu, Yuanhong; Chen, Hehong; Tian, Junfeng; Qian, Qi; Zhang, Ji; Huang, Fei; Zhou, Jingren				Zhou, Mingyuan/AAE-8717-2021; Shi, Pengcheng/AAY-3011-2021; wang, junyang/HMP-6590-2023; Yuanhong, Xu/AAY-4944-2020; Xu, Haiyang/AAC-2095-2021; Tian, Junfeng/AGY-1359-2022; Yan, Ming/LDT-2692-2024						mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality								Arxiv											3	3;2024-03-29;https://www.arxiv.org/abs/2304.14178v3| 2;2024-03-22;https://www.arxiv.org/abs/2304.14178v2| 1;2023-04-27;https://www.arxiv.org/abs/2304.14178v1	arXiv:2304.14178			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. 																																	2024-04-15	PPRN:65759635		
J	Guo, Daya; Zhu, Qihao; Yang, Dejian; Xie, Zhenda; Dong, Kai; Zhang, Wentao; Chen, Guanting; Bi, Xiao; Wu, Y.; Li, Y.K.; Luo, Fuli; Xiong, Yingfei; Liang, Wenfeng				Guo, Daya/HPG-8192-2023; Bi, Xiao/GQG-9410-2022; Xie, Zhenda/O-1198-2013; Zhu, Qihao/JWO-8071-2024						DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence								Arxiv											2	2;2024-01-26;https://www.arxiv.org/abs/2401.14196v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.14196v1	arXiv:2401.14196			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 26 2024	2024	The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.																																	2024-02-14	PPRN:87335722		
J	Snell, Charlie; Lee, Jaehoon; Xu, Kelvin; Kumar, Aviral										Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters								Arxiv											1	1;2024-08-06;https://www.arxiv.org/abs/2408.03314v1	arXiv:2408.03314			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 06 2024	2024	Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.																																	2024-08-11	PPRN:91255149		
J	Liu, Yuan; Duan, Haodong; Zhang, Yuanhan; Li, Bo; Zhang, Songyang; Zhao, Wangbo; Yuan, Yike; Wang, Jiaqi; He, Conghui; Liu, Ziwei; Chen, Kai; Lin, Dahua				Lin, Dahua/W-6576-2019; He, Conghui/AAZ-3323-2021; WANG, JIAQI/KBB-8837-2024; Duan, Haodong/ITV-1505-2023; Zhang, Songyang/GPX-5621-2022; Liu, Ziwei/AAG-6939-2021						MMBench: Is Your Multi-modal Model an All-around Player?								Arxiv											2	2;2024-04-29;https://www.arxiv.org/abs/2307.06281v4| 1;2023-07-12;https://www.arxiv.org/abs/2307.06281v1	arXiv:2307.06281			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 29 2024	2024	Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model’s abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs’ performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. 																																	2024-05-14	PPRN:73886632		
J	Huang, Lei; Yu, Weijiang; Ma, Weitao; Zhong, Weihong; Feng, Zhangyin; Wang, Haotian; Chen, Qianglong; Peng, Weihua; Feng, Xiaocheng; Qin, Bing; Liu, Ting				Haotian, Wang/HNQ-4046-2023						A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions								Arxiv											2	2;2024-11-19;https://www.arxiv.org/abs/2311.05232v2| 1;2023-11-09;https://www.arxiv.org/abs/2311.05232v1	arXiv:2311.05232			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 19 2024	2024	The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.																																	2024-12-28	PPRN:86112138		
J	Zhu, Lianghui; Liao, Bencheng; Zhang, Qian; Wang, Xinlong; Liu, Wenyu; Wang, Xinggang				Wenyu, Liu/GRS-3009-2022; Wang, Xinlong/AFI-8800-2022; Zhu, Lianghui/OZF-3258-2025; Wang, Xinggang/LSL-0946-2024						Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model								Arxiv											3	3;2024-11-14;https://www.arxiv.org/abs/2401.09417v3| 2;2024-02-10;https://www.arxiv.org/abs/2401.09417v2| 1;2024-01-17;https://www.arxiv.org/abs/2401.09417v1	arXiv:2401.09417			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 14 2024	2024	Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k  semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8× faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248×1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code and models are released at https://github.com/hustvl/Vim																																	2024-12-21	PPRN:87208488		
J	Lin, Bin; Ye, Yang; Zhu, Bin; Cui, Jiaxi; Ning, Munan; Jin, Peng; Yuan, Li				Jin, Peng/IAP-3718-2023; Lin, Bin/V-3431-2019; Yuan, Li/AET-1324-2022; cui, jiaxi/H-8790-2015						Video-LLaVA: Learning United Visual Representation by Alignment Before Projection								Arxiv											2	2;2024-10-01;https://www.arxiv.org/abs/2311.10122v3| 1;2023-11-16;https://www.arxiv.org/abs/2311.10122v1	arXiv:2311.10122			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 01 2024	2024	The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.  																																	2024-10-12	PPRN:86199199		
J	Hui, Binyuan; Yang, Jian; Cui, Zeyu; Yang, Jiaxi; Liu, Dayiheng; Zhang, Lei; Liu, Tianyu; Zhang, Jiajun; Yu, Bowen; Lu, Keming; Dang, Kai; Fan, Yang; Zhang, Yichang; Yang, An; Men, Rui; Huang, Fei; Zheng, Bo; Miao, Yibo; Quan, Shanghaoran; Feng, Yunlong; Ren, Xingzhang; Ren, Xuancheng; Zhou, Jingren; Lin, Junyang				Men, Rui/ABD-4990-2021; Zhou, Mingyuan/AAE-8717-2021; Bowen, Yu/MFH-7462-2025; Yang, An/NXY-1481-2025; Zheng, Bo/JDW-6453-2023; Bin/B-6414-2019; Yang, Jianyu/KBD-3342-2024						Qwen2.5-Coder Technical Report								Arxiv											2	2;2024-11-12;https://www.arxiv.org/abs/2409.12186v3| 1;2024-09-18;https://www.arxiv.org/abs/2409.12186v1	arXiv:2409.12186			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 12 2024	2024	In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pre- trained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.																																	2024-12-18	PPRN:91948834		
J	Zhang, Renrui; Han, Jiaming; Liu, Chris; Zhou, Aojun; Lu, Pan; Qiao, Yu; Li, Hongsheng; Gao, Peng				zhang, hongliang/AAC-6827-2021; Zhang, Zhuosheng/AAF-4919-2020; Gao, Peng/B-4675-2012						LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention								Arxiv											2	2;2024-09-18;https://www.arxiv.org/abs/2303.16199v3| 1;2023-03-28;https://www.arxiv.org/abs/2303.16199v1	arXiv:2303.16199			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 18 2024	2024	With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT. To this end, we present LLaMA-Adapter, , a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning. Specifically, a zero-initialized attention mechanism is proposed. It adopts a learnable zero gating to adaptively inject the instructional cues into LLaMA within self-attention layers, contributing to a stable training process and superior final performance. In this way, LLaMA-Adapter can generate high-quality responses to diverse language instructions, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, by incorporating an image encoder, our approach can be simply extended to a Multi-modal LLM for image-conditioned instruction following, which achieves superior multi-modal reasoning capacity on several popular benchmarks (MME, MMBench, LVLM-eHub). Furthermore, we also verify the proposed zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa, CLIP) on traditional vision and language tasks, demonstrating the effectiveness and generalizability of our approach. 																																	2024-09-30	PPRN:50138518		
J	Chao, Patrick; Robey, Alexander; Dobriban, Edgar; Hassani, Hamed; Pappas, George J.; Wong, Eric										Jailbreaking Black Box Large Language Models in Twenty Queries								Arxiv											3	3;2024-07-18;https://www.arxiv.org/abs/2310.08419v4| 2;2024-07-03;https://www.arxiv.org/abs/2310.08419v3| 1;2023-10-13;https://www.arxiv.org/abs/2310.08419v2	arXiv:2310.08419			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 18 2024	2024	There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.																																	2024-07-27	PPRN:85660231		
J	Guo, Yuwei; Yang, Ceyuan; Rao, Anyi; Liang, Zhengyang; Wang, Yaohui; Qiao, Yu; Agrawala, Maneesh; Lin, Dahua; Dai, Bo				Lin, Dahua/W-6576-2019; Rao, Anyi/IXE-1082-2023; Qiao, Yu/ABD-5787-2021						AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning								Arxiv											2	2;2024-02-08;https://www.arxiv.org/abs/2307.04725v2| 1;2023-07-10;https://www.arxiv.org/abs/2307.04725v1	arXiv:2307.04725			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 08 2024	2024	With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff.																																	2024-05-25	PPRN:73866716		
J	Zhang, Shengyu; Dong, Linfeng; Li, Xiaoya; Zhang, Sen; Sun, Xiaofei; Wang, Shuhe; Li, Jiwei; Hu, Runyi; Zhang, Tianwei; Wu, Fei; Wang, Guoyin				Shengyu, Zhang/KUC-9925-2024; Wang, Shuhe/AAG-5976-2021; Zhang, Tianwei/AAV-8818-2020; Sun, Xiaofei/S-9786-2018; Li, Jiwei/AAC-6522-2021						Instruction Tuning for Large Language Models: A Survey								Arxiv											7	7;2024-11-11;https://www.arxiv.org/abs/2308.10792v7| 6;2024-10-16;https://www.arxiv.org/abs/2308.10792v6| 5;2024-03-14;https://www.arxiv.org/abs/2308.10792v5| 4;2023-10-09;https://www.arxiv.org/abs/2308.10792v4| 3;2023-10-04;https://www.arxiv.org/abs/2308.10792v3| 2;2023-09-21;https://www.arxiv.org/abs/2308.10792v2| 1;2023-08-21;https://www.arxiv.org/abs/2308.10792v1	arXiv:2308.10792			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 11 2024	2024	This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)1, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of ( INSTRUCTION , OUTPUT) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users’ objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.																																	2024-12-19	PPRN:81868041		
J	Yu, Weihao; Yang, Zhengyuan; Li, Linjie; Wang, Jianfeng; Lin, Kevin; Liu, Zicheng; Wang, Xinchao; Wang, Lijuan				Li, Linjie/ABC-4651-2021; Lin, Kevin/JFS-1634-2023; Wang, Xinchao/L-7655-2018; Yang, Zhengyuan/AGQ-1232-2022; Yu, Weihao/HJH-1824-2023						MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities								Arxiv											4	4;2024-12-01;https://www.arxiv.org/abs/2308.02490v4| 3;2023-10-24;https://www.arxiv.org/abs/2308.02490v3| 2;2023-10-13;https://www.arxiv.org/abs/2308.02490v2| 1;2023-08-04;https://www.arxiv.org/abs/2308.02490v1	arXiv:2308.02490			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 01 2024	2024	We propose MM-Vet1, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM- Vet, designed based on the insight that the intriguing ability to solve complicated tasks often stems from a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from their combinations. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM- Vet, providing insights into the capabilities of different LMM system paradigms and model designs. 																																	2025-01-11	PPRN:74279774		
J	Maaz, Muhammad; Rasheed, Hanoona; Khan, Salman; Khan, Fahad Shahbaz				Maaz, Muhammad/GOK-1100-2022; Khan, Salman/M-4834-2016; Khan, Fahad Shahbaz/ABD-6646-2021						Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2306.05424v2| 1;2023-06-08;https://www.arxiv.org/abs/2306.05424v1	arXiv:2306.05424			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 10 2024	2024	Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image -based conversation models, this work addresses the under -explored field of video -based conversation by introducing Video-ChatGPT. It is a multimodal model that merges a video -adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video -instruction pairs used to train Video-ChatGPT acquired via manual and semi -automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for videobased dialogue models to objectively analyze the strengths and weaknesses of video -based dialogue models. 																																	2024-07-11	PPRN:73235773		
J	Kim, Moo Jin; Pertsch, Karl; Karamcheti, Siddharth; Xiao, Ted; Balakrishna, Ashwin; Nair, Suraj; Rafailov, Rafael; Foster, Ethan; Lam, Grace; Sanketi, Pannag; Vuong, Quan; Kollar, Thomas; Burchfiel, Benjamin; Tedrake, Russ; Sadigh, Dorsa; Levine, Sergey; Liang, Percy; Finn, Chelsea				Kim, Moo-Jin/KCK-0674-2024						OpenVLA: An Open-Source Vision-Language-Action Model								Arxiv											1	1;2024-06-13;https://www.arxiv.org/abs/2406.09246v1	arXiv:2406.09246			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 13 2024	2024	Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.																																	2024-07-10	PPRN:89301690		
J	Wang, Weihan; Lv, Qingsong; Yu, Wenmeng; Hong, Wenyi; Qi, Ji; Wang, Yan; Ji, Junhui; Yang, Zhuoyi; Zhao, Lei; Song, Xixuan; Xu, Jiazheng; Chen, Keqin; Xu, Bin; Li, Juanzi; Dong, Yuxiao; Ding, Ming; Tang, Jie				Zhao, Lei/HCI-5629-2022						CogVLM: Visual Expert for Pretrained Language Models								Arxiv											2	2;2024-02-04;https://www.arxiv.org/abs/2311.03079v2| 1;2023-11-06;https://www.arxiv.org/abs/2311.03079v1	arXiv:2311.03079			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 04 2024	2024	We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables a deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state -of -the -art performance on 17 classic cross -modal benchmarks, including 1) image captioning datasets: NoCaps, Flicker30k, 2) VQA datasets: OKVQA, TextVQA, OCRVQA, ScienceQA, 3) LVLM benchmarks: MMVet, MMBench, SEED-Bench, LLaVABench, POPE, MMMU, MathVista, 4) visual grounding datasets: RefCOCO, RefCOCO+, RefCOCOg, Visual7W. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.																																	2024-05-25	PPRN:86048325		
J	Hafner, Danijar; Pasukonis, Jurgis; Ba, Jimmy; Lillicrap, Timothy										Mastering Diverse Domains through World Models								Arxiv											1	1;2024-04-17;https://www.arxiv.org/abs/2301.04104v2	arXiv:2301.04104			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 17 2024	2024	Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.																																	2024-04-27	PPRN:88557367		
J	Yao, Yuan; Yu, Tianyu; Zhang, Ao; Wang, Chongyi; Cui, Junbo; Zhu, Hongji; Cai, Tianchi; Li, Haoyu; Zhao, Weilin; He, Zhihui; Chen, Qianyu; Zhou, Huarong; Zou, Zhensheng; Zhang, Haoye; Hu, Shengding; Zheng, Zhi; Zhou, Jie; Cai, Jie; Han, Xu; Zeng, Guoyang; Li, Dahai; Liu, Zhiyuan; Sun, Maosong				Li, Dahai/HCH-7167-2022; zhao, weilin/JMC-5864-2023; Cai, Jie/ABB-4771-2020; He, Zhihui/LVR-7522-2024; Hu, Shengding/JRY-6064-2023						MiniCPM-V: A GPT-4V Level MLLM on Your Phone								Arxiv											1	1;2024-08-03;https://www.arxiv.org/abs/2408.01800v1	arXiv:2408.01800			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 03 2024	2024	The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain preventing MLLMs from being practical in real-world applications. The most notable challenge comes from the huge cost of running an MLLM with a massive number of parameters and extensive computation. As a result, most MLLMs need to be deployed on high-performing cloud servers, which greatly limits their application scopes such as mobile, offline, energy-sensitive, and privacy-protective scenarios. In this work, we present MiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By integrating the latest MLLM techniques in architecture, pretraining and alignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1) Strong performance, outperforming GPT-4V-1106, Gemini Pro and Claude 3 on OpenCompass, a comprehensive evaluation over 11 popular benchmarks, (2) strong OCR capability and 1.8M pixel high-resolution image perception at any aspect ratio, (3) trustworthy behavior with low hallucination rates, (4) multilingual support for 30+ languages, and (5) efficient deployment on mobile phones. More importantly, MiniCPM-V can be viewed as a representative example of a promising trend: The model sizes for achieving usable (e.g., GPT-4V) level performance are rapidly decreasing, along with the fast growth of end-side computation capacity. This jointly shows that GPT-4V level MLLMs deployed on end devices are becoming increasingly possible, unlocking a wider spectrum of real-world AI applications in the near future.																																	2024-08-11	PPRN:91249176		
J	Hong, Sirui; Zhuge, Mingchen; Chen, Jiaqi; Zheng, Xiawu; Cheng, Yuheng; Zhang, Ceyao; Wang, Jinlin; Wang, Zili; Yau, Steven Ka Shing; Lin, Zijuan; Zhou, Liyang; Ran, Chenyu; Xiao, Lingfeng; Wu, Chenglin; Schmidhuber, Jurgen				Chen, Jiaqi/HTT-0960-2023; Wang, Jinlin/HTS-9926-2023						MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework								Arxiv											4	4;2024-11-01;https://www.arxiv.org/abs/2308.00352v7| 3;2024-10-21;https://www.arxiv.org/abs/2308.00352v6| 2;2023-11-06;https://www.arxiv.org/abs/2308.00352v5| 1;2023-08-02;https://www.arxiv.org/abs/2308.00352v2	arXiv:2308.00352			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 01 2024	2024	Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. 																																	2024-12-11	PPRN:74218440		
J	Wang, Liang; Yang, Nan; Huang, Xiaolong; Jiao, Binxing; Yang, Linjun; Jiang, Daxin; Majumder, Rangan; Wei, Furu				Yang, Linjun/MTA-5431-2025						Text Embeddings by Weakly-Supervised Contrastive Pre-training								Arxiv											2	2;2024-02-22;https://www.arxiv.org/abs/2212.03533v2| 1;2022-12-07;https://www.arxiv.org/abs/2212.03533v1	arXiv:2212.03533			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	This paper presents E51, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single -vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero -shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero -shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40× more parameters.																																	2024-03-21	PPRN:24983130		
J	Chen, Zhe; Wang, Weiyun; Cao, Yue; Liu, Yangzhou; Gao, Zhangwei; Cui, Erfei; Zhu, Jinguo; Ye, Shenglong; Tian, Hao; Liu, Zhaoyang; Gu, Lixin; Wang, Xuehui; Li, Qingyun; Ren, Yimin; Chen, Zixuan; Luo, Jiapeng; Wang, Jiahao; Jiang, Tan; Wang, Bo; He, Conghui; Shi, Botian; Zhang, Xingcheng; Lv, Han; Wang, Yi; Shao, Wenqi; Chu, Pei; Tu, Zhongying; He, Tong; Wu, Zhiyong; Deng, Huipeng; Ge, Jiaye; Chen, Kai; Dou, Min; Lu, Lewei; Zhu, Xizhou; Lu, Tong; Lin, Dahua; Qiao, Yu; Dai, Jifeng; Wang, Wenhai				Dai, Jifeng/HGU-8741-2022; Ye, Shenglong/NFT-0127-2025; Wang, Wen-Jing/HOH-7164-2023; TIAN, Hao/GMU-6101-2022; Shi, Botian/HTT-0363-2023; He, Conghui/AAZ-3323-2021; Jiapeng, Luo/AAC-8952-2021; Li, QingYun/KLY-9523-2024						Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling								Arxiv											1	1;2024-12-17;https://www.arxiv.org/abs/2412.05271v3	arXiv:2412.05271			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 17 2024	2024	We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. 																																	2025-01-24	PPRN:120004512		
J	Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong				Yin, Shukang/JCP-4961-2023; Zhao, Sirui/OYE-9902-2025; Zheng, Yefeng/ABG-7053-2020						A Survey on Multimodal Large Language Models								Arxiv											2	2;2024-11-26;https://www.arxiv.org/abs/2306.13549v3| 1;2023-06-23;https://www.arxiv.org/abs/2306.13549v1	arXiv:2306.13549			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 26 2024	2024	Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and Optical Character Recognition (OCR)-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions.																																	2025-01-08	PPRN:73493885		
J	Ethayarajh, Kawin; Xu, Winnie; Muennighoff, Niklas; Jurafsky, Dan; Kiela, Douwe										KTO: Model Alignment as Prospect Theoretic Optimization								Arxiv											4	4;2024-11-19;https://www.arxiv.org/abs/2402.01306v4| 3;2024-09-03;https://www.arxiv.org/abs/2402.01306v3| 2;2024-06-03;https://www.arxiv.org/abs/2402.01306v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01306v1	arXiv:2402.01306			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Nov 19 2024	2024	Kahneman & Tversky’s prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases—the success of these objectives (e.g., DPO) over cross- entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call human-aware losses (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.																																	2024-12-28	PPRN:87509294		
J	Naveed, Humza; Khan, Asad Ullah; Qiu, Shi; Saqib, Muhammad; Anwar, Saeed; Usman, Muhammad; Akhtar, Naveed; Barnes, Nick; Mian, Ajmal				Khan, Asad/N-2382-2019; Mian, Ajmal/OUI-4361-2025; Saqib, Muhammad/KUD-3995-2024; AKHTAR, NAVEED/AAT-1283-2020; Barnes, Nick/Y-2744-2018						A Comprehensive Overview of Large Language Models								Arxiv											9	9;2024-04-09;https://www.arxiv.org/abs/2307.06435v9| 8;2024-02-20;https://www.arxiv.org/abs/2307.06435v8| 7;2023-12-27;https://www.arxiv.org/abs/2307.06435v7| 6;2023-11-23;https://www.arxiv.org/abs/2307.06435v6| 5;2023-11-02;https://www.arxiv.org/abs/2307.06435v5| 4;2023-10-05;https://www.arxiv.org/abs/2307.06435v4| 3;2023-09-13;https://www.arxiv.org/abs/2307.06435v3| 2;2023-07-12;https://www.arxiv.org/abs/2307.06435v1| 1;2024-10-17;https://www.arxiv.org/abs/2307.06435v10	arXiv:2307.06435			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.																																	2024-11-07	PPRN:73905379		
J	Bi, Xiao; Chen, Deli; Chen, Guanting; Chen, Shanhuang; Dai, Damai; Deng, Chengqi; Ding, Honghui; Dong, Kai; Du, Qiushi; Fu, Zhe; Gao, Huazuo; Gao, Kaige; Gao, Wenjun; Ge, Ruiqi; Guan, Kang; Guo, Daya; Guo, Jianzhong; Hao, Guangbo; Hao, Zhewen; He, Ying; Hu, Wenjie; Huang, Panpan; Li, Erhang; Li, Guowei; Li, Jiashi; Li, Yao; Li, Y.K.; Liang, Wenfeng; Lin, Fangyun; Liu, A.X.; Liu, Bo; Liu, Wen; Liu, Xiaodong; Liu, Xin; Liu, Yiyuan; Lu, Haoyu; Lu, Shanghao; Luo, Fuli; Ma, Shirong; Nie, Xiaotao; Pei, Tian; Piao, Yishi; Qiu, Junjie; Qu, Hui; Ren, Tongzheng; Ren, Zehui; Ruan, Chong; Sha, Zhangli; Shao, Zhihong; Song, Junxiao; Su, Xuecheng; Sun, Jingxiang; Sun, Yaofeng; Tang, Minghui; Wang, Bingxuan; Wang, Peiyi; Wang, Shiyu; Wang, Yaohui; Wang, Yongji; Wu, Tong; Wu, Y.; Xie, Xin; Xie, Zhenda; Xie, Ziwei; Xiong, Yiliang; Xu, Hanwei; Xu, R.X.; Xu, Yanhong; Yang, Dejian; You, Yuxiang; Yu, Shuiping; Yu, Xingkai; Zhang, B.; Zhang, Haowei; Zhang, Lecong; Zhang, Liyue; Zhang, Mingchuan; Zhang, Minghua; Zhang, Wentao; Zhang, Yichao; Zhao, Chenggang; Zhao, Yao; Zhou, Shangyan; Zhou, Shunfeng; Zhu, Qihao; Zou, Yuheng				Bi, Xiao/GQG-9410-2022; Dai, Damai/KEJ-3256-2024; Yu, Xingkai/AAO-5118-2020; wang, peiyi/LNR-6224-2024; Qiu, Junjie/AAJ-9593-2020; Hao, Guangbo/D-2429-2012; Huang, Panpan/AAQ-5010-2020; gao, wenjun/KIA-6581-2024; Zhang, Mingchuan/I-6902-2018; zhang, liyue/AAU-6549-2021; Wang, Xiaodong/E-4202-2017; Xie, Zhenda/O-1198-2013; Wang, Yongji/JUU-5791-2023; Guo, Daya/HPG-8192-2023; Liu, Yiyuan/AAT-7514-2021; Zhang, Minghua/HDM-8534-2022						DeepSeek LLM: Scaling Open-Source Language Models with Longtermism								Arxiv											1	1;2024-01-05;https://www.arxiv.org/abs/2401.02954v1	arXiv:2401.02954			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 05 2024	2024	The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.																																	2024-01-14	PPRN:86996974		
J	Shi, Yichun; Wang, Peng; Ye, Jianglong; Mai, Long; Li, Kejie; Yang, Xiao				Li, Kejie/D-3595-2013; Shi, Yichun/AGV-8080-2022						MVDREAM: MULTI-VIEW DIFFUSION FOR 3D GENERATION								Arxiv											4	4;2024-04-18;https://www.arxiv.org/abs/2308.16512v4| 3;2024-03-16;https://www.arxiv.org/abs/2308.16512v3| 2;2023-10-02;https://www.arxiv.org/abs/2308.16512v2| 1;2023-08-31;https://www.arxiv.org/abs/2308.16512v1	arXiv:2308.16512			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 18 2024	2024	We introduce MVDream, a diffusion model that is able to generate consistent multiview images from a given text prompt. Learning from both 2D and 3D data, a multiview diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that such a multi -view diffusion model is implicitly a generalizable 3D prior agnostic to 3D representations. It can be applied to 3D generation via Score Distillation Sampling, significantly enhancing the consistency and stability of existing 2D-lifting methods. It can also learn new concepts from a few 2D examples, akin to DreamBooth, but for 3D generation.																																	2024-04-28	PPRN:84625310		
J	Yu, Longhui; Jiang, Weisen; Shi, Han; Yu, Jincheng; Liu, Zhengying; Zhang, Yu; Kwok, James T.; Li, Zhenguo; Weller, Adrian; Liu, Weiyang				Jiang, Weisen/JGE-0023-2023; Lv, Zhengtong/AAW-9611-2020						MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models								Arxiv											4	4;2024-05-03;https://www.arxiv.org/abs/2309.12284v4| 3;2023-10-09;https://www.arxiv.org/abs/2309.12284v3| 2;2023-09-22;https://www.arxiv.org/abs/2309.12284v2| 1;2023-09-21;https://www.arxiv.org/abs/2309.12284v1	arXiv:2309.12284			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 03 2024	2024	Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.																																	2024-05-24	PPRN:85081607		
J	Chen, Zhe; Wang, Weiyun; Tian, Hao; Ye, Shenglong; Gao, Zhangwei; Cui, Erfei; Tong, Wenwen; Hu, Kongzhi; Luo, Jiapeng; Ma, Zheng; Ma, Ji; Wang, Jiaqi; Dong, Xiaoyi; Yan, Hang; Guo, Hewei; He, Conghui; Jin, Zhenjiang; Xu, Chao; Wang, Bin; Wei, Xingjian; Li, Wei; Zhang, Wenjian; Lu, Lewei; Zhu, Xizhou; Lu, Tong; Lin, Dahua; Qiao, Yu; Dai, Jifeng; Wang, Wenhai				Jiapeng, Luo/AAC-8952-2021; Ye, Shenglong/NFT-0127-2025; Qiao, Yu/ABD-5787-2021; Wang, Bin/MVU-8917-2025; TIAN, Hao/GMU-6101-2022; He, Conghui/AAZ-3323-2021; Dong, Xiaoyi/AAC-8666-2019; WANG, JIAQI/KBB-8837-2024; Wang, Wen-Jing/HOH-7164-2023; Lin, Dahua/W-6576-2019; Dai, Jifeng/HGU-8741-2022						How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites								Arxiv											1	1;2024-04-25;https://www.arxiv.org/abs/2404.16821v1	arXiv:2404.16821			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 25 2024	2024	In this report, we introduce InternVL 1.5, an opensource multimodal large language model (MLLM) to bridge the capability gap between open -source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model—InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High -Resolution: we divide images into tiles ranging from 1 to 40 of 448×448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High -Quality Bilingual Dataset: we carefully collected a high -quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question -answer pairs, significantly enhancing performance in OCR- and Chineserelated tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open -source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.																																	2024-05-04	PPRN:88650779		
J	Jimenez, Carlos E.; Yang, John; Wettig, Alexander; Yao, Shunyu; Pei, Kexin; Press, Ofir; Narasimhan, Karthik				Yao, ShunYu/GWV-1046-2022						SWE-bench: Can Language Models Resolve Real-World GitHub Issues?								Arxiv											3	3;2024-11-11;https://www.arxiv.org/abs/2310.06770v3| 2;2024-04-05;https://www.arxiv.org/abs/2310.06770v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06770v1	arXiv:2310.06770			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 11 2024	2024	Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.																																	2024-12-18	PPRN:85525762		
J	Lu, Pan; Bansal, Hritik; Xia, Tony; Liu, Jiacheng; Li, Chunyuan; Hajishirzi, Hannaneh; Cheng, Hao; Chang, Kai-Wei; Galley, Michel; Gao, Jianfeng				Gao, Jianfeng/AAP-8200-2021; Li, Chunyuan/KHY-0771-2024; Liu, Jiacheng/ISS-1763-2023; Chang, Kai-Wei/AAJ-7874-2020						MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts								Arxiv											3	3;2024-01-21;https://www.arxiv.org/abs/2310.02255v3| 2;2023-10-25;https://www.arxiv.org/abs/2310.02255v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02255v1	arXiv:2310.02255			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 21 2024	2024	Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MATHVISTA, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 ex-amples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MATHVISTA, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical rea-soning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MATHVISTA will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research.																																	2024-05-25	PPRN:85377844		
J	Xiao, Guangxuan; Tian, Yuandong; Chen, Beidi; Han, Song; Lewis, Mike				Han, Song/AAR-9464-2020; Tian, Yuan/B-1553-2009						Efficient Streaming Language Models with Attention Sinks								Arxiv											4	4;2024-04-07;https://www.arxiv.org/abs/2309.17453v4| 3;2023-12-12;https://www.arxiv.org/abs/2309.17453v3| 2;2023-11-21;https://www.arxiv.org/abs/2309.17453v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17453v1	arXiv:2309.17453			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 07 2024	2024	Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a "sink" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. 																																	2024-04-21	PPRN:85339013		
J	Liu, Yue; Tian, Yunjie; Zhao, Yuzhong; Yu, Hongtian; Xie, Lingxi; Wang, Yaowei; Ye, Qixiang; Liu, Yunfan				Xie, Lingxi/ABF-6996-2020; li, Yunfan/HZJ-8983-2023						VMamba: Visual State Space Model								Arxiv											3	3;2024-05-26;https://www.arxiv.org/abs/2401.10166v3| 2;2024-04-10;https://www.arxiv.org/abs/2401.10166v2| 1;2024-01-18;https://www.arxiv.org/abs/2401.10166v1	arXiv:2401.10166			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	May 26 2024	2024	Designing computationally efficient network architectures persists as an ongoing necessity in computer vision. In this paper, we transplant Mamba, a state-space language model, into VMamba, a vision backbone that works in linear time complexity. At the core of VMamba lies a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D helps bridge the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the gathering of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments showcase VMamba's promising performance across diverse visual perception tasks, highlighting its advantages in input scaling efficiency compared to existing benchmark models.																																	2024-06-10	PPRN:87221709		
J	Tang, Jiaxiang; Ren, Jiawei; Zhou, Hang; Liu, Ziwei; Zeng, Gang				Liu, Ziwei/AAG-6939-2021						DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation								Arxiv											2	2;2024-03-29;https://www.arxiv.org/abs/2309.16653v2| 1;2023-09-28;https://www.arxiv.org/abs/2309.16653v1	arXiv:2309.16653			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 29 2024	2024	Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.																																	2024-04-17	PPRN:85324100		
J	Javadi-Abhari, Ali; Treinish, Matthew; Krsulich, Kevin; Wood, Christopher J.; Lishman, Jake; Gacon, Julien; Martiel, Simon; Nation, Paul D.; Bishop, Lev S.; Cross, Andrew W.; Johnson, Blake R.; Gambetta, Jay M.				Wood, Christopher/AAQ-1777-2021; Gambetta, Jay/D-2794-2009; Nation, Paul/E-9119-2010; Bishop, Lev/C-5476-2009						Quantum computing with Qiskit								Arxiv											2	2;2024-06-19;https://www.arxiv.org/abs/2405.08810v3| 1;2024-05-15;https://www.arxiv.org/abs/2405.08810v2	arXiv:2405.08810			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 19 2024	2024	We describe Qiskit, a software development kit for quantum information science. We discuss the key design decisions that have shaped its development, and examine the software architecture and its core components. We demonstrate an end-to-end workflow for solving a problem in condensed matter physics on a quantum computer that serves to highlight some of Qiskit’s capabilities, for example the representation and optimization of circuits at various abstraction levels, its scalability and retargetability to new gates, and the use of quantum-classical computations via dynamic circuits. Lastly, we discuss some of the ecosystem of tools and plugins that extend Qiskit for various tasks, and the future ahead.																																	2024-07-10	PPRN:89076875		
J	Lee, Harrison; Phatale, Samrat; Mansoor, Hassan; Mesnard, Thomas; Ferret, Johan; Lu, Kellie; Bishop, Colton; Hall, Ethan; Carbune, Victor; Rastogi, Abhinav; Prakash, Sushant										RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback								Arxiv											3	3;2024-09-03;https://www.arxiv.org/abs/2309.00267v3| 2;2023-12-01;https://www.arxiv.org/abs/2309.00267v2| 1;2023-09-01;https://www.arxiv.org/abs/2309.00267v1	arXiv:2309.00267			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 03 2024	2024	Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards "self-improvement" by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.																																	2024-09-11	PPRN:84667573		
J	Bai, Yushi; Lv, Xin; Zhang, Jiajie; Lyu, Hongchang; Tang, Jiankai; Huang, Zhidian; Du, Zhengxiao; Liu, Xiao; Zeng, Aohan; Hou, Lei; Dong, Yuxiao; Tang, Jie; Li, Juanzi				Zhang, Jiajie/KIE-1997-2024; Li, Zhiyuan/ESQ-7168-2022						LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding								Arxiv											2	2;2024-06-19;https://www.arxiv.org/abs/2308.14508v2| 1;2023-08-28;https://www.arxiv.org/abs/2308.14508v1	arXiv:2308.14508			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 19 2024	2024	Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs’ long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including singledoc QA, multi-doc QA, summarization, fewshot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other opensourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.																																	2024-07-10	PPRN:84415850		
J	Chiang, Wei-Lin; Zheng, Lianmin; Sheng, Ying; Angelopoulos, Anastasios Nikolas; Li, Tianle; Li, Dacheng; Zhang, Hao; Zhu, Banghua; Jordan, Michael; Gonzalez, Joseph E.; Stoica, Ion				Angelopoulos, Anastasios/AAT-5355-2020						Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.04132v1	arXiv:2403.04132			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. 																																	2024-04-05	PPRN:88055859		
J	Team, Chameleon										Chameleon: Mixed-Modal Early-Fusion Foundation Models								Arxiv											1	1;2024-05-16;https://www.arxiv.org/abs/2405.09818v1	arXiv:2405.09818			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 16 2024	2024	We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.																																	2024-06-12	PPRN:89086346		
J	Zhou, Shuyan; Xu, Frank F.; Zhu, Hao; Zhou, Xuhui; Lo, Robert; Sridhar, Abishek; Cheng, Xianyi; Ou, Tianyue; Bisk, Yonatan; Fried, Daniel; Alon, Uri; Neubig, Graham				Lo, Robert/OMN-0320-2025						WebArena: A Realistic Web Environment for Building Autonomous Agents								Arxiv											3	3;2024-04-16;https://www.arxiv.org/abs/2307.13854v4| 2;2023-10-25;https://www.arxiv.org/abs/2307.13854v3| 1;2023-07-25;https://www.arxiv.org/abs/2307.13854v1	arXiv:2307.13854			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.																																	2024-04-26	PPRN:74117438		
J	Chi, Cheng; Xu, Zhenjia; Feng, Siyuan; Cousineau, Eric; Du, Yilun; Burchfiel, Benjamin; Tedrake, Russ; Song, Shuran				Feng, Siyuan/AAA-1320-2022						Diffusion Policy: Visuomotor Policy Learning via Action Diffusion								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2303.04137v5| 1;2023-03-10;https://www.arxiv.org/abs/2303.04137v2	arXiv:2303.04137			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 14 2024	2024	This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu																																	2024-04-11	PPRN:44231889		
J	Zhang, Zhuosheng; Zhang, Aston; Li, Mu; Zhao, Hai; Karypis, George; Smola, Alex				Zhang, Zhuosheng/AAF-4919-2020						Multimodal Chain-of-Thought Reasoning in Language Models								Arxiv											2	2;2024-05-20;https://www.arxiv.org/abs/2302.00923v5| 1;2023-02-02;https://www.arxiv.org/abs/2302.00923v1	arXiv:2302.00923			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	May 20 2024	2024	Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed.																																	2024-06-01	PPRN:36115735		
J	Qian, Chen; Liu, Wei; Liu, Hongzhang; Chen, Nuo; Dang, Yufan; Li, Jiahao; Yang, Cheng; Chen, Weize; Su, Yusheng; Cong, Xin; Xu, Juyuan; Li, Dahai; Liu, Zhiyuan; Sun, Maosong				SU, YU-SHENG/E-6570-2019; Liu, Zhiyuan/I-2233-2014; Liu, Wei/KFQ-9997-2024; Li, Dahai/HCH-7167-2022						ChatDev: Communicative Agents for Software Development								Arxiv											4	4;2024-06-05;https://www.arxiv.org/abs/2307.07924v5| 3;2023-12-19;https://www.arxiv.org/abs/2307.07924v4| 2;2023-08-28;https://www.arxiv.org/abs/2307.07924v3| 1;2023-07-18;https://www.arxiv.org/abs/2307.07924v2	arXiv:2307.07924			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 05 2024	2024	Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. 																																	2024-06-22	PPRN:73994407		
J	Jain, Naman; Han, King; Gu, Alex; Li, Wen-Ding; Yan, Fanjia; Zhang, Tianjun; Wang, Sida I; Solar-Lezama, Armando; Sen, Koushik; Stoica, Ion										LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code								Arxiv											1	1;2024-03-12;https://www.arxiv.org/abs/2403.07974v1	arXiv:2403.07974			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 12 2024	2024	Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model																																	2024-04-11	PPRN:88132139		
J	Chen, Zhe; Wu, Jiannan; Wang, Wenhai; Su, Weijie; Chen, Guo; Xing, Sen; Zhong, Muyan; Zhang, Qinglong; Zhu, Xizhou; Lu, Lewei; Li, Bin; Luo, Ping; Lu, Tong; Qiao, Yu; Dai, Jifeng				Qiao, Yu/ABD-5787-2021; Su, Weijie/LOS-6390-2024; wang, wenhai/GUX-3226-2022; Dai, Jifeng/HGU-8741-2022; pluo/GPG-2707-2022						InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks								Arxiv											3	3;2024-01-15;https://www.arxiv.org/abs/2312.14238v3| 2;2023-12-26;https://www.arxiv.org/abs/2312.14238v2| 1;2023-12-21;https://www.arxiv.org/abs/2312.14238v1	arXiv:2312.14238			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 15 2024	2024	The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (InternVL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the develop-ment of multi-modal large models.																																	2024-02-02	PPRN:86784296		
J	Fu, Chaoyou; Dai, Yuhan; Luo, Yongdong; Li, Lei; Ren, Shuhuai; Zhang, Renrui; Wang, Zihan; Zhou, Chenyu; Shen, Yunhang; Zhang, Mengdan; Chen, Peixian; Li, Yanwei; Lin, Shaohui; Zhao, Sirui; Li, Ke; Xu, Tong; Zheng, Xiawu; Chen, Enhong; Ji, Rongrong; Sun, Xing				Lin, Shaohui/HKF-5868-2023; Zhang, Zhuosheng/AAF-4919-2020; Zhou, chenyu/HPF-7941-2023; Ren, Shuhuai/KDO-1734-2024; Li, Yanwei/AAW-7672-2020; Zhao, Sirui/OYE-9902-2025; Zheng, Yefeng/ABG-7053-2020; Wang, Zihan/HGE-3658-2022; Zhang, Mengdan/LFV-0431-2024; Shen, Yunhang/ADW-0834-2022						Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis								Arxiv											2	2;2024-06-16;https://www.arxiv.org/abs/2405.21075v2| 1;2024-05-31;https://www.arxiv.org/abs/2405.21075v1	arXiv:2405.21075			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 16 2024	2024	In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 254 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. 																																	2024-07-04	PPRN:89131704		
J	Guo, Taicheng; Chen, Xiuying; Wang, Yaqi; Chang, Ruidi; Pei, Shichao; Chawla, Nitesh V.; Wiest, Olaf; Zhang, Xiangliang				Guo, Taicheng/IUM-4515-2023; Chen, Xiuying/ISU-7033-2023						Large Language Model based Multi-Agents: A Survey of Progress and Challenges								Arxiv											2	2;2024-04-19;https://www.arxiv.org/abs/2402.01680v2| 1;2024-01-21;https://www.arxiv.org/abs/2402.01680v1	arXiv:2402.01680			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 19 2024	2024	Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.																																	2024-04-30	PPRN:87522650		
J	Sun, Mingjie; Liu, Zhuang; Bair, Anna; Kolter, J.Zico				SUN, MINGJIE/GQQ-0374-2022						A Simple and Effective Pruning Approach for Large Language Models								Arxiv											3	3;2024-05-06;https://www.arxiv.org/abs/2306.11695v3| 2;2023-10-06;https://www.arxiv.org/abs/2306.11695v2| 1;2023-06-20;https://www.arxiv.org/abs/2306.11695v1	arXiv:2306.11695			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by W eights and a ctivations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is . We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda. .																																	2024-05-24	PPRN:73435749		
J	Liu, Xiaogeng; Xu, Nan; Chen, Muhao; Xiao, Chaowei				Liu, Xiaogeng/KIJ-1671-2024; Chen, Muhao/AAA-3634-2021; XU, nan/KDP-0628-2024; Xiao, Chaowei/AAT-8772-2021						AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models								Arxiv											2	2;2024-03-20;https://www.arxiv.org/abs/2310.04451v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.04451v1	arXiv:2310.04451			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 20 2024	2024	The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.																																	2024-04-13	PPRN:85583223		
J	Brown, Bradley; Juravsky, Jordan; Ehrlich, Ryan; Clark, Ronald; Le, Quoc V.; Re, Christopher; Mirhoseini, Azalia				Clark, Ronald/Y-5623-2019						Large Language Monkeys: Scaling Inference Compute with Repeated Sampling								Arxiv											3	3;2024-12-30;https://www.arxiv.org/abs/2407.21787v3| 2;2024-09-16;https://www.arxiv.org/abs/2407.21787v2| 1;2024-07-31;https://www.arxiv.org/abs/2407.21787v1	arXiv:2407.21787			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 30 2024	2024	Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.																																	2025-02-15	PPRN:91178483		
J	Ren, Tianhe; Liu, Shilong; Zeng, Ailing; Lin, Jing; Cao, He; Li, Kunchang; Chen, Jiayu; Huang, Xinyu; Chen, Yukang; Yan, Feng; Zeng, Zhaoyang; Zhang, Hao; Li, Feng; Yang, Jie; Li, Hongyang; Jiang, Qing; Whitehouse, Chenxi; Wang, Zhenxuan; Zhang, Lei				yan, feng/JAX-8275-2023; Li, Kunchang/KFA-4043-2024; Chen, Yukang/HKW-0344-2023; Huang, Xinyu/AAT-8168-2020; Liu, Shilong/GVS-1257-2022; Zeng, Zhao/D-1959-2010; Chen, Jiayu/HSF-1611-2023						Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks								Arxiv											1	1;2024-01-25;https://www.arxiv.org/abs/2401.14159v1	arXiv:2401.14159			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.																																	2024-05-25	PPRN:87331044		
J	Xiao, Shitao; Liu, Zheng; Zhang, Peitian; Muennighoff, Niklas; Lian, Defu; Nie, Jian-Yun				Lian, Defu/AFN-4573-2022; Liu, Zheng/AHI-3660-2022						C-Pack: Packaged Resources To Advance General Chinese Embedding								Arxiv											3	3;2024-05-12;https://www.arxiv.org/abs/2309.07597v4| 2;2024-04-20;https://www.arxiv.org/abs/2309.07597v3| 1;2023-12-15;https://www.arxiv.org/abs/2309.07597v2	arXiv:2309.07597			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 12 2024	2024	We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. 																																	2024-05-29	PPRN:86651217		
J	Black, Kevin; Brown, Noah; Driess, Danny; Esmail, Adnan; Equi, Michael; Finn, Chelsea; Fusai, Niccolo; Groom, Lachy; Hausman, Karol; Ichter, Brian; Jakubczak, Szymon; Jones, Tim; Ke, Liyiming; Levine, Sergey; Li-Bell, Adrian; Mothukuri, Mohith; Nair, Suraj; Pertsch, Karl; Shi, Lucy Xiaoyang; Tanner, James; Vuong, Quan; Walling, Anna; Wang, Haohuan; Zhilinsky, Ury		Phys Intelligence		wang, haoxue/JHS-6205-2023						π0 : A Vision-Language-Action Flow Model for General Robot Control								Arxiv											1	1;2024-10-31;https://www.arxiv.org/abs/2410.24164v1	arXiv:2410.24164			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.																																	2024-12-06	PPRN:118965314		
J	Yang, Jingkang; Zhou, Kaiyang; Li, Yixuan; Liu, Ziwei				Yang, Jingkang/HJZ-3689-2023; Zhou, Kaiyang/KTI-8952-2024; Liu, Ziwei/AAG-6939-2021						Generalized Out-of-Distribution Detection: A Survey								Arxiv											2	2;2024-01-23;https://www.arxiv.org/abs/2110.11334v3| 1;2021-10-21;https://www.arxiv.org/abs/2110.11334v2	arXiv:2110.11334			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 23 2024	2024	Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.																																	2024-05-25	PPRN:11920070		
J	Lin, Ji; Tang, Jiaming; Tang, Haotian; Yang, Shang; Chen, Wei-Ming; Wang, Wei-Chen; Xiao, Guangxuan; Dang, Xingyu; Gan, Chuang; Han, Song				Chen, Wei-Ming/ABD-2594-2021; Wang, Wei-Chen/ABC-8145-2020						AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration								Arxiv											5	5;2024-07-18;https://www.arxiv.org/abs/2306.00978v5| 4;2024-04-23;https://www.arxiv.org/abs/2306.00978v4| 3;2024-04-21;https://www.arxiv.org/abs/2306.00978v3| 2;2023-10-03;https://www.arxiv.org/abs/2306.00978v2| 1;2023-06-01;https://www.arxiv.org/abs/2306.00978v1	arXiv:2306.00978			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 18 2024	2024	Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.																																	2024-07-26	PPRN:72815241		
J	Sun, Peize; Jiang, Yi; Chen, Shoufa; Zhang, Shilong; Peng, Bingyue; Luo, Ping; Yuan, Zehuan				pluo/GPG-2707-2022; ZHANG, Shilong/NJT-1608-2025; CHEN, SHOUFA/HSH-2485-2023						Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation								Arxiv											1	1;2024-06-10;https://www.arxiv.org/abs/2406.06525v1	arXiv:2406.06525			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	We introduce LlamaGen, a new family of image generation models that apply original “next -token prediction” paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class -conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256×256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text -conditional image generation model with 775M parameters, from two -stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open -source community of visual generation and multimodal foundation models.																																	2024-07-04	PPRN:89268814		
J	Yang, Chengrun; Wang, Xuezhi; Lu, Yifeng; Liu, Hanxiao; Le, Quoc V.; Zhou, Denny; Chen, Xinyun				Chen, Xinyun/ABZ-9877-2022; Yang, Chengrun/HII-3621-2022						Large Language Models as Optimizers								Arxiv											3	3;2024-04-15;https://www.arxiv.org/abs/2309.03409v3| 2;2023-12-07;https://www.arxiv.org/abs/2309.03409v2| 1;2023-09-07;https://www.arxiv.org/abs/2309.03409v1	arXiv:2309.03409			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Apr 15 2024	2024	Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.  [GRAHICS]																																	2024-04-25	PPRN:84854095		
J	Yang, An; Zhang, Beichen; Hui, Binyuan; Gao, Bofei; Yu, Bowen; Li, Chengpeng; Liu, Dayiheng; Tu, Jianhong; Zhou, Jingren; Lin, Junyang; Lu, Keming; Xue, Mingfeng; Lin, Runji; Liu, Tianyu; Ren, Xingzhang; Zhang, Zhenru				Liu, Tianyu/ABD-7412-2020; Bowen, Yu/MFH-7462-2025; 李, 程鹏/HLP-9229-2023; Bin/B-6414-2019						Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement								Arxiv											1	1;2024-09-18;https://www.arxiv.org/abs/2409.12122v1	arXiv:2409.12122			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 18 2024	2024	In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it’s possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model’s performance. Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems. The flagship model, Qwen2.5-Math-72B-Instruct, significantly outperforms both open-source models and leading closed-source models (e.g., GPT4o, Gemini Math-Specialized 1.5 Pro). Particularly in the challenging AMC 2023, with the assistance of RM, Qwen2.5-Math-72B-Instruct successfully solves almost all the problems. Qwen2.5-Math-7B-Instruct surpasses Qwen2-Math-Instruct 72B in performance. Under CoT and TIR settings, it achieves MATH scores of 83.6 and 85.3, respectively. Even our smallest 1.5B model, achieving a MATH score of around 80 when utilizing the Python Interpreter, outperforms the majority of current models in this domain. We hope that Qwen2.5-Math can contribute to the community for solving complex mathematical problems. The base models, instruct models, and reward model of the Qwen2.5-Math series are available on Hugging Face1 and ModelScope2 , and the evaluation scripts on GitHub3. We have also developed a demo that supports the TIR mode in QwenAgent4, which allows running code locally to experience Tool-Integrated Reasoning capabilities of Qwen2.5-Math.																																	2024-09-30	PPRN:91949585		
J	Chen, Jianlv; Xiao, Shitao; Zhang, Peitian; Luo, Kun; Lian, Defu; Liu, Zheng				Liu, Zheng/AHI-3660-2022; 罗, 坤/AAV-6532-2021; Lian, Defu/AFN-4573-2022						BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation								Arxiv											4	4;2024-06-28;https://www.arxiv.org/abs/2402.03216v4| 3;2024-02-10;https://www.arxiv.org/abs/2402.03216v3| 2;2024-02-08;https://www.arxiv.org/abs/2402.03216v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.03216v1	arXiv:2402.03216			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 28 2024	2024	In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three comBGE M3-Embedding mon retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross -lingual, and longdocument retrieval benchmarks. 1																																	2024-07-17	PPRN:87522387		
J	Liu, Yi; Deng, Gelei; Xu, Zhengzi; Li, Yuekang; Zheng, Yaowen; Zhang, Ying; Zhao, Lida; Zhang, Tianwei; Wang, Kailong; Liu, Yang				Huawei, Zhang/K-8769-2017; Liu, Yang/D-2306-2013; Wang, Kailong/NLO-6290-2025; LIU, YI/LUY-0513-2024; Li, Yuekang/U-9646-2019						Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study								Arxiv											2	2;2024-03-10;https://www.arxiv.org/abs/2305.13860v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.13860v1	arXiv:2305.13860			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 10 2024	2024	Large Language Models (LLMs), like CHATGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of CHATGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with CHATGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of CHATGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use -case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.																																	2024-04-08	PPRN:71678816		
J	Liu, Yuan; Lin, Cheng; Zeng, Zijiao; Long, Xiaoxiao; Liu, Lingjie; Komura, Taku; Wang, Wenping				Komura, Taku/ODJ-6678-2025; Liu, Lingjie/LIC-0932-2024						SyncDreamer: Generating Multiview-consistent Images from a Single-view Image								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2309.03453v2| 1;2023-09-07;https://www.arxiv.org/abs/2309.03453v1	arXiv:2309.03453			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	In this paper, we present a novel diffusion model called that generates multiview-consistent images from a single-view image. Using pretrained large-scale 2D diffusion models, recent work Zero123 demonstrates the ability to generate plausible novel views from a single-view image of an object. However, maintaining consistency in geometry and colors for the generated images remains a challenge. To address this issue, we propose a synchronized multiview diffusion model that models the joint probability distribution of multiview images, enabling the generation of multiview-consistent images in a single reverse process. SyncDreamer synchronizes the intermediate states of all the generated images at every step of the reverse process through a 3D-aware feature attention mechanism that correlates the corresponding features across different views. Experiments show that SyncDreamer generates images with high consistency across different views, thus making it well-suited for various 3D generation tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.																																	2024-04-25	PPRN:84856765		
J	Wang, Lei; Ma, Chen; Feng, Xueyang; Zhang, Zeyu; Yang, Hao; Zhang, Jingsen; Chen, Zhiyuan; Tang, Jiakai; Chen, Xu; Lin, Yankai; Zhao, Wayne Xin; Wei, Zhewei; Wen, Ji-Rong				Wang, Lei/MIO-9469-2025; Xia, Lianghao/IWV-0954-2023; Feng, Xueyang/G-1295-2015; Chen, Xu/MTC-1347-2025; Zhang, Zeyu/HDO-2331-2022; Zhang, Jingsen/KIG-5904-2024; tang, jiakai/LSI-8458-2024						A Survey on Large Language Model based Autonomous Agents								Arxiv											6	6;2025-03-02;https://www.arxiv.org/abs/2308.11432v7| 5;2024-04-04;https://www.arxiv.org/abs/2308.11432v5| 4;2024-03-25;https://www.arxiv.org/abs/2308.11432v4| 3;2024-03-12;https://www.arxiv.org/abs/2308.11432v3| 2;2023-09-07;https://www.arxiv.org/abs/2308.11432v2| 1;2023-08-22;https://www.arxiv.org/abs/2308.11432v1	arXiv:2308.11432			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 25 2024	2024	Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.																																	2025-08-07	PPRN:82375753		
J	Han, Zeyu; Gao, Chao; Liu, Jinyang; Zhang, Jeff Jun; Zhang, Sai Qian				LIU, JINYANG/KHE-5101-2024; HAN, ZEYU/OZF-3193-2025						Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey								Arxiv											1	1;2024-03-21;https://www.arxiv.org/abs/2403.14608v1	arXiv:2403.14608			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.																																	2024-04-13	PPRN:88260115		
J	Zhang, Peiyuan; Zeng, Guangtao; Wang, Tianduo; Lu, Wei				zeng, guangtao/OPF-2200-2025; Zhang, Peiyuan/OOK-8916-2025; Lu, Wei/F-2504-2016						TinyLlama: An Open-Source Small Language Model								Arxiv											1	1;2024-01-04;https://www.arxiv.org/abs/2401.02385v1	arXiv:2401.02385			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 04 2024	2024	We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2 (Touvron et al., 2023b), TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention (Dao, 2023)), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes.																																	2024-05-25	PPRN:86970362		
J	Wang, Yubo; Ma, Xueguang; Zhang, Ge; Ni, Yuansheng; Chandra, Abhranil; Guo, Shiguang; Ren, Weiming; Arulraj, Aaran; He, Xuan; Jiang, Ziyan; Li, Tianle; Ku, Max; Wang, Kai; Zhuang, Alex; Fan, Rongqi; Yue, Xiang; Chen, Wenhu				Ma, Xueguang/GPF-7396-2022						MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark								Arxiv											4	4;2024-11-06;https://www.arxiv.org/abs/2406.01574v6| 3;2024-10-07;https://www.arxiv.org/abs/2406.01574v5| 2;2024-06-23;https://www.arxiv.org/abs/2406.01574v4| 1;2024-06-05;https://www.arxiv.org/abs/2406.01574v3	arXiv:2406.01574			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 06 2024	2024	In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.																																	2024-11-30	PPRN:89259203		
J	Meng, Yu; Xia, Mengzhou; Chen, Danqi				CHEN, DANQI/C-6441-2013						SimPO: Simple Preference Optimization with a Reference-Free Reward								Arxiv											3	3;2024-11-01;https://www.arxiv.org/abs/2405.14734v3| 2;2024-07-08;https://www.arxiv.org/abs/2405.14734v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14734v1	arXiv:2405.14734			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a 72.4% length-controlled win rate on AlpacaEval 2, a 59.1% win rate on Arena-Hard, and ranks 1st on Chatbot Arena among <10B models with real user votes.1																																	2024-12-16	PPRN:88982389		
J	Shen, Xinyue; Chen, Zeyuan; Backes, Michael; Shen, Yun; Zhang, Yang				Chen, Zeyuan/NSV-1807-2025						"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models								Arxiv											2	2;2024-05-15;https://www.arxiv.org/abs/2308.03825v2| 1;2023-08-07;https://www.arxiv.org/abs/2308.03825v1	arXiv:2308.03825			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 15 2024	2024	The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.1																																	2024-06-01	PPRN:74384739		
J	Chu, Yunfei; Xu, Jin; Yang, Qian; Wei, Haojie; Wei, Xipin; Guo, Zhifang; Leng, Yichong; Lv, Yuanjun; He, Jinzheng; Lin, Junyang; Zhou, Chang; Zhou, Jingren				Yang, Qian/T-1026-2017; Chu, Yunfei/C-8002-2013; wei, haojie/AAH-8275-2019; Zhou, Mingyuan/AAE-8717-2021						Qwen2-Audio Technical Report								Arxiv											1	1;2024-07-15;https://www.arxiv.org/abs/2407.10759v1	arXiv:2407.10759			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 15 2024	2024	We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.																																	2024-07-23	PPRN:90823206		
J	Lozhkov, Anton; Li, Raymond; Allal, Loubna Ben; Cassano, Federico; Lamy-Poirier, Joel; Tazi, Nouamane; Tang, Ao; Pykhtar, Dmytro; Liu, Jiawei; Wei, Yuxiang; Liu, Tianyang; Tian, Max; Kocetkov, Denis; Zucker, Arthur; Belkada, Younes; Wang, Zijian; Liu, Qian; Abulkhanov, Dmitry; Paul, Indraneil; Li, Zhuang; Li, Wen-Ding; Risdal, Megan; Li, Jia; Zhu, Jian; Zhuo, Terry Yue; Zheltonozhskii, Evgenii; Dade, Nii Osae Osae; Yu, Wenhao; Krauss, Lucas; Jain, Naman; Su, Yixuan; He, Xuanli; Dey, Manan; Abati, Edoardo; Chai, Yekun; Muennighoff, Niklas; Tang, Xiangru; Oblokulov, Muhtasham; Akiki, Christopher; Marone, Marc; Mou, Chenghao; Mishra, Mayank; Gu, Alex; Hui, Binyuan; Dao, Tri; Zebaze, Armel; Dehaene, Olivier; Patry, Nicolas; Xu, Canwen; McAuley, Julian; Hu, Han; Scholak, Torsten; Paquet, Sebastien; Robinson, Jennifer; Anderson, Carolyn Jane; Chapados, Nicolas; Patwary, Mostofa; Tajbakhsh, Nima; Jernite, Yacine; Ferrandis, Carlos Munoz; Zhang, Lingming; Hughes, Sean; Wolf, Thomas; Guha, Arjun; von Werra, Leandro; de Vries, Harm				Wei, Yuxiang/KYQ-5192-2024; He, Xuanli/JGM-1644-2023; Chai, Yekun/HJG-7185-2022; Liu, Jiawei/JVZ-3421-2024; Zheltonozhskii, Evgenii/ABB-8740-2020; Tajbakhsh, Nima/AAC-1354-2019; Bin/B-6414-2019; Liu, Tianyang/JOZ-0867-2023; Li, Raymond/ADD-4734-2022; Hu, Han/KMA-4343-2024						StarCoder 2 and The Stack v2: The Next Generation								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19173v1	arXiv:2402.19173			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Feb 29 2024	2024	The BigCode project,1 an open -scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH),2 we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high -quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4× larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder215B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder33B is the best -performing model at code completion for high -resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low -resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.																																	2024-03-28	PPRN:88007028		
J	Yang, Lihe; Kang, Bingyi; Huang, Zilong; Zhao, Zhen; Xu, Xiaogang; Feng, Jiashi; Zhao, Hengshuang				Feng, Jiashi/AGX-6209-2022; Huang, Zilong/AAW-6071-2021; Kang, Bingyi/AAR-2697-2020						Depth Anything V2								Arxiv											1	1;2024-10-20;https://www.arxiv.org/abs/2406.09414v2	arXiv:2406.09414			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 20 2024	2024	This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.																																	2024-11-20	PPRN:118760022		
J	Lambert, Nathan; Pyatkin, Valentina; Morrison, Jacob; Miranda, Lj; Lin, Bill Yuchen; Chandu, Khyathi; Dziri, Nouha; Kumar, Sachin; Zick, Tom; Choi, Yejin; Smith, Noah A.; Hajishirzi, Hannaneh										RewardBench: Evaluating Reward Models for Language Modeling								Arxiv											2	2;2024-06-08;https://www.arxiv.org/abs/2403.13787v2| 1;2024-03-20;https://www.arxiv.org/abs/2403.13787v1	arXiv:2403.13787			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 08 2024	2024	Reward models (RMs) are at the crux of successfully using RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we present RewardBench, a benchmark dataset and code-base for evaluation. The RewardBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.																																	2024-07-04	PPRN:88249783		
J	Liang, Tian; He, Zhiwei; Jiao, Wenxiang; Wang, Xing; Wang, Yan; Wang, Rui; Yang, Yujiu; Shi, Shuming; Tu, Zhaopeng				Yang, Yujiu/JGM-0303-2023; Tu, Zhaopeng/AAS-4259-2021						Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate								Arxiv											4	4;2024-10-09;https://www.arxiv.org/abs/2305.19118v4| 3;2024-07-17;https://www.arxiv.org/abs/2305.19118v3| 2;2024-06-19;https://www.arxiv.org/abs/2305.19118v2| 1;2023-05-30;https://www.arxiv.org/abs/2305.19118v1	arXiv:2305.19118			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 09 2024	2024	Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counterintuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at https://github. com/Skytliang/Multi-Agents-Debate .																																	2024-10-29	PPRN:72767955		
J	Mazeika, Mantas; Phan, Long; Yin, Xuwang; Zou, Andy; Wang, Zifan; Mu, Norman; Sakhaee, Elham; Li, Nathaniel; Basart, Steven; Li, Bo; Forsyth, David; Hendrycks, Dan				Zou, Andy/MGU-4410-2025; wang, zifan/HHS-5709-2022						HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2402.04249v1	arXiv:2402.04249			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 06 2024	2024	Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a largescale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github. com/centerforaisafety/HarmBench.																																	2024-05-25	PPRN:87533351		
J	Wu, Shengqiong; Fei, Hao; Qu, Leigang; Ji, Wei; Chua, Tat-Seng				JI, WEI/KEI-2602-2024; Wang, Meng/AEZ-9059-2022; Fei, Hao/IZD-5292-2023						NExT-GPT: Any-to-Any Multimodal LLM								Arxiv											2	2;2024-06-25;https://www.arxiv.org/abs/2309.05519v3| 1;2023-09-13;https://www.arxiv.org/abs/2309.05519v2	arXiv:2309.05519			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 25 2024	2024	While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only inputside multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end -to -end general-purpose any-to-any MM-LLM system, NExT-GPT . We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio. By leveraging the existing well-trained high-performing encoders and decoders, NExTGPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modalityswitching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross -modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human -like AI research in the community. Project website: https://next-gpt.github.io/																																	2024-07-15	PPRN:85006117		
J	Chung, Hyungjin; Kim, Jeongsol; Mccann, Michael T.; Klasky, Marc L.; Ye, Jong Chul				McCann, Michael/AAG-5257-2021; Ye, Jong/C-1623-2011; Chung, Hyungjin/AAL-1161-2021						DIFFUSION POSTERIOR SAMPLING FOR GENERAL NOISY INVERSE PROBLEMS								Arxiv											2	2;2024-05-20;https://www.arxiv.org/abs/2209.14687v4| 1;2022-09-29;https://www.arxiv.org/abs/2209.14687v1	arXiv:2209.14687			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	May 20 2024	2024	Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.																																	2024-06-01	PPRN:18860221		
J	Chen, Lin; Li, Jinsong; Dong, Xiaoyi; Zhang, Pan; Zang, Yuhang; Chen, Zehui; Duan, Haodong; Wang, Jiaqi; Qiao, Yu; Lin, Dahua; Zhao, Feng				WANG, JIAQI/KBB-8837-2024; Zhao, Feng/NGQ-9015-2025; Dong, Xiaoyi/AAC-8666-2019; Lin, Dahua/W-6576-2019; Duan, Haodong/ITV-1505-2023; Chen, Zehui/HDN-3605-2022; Zang, Yuhang/AES-3018-2022						Are We on the Right Way for Evaluating Large Vision-Language Models?								Arxiv											2	2;2024-04-09;https://www.arxiv.org/abs/2403.20330v2| 1;2024-03-29;https://www.arxiv.org/abs/2403.20330v1	arXiv:2403.20330			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 09 2024	2024	Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.9% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.																																	2024-05-22	PPRN:88342970		
J	Lu, Chris; Lu, Cong; Lange, Robert Tjarko; Foerster, Jakob; Clune, Jeff; Ha, David										The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery								Arxiv											2	2;2024-09-01;https://www.arxiv.org/abs/2408.06292v3| 1;2024-08-15;https://www.arxiv.org/abs/2408.06292v2	arXiv:2408.06292			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 01 2024	2024	One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models (LLMs) to perform research independently and communicate their findings. We introduce THE AI SCIENTIST, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion and add them to a growing archive of knowledge, acting like the human scientific community. We demonstrate the versatility of this approach by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a meager cost of less than $15 per paper, illustrating the potential for our framework to democratize research and significantly accelerate scientific progress. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. THE AI SCIENTIST can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world’s most challenging problems. 																																	2024-11-30	PPRN:91413953		
J	Yu, Jiahao; Lin, Xingwei; Yu, Zheng; Xing, Xinyu				Lin, Xingwei/LNP-6081-2024; Yu, Jiahao/HPG-6396-2023						GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts								Arxiv											4	4;2024-06-27;https://www.arxiv.org/abs/2309.10253v4| 3;2024-06-24;https://www.arxiv.org/abs/2309.10253v3| 2;2023-10-04;https://www.arxiv.org/abs/2309.10253v2| 1;2023-09-19;https://www.arxiv.org/abs/2309.10253v1	arXiv:2309.10253			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 27 2024	2024	Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial “jailbreak” attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFUZZER, a novel blackbox jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFUZZER automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFUZZER starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFUZZER : a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFUZZER against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFUZZER consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFUZZER achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFUZZER will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.																																	2024-07-15	PPRN:85050448		
J	Cheng, Zesen; Leng, Sicong; Zhang, Hang; Xin, Yifei; Li, Xin; Chen, Guanzheng; Zhu, Yongxin; Zhang, Wenqi; Luo, Ziyang; Zhao, Deli; Bing, Lidong				Leng, Sicong/MNO-5959-2025; Zhang, Wen/K-1263-2016						VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2406.07476v2| 1;2024-06-11;https://www.arxiv.org/abs/2406.07476v1	arXiv:2406.07476			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks. Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector , which effectively captures the intricate spatial and temporal dynamics of video data. Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues. Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks. Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audiovideo question-answering (AQA & OE-AVQA) benchmarks over existing models. These advancements underline VideoLLaMA 2’s superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems. All models are public to facilitate further research.																																	2024-07-04	PPRN:89283313		
J	Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Mueller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel; Boesel, Frederic; Podell, Dustin; Dockhorn, Tim; English, Zion; Lacey, Kyle; Goodwin, Alex; Marek, Yannik; Rombach, Robin				Esser, Patrick/AAD-7144-2019						Scaling Rectified Flow Transformers for High-Resolution Image Synthesis								Arxiv											1	1;2024-03-05;https://www.arxiv.org/abs/2403.03206v1	arXiv:2403.03206			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 05 2024	2024	Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.																																	2024-04-03	PPRN:88035105		
J	Ma, Jun; Li, Feifei; Wang, Bo				Wang, Bo/HDO-6738-2022; Li, Feifei/C-3476-2017; Ma, Jun/LHA-0128-2024						U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation								Arxiv											1	1;2024-01-09;https://www.arxiv.org/abs/2401.04722v1	arXiv:2401.04722			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jan 09 2024	2024	Convolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results reveal that U-Mamba outperforms state-of-the-art CNN-based and Transformer-based segmentation networks across all tasks. This opens new avenues for efficient long-range dependency modeling in biomedical image analysis. 																																	2024-01-25	PPRN:87081710		
J	Dao, Tri; Gu, Albert										Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality								Arxiv											1	1;2024-05-31;https://www.arxiv.org/abs/2405.21060v1	arXiv:2405.21060			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 31 2024	2024	While Transformers have been the main architecture behind deep learning’s success in language modeling, state -space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well -studied class of structured semiseparable matrices. . Our state space duality (SSD) framework allows us to design a new architecture ( Mamba -2 ) whose core layer is an a refinement of Mamba’s selective SSM that is 2-8× faster, while continuing to be competitive with Transformers on language modeling.																																	2024-06-19	PPRN:89128390		
J	Sheng, Guangming; Zhang, Chi; Ye, Zilingfeng; Wu, Xibin; Zhang, Wang; Zhang, Ru; Peng, Yanghua; Lin, Haibin; Wu, Chuan				Yang, Jacky/HKE-6819-2023; Zhang, Rushan/OFN-9163-2025						HybridFlow: A Flexible and Efficient RLHF Framework								Arxiv											2	2;2024-10-02;https://www.arxiv.org/abs/2409.19256v2| 1;2024-09-28;https://www.arxiv.org/abs/2409.19256v1	arXiv:2409.19256			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 02 2024	2024	Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, , which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. flow . We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53×∼20.57× ×∼ 20.57 × throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at https://github.com/volcengine/verl																																	2024-10-16	PPRN:100732627		
J	Huang, Jie; Chen, Xinyun; Mishra, Swaroop; Zheng, Huaixiu Steven; Yu, Adams Wei; Song, Xinying; Zhou, Denny				Chen, Xinyun/ABZ-9877-2022; Huang, Jie/JYO-7241-2024; Yu, Adams Wei/JHU-2625-2023						Large Language Models Cannot Self-Correct Reasoning Yet								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2310.01798v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.01798v1	arXiv:2310.01798			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.																																	2024-04-11	PPRN:85376709		
J	Wang, Xinlong; Zhang, Xiaosong; Luo, Zhengxiong; Sun, Quan; Cui, Yufeng; Wang, Jinsheng; Zhang, Fan; Wang, Yueze; Li, Zhen; Yu, Qiying; Zhao, Yingli; Ao, Yulong; Min, Xuebin; Li, Tao; Wu, Boya; Zhao, Bo; Zhang, Bowen; Wang, Liangdong; Liu, Guang; He, Zheqi; Yang, Xi; Liu, Jingjing; Lin, Yonghua; Huang, Tiejun; Wang, Zhongyuan		BAAI		Wang, Zhongyuan/HTL-5239-2023; Luo, Zhengxiong/HLP-8344-2023						Emu3: Next-Token Prediction is All You Need								Arxiv											1	1;2024-09-27;https://www.arxiv.org/abs/2409.18869v1	arXiv:2409.18869			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 27 2024	2024	While next-token prediction is considered a promising path towards artificial general intelligence, it has struggled to excel in multimodal tasks, which are still dominated by diffusion models (e.g., Stable Diffusion) and compositional approaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences. Emu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship models such as SDXL and LLaVA-1.6, while eliminating the need for diffusion or compositional architectures. Emu3 is also capable of generating high-fidelity video via predicting the next token in a video sequence. We simplify complex multimodal model designs by converging on a singular focus: tokens, unlocking great potential for scaling both during training and inference. Our results demonstrate that next-token prediction is a promising path towards building general multimodal intelligence beyond language. We open-source key techniques and models to support further research in this direction.																																	2024-10-09	PPRN:100708428		
J	Li, Feng; Zhang, Renrui; Zhang, Hao; Zhang, Yuanhan; Li, Bo; Li, Wei; Ma, Zejun; Li, Chunyuan				Zhang, Zhuosheng/AAF-4919-2020						LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models								Arxiv											2	2;2024-07-28;https://www.arxiv.org/abs/2407.07895v2| 1;2024-07-10;https://www.arxiv.org/abs/2407.07895v1	arXiv:2407.07895			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 28 2024	2024	Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multi-image scenarios remains less explored. Additionally, prior LMM research separately tackles different scenarios, leaving it impossible to generalize cross scenarios with new emerging capabilities. To this end, we introduce LLaVA-NeXT-Interleave, , which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and M ulti-patch (single-image) scenarios in LMMs. To enable these capabilities, we regard the interleaved data format as a general template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4 primary domains with 14 tasks and 41 datasets. We also curate the LLaVAInterleave Bench to comprehensively evaluate the multiimage performance of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading results in multi-image, video, and 3D benchmarks, while maintaining the performance of single-image tasks. Besides, our model also exhibits several emerging capabilities, e.g., transferring tasks across different settings and modalities. 																																	2024-08-03	PPRN:90763521		
J	Hong, Yicong; Zhang, Kai; Gu, Jiuxiang; Bi, Sai; Zhou, Yang; Liu, Difan; Liu, Feng; Sunkavalli, Kalyan; Bui, Trung; Tan, Hao				Zhou, Yang/ABA-8626-2020						LRM: Large Reconstruction Model for Single Image to 3D								Arxiv											2	2;2024-03-09;https://www.arxiv.org/abs/2311.04400v2| 1;2023-11-08;https://www.arxiv.org/abs/2311.04400v1	arXiv:2311.04400			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 09 2024	2024	We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.																																	2024-04-04	PPRN:86097050		
J	Ma, Xin; Wang, Yaohui; Jia, Gengyun; Chen, Xinyuan; Liu, Ziwei; Li, Yuan-Fang; Chen, Cunjian; Qiao, Yu				yaohui, Wang/HJA-6302-2022; Jia, Gengyun/MZR-7814-2025; Liu, Ziwei/AAG-6939-2021; Ma, Xin/JOK-4165-2023; Qiao, Yu/ABD-5787-2021; CHEN, CUNJIAN/JBS-6301-2023; Li, Yuan-Fang/T-7532-2019						Latte: Latent Diffusion Transformer for Video Generation								Arxiv											1	1;2024-01-05;https://www.arxiv.org/abs/2401.03048v1	arXiv:2401.03048			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 05 2024	2024	We propose a novel Latent Diffusion Transformer, namely Latte, for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to text-to-video generation (T2V) task, where Latte achieves comparable results compared to recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation.																																	2024-01-25	PPRN:87069630		
J	Hu, Shengding; Tu, Yuge; Han, Xu; He, Chaoqun; Cui, Ganqu; Long, Xiang; Zheng, Zhi; Fang, Yewei; Huang, Yuxiang; Zhao, Weilin; Zhang, Xinrong; Thai, Zheng Leng; Zhang, Kaihuo; Wang, Chongyi; Yao, Yuan; Zhao, Chenyang; Zhou, Jie; Cai, Jie; Zhai, Zhongwu; Ding, Ning; Jia, Chao; Zeng, Guoyang; Li, Dahai; Liu, Zhiyuan; Sun, Maosong				Liu, Zhiyuan/I-2233-2014; Li, Dahai/HCH-7167-2022; Zhang, Qingyun/JKH-5964-2023; Hu, Shengding/JRY-6064-2023; zhao, weilin/JMC-5864-2023; Zhang, Xinrong/JPK-2282-2023						MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies								Arxiv											3	3;2024-06-03;https://www.arxiv.org/abs/2404.06395v3| 2;2024-04-22;https://www.arxiv.org/abs/2404.06395v2| 1;2024-04-09;https://www.arxiv.org/abs/2404.06395v1	arXiv:2404.06395			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly 1.																																	2024-06-22	PPRN:88468168		
J	Liu, Yong; Hu, Tengge; Zhang, Haoran; Wu, Haixu; Wang, Shiyu; Ma, Lintao; Long, Mingsheng										iTransformer: Inverted Transformers Are Effective for Time Series Forecasting								Arxiv											4	4;2024-03-14;https://www.arxiv.org/abs/2310.06625v4| 3;2024-03-09;https://www.arxiv.org/abs/2310.06625v3| 2;2023-12-01;https://www.arxiv.org/abs/2310.06625v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06625v1	arXiv:2310.06625			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 14 2024	2024	The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting.																																	2024-04-11	PPRN:85521908		
J	Wang, Liang; Yang, Nan; Huang, Xiaolong; Yang, Linjun; Majumder, Rangan; Wei, Furu				Yang, Linjun/MTA-5431-2025						Improving Text Embeddings with Large Language Models								Arxiv											3	3;2024-05-31;https://www.arxiv.org/abs/2401.00368v3| 2;2024-01-19;https://www.arxiv.org/abs/2401.00368v2| 1;2023-12-31;https://www.arxiv.org/abs/2401.00368v1	arXiv:2401.00368			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 31 2024	2024	In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.																																	2024-11-09	PPRN:86904455		
J	Jin, Ming; Wang, Shiyu; Ma, Lintao; Chu, Zhixuan; Zhang, James Y.; Shi, Xiaoming; Chen, Pin-Yu; Liang, Yuxuan; Li, Yuan-Fang; Pan, Shirui; Wen, Qingsong				Chen, Pin-Yu/AAA-1059-2020; Wen, Qingsong/LTF-7625-2024; Shi, Xiaoming/AAU-4105-2021; Li, Yuan-Fang/T-7532-2019; Liang, Yuxuan/KXR-3882-2024						Time-LLM: Time Series Forecasting by Reprogramming Large Language Models								Arxiv											2	2;2024-01-29;https://www.arxiv.org/abs/2310.01728v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.01728v1	arXiv:2310.01728			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 29 2024	2024	Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present TIME-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM’s ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that TIME-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, TIME-LLM excels in both few-shot and zero-shot learning scenarios. The code is made available at https://github.com/KimMeen/Time-LLM																																	2024-02-15	PPRN:85377888		
J	Black, Kevin; Janner, Michael; Du, Yilun; Kostrikov, Ilya; Levine, Sergey										Training Diffusion Models with Reinforcement Learning								Arxiv											3	3;2024-01-04;https://www.arxiv.org/abs/2305.13301v4| 2;2023-10-01;https://www.arxiv.org/abs/2305.13301v3| 1;2023-05-22;https://www.arxiv.org/abs/2305.13301v2	arXiv:2305.13301			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 04 2024	2024	Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation.																																	2024-01-13	PPRN:71743459		
J	Dai, Damai; Deng, Chengqi; Zhao, Chenggang; Xu, R. X.; Gao, Huazuo; Chen, Deli; Li, Jiashi; Zeng, Wangding; Yu, Xingkai; Wu, Y.; Xie, Zhenda; Li, Y. K.; Huang, Panpan; Luo, Fuli; Ruan, Chong; Sui, Zhifang; Liang, Wenfeng				Huang, Panpan/AAQ-5010-2020; Xie, Zhenda/O-1198-2013; Yu, Xingkai/AAO-5118-2020						DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models								Arxiv											1	1;2024-01-11;https://www.arxiv.org/abs/2401.06066v1	arXiv:2401.06066			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-K out of N experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating KS experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5× expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.																																	2024-01-26	PPRN:87124393		
J	Liu, Shih-Yang; Wang, Chien-Yi; Yin, Hongxu; Molchanov, Pavlo; Wang, Yu-Chiang Frank; Cheng, Kwang-Ting; Chen, Min-Hung				Chen, Long/HZJ-7271-2023; Yin, Hongxu/AAZ-3328-2020; Chen, Min-Hung/W-2192-2019						DoRA: Weight-Decomposed Low-Rank Adaptation								Arxiv											6	6;2024-07-09;https://www.arxiv.org/abs/2402.09353v6| 5;2024-06-03;https://www.arxiv.org/abs/2402.09353v5| 4;2024-04-28;https://www.arxiv.org/abs/2402.09353v4| 3;2024-03-05;https://www.arxiv.org/abs/2402.09353v3| 2;2024-03-01;https://www.arxiv.org/abs/2402.09353v2| 1;2024-02-14;https://www.arxiv.org/abs/2402.09353v1	arXiv:2402.09353			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 09 2024	2024	Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing ours, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. ours~consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. 																																	2024-07-21	PPRN:87688581		
J	Yang, Ling; Zhang, Zhilong; Song, Yang; Hong, Shenda; Xu, Runsheng; Zhao, Yue; Zhang, Wentao; Cui, Bin; Yang, Ming-Hsuan				Zhang, Zhilong/AEC-8768-2022; Yang, Ming-Hsuan/T-9533-2019						Diffusion Models: A Comprehensive Survey of Methods and Applications								Arxiv											5	5;2022-09-07;https://www.arxiv.org/abs/2209.00796v3| 4;2024-12-02;https://www.arxiv.org/abs/2209.00796v14| 3;2024-06-24;https://www.arxiv.org/abs/2209.00796v13| 2;2024-02-06;https://www.arxiv.org/abs/2209.00796v12| 1;2023-10-11;https://www.arxiv.org/abs/2209.00796v11	arXiv:2209.00796			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration.																																	2025-01-11	PPRN:13568951		
J	Wang, Qixun; Bai, Xu; Wang, Haofan; Qin, Zekui; Chen, Anthony; Li, Huaxia; Tang, Xu; Hu, Yao				Wong, Howard/HZJ-8545-2023; Wang, Qixun/ITT-4329-2023						InstantID: Zero-shot Identity-Preserving Generation in Seconds								Arxiv											2	2;2024-02-02;https://www.arxiv.org/abs/2401.07519v2| 1;2024-01-15;https://www.arxiv.org/abs/2401.07519v1	arXiv:2401.07519			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. 																																	2024-05-25	PPRN:87197055		
J	Ma, Yecheng Jason; Liang, William; Wang, Guanzhi; Huang, De-An; Bastani, Osbert; Jayaraman, Dinesh; Zhu, Yuke; Fan, Linxi ''Jim''; Anandkumar, Anima				Jayaraman, Dinesh/AAI-2527-2021; Wang, Guanzhi/KQU-0869-2024						Eureka: Human-Level Reward Design via Coding Large Language Models								Arxiv											2	2;2024-04-30;https://www.arxiv.org/abs/2310.12931v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.12931v1	arXiv:2310.12931			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.																																	2024-05-18	PPRN:85723610		
J	Tong, Shengbang; Brown, Ellis; Wu, Penghao; Woo, Sanghyun; Middepogu, Manoj; Akula, Sai Charitha; Yang, Jihan; Yang, Shusheng; Iyer, Adithya; Pan, Xichen; Wang, Ziteng; Fergus, Rob; Lecun, Yann; Xie, Saining				Yang, Jihan/JQI-4498-2023						<italic>Cambrian</italic>-1: A Fully Open, <italic>Vision-Centric</italic> Exploration of Multimodal LLMs								Arxiv											2	2;2024-12-04;https://www.arxiv.org/abs/2406.16860v2| 1;2024-06-24;https://www.arxiv.org/abs/2406.16860v1	arXiv:2406.16860			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 04 2024	2024	We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.																																	2025-01-15	PPRN:89406634		
J	Liu, Yang; Yao, Yuanshun; Ton, Jean-Francois; Zhang, Xiaoying; Guo, Ruocheng; Cheng, Hao; Klochkov, Yegor; Taufiq, Muhammad Faaiz; Li, Hang				张, 晓颖/ABD-9142-2021						Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment								Arxiv											2	2;2023-08-10;https://www.arxiv.org/abs/2308.05374v1| 1;2024-03-01;	arXiv:2308.05374			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 01 2024	2024	Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.																																	2025-11-07	PPRN:75400242		
J	Abdin, Marah; Aneja, Jyoti; Behl, Harkirat; Bubeck, Sebastien; Eldan, Ronen; Gunasekar, Suriya; Harrison, Michael; Hewett, Russell J.; Javaheripi, Mojan; Kauffmann, Piero; Lee, James R.; Lee, Yin Tat; Li, Yuanzhi; Liu, Weishung; Mendes, Caio C.T.; Nguyen, Anh; Price, Eric; de Rosa, Gustavo; Saarikivi, Olli; Salim, Adil; Shah, Shital; Wang, Xin; Ward, Rachel; Wu, Yue; Yu, Dingli; Zhang, Cyril; Zhang, Yi				Yu, Ding/AFR-1839-2022						Phi-4 Technical Report								Arxiv											1	1;2024-12-12;https://www.arxiv.org/abs/2412.08905v1	arXiv:2412.08905			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 12 2024	2024	We present phi-4 , a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size – especially on reasoning-focused benchmarks – due to improved data, training curriculum, and innovations in the post-training scheme.																																	2025-01-22	PPRN:119948178		
J	Turner, Alexander Matt; Thiergart, Lisa; Leech, Gavin; Udell, David; Vazquez, Juan J.; Mini, Ulisse; MacDiarmid, Monte				Leech, Gavin/HPF-1184-2023						Steering Language Models With Activation Engineering								Arxiv											5	5;2024-10-10;https://www.arxiv.org/abs/2308.10248v5| 4;2024-06-04;https://www.arxiv.org/abs/2308.10248v4| 3;2023-11-13;https://www.arxiv.org/abs/2308.10248v3| 2;2023-09-01;https://www.arxiv.org/abs/2308.10248v2| 1;2023-08-20;https://www.arxiv.org/abs/2308.10248v1	arXiv:2308.10248			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 10 2024	2024	Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model's capabilities. To reduce this gap, we introduce activation engineering: the inference-time modification of activations in order to control (or steer) model outputs. Specifically, we introduce the Activation Addition (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as "Love" versus "Hate") to compute a steering vector (Subramani et al. 2022). By tactically adding in e.g. the "Love" - "Hate" steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level output properties (like topic and sentiment) while preserving performance on off-target tasks. ActAdd is lightweight: it does not require any machine optimization and works with a single pair of data points, which enables rapid iteration over steering. ActAdd demonstrates the power of activation engineering.																																	2024-11-01	PPRN:81920456		
J	Wei, Zeming; Wang, Yifei; Li, Ang; Mo, Yichuan; Wang, Yisen				Wei, Zeming/JDW-5614-2023						Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations								Arxiv											3	3;2024-05-25;https://www.arxiv.org/abs/2310.06387v3| 2;2024-05-04;https://www.arxiv.org/abs/2310.06387v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06387v1	arXiv:2310.06387			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 25 2024	2024	Large Language Models (LLMs) have shown remarkable success in various tasks, yet their safety and the risk of generating harmful content remain pressing concerns. In this paper, we delve into the potential of In-Context Learning (ICL) to modulate the alignment of LLMs. Specifically, we propose the In-Context Attack (ICA) which employs harmful demonstrations to subvert LLMs, and the In-Context Defense (ICD) which bolsters model resilience through examples that demonstrate refusal to produce harmful responses. We offer theoretical insights to elucidate how a limited set of in-context demonstrations can pivotally influence the safety alignment of LLMs. Through extensive experiments, we demonstrate the efficacy of ICA and ICD in respectively elevating and mitigating the success rates of jailbreaking prompts. Our findings illuminate the profound influence of ICL on LLM behavior, opening new avenues for improving the safety of LLMs.																																	2024-06-09	PPRN:85525756		
J	Liu, Yixin; Zhang, Kai; Li, Yuan; Yan, Zhiling; Gao, Chujie; Chen, Ruoxi; Yuan, Zhengqing; Huang, Yue; Sun, Hanchi; Gao, Jianfeng; He, Lifang; Sun, Lichao				Gao, Jianfeng/AAP-8200-2021; He, Lifang/D-8175-2016; Yuan, Zhengqing/HTS-6231-2023						Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models								Arxiv											1	1;2024-04-17;https://www.arxiv.org/abs/2402.17177v3	arXiv:2402.17177			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 17 2024	2024	Sora is a text -to -video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model’s background, related technologies, applications, remaining challenges, and future directions of text -to -video AI models. We first trace Sora’s development and investigate the underlying technologies used to build this “world simulator”. Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film -making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human -AI interaction, boosting productivity and creativity of video generation.   [GRAPHICS}																																	2024-04-28	PPRN:88565996		
J	Robey, Alexander; Wong, Eric; Hassani, Hamed; Pappas, George J.										SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks								Arxiv											4	4;2024-06-11;https://www.arxiv.org/abs/2310.03684v4| 3;2023-10-13;https://www.arxiv.org/abs/2310.03684v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03684v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03684v1	arXiv:2310.03684			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 11 2024	2024	Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. Across a range of popular LLMs, SmoothLLM sets the state-of-the-art for robustness against the GCG, PAIR, RandomSearch, and AmpleGCG jailbreaks. SmoothLLM is also resistant against adaptive GCG attacks, exhibits a small, though non-negligible trade-off between robustness and nominal performance, and is compatible with any LLM.																																	2024-07-02	PPRN:85435838		
J	Ghosh, Dibya; Walke, Homer; Pertsch, Karl; Black, Kevin; Mees, Oier; Dasari, Sudeep; Hejna, Joey; Kreiman, Tobias; Doshi, Ria; Xu, Charles; Luo, Jianlan; Tan, You Liang; Chen, Lawrence Yunliang; Sanketi, Pannag; Vuong, Quan; Xiao, Ted; Sadigh, Dorsa; Finn, Chelsea; Levine, Sergey		Octo Model Team		Chen, Lawrence Yunliang/ACJ-4018-2022						Octo: An Open-Source Generalist Robot Policy								Arxiv											1	1;2024-05-26;https://www.arxiv.org/abs/2405.12213v2	arXiv:2405.12213			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 26 2024	2024	Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.																																	2024-06-08	PPRN:89052588		
J	Chen, Zixiang; Deng, Yihe; Yuan, Huizhuo; Ji, Kaixuan; Gu, Quanquan				Ji, Kaixuan/HZH-5940-2023; CHEN, ZIXIANG/OKT-0372-2025						Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models								Arxiv											3	3;2024-06-14;https://www.arxiv.org/abs/2401.01335v3| 2;2024-02-12;https://www.arxiv.org/abs/2401.01335v2| 1;2024-01-02;https://www.arxiv.org/abs/2401.01335v1	arXiv:2401.01335			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. 																																	2024-07-04	PPRN:86916307		
J	Li, Yanwei; Zhang, Yuechen; Wang, Chengyao; Zhong, Zhisheng; Chen, Yixin; Chu, Ruihang; Liu, Shaoteng; Jia, Jiaya				Jia, Jiaya/I-3251-2012						Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models								Arxiv											1	1;2024-03-27;https://www.arxiv.org/abs/2403.18814v1	arXiv:2403.18814			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 27 2024	2024	In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab-research/MiniGemini.																																	2024-04-14	PPRN:88333625		
J	Sun, Lichao; Huang, Yue; Wang, Haoran; Wu, Siyuan; Zhang, Qihui; Li, Yuan; Gao, Chujie; Huang, Yixin; Lyu, Wenhan; Zhang, Yixuan; Li, Xiner; Sun, Hanchi; Liu, Zhengliang; Liu, Yixin; Wang, Yijue; Zhang, Zhikun; Vidgen, Bertie; Kailkhura, Bhavya; Xiong, Caiming; Xiao, Chaowei; Li, Chunyuan; Xing, Eric; Huang, Furong; Liu, Hao; Ji, Heng; Wang, Hongyi; Zhang, Huan; Yao, Huaxiu; Kellis, Manolis; Zitnik, Marinka; Jiang, Meng; Bansal, Mohit; Zou, James; Pei, Jian; Liu, Jian; Gao, Jianfeng; Han, Jiawei; Zhao, Jieyu; Tang, Jiliang; Wang, Jindong; Vanschoren, Joaquin; Mitchell, John; Shu, Kai; Xu, Kaidi; Chang, Kai-Wei; He, Lifang; Huang, Lifu; Backes, Michael; Gong, Neil Zhenqiang; Yu, Philip S.; Chen, Pin-Yu; Gu, Quanquan; Xu, Ran; Ying, Rex; Ji, Shuiwang; Jana, Suman; Chen, Tianlong; Liu, Tianming; Zhou, Tianyi; Wang, William; Li, Xiang; Zhang, Xiangliang; Wang, Xiao; Xie, Xing; Chen, Xun; Wang, Xuyu; Liu, Yan; Ye, Yanfang; Cao, Yinzhi; Chen, Yong; Zhao, Yue				wang, jindong/ACD-8485-2022; Li, Xiner/IUO-5870-2023; Wang, Hongyi/KMA-5952-2024; Zhao, Yue/AAM-2787-2020; Wang, Xuyu/KGK-8071-2024; Bansal, Mohit/Q-9105-2016; Backes, Michael/N-5126-2016; Chang, Kai-Wei/AAJ-7874-2020; Liu, Yixin/HPD-6922-2023; Li, Xiang/J-6924-2019; Chen, Pin-Yu/AAA-1059-2020; He, Lifang/D-8175-2016; Yu, Philip/A-2815-2012; Zhang, Xiangliang/GXF-6961-2022; Xiao, Chaowei/AAT-8772-2021; SHU, Kai/OEN-5324-2025; Zhao, jieyu/LXB-1295-2024						TrustLLM: Trustworthiness in Large Language Models								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2401.05561v4| 2;2024-01-25;https://www.arxiv.org/abs/2401.05561v3| 1;2024-01-13;https://www.arxiv.org/abs/2401.05561v2	arXiv:2401.05561			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 18 2024	2024	Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TRUSTLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TRUSTLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. For instance, LLMs like GPT-4, ERNIE, and Llama2, which exhibit strong performance in stereotype categorization, tend to reject stereotypical statements more reliably. Similarly, Llama2-70b and GPT-4, known for their proficiency in natural language inference, demonstrate enhanced resilience to adversarial attacks. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Notably, Llama2 demonstrates superior trustworthiness in several tasks, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like moderator, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs, such as Llama2, may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we’ve uncovered key insights into the multifaceted trustworthiness in LLMs. In terms of truthfulness, LLMs often struggle to provide truthful responses due to the noise, misinformation, or outdated information in their training data. Notably, LLMs enhanced with external knowledge sources show a marked performance improvement. For safety, most open-source LLMs significantly lag behind that of proprietary LLMs, particularly in areas like jailbreak, toxicity, and misuse. Also, the challenge of balancing safety without over-caution remains. Regarding fairness, most LLMs exhibit unsatisfactory performance in stereotype recognition, with even the best -performing (GPT-4) having an overall accuracy of only 65%. The robustness of LLMs shows significant variability, especially in open-ended tasks and out-of-distribution tasks. Regarding privacy, while LLMs show an awareness of privacy norms, the understanding and handling of private information vary widely, with some models even demonstrating information leakage when tested on the Enron Email Dataset. Lastly, in machine ethics, LLMs exhibit a basic moral understanding but fall short in complex ethical scenarios. These insights underscore the complexity of trustworthiness in LLMs and highlight the need for continued research efforts to enhance their reliability and ethical alignment. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community as well as various practitioners to foster collaboration is imperative to advance the trustworthiness of LLMs. 																																	2024-05-10	PPRN:87186555		
J	Besta, Maciej; Blach, Nils; Kubicek, Ales; Gerstenberger, Robert; Podstawski, Michal; Gianinazzi, Lukas; Gajda, Joanna; Lehmann, Tomasz; Niewiadomski, Hubert; Nyczyk, Piotr; Hoefler, Torsten				Lehmann, Tomasz/GYA-2602-2022; Podstawski, Michal/ODM-9290-2025; Hoefler, Torsten/HKF-3023-2023						Graph of Thoughts: Solving Elaborate Problems with Large Language Models								Arxiv											3	3;2024-02-06;https://www.arxiv.org/abs/2308.09687v4| 2;2023-11-24;https://www.arxiv.org/abs/2308.09687v3| 1;2023-08-21;https://www.arxiv.org/abs/2308.09687v2	arXiv:2308.09687			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.																																	2024-05-25	PPRN:81977983		
J	Cai, Tianle; Li, Yuhong; Geng, Zhengyang; Peng, Hongwu; Lee, Jason D.; Chen, Deming; Dao, Tri										Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads								Arxiv											2	2;2024-06-14;https://www.arxiv.org/abs/2401.10774v3| 1;2024-01-19;https://www.arxiv.org/abs/2401.10774v1	arXiv:2401.10774			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 14 2024	2024	Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities. Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.																																	2024-07-04	PPRN:87309342		
J	Yue, Xiang; Ni, Yuansheng; Zhang, Kai; Zheng, Tianyu; Liu, Ruoqi; Zhang, Ge; Stevens, Samuel; Jiang, Dongfu; Ren, Weiming; Sun, Yuxuan; Wei, Cong; Yu, Botao; Yuan, Ruibin; Sun, Renliang; Yin, Ming; Zheng, Boyuan; Yang, Zhenzhu; Liu, Yibo; Huang, Wenhao; Sun, Huan; Su, Yu; Chen, Wenhu				Jiang, Dongfu/JTV-4943-2023; Zhang, Kai/KOD-2592-2024; Yue, Xiang/AAG-6582-2019; Zhang, Ge/N-4150-2013; Huang, Wenhao/GWU-9337-2022; Yu, Botao/LIH-7233-2024; Zheng, Tianyu/JXM-4664-2024						MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI								Arxiv											4	4;2024-06-13;https://www.arxiv.org/abs/2311.16502v4| 3;2023-12-21;https://www.arxiv.org/abs/2311.16502v3| 2;2023-12-18;https://www.arxiv.org/abs/2311.16502v2| 1;2023-11-27;https://www.arxiv.org/abs/2311.16502v1	arXiv:2311.16502			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 13 2024	2024	We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.																																	2024-09-18	PPRN:86305622		
J	Yu, Lijun; Lezama, Jose; Gundavarapu, Nitesh B.; Versari, Luca; Sohn, Kihyuk; Minnen, David; Cheng, Yong; Birodkar, Vighnesh; Gupta, Agrim; Gu, Xiuye; Hauptmann, Alexander G.; Gong, Boqing; Yang, Ming-Hsuan; Essa, Irfan; Ross, David A.; Jiang, Lu				Yu, Lijun/AAJ-6691-2020; Yang, Ming-Hsuan/T-9533-2019						Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation								Arxiv											3	3;2024-03-29;https://www.arxiv.org/abs/2310.05737v3| 2;2024-03-13;https://www.arxiv.org/abs/2310.05737v2| 1;2023-10-09;https://www.arxiv.org/abs/2310.05737v1	arXiv:2310.05737			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.																																	2024-04-15	PPRN:85583140		
J	Kovachki, Nikola; Li, Zongyi; Liu, Burigede; Azizzadenesheli, Kamyar; Bhattacharya, Kaushik; Stuart, Andrew; Anandkumar, Anima				Bhattacharya, Kaushik/AAK-4639-2021; Li, Zongyi/AAY-3602-2020						Neural Operator: Learning Maps Between Function Spaces								Arxiv											2	2;2024-05-02;https://www.arxiv.org/abs/2108.08481v6| 1;2021-08-19;https://www.arxiv.org/abs/2108.08481v3	arXiv:2108.08481			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.																																	2024-05-20	PPRN:11989845		
J	Hou, Xinyi; Zhao, Yanjie; Liu, Yue; Yang, Zhou; Wang, Kailong; Li, Li; Luo, Xiapu; Lo, David; Grundy, John; Wang, Haoyu				Wang, Kailong/NLO-6290-2025; Li, Li/AAR-3316-2020; hou, xinyi/HPC-4626-2023; Grundy, John/AAF-1716-2019; Zhao, Yanjie/HNJ-6191-2023; Liu, Yue/AAN-1054-2020; Lo, David/IYT-3538-2023; Wang, Haoyu/GXM-4665-2022; Yang, Zhou/KCY-4504-2024						Large Language Models for Software Engineering: A Systematic Literature Review								Arxiv											6	6;2024-04-10;https://www.arxiv.org/abs/2308.10620v6| 5;2024-03-10;https://www.arxiv.org/abs/2308.10620v5| 4;2023-09-12;https://www.arxiv.org/abs/2308.10620v4| 3;2023-09-03;https://www.arxiv.org/abs/2308.10620v3| 2;2023-08-28;https://www.arxiv.org/abs/2308.10620v2| 1;2023-08-21;https://www.arxiv.org/abs/2308.10620v1	arXiv:2308.10620			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 10 2024	2024	Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE_SLR.																																	2024-04-24	PPRN:81787605		
J	Ji, Jiaming; Qiu, Tianyi; Chen, Boyuan; Zhang, Borong; Lou, Hantao; Wang, Kaile; Duan, Yawen; He, Zhonghao; Zhou, Jiayi; Zhang, Zhaowei; Zeng, Fanzhi; Dai, Juntao; Pan, Xuehai; Ng, Kwan Yee; O'Gara, Aidan; Xu, Hua; Tse, Brian; Fu, Jie; McAleer, Stephen; Yang, Yaodong; Wang, Yizhou; Zhu, Song-Chun; Guo, Yike; Gao, Wen				wang, kaile/NXY-1083-2025; fu, jie/ADX-9945-2022; Zeng, Fanzhi/HMD-5107-2023; Chen, Bo/B-7817-2018; Wang, Yizhou/JCO-3927-2023						AI Alignment: A Comprehensive Survey								Arxiv											2	2;2024-01-02;https://www.arxiv.org/abs/2310.19852v3| 1;2023-11-01;https://www.arxiv.org/abs/2310.19852v2	arXiv:2310.19852			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 02 2024	2024	AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems’ alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. Specifically, we survey traditional preference modeling methods and reinforcement learning from human feedback, and further discuss potential frameworks to reach scalable oversight for tasks where effective human oversight is hard to obtain. Within learning under distribution shift, we also cover data distribution interventions such as adversarial training that help expand the distribution of training data, and algorithmic interventions to combat goal misgeneralization. On backward alignment, we discuss assurance techniques and governance practices. Specifically, we survey assurance methods of AI systems throughout their lifecycle, covering safety evaluation, interpretability, and human value compliance. We discuss the current and prospective governance practices that are adopted by different governments, industry actors, and other third parties, aimed at managing existing and future AI risks. This survey aims to provide a comprehensive yet beginner-friendly review of alignment research topics. Based on this, we also release and continually update the website www.alignmentsurvey.com which features tutorials, collections of papers, blog posts, and other resources.																																	2024-01-11	PPRN:85928869		
J	Liu, Fuxiao; Lin, Kevin; Li, Linjie; Wang, Jianfeng; Yacoob, Yaser; Wang, Lijuan				Li, Linjie/ABC-4651-2021; Lin, Kevin/JFS-1634-2023						Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning								Arxiv											4	4;2024-03-19;https://www.arxiv.org/abs/2306.14565v4| 3;2023-09-29;https://www.arxiv.org/abs/2306.14565v3| 2;2023-09-14;https://www.arxiv.org/abs/2306.14565v2| 1;2023-06-26;https://www.arxiv.org/abs/2306.14565v1	arXiv:2306.14565			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV-Instruction.																																	2024-04-12	PPRN:73545815		
J	Tonmoy, S.Towhidul Islam; Zaman, S M Mehedi; Jain, Vinija; Rani, Anku; Rawte, Vipula; Chadha, Aman; Das, Amitava				Chadha, Dr. Aman/GNM-9565-2022; Zaman, S M Mehedi/OON-0475-2025						A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models								Arxiv											2	2;2024-01-08;https://www.arxiv.org/abs/2401.01313v3| 1;2024-01-03;https://www.arxiv.org/abs/2401.01313v2	arXiv:2401.01313			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 08 2024	2024	As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to “hallucinate” – generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people’s lives (Jain, 2023). The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, customer support conversations, financial analysis reports, and providing erroneous legal advice. Small errors could lead to harm, revealing the LLMs’ lack of actual comprehension despite advances in self-learning. This paper presents a comprehensive survey of over thirty-two techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval-Augmented Generation (RAG) (Lewis et al., 2021), Knowledge Retrieval (Varshney et al., 2023), CoNLI (Lei et al., 2023), and CoVe (Dhuliawala et al., 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.																																	2024-01-21	PPRN:86944455		
J	Lin, Bin; Tang, Zhenyu; Ye, Yang; Huang, Jinfa; Zhang, Junwu; Pang, Yatian; Jin, Peng; Ning, Munan; Luo, Jiebo; Yuan, Li				Jin, Peng/HRA-0993-2023; Tang, Zhenyu/ACY-2335-2022; zhang, junwu/IWM-6534-2023; Yuan, Li/AET-1324-2022						MoE-LLaVA: Mixture of Experts for Large Vision-Language Models								Arxiv											5	5;2024-12-23;https://www.arxiv.org/abs/2401.15947v5| 4;2024-07-06;https://www.arxiv.org/abs/2401.15947v4| 3;2024-02-17;https://www.arxiv.org/abs/2401.15947v3| 2;2024-02-04;https://www.arxiv.org/abs/2401.15947v2| 1;2024-01-29;https://www.arxiv.org/abs/2401.15947v1	arXiv:2401.15947			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 23 2024	2024	Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovatively addresses the common issue of performance degradation in multi-modal sparsity learning, consequently constructing a sparse model with an outrageous number of parameters but a constant computational cost. Furthermore, we present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments show the significant performance of MoE-LLaVA in a variety of visual understanding and object hallucination benchmarks. Remarkably, with only approximately 3B sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. 																																	2025-02-02	PPRN:87393399		
J	Penedo, Guilherme; Kydlicek, Hynek; Ben allal, Loubna; Lozhkov, Anton; Mitchell, Margaret; Raffel, Colin; Von Werra, Leandro; Wolf, Thomas										The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale								Arxiv											2	2;2024-10-31;https://www.arxiv.org/abs/2406.17557v2| 1;2024-06-25;https://www.arxiv.org/abs/2406.17557v1	arXiv:2406.17557			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 31 2024	2024	The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.																																	2024-12-06	PPRN:89524669		
J	He, Chaoqun; Luo, Renjie; Bai, Yuzhuo; Hu, Shengding; Thai, Zhen Leng; Shen, Junhao; Hu, Jinyi; Han, Xu; Huang, Yujie; Zhang, Yuxiang; Liu, Jie; Qi, Lei; Liu, Zhiyuan; Sun, Maosong				Hu, Shengding/JRY-6064-2023; SHEN, Junhao/OLQ-0931-2025; Hu, Jinyi/GXF-7296-2022; Liu, Zhiyuan/I-2233-2014; Huang, Yujie/LVR-2899-2024						OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems								Arxiv											1	1;2024-02-21;https://www.arxiv.org/abs/2402.14008v1	arXiv:2402.14008			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors.																																	2024-05-22	PPRN:87793730		
J	Hsieh, Cheng-Ping; Sun, Simeng; Kriman, Samuel; Acharya, Shantanu; Rekesh, Dima; Jia, Fei; Zhang, Yang; Ginsburg, Boris				Acharya, Shantanu/NHQ-1893-2025						RULER: What's the Real Context Size of Your Long-Context Language Models?								Arxiv											3	3;2024-08-06;https://www.arxiv.org/abs/2404.06654v3| 2;2024-04-11;https://www.arxiv.org/abs/2404.06654v2| 1;2024-04-09;https://www.arxiv.org/abs/2404.06654v1	arXiv:2404.06654			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 06 2024	2024	The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the "needle") from long distractor texts (the "haystack"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.																																	2024-08-17	PPRN:88477813		
J	Xiong, Miao; Hu, Zhiyuan; Lu, Xinyang; Li, Yifei; Fu, Jie; He, Junxian; Hooi, Bryan				Hooi, Bryan/AAU-5707-2020; Hu, Zhiyuan/JPX-7229-2023; Li, Yifei/AAB-4210-2019; HE, Junxian/OHV-2278-2025						Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs								Arxiv											2	2;2024-03-17;https://www.arxiv.org/abs/2306.13063v2| 1;2023-06-22;https://www.arxiv.org/abs/2306.13063v1	arXiv:2306.13063			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 17 2024	2024	Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.																																	2024-04-11	PPRN:73469205		
J	Wang, Zihao; Cai, Shaofei; Chen, Guanzhou; Liu, Anji; Ma, Xiaojian; Liang, Yitao		Team CraftJarvis		Liang, Yitao/GWN-1575-2022; Wang, Zihao/S-7875-2019						Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents								Arxiv											3	3;2024-07-08;https://www.arxiv.org/abs/2302.01560v3| 2;2023-10-29;https://www.arxiv.org/abs/2302.01560v2| 1;2023-02-03;https://www.arxiv.org/abs/2302.01560v1	arXiv:2302.01560			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 08 2024	2024	We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "Describe, Explain, Plan and Select" (DEPS), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated plan by integrating description of the plan execution process and providing self-explanation of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal selector, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the ObtainDiamond grand challenge with our approach. 																																	2024-07-21	PPRN:36135136		
J	Cui, Yiming; Yang, Ziqing; Yao, Xin				Cui, Yiming/AAA-5499-2022						EFFICIENT AND EFFECTIVE TEXT ENCODING FOR CHINESE LLAMA AND ALPACA								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/2304.08177v3| 1;2023-04-17;https://www.arxiv.org/abs/2304.08177v1	arXiv:2304.08177			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, significantly enhancing the model's ability to comprehend and execute instructions. Our experimental results indicate that the newly proposed model markedly enhances the original LLaMA's proficiency in understanding and generating Chinese content. Additionally, the results on the C-Eval dataset yield competitive performance among the models with several times the size of ours. We have made our pre-trained models, training scripts, and other resources available through GitHub, fostering open research for our community.  																																	2024-03-23	PPRN:63495344		
J	Fu, Zipeng; Zhao, Tony Z.; Finn, Chelsea										Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation								Arxiv											1	1;2024-01-04;https://www.arxiv.org/abs/2401.02117v1	arXiv:2401.02117			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 04 2024	2024	Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system [104] with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet.																																	2024-05-25	PPRN:86964956		
J	Wu, Guanjun; Yi, Taoran; Fang, Jiemin; Xie, Lingxi; Zhang, Xiaopeng; Wei, Wei; Liu, Wenyu; Tian, Qi; Wang, Xinggang				Yi, Tao/MHP-8239-2025; Wenyu, Liu/GRS-3009-2022; wu, guanjun/LEN-0332-2024; Xie, Lingxi/ABF-6996-2020; Wang, Xinggang/LSL-0946-2024						4D Gaussian Splatting for Real-Time Dynamic Scene Rendering								Arxiv											3	3;2024-07-15;https://www.arxiv.org/abs/2310.08528v3| 2;2023-12-07;https://www.arxiv.org/abs/2310.08528v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08528v1	arXiv:2310.08528			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 15 2024	2024	Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800$times$800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.																																	2024-07-23	PPRN:85604367		
J	Deitke, Matt; Clark, Christopher; Lee, Sangho; Tripathi, Rohun; Yang, Yue; Park, Jae Sung; Salehi, Mohammadreza; Muennighoff, Niklas; Lo, Kyle; Soldaini, Luca; Lu, Jiasen; Anderson, Taira; Bransom, Erin; Ehsani, Kiana; Ngo, Huong; Chen, Yensung; Patel, Ajay; Yatskar, Mark; Callison-Burch, Chris; Head, Andrew; Hendrix, Rose; Bastani, Favyen; Vanderbilt, Eli; Lambert, Nathan; Chou, Yvonne; Chheda, Arnavi; Sparks, Jenna; Skjonsberg, Sam; Schmitz, Michael; Sarnat, Aaron; Bischoff, Byron; Walsh, Pete; Newell, Chris; Wolters, Piper; Gupta, Tanmay; Zeng, Kuo-Hao; Borchardt, Jon; Groeneveld, Dirk; Dumas, Jen; Nam, Crystal; Lebrecht, Sophie; Wittlif, Caitlin; Schoenick, Carissa; Michel, Oscar; Krishna, Ranjay; Weihs, Luca; Smith, Noah A.; Hajishirzi, Hannaneh; Girshick, Ross; Farhadi, Ali; Kembhavi, Aniruddha				Callison-Burch, Chris/A-3393-2010; Ehsani, Kiana/X-4345-2019; Patel, Ajay/HNR-6989-2023; Lee, Sang Ho/HNC-3447-2023						Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models								Arxiv											1	1;2024-09-25;https://www.arxiv.org/abs/2409.17146v1	arXiv:2409.17146			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 25 2024	2024	Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation. We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future.  																																	2024-10-08	PPRN:98871722		
J	Zheng, Yaowei; Zhang, Richong; Zhang, Junhao; Ye, Yanhan; Luo, Zheyan; Feng, Zhangchi; Ma, Yongqiang				Feng, Zhangchi/NSV-0061-2025						LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models								Arxiv											2	2;2024-06-24;https://www.arxiv.org/abs/2403.13372v3| 1;2024-03-21;https://www.arxiv.org/abs/2403.13372v2	arXiv:2403.13372			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 24 2024	2024	Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. 																																	2024-07-12	PPRN:88257948		
J	Zhu, Qihao; Guo, Daya; Shao, Zhihong; Yang, Dejian; Wang, Peiyi; Xu, Runxin; Wu, Y.; Li, Yukun; Gao, Huazuo; Ma, Shirong; Zeng, Wangding; Bi, Xiao; Gu, Zihui; Xu, Hanwei; Dai, Damai; Dong, Kai; Zhang, Liyue; Piao, Yishi; Gou, Zhibin; Xie, Zhenda; Hao, Zhewen; Wang, Bingxuan; Song, Junxiao; Chen, Deli; Xie, Xin; Guan, Kang; You, Yuxiang; Liu, Aixin; Du, Qiushi; Gao, Wenjun; Lu, Xuan; Chen, Qinyu; Wang, Yaohui; Deng, Chengqi; Li, Jiashi; Zhao, Chenggang; Ruan, Chong; Luo, Fuli; Liang, Wenfeng				Xie, Zhenda/O-1198-2013; yaohui, Wang/HJA-6302-2022; chen, qinyu/HNQ-3923-2023; wang, peiyi/LNR-6224-2024; Dai, Damai/KEJ-3256-2024; zhang, liyue/AAU-6549-2021; gao, wenjun/KIA-6581-2024; Li, Yukun/J-2135-2015; Bi, Xiao/GQG-9410-2022; Zhu, Qihao/JWO-8071-2024; Guo, Daya/HPG-8192-2023						DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.11931v1	arXiv:2406.11931			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.																																	2024-07-06	PPRN:89361621		
J	Gao, Leo; la Tour, Tom Dupre; Tillman, Henk; Goh, Gabriel; Troll, Rajan; Radford, Alec; Sutskever, Ilya; Leike, Jan; Wu, Jeffrey				la Tour, Tom/ABH-9892-2020						Scaling and evaluating sparse autoencoders								Arxiv											1	1;2024-06-06;https://www.arxiv.org/abs/2406.04093v1	arXiv:2406.04093			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 06 2024	2024	Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.																																	2024-06-22	PPRN:89248706		
J	Lai, Xin; Tian, Zhuotao; Chen, Yukang; Li, Yanwei; Yuan, Yuhui; Liu, Shu; Jia, Jiaya				Tian, Zhuotao/HJP-1597-2023; Jia, Jiaya/I-3251-2012; Chen, Yukang/HKW-0344-2023						LISA: Reasoning Segmentation via Large Language Model								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2308.00692v3| 1;2023-08-01;https://www.arxiv.org/abs/2308.00692v1	arXiv:2308.00692			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 01 2024	2024	Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task — reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a token and propose the embeddingas-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero -shot capability when trained exclusively on reasoningfree datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. 																																	2024-05-18	PPRN:74200090		
J	Gou, Zhibin; Shao, Zhihong; Gong, Yeyun; Shen, Yelong; Yang, Yujiu; Duan, Nan; Chen, Weizhu				Yang, Yujiu/JGM-0303-2023; Duan, Nan/AAR-2231-2020						CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing								Arxiv											4	4;2024-02-21;https://www.arxiv.org/abs/2305.11738v4| 3;2024-02-16;https://www.arxiv.org/abs/2305.11738v3| 2;2023-09-30;https://www.arxiv.org/abs/2305.11738v2| 1;2023-05-19;https://www.arxiv.org/abs/2305.11738v1	arXiv:2305.11738			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 16 2024	2024	Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.																																	2024-03-21	PPRN:70570043		
J	Wei, Yuxiang; Wang, Zhe; Liu, Jiawei; Ding, Yifeng; Zhang, Lingming				Wei, Yuxiang/KYQ-5192-2024; Liu, Jiawei/JVZ-3421-2024						Magicoder: Empowering Code Generation with OSS-Instruct								Arxiv											2	2;2024-06-07;https://www.arxiv.org/abs/2312.02120v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02120v1	arXiv:2312.02120			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.																																	2024-06-22	PPRN:86378093		
J	Maiolino, Roberto; Scholtz, Jan; Curtis-Lake, Emma; Carniani, Stefano; Baker, William; de Graaff, Anna; Tacchella, Sandro; Ubler, Hannah; D'Eugenio, Francesco; Witstok, Joris; Curti, Mirko; Arribas, Santiago; Bunker, Andrew J.; Charlot, Stephane; Chevallard, Jacopo; Eisenstein, Daniel J.; Egami, Eiichi; Ji, Zhiyuan; Jones, Gareth C.; Lyu, Jianwei; Rawle, Tim; Robertson, Brant; Rujopakarn, Wiphu; Perna, Michele; Sun, Fengwu; Venturi, Giacomo; Williams, Christina C.; Willott, Chris				ji, zhiyuan/HGC-6180-2022; Jones, Gareth/AAD-7663-2022; Rujopakarn, Wiphu/AAV-2176-2021; Robertson, Brant/AAA-6124-2022; Arribas, Santiago/F-9277-2015; D'Eugenio, Francesco/H-2606-2019; Lyu, Jianwei/KGL-5057-2024; Baker, William/J-7584-2014; Venturi, Giacomo/AAB-4352-2021; Tacchella, Sandro/AAT-1602-2021						JADES. The diverse population of infant Black Holes at 4<z<11: merging, tiny, poor, but mighty								Arxiv											1	1;2024-08-23;https://www.arxiv.org/abs/2308.01230v4	arXiv:2308.01230			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 23 2024	2024	Spectroscopy with the James Webb Space Telescope has opened the possibility to identify moderate luminosity Active Galactic Nuclei (AGN) in the early Universe, at and beyond the epoch of reionization, complementing previous surveys of much more luminous (and much rarer) quasars. We present 12 new AGN at 4<z<7 in the JADES survey (in addition to the previously identified AGN in GN-z11 at z=10.6) revealed through the detection of a Broad Line Region as seen in the Balmer emission lines. The depth of JADES, together with the use of three different spectral resolutions, enables us to probe a lower mass regime relative to previous studies. In a few cases we find evidence for two broad components of Hα,which suggests that these could be candidate merging black holes (BHs), although a BLR complex geometry cannot be excluded. The inferred BH masses range between 8 × 107M⊙ down to 4 × 105 M⊙, interestingly probing the regime expected for Direct Collapse Black Holes. The inferred AGN bolometric luminosities (∼1044−1045erg/s) imply accretion rates that are < 0.5 times the Eddington rate in most cases. However, small BHs, with MBH ∼ 106 M⊙, tend to accrete at Eddington or super-Eddington rates. These BHs at z∼4-11 are over-massive relative to their host galaxies stellar masses when compared to the local MBH−Mstar relation, even approaching MBH ∼ Mstar, as expected from heavy BH seeds and/or super-Eddington accretion scenarios. However, we find that these early BH tend to be more consistent with the local relation between MBH and velocity dispersion, as well as between MBH and dynamical mass, suggesting that these are more fundamental and universal relations. On the BPT excitation-diagnostic diagram these AGN are located in the region that is that is locally occupied by star-forming galaxies, implying that they would be missed by the standard classification techniques if they did not display broad lines. Their location on the diagram is consistent with what expected for AGN hosted in metal poor galaxies (Z∼0.1−0.2 Z⊙). The fraction of broad line AGN with LAGN>1044erg/s among galaxies in the redshift range 4<z<6 is about 10%, suggesting that the contribution of AGN and their hosts to the reionization of the Universe is>10%.																																	2025-01-08	PPRN:74218775		
J	Zhou, Chunting; Yu, Lili; Babu, Arun; Tirumala, Kushal; Yasunaga, Michihiro; Shamis, Leonid; Kahn, Jacob; Ma, Xuezhe; Zettlemoyer, Luke; Levy, Omer				Ma, Xuezhe/GWN-1885-2022; Yasunaga, Michihiro/GPW-9499-2022						Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model								Arxiv											1	1;2024-08-20;https://www.arxiv.org/abs/2408.11039v1	arXiv:2408.11039			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 20 2024	2024	We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.																																	2024-08-30	PPRN:91498814		
J	Zhu, Kaijie; Wang, Jindong; Zhou, Jiaheng; Wang, Zeek; Chen, Hao; Wang, Yidong; Yang, Linyi; Ye, Wei; Gong, Neil Zhenqiang; Zhang, Yue; Xie, Xing				wang, jindong/ACD-8485-2022; Zhu, Kaijie/KHX-8423-2024						PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts								Arxiv											4	4;2024-07-16;https://www.arxiv.org/abs/2306.04528v5| 3;2023-10-18;https://www.arxiv.org/abs/2306.04528v4| 2;2023-08-24;https://www.arxiv.org/abs/2306.04528v3| 1;2023-06-07;https://www.arxiv.org/abs/2306.04528v2	arXiv:2306.04528			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 16 2024	2024	The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptRobust, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present a comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users.																																	2024-07-25	PPRN:73040087		
J	Zeng, Yi; Lin, Hongpeng; Zhang, Jingwen; Yang, Diyi; Jia, Ruoxi; Shi, Weiyan				zeng, yi/KFS-5661-2024; Zhang, Jingwen/ABC-4769-2021; Shi, Weiyan/JOK-7836-2023						How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs								Arxiv											2	2;2024-01-23;https://www.arxiv.org/abs/2401.06373v2| 1;2024-01-12;https://www.arxiv.org/abs/2401.06373v1	arXiv:2401.06373			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 23 2024	2024	Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs																																	2024-05-25	PPRN:87156156		
J	Yang, John; Jimenez, Carlos E.; Wettig, Alexander; Lieret, Kilian; Yao, Shunyu; Narasimhan, Karthik; Press, Ofir										SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering								Arxiv											3	3;2024-11-11;https://www.arxiv.org/abs/2405.15793v3| 2;2024-05-30;https://www.arxiv.org/abs/2405.15793v2| 1;2024-05-06;https://www.arxiv.org/abs/2405.15793v1	arXiv:2405.15793			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 11 2024	2024	Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.																																	2024-12-18	PPRN:89052255		
J	Towers, Mark; Kwiatkowski, Ariel; Terry, Jordan; Balis, John U.; De Cola, Gianluca; Deleu, Tristan; Goulao, Manuel; Kallinteris, Andreas; Krimmel, Markus; Arjun, KG; Perez-Vicente, Rodrigo; Pierre, Andrea; Schulhoff, Sander; Tai, Jun Jet; Tan, Hannah; Younis, Omar G.				T, Jun/AAG-4954-2021						Gymnasium: A Standard Interface for Reinforcement Learning Environments								Arxiv											3	3;2024-11-08;https://www.arxiv.org/abs/2407.17032v3| 2;2024-10-09;https://www.arxiv.org/abs/2407.17032v2| 1;2024-07-24;https://www.arxiv.org/abs/2407.17032v1	arXiv:2407.17032			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 08 2024	2024	Reinforcement Learning (RL) is a continuously growing field that has the potential to revolutionize many areas of artificial intelligence. However, despite its promise, RL research is often hindered by the lack of standardization in environment and algorithm implementations. This makes it difficult for researchers to compare and build upon each other's work, slowing down progress in the field. Gymnasium is an open-source library that provides a standard API for RL environments, aiming to tackle this issue. Gymnasium's main feature is a set of abstractions that allow for wide interoperability between environments and training algorithms, making it easier for researchers to develop and test RL algorithms. In addition, Gymnasium provides a collection of easy-to-use environments, tools for easily customizing environments, and tools to ensure the reproducibility and robustness of RL research. Through this unified framework, Gymnasium significantly streamlines the process of developing and testing RL algorithms, enabling researchers to focus more on innovation and less on implementation details. By providing a standardized platform for RL research, Gymnasium helps to drive forward the field of reinforcement learning and unlock its full potential. 																																	2024-12-16	PPRN:91057907		
J	Zhang, Duzhen; Yu, Yahan; Dong, Jiahua; Li, Chenxing; Su, Dan; Chu, Chenhui; Yu, Dong				Zhang, Duzhen/LXV-7949-2024						MM-LLMs: Recent Advances in MultiModal Large Language Models								Arxiv											3	3;2024-05-28;https://www.arxiv.org/abs/2401.13601v5| 2;2024-02-20;https://www.arxiv.org/abs/2401.13601v4| 1;2024-01-25;https://www.arxiv.org/abs/2401.13601v2	arXiv:2401.13601			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off -the -shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision -making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website1 1 for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.																																	2024-06-13	PPRN:87335927		
J	Azerbayev, Zhangir; Schoelkopf, Hailey; Paster, Keiran; Dos Santos, Marco; McAleer, Stephen; Jiang, Albert Q.; Deng, Jia; Biderman, Stella; Welleck, Sean										Llemma: An Open Language Model For Mathematics								Arxiv											3	3;2024-03-15;https://www.arxiv.org/abs/2310.10631v3| 2;2023-12-01;https://www.arxiv.org/abs/2310.10631v2| 1;2023-10-16;https://www.arxiv.org/abs/2310.10631v1	arXiv:2310.10631			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 15 2024	2024	We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.																																	2024-04-11	PPRN:85662240		
J	Hong, Jiwoo; Lee, Noah; Thorne, James										ORPO: Monolithic Preference Optimization without Reference Model								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2403.07691v2| 1;2024-03-12;https://www.arxiv.org/abs/2403.07691v1	arXiv:2403.07691			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 14 2024	2024	While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference -aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model -free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi -2 (2.7B), Llama -2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on AlpacaEval2.0 (Figure 1), 66.19% on IFEval (instruction -level loose, Table 6), and 7.32 in MT -Bench (Figure 12). We release code1 and model checkpoints for Mistral-ORPO- α (7B)2 and Mistral-ORPO-β (7B).3																																	2024-04-11	PPRN:88118714		
J	Wang, Peiyi; Li, Lei; Shao, Zhihong; Xu, R.X.; Dai, Damai; Li, Yifei; Chen, Deli; Wu, Y.; Sui, Zhifang				Li, Lei/LMN-0940-2024; Dai, Damai/KEJ-3256-2024; wang, peiyi/LNR-6224-2024						Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations								Arxiv											3	3;2024-02-19;https://www.arxiv.org/abs/2312.08935v3| 2;2023-12-28;https://www.arxiv.org/abs/2312.08935v2| 1;2023-12-14;https://www.arxiv.org/abs/2312.08935v1	arXiv:2312.08935			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 19 2024	2024	In this paper, we present an innovative process -oriented math process reward model called MATH -SHEPHERD, which assigns a reward score to each step of math problem solutions. The training of MATH -SHEPHERD is achieved using automatically constructed process -wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of MATH -SHEPHERD in two scenarios: 1) Verification: MATH -SHEPHERD is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) Reinforcement Learning: MATH -SHEPHERD is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With MATH -SHEPHERD, a series of open -source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with MATH -SHEPHERD significantly improves the accuracy of Mistral -7B (77.9%→84.1% on GSM8K and 28.6%→33.0% on MATH). The accuracy can be further enhanced to 89.1% and 43.5% on GSM8K and MATH with the verification of MATH -SHEPHERD, respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.																																	2024-03-16	PPRN:86587277		
J	Cui, Jiaxi; Ning, Munan; Li, Zongjian; Chen, Bohua; Yan, Yang; Li, Hao; Ling, Bin; Tian, Yonghong; Yuan, Li				Li, Yongjiang/AAF-9313-2020; TIAN, Yonghong/M-4937-2013; Yuan, Li/AET-1324-2022; cui, jiaxi/H-8790-2015						Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2306.16092v2| 1;2023-06-28;https://www.arxiv.org/abs/2306.16092v1	arXiv:2306.16092			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services. By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model. This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally, Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case consultations, demonstrating our robust capability for legal consultation.																																	2024-06-16	PPRN:73584528		
J	Nguyen, Thanh Tam; Huynh, Thanh Trung; Ren, Zhao; Nguyen, Phi Le; Liew, Alan Wee-Chung; Yin, Hongzhi; Nguyen, Quoc Viet Hung				Nguyen, Tien Thai/AAB-7638-2022; Liew, Alan/F-6988-2011; Nguyen, Quoc/L-8695-2017						A Survey of Machine Unlearning								Arxiv											2	2;2024-09-17;https://www.arxiv.org/abs/2209.02299v6| 1;2022-09-08;https://www.arxiv.org/abs/2209.02299v3	arXiv:2209.02299			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 17 2024	2024	Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. 																																	2024-09-29	PPRN:13568802		
J	Wang, Liang; Yang, Nan; Huang, Xiaolong; Yang, Linjun; Majumder, Rangan; Wei, Furu				Yang, Linjun/MTA-5431-2025						Multilingual E5 Text Embeddings: A Technical Report								Arxiv											1	1;2024-02-08;https://www.arxiv.org/abs/2402.05672v1	arXiv:2402.05672			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 08 2024	2024	This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. 																																	2024-05-25	PPRN:87572495		
J	Liu, Hanchao; Xue, Wenyuan; Chen, Yifei; Chen, Dapeng; Zhao, Xiutian; Wang, Ke; Hou, Liping; Li, Rongjun; Peng, Wei				Chen, Dapeng/U-5866-2018; Peng, Wei/OEO-9234-2025; Wang, Ke/AAW-4079-2020; Liu, Hanchao/ABE-4637-2020; Chen, Yifei/HPC-5552-2023						A Survey on Hallucination in Large Vision-Language Models								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2402.00253v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.00253v1	arXiv:2402.00253			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	Recent development of Large Vision -Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, “hallucination”, or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.																																	2024-05-25	PPRN:87450869		
J	Wang, Yi; He, Yinan; Li, Yizhuo; Li, Kunchang; Yu, Jiashuo; Ma, Xin; Li, Xinhao; Chen, Guo; Chen, Xinyuan; Wang, Yaohui; He, Conghui; Luo, Ping; Liu, Ziwei; Wang, Yali; Wang, Limin; Qiao, Yu				pluo/GPG-2707-2022; Li, Yizhuo/AAL-5705-2021; Li, Kunchang/KFA-4043-2024; He, Conghui/AAZ-3323-2021; Liu, Ziwei/AAG-6939-2021; Wang, Limin/AAE-3419-2019; Ma, Xinzhu/MGA-5961-2025; Qiao, Yu/ABD-5787-2021						InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation								Arxiv											2	2;2024-01-04;https://www.arxiv.org/abs/2307.06942v2| 1;2023-07-13;https://www.arxiv.org/abs/2307.06942v1	arXiv:2307.06942			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 04 2024	2024	This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.																																	2024-01-13	PPRN:73909336		
J	Luo, Liangchen; Liu, Yinxiao; Liu, Rosanne; Phatale, Samrat; Guo, Meiqi; Lara, Harsh; Li, Yunxuan; Shu, Lei; Zhu, Yun; Meng, Lei; Sun, Jiao; Rastogi, Abhinav				LI, Yunxuan/HKM-8952-2023						Improve Mathematical Reasoning in Language Models by Automated Process Supervision								Arxiv											2	2;2024-12-11;https://www.arxiv.org/abs/2406.06592v2| 1;2024-06-05;https://www.arxiv.org/abs/2406.06592v1	arXiv:2406.06592			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 11 2024	2024	Complex multi-step reasoning tasks, such as solving mathematical problems or generating code, remain a significant hurdle for even the most advanced large language models (LLMs). Verifying LLM outputs with an Outcome Reward Model (ORM) is a standard inference-time technique aimed at enhancing the reasoning performance of LLMs. However, this still proves insufficient for reasoning tasks with a lengthy or multi-hop reasoning chain, where the intermediate outcomes are neither properly rewarded nor penalized. Process supervision addresses this limitation by assigning intermediate rewards during the reasoning process. To date, the methods used to collect process supervision data have relied on either human annotation or per-step Monte Carlo estimation, both prohibitively expensive to scale, thus hindering the broad application of this technique. In response to this challenge, we propose a novel divide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm named OmegaPRM for the efficient collection of high-quality process supervision data. This algorithm swiftly identifies the first error in the Chain of Thought (CoT) with binary search and balances the positive and negative examples, thereby ensuring both efficiency and quality. As a result, we are able to collect over 1.5 million process supervision annotations to train Process Reward Models (PRMs). This fully automated process supervision alongside the weighted self-consistency algorithm is able to enhance LLMs’ math reasoning performances. We improved the success rates of the instruction-tuned Gemini Pro model from 51% to 69.4% on MATH500 and from 86.4% to 93.6% on GSM8K. Similarly, we boosted the success rates of Gemma2 27B from 42.3% to 58.2% on MATH500 and from 74.0% to 92.2% on GSM8K. The entire process operates without any human intervention or supervision, making our method both financially and computationally cost-effective compared to existing methods.																																	2025-01-20	PPRN:89282871		
J	Zhang, Chenshuang; Zhang, Chaoning; Zhang, Mengchun; Kweon, In So; Kim, Junmo				Kweon, In/C-2023-2011; Chien, Chian-Shiu/AAT-3192-2020; Zhang, Chaoning/ABG-1572-2022						Text-to-image Diffusion Models in Generative AI: A Survey								Arxiv											2	2;2024-11-08;https://www.arxiv.org/abs/2303.07909v3| 1;2023-03-14;https://www.arxiv.org/abs/2303.07909v1	arXiv:2303.07909			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 08 2024	2024	This survey reviews the progress of diffusion models in generating images from text, i.e. text-to-image diffusion models. As a self-contained work, this survey starts with a brief introduction of how diffusion models work for image synthesis, followed by the background for text-conditioned image synthesis. Based on that, we present an organized review of pioneering methods and their improvements on text-to-image generation. We further summarize applications beyond image generation, such as text- guided generation for various modalities like videos, and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.																																	2024-12-18	PPRN:46815147		
J	Shumailov, Ilia; Shumaylov, Zakhar; Zhao, Yiren; Gal, Yarin; Papernot, Nicolas; Anderson, Ross										The Curse of Recursion: Training on Generated Data Makes Models Forget								Arxiv											2	2;2024-04-14;https://www.arxiv.org/abs/2305.17493v3| 1;2023-05-27;https://www.arxiv.org/abs/2305.17493v1	arXiv:2305.17493			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 14 2024	2024	Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model -generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as model collapse1 and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.																																	2024-04-25	PPRN:72752481		
J	Mehrotra, Anay; Zampetakis, Manolis; Kassianik, Paul; Nelson, Blaine; Anderson, Hyrum; Singer, Yaron; Karbasi, Amin										Tree of Attacks: Jailbreaking Black-Box LLMs Automatically								Arxiv											3	3;2024-10-31;https://www.arxiv.org/abs/2312.02119v3| 2;2024-02-21;https://www.arxiv.org/abs/2312.02119v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02119v1	arXiv:2312.02119			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art guardrails, e.g., LlamaGuard.																																	2024-12-06	PPRN:86379382		
J	Bai, Zechen; Wang, Pichao; Xiao, Tianjun; He, Tong; Han, Zongbo; Zhang, Zheng; Shou, Mike Zheng				Xiao, Tianjun/ABF-7623-2020; Shou, Mike Zheng/LXW-9197-2024						Hallucination of Multimodal Large Language Models: A Survey								Arxiv											1	1;2024-04-29;https://www.arxiv.org/abs/2404.18930v1	arXiv:2404.18930			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 29 2024	2024	This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real -world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.																																	2024-05-14	PPRN:88695168		
J	Xu, Jiale; Cheng, Weihao; Gao, Yiming; Wang, Xintao; Gao, Shenghua; Shan, Ying				cheng, weihao/MUO-0523-2025; Gao, Shenghua/JED-9248-2023						InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models								Arxiv											2	2;2024-04-14;https://www.arxiv.org/abs/2404.07191v2| 1;2024-04-10;https://www.arxiv.org/abs/2404.07191v1	arXiv:2404.07191			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 14 2024	2024	We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.																																	2024-04-25	PPRN:88478413		
J	Xu, Xiaohan; Li, Ming; Tao, Chongyang; Shen, Tao; Cheng, Reynold; Li, Jinyang; Xu, Can; Tao, Dacheng; Zhou, Tianyi				Tao, Dacheng/A-5449-2012; Tao, Chongyang/MBG-8179-2025						A Survey on Knowledge Distillation of Large Language Models								Arxiv											1	1;2024-02-20;https://www.arxiv.org/abs/2402.13116v1	arXiv:2402.13116			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 20 2024	2024	This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential for more accessible, efficient, and sustainable AI solutions, fostering a more inclusive and equitable landscape in AI advancements. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.																																	2024-11-09	PPRN:87776626		
J	Darcet, Timothee; Oquab, Maxime; Mairal, Julien; Bojanowski, Piotr				Mairal, Julien/AAL-5611-2021						Vision Transformers Need Registers								Arxiv											2	2;2024-04-12;https://www.arxiv.org/abs/2309.16588v2| 1;2023-09-28;https://www.arxiv.org/abs/2309.16588v1	arXiv:2309.16588			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 12 2024	2024	Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.																																	2024-04-26	PPRN:85322288		
J	Ma, Jun; He, Yuting; Li, Feifei; Han, Lin; You, Chenyu; Wang, Bo				Ma, Jun/LHA-0128-2024; Wang, Bo/HDO-6738-2022; He, Yuting/U-1207-2018; Li, Feifei/C-3476-2017; You, Chenyu/AFK-5803-2022						Segment Anything in Medical Images								Arxiv											2	2;2024-04-01;https://www.arxiv.org/abs/2304.12306v3| 1;2023-04-24;https://www.arxiv.org/abs/2304.12306v1	arXiv:2304.12306			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 01 2024	2024	Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans.																																	2024-04-17	PPRN:65250350		
J	Zhao, Penghao; Zhang, Hailin; Yu, Qinhan; Wang, Zhengren; Geng, Yunteng; Fu, Fangcheng; Yang, Ling; Zhang, Wentao; Jiang, Jie; Cui, Bin				Wang, ZhiWei/AAE-4028-2019; zhang, hailin/HMP-1435-2023; Fu, Fangcheng/HPG-7520-2023						Retrieval-Augmented Generation for AI-Generated Content: A Survey								Arxiv											6	6;2024-06-21;https://www.arxiv.org/abs/2402.19473v6| 5;2024-05-31;https://www.arxiv.org/abs/2402.19473v5| 4;2024-05-02;https://www.arxiv.org/abs/2402.19473v4| 3;2024-04-14;https://www.arxiv.org/abs/2402.19473v3| 2;2024-03-27;https://www.arxiv.org/abs/2402.19473v2| 1;2024-02-29;https://www.arxiv.org/abs/2402.19473v1	arXiv:2402.19473			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 21 2024	2024	Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research.																																	2024-07-11	PPRN:87985783		
J	Huang, Xu; Liu, Weiwen; Chen, Xiaolong; Wang, Xingmei; Wang, Hao; Lian, Defu; Wang, Yasheng; Tang, Ruiming; Chen, Enhong				Liu, Weiwen/LMQ-1488-2024; Lian, Defu/AFN-4573-2022						Understanding the planning of LLM agents: A survey								Arxiv											2	2;2024-02-05;https://www.arxiv.org/abs/2402.02716v1| 1;2024-02-05;https://www.arxiv.org/abs/2402.02716v1	arXiv:2402.02716			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 05 2024	2024	As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.																																	2024-05-25	PPRN:87521778		
J	Ahmadian, Arash; Cremer, Chris; Galle, Matthias; Fadaee, Marzieh; Kreutzer, Julia; Pietquin, Olivier; Ustun, Ahmet; Hooker, Sara				Pietquin, Olivier/L-3108-2015						Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs								Arxiv											2	2;2024-02-26;https://www.arxiv.org/abs/2402.14740v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.14740v1	arXiv:2402.14740			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the formulation of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed "RL-free" methods such as DPO and RAFT. Our work suggests that careful adaptation to LLMs alignment characteristics enables benefiting from online RL optimization at low cost.																																	2024-03-24	PPRN:87799145		
J	Li, Haonan; Zhang, Yixuan; Koto, Fajri; Yang, Yifei; Zhao, Hai; Gong, Yeyun; Duan, Nan; Baldwin, Timothy				YANG, Yifei/HKN-6975-2023; Li, Haonan/IQV-4567-2023; Duan, Nan/AAR-2231-2020; Koto, Fajri/ISS-6497-2023						CMMLU: Measuring massive multitask language understanding in Chinese								Arxiv											2	2;2024-01-17;https://www.arxiv.org/abs/2306.09212v2| 1;2023-06-15;https://www.arxiv.org/abs/2306.09212v1	arXiv:2306.09212			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 17 2024	2024	As the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming simultaneously more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of 60% even, which is the pass mark for Chinese exams. This highlights that there is significant room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models’ performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models in the Chinese context. 1																																	2024-05-25	PPRN:73362275		
J	Ansari, Abdul Fatir; Stella, Lorenzo; Turkmen, Caner; Zhang, Xiyuan; Mercado, Pedro; Shen, Huibin; Shchur, Oleksandr; Rangapuram, Syama Sundar; Arango, Sebastian Pineda; Kapoor, Shubham; Zschiegner, Jasper; Maddix, Danielle C.; Wang, Hao; Mahoney, Michael W.; Torkkola, Kari; Wilson, Andrew Gordon; Bohlke-Schneider, Michael; Wang, Yuyang				Pineda Arango, Sebastian/KIB-2459-2024						Chronos: Learning the Language of Time Series								Arxiv											3	3;2024-11-04;https://www.arxiv.org/abs/2403.07815v3| 2;2024-05-02;https://www.arxiv.org/abs/2403.07815v2| 1;2024-03-12;https://www.arxiv.org/abs/2403.07815v1	arXiv:2403.07815			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	We introduce CHRONOS, a simple yet effective framework for pretrained probabilistic time series models. CHRONOS tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained CHRONOS models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that CHRONOS models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that CHRONOS models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.																																	2024-12-09	PPRN:88120003		
J	Polyak, Adam; Zohar, Amit; Brown, Andrew; Tjandra, Andros; Sinha, Animesh; Lee, Ann; Vyas, Apoorv; Shi, Bowen; Ma, Chih-Yao; Chuang, Ching-Yao; Yan, David; Choudhary, Dhruv; Wang, Dingkang; Sethi, Geet; Pang, Guan; Ma, Haoyu; Misra, Ishan; Hou, Ji; Wang, Jialiang; Jagadeesh, Kiran; Li, Kunpeng; Zhang, Luxin; Singh, Mannat; Williamson, Mary; Le, Matt; Yu, Matthew; Singh, Mitesh Kumar; Zhang, Peizhao; Vajda, Peter; Duval, Quentin; Girdhar, Rohit; Sumbaly, Roshan; Rambhatla, Sai Saketh; Tsai, Sam; Azadi, Samaneh; Datta, Samyak; Chen, Sanyuan; Bell, Sean; Ramaswamy, Sharadh; Sheynin, Shelly; Bhattacharya, Siddharth; Motwani, Simran; Xu, Tao; Li, Tianhe; Hou, Tingbo; Hsu, Wei-Ning; Yin, Xi; Dai, Xiaoliang; Taigman, Yaniv; Luo, Yaqiao; Liu, Yen-Cheng; Wu, Yi-Chiao; Zhao, Yue; Kirstain, Yuval; He, Zecheng; He, Zijian; Pumarola, Albert; Thabet, Ali; Sanakoyeu, Artsiom; Mallya, Arun; Guo, Baishan; Araya, Boris; Kerr, Breena; Wood, Carleigh; Liu, Ce; Peng, Cen; Vengertsev, Dimitry; Schonfeld, Edgar; Blanchard, Elliot; Juefei-Xu, Felix; Nord, Fraylie; Liang, Jeff; Hoffman, John; Kohler, Jonas; Fire, Kaolin; Sivakumar, Karthik; Chen, Lawrence; Yu, Licheng; Gao, Luya; Georgopoulos, Markos; Moritz, Rashel; Sampson, Sara K.; Li, Shikai; Parmeggiani, Simone; Fine, Steve; Fowler, Tara; Petrovic, Vladan; Du, Yuming				Yin, Xi/GQO-8468-2022; Tjandra, Andros/AAH-6013-2019; Datta, Sanjib/AHC-5269-2022; Hoffman, John/D-8012-2014; Lee, Ann/AEI-4838-2022; Wang, Dingkang/AAO-4274-2020; Hou, Tingbo/H-6978-2012; li, shikai/B-9317-2009; Li, TIANHE/MVU-2493-2025; He, Zijian/MXK-7882-2025; Singh, Mitesh/OFO-1010-2025; Ma, Haoyu/HGA-9740-2022; Zhang, Luxin/JRY-5718-2023; Li, Kunpeng/HTR-2821-2023; wang, jialiang/LZG-6158-2025; Wu, Yi-Chiao/AAS-7506-2020; Chen, Sanyuan/GLR-3754-2022						Movie Gen: A Cast of Media Foundation Models								Arxiv											1	1;2024-10-17;https://www.arxiv.org/abs/2410.13720v1	arXiv:2410.13720			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 17 2024	2024	We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.																																	2025-01-24	PPRN:115586141		
J	Wang, Ao; Chen, Hui; Liu, Lihao; Chen, Kai; Lin, Zijia; Han, Jungong; Ding, Guiguang				Ding, Guiguang/KIL-3528-2024; Chen, Hui/AAU-9595-2021						YOLOv10: Real-Time End-to-End Object Detection								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14458v1	arXiv:2405.14458			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8× faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8× smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\% less latency and 25% fewer parameters for the same performance.																																	2024-06-05	PPRN:88989911		
J	Kaiser, Lukasz; Babaeizadeh, Mohammad; Milos, Piotr; Osinski, Blazej; Campbell, Roy H; Czechowski, Konrad; Erhan, Dumitru; Finn, Chelsea; Kozakowski, Piotr; Levine, Sergey; Mohiuddin, Afroz; Sepassi, Ryan; Tucker, George; Michalewski, Henryk				Campbell, Roy/O-1141-2019; Michalewski, Henryk/L-4287-2018; Czechowski, Konrad/AAK-9656-2020; Milos, Piotr/AAQ-3048-2020; Erhan, Dumitru/MTE-2999-2025						Model-Based Reinforcement Learning for Atari								Arxiv											2	2;2024-04-03;https://www.arxiv.org/abs/1903.00374v5| 1;2020-02-19;https://www.arxiv.org/abs/1903.00374v4	arXiv:1903.00374			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction - substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.																																	2024-04-18	PPRN:13071802		
J	Zheng, Qinkai; Xia, Xiao; Zou, Xu; Dong, Yuxiao; Wang, Shan; Xue, Yufei; Wang, Zihan; Shen, Lei; Wang, Andi; Li, Yang; Su, Teng; Yang, Zhilin; Tang, Jie				Zou, Xu/JQW-6611-2023; tang, jie/KIE-8633-2024; Zheng, Qinkai/AAI-9267-2021						CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X								Arxiv											2	2;2024-07-10;https://www.arxiv.org/abs/2303.17568v2| 1;2023-03-30;https://www.arxiv.org/abs/2303.17568v1	arXiv:2303.17568			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 10 2024	2024	Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users.																																	2024-07-21	PPRN:52094453		
J	Beyer, Lucas; Steiner, Andreas; Pinto, Andre Susano; Kolesnikov, Alexander; Wang, Xiao; Salz, Daniel; Neumann, Maxim; Alabdulmohsin, Ibrahim; Tschannen, Michael; Bugliarello, Emanuele; Unterthiner, Thomas; Keysers, Daniel; Koppula, Skanda; Liu, Fangyu; Grycner, Adam; Gritsenko, Alexey; Houlsby, Neil; Kumar, Manoj; Rong, Keran; Eisenschlos, Julian; Kabra, Rishabh; Bauer, Matthias; Bosnjak, Matko; Chen, Xi; Minderer, Matthias; Voigtlaender, Paul; Bica, Ioana; Balazevic, Ivana; Puigcerver, Joan; Papalampidi, Pinelopi; Henaff, Olivier; Xiong, Xi; Soricut, Radu; Harmsen, Jeremiah; Zhai, Xiaohua				Liu, Fangyu/AHA-5291-2022; Unterthiner, Thomas/K-7231-2018; Puigcerver, Joan/L-9908-2014; Zhai, Xiaohua/AAQ-1391-2020						PaliGemma: A versatile 3B VLM for transfer								Arxiv											2	2;2024-10-10;https://www.arxiv.org/abs/2407.07726v2| 1;2024-07-10;https://www.arxiv.org/abs/2407.07726v1	arXiv:2407.07726			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 10 2024	2024	PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.																																	2024-11-01	PPRN:90760160		
J	Freed, Daniel S.; Moore, Gregory W.; Teleman, Constantin										Topological symmetry in quantum field theory								Arxiv											2	2;2024-07-31;https://www.arxiv.org/abs/2209.07471v4| 1;2022-09-15;https://www.arxiv.org/abs/2209.07471v2	arXiv:2209.07471			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 31 2024	2024	We introduce a framework for internal topological symmetries in quantum field theory, including "noninvertible symmetries" and "categorical symmetries". This leads to a calculus of topological defects which takes full advantage of well-developed theorems and techniques in topological field theory. Our discussion focuses on finite symmetries, and we give indications for a generalization to other symmetries. We treat quotients and quotient defects (often called "gauging" and "condensation defects"), finite electromagnetic duality, and duality defects, among other topics. We include an appendix on finite homotopy theories, which are often used to encode finite symmetries and for which computations can be carried out using methods of algebraic topology. Throughout we emphasize exposition and examples over a detailed technical treatment.																																	2024-08-09	PPRN:23937726		
J	Li, Xuan; Zhou, Zhanke; Zhu, Jianing; Yao, Jiangchao; Liu, Tongliang; Han, Bo				Liu, Tongliang/AAA-1506-2021; Zhu, Jianing/JGM-5607-2023; Yao, Jiangchao/JOZ-1621-2023; Zhuang, Peiyu/HII-5769-2022						DeepInception: Hypnotize Large Language Model to Be Jailbreaker								Arxiv											5	5;2024-11-28;https://www.arxiv.org/abs/2311.03191v5| 4;2024-05-23;https://www.arxiv.org/abs/2311.03191v4| 3;2024-02-06;https://www.arxiv.org/abs/2311.03191v3| 2;2023-12-05;https://www.arxiv.org/abs/2311.03191v2| 1;2023-11-06;https://www.arxiv.org/abs/2311.03191v1	arXiv:2311.03191			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 28 2024	2024	Large language models (LLMs) have succeeded significantly in various applications but remain susceptible to adversarial jailbreaks that void their safety guardrails. Previous attempts to exploit these vulnerabilities often rely on high-cost computational extrapolations, which may not be practical or efficient. In this paper, inspired by the authority influence demonstrated in the Milgram experiment, we present a lightweight method to take advantage of the LLMs’ personification capabilities to construct a virtual, nested scene, allowing it to realize an adaptive way to escape the usage control in a normal scenario. Empirically, the contents induced by our approach can achieve leading harmfulness rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs, e.g., Llama-2, Llama-3, GPT-3.5, GPT-4, and GPT-4o. [GRAPHICS]																																	2025-01-10	PPRN:86063359		
J	Liu, Yuliang; Li, Zhang; Huang, Mingxin; Yang, Biao; Yu, Wenwen; Li, Chunyuan; Yin, Xucheng; Liu, Cheng-lin; Jin, Lianwen; Bai, Xiang				Jin, Lianwen/AAJ-6536-2020; li, chunyuan/IQW-1618-2023; Liu, Cheng-Lin/JCO-6642-2023; Yu, Wenwen/LCE-3187-2024						OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models								Arxiv											4	4;2024-08-26;https://www.arxiv.org/abs/2305.07895v7| 3;2024-08-14;https://www.arxiv.org/abs/2305.07895v6| 2;2024-01-17;https://www.arxiv.org/abs/2305.07895v5| 1;2023-05-13;https://www.arxiv.org/abs/2305.07895v1	arXiv:2305.07895			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 26 2024	2024	Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. However, their effectiveness in text-related visual tasks remains relatively unexplored. In this paper, we conducted a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). To facilitate the assessment of Optical Character Recognition (OCR) capabilities in Large Multimodal Models, we propose OCRBench, a comprehensive evaluation benchmark. OCRBench contains 29 datasets, making it the most comprehensive OCR evaluation benchmark available. Furthermore, our study reveals both the strengths and weaknesses of these models, particularly in handling multilingual text, handwritten text, non-semantic text, and mathematical expression recognition. Most importantly, the baseline results presented in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. 																																	2025-01-23	PPRN:69649778		
J	Sclar, Melanie; Choi, Yejin; Tsvetkov, Yulia; Suhr, Alane				Sclar, Melanie/LXU-4864-2024						Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: <italic>How I learned to start worrying about prompt formatting</italic>								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2310.11324v2| 1;2023-10-17;https://www.arxiv.org/abs/2310.11324v1	arXiv:2310.11324			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.																																	2024-11-03	PPRN:85682838		
J	Kondratyuk, Dan; Yu, Lijun; Gu, Xiuye; Lezama, José; Huang, Jonathan; Schindler, Grant; Hornung, Rachel; Birodkar, Vighnesh; Yan, Jimmy; Chiu, Ming-Chang; Somandepalli, Krishna; Akbari, Hassan; Alon, Yair; Cheng, Yong; Dillon, Josh; Gupta, Agrim; Hahn, Meera; Hauth, Anja; Hendon, David; Martinez, Alonso; Minnen, David; Sirotenko, Mikhail; Sohn, Kihyuk; Yang, Xuan; Adam, Hartwig; Yang, Ming-Hsuan; Essa, Irfan; Wang, Huisheng; Ross, David A.; Seybold, Bryan; Jiang, Lu				Somandepalli, Krishna/AAB-6789-2020; Yu, Lijun/AAJ-6691-2020; Yang, Ming-Hsuan/T-9533-2019						VideoPoet: A Large Language Model for Zero-Shot Video Generation								Arxiv											3	3;2024-03-14;https://www.arxiv.org/abs/2312.14125v2| 2;2023-12-21;https://www.arxiv.org/abs/2312.14125v1| 1;2024-03-01;	arXiv:2312.14125			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 01 2024	2024	We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/																																	2025-11-07	PPRN:86786930		
J	Bar-Tal, Omer; Chefer, Hila; Tov, Omer; Herrmann, Charles; Paiss, Roni; Zada, Shiran; Ephrat, Ariel; Hur, Junhwa; Liu, Guanghui; Raj, Amit; Li, Yuanzhen; Rubinstein, Michael; Michaeli, Tomer; Wang, Oliver; Sun, Deqing; Dekel, Tali; Mosseri, Inbar				Liu, Guanghui/HPC-0957-2023; Sun, Deqing/KLD-7402-2024						Lumiere: A Space-Time Diffusion Model for Video Generation								Arxiv											2	2;2024-02-05;https://www.arxiv.org/abs/2401.12945v2| 1;2024-01-23;https://www.arxiv.org/abs/2401.12945v1	arXiv:2401.12945			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 05 2024	2024	We introduce Lumiere - a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion - a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution - an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.																																	2024-05-25	PPRN:87299793		
J	Laurencon, Hugo; Tronchon, Leo; Cord, Matthieu; Sanh, Victor										What matters when building vision-language models?								Arxiv											1	1;2024-05-03;https://www.arxiv.org/abs/2405.02246v1	arXiv:2405.02246			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 03 2024	2024	The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.																																	2024-07-04	PPRN:88761548		
J	Ruan, Jiacheng; Li, Jincheng; Xiang, Suncheng				Ruan, Jiacheng/KAM-3815-2024						VM-UNet: Vision Mamba UNet for Medical Image Segmentation								Arxiv											2	2;2024-11-08;https://www.arxiv.org/abs/2402.02491v2| 1;2024-02-04;https://www.arxiv.org/abs/2402.02491v1	arXiv:2402.02491			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 08 2024	2024	the realm of medical image segmentation, both CNN-based and Transformer-based models have been extensively explored. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, leveraging state space models, we propose a Ushape architecture model for medical image segmentation, named Vision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block is introduced as the foundation block to capture extensive contextual information, and an asymmetrical encoder-decoder structure is constructed with fewer convolution layers to save calculation cost. We conduct comprehensive experiments on the ISIC17, ISIC18, and Synapse datasets, and the results indicate that VM-UNet performs competitively in medical image segmentation tasks. To our best knowledge, this is the first medical image segmentation model constructed based on the pure SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based segmentation systems.																																	2024-12-18	PPRN:87521780		
J	Zhang, Peiyuan; Zhang, Kaichen; Li, Bo; Zeng, Guangtao; Yang, Jingkang; Zhang, Yuanhan; Wang, Ziyue; Tan, Haoran; Li, Chunyuan; Liu, Ziwei				zeng, guangtao/OPF-2200-2025; Liu, Ziwei/AAG-6939-2021; wang, ziyue/IQT-0730-2023; Yang, Jingkang/HJZ-3689-2023; Zhang, Peiyuan/OOK-8916-2025						Long Context Transfer from Language to Vision								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2406.16852v2| 1;2024-06-24;https://www.arxiv.org/abs/2406.16852v1	arXiv:2406.16852			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Video sequences offer valuable temporal information, but existing large multimodal models (LMMs) fall short in understanding extremely long videos. Many works address this by reducing the number of visual tokens using visual resamplers. Alternatively, in this paper, we approach this problem from the perspective of the language model. By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training. We call this phenomenon long context transfer and carefully ablate its properties. To effectively measure LMMs' ability to generalize to long contexts in the vision modality, we develop V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model's NIAH test. Our proposed Long Video Assistant (LongVA) can process 2000 frames or over 200K visual tokens without additional complexities. With its extended context length, LongVA achieves state-of-the-art performance on Video-MME among 7B-scale models by densely sampling more input frames. 																																	2024-07-18	PPRN:89414675		
J	Li, Tianle; Chiang, Wei-Lin; Frick, Evan; Dunlap, Lisa; Wu, Tianhao; Zhu, Banghua; Gonzalez, Joseph E.; Stoica, Ion				Wu, Tianhao/AAX-8318-2021						From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.11939v1	arXiv:2406.11939			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	The rapid evolution of language models has necessitated the development of more challenging benchmarks. Current static benchmarks often struggle to consistently distinguish between the capabilities of different models and fail to align with real-world user preferences. On the other hand, live crowd-sourced platforms like the Chatbot Arena collect a wide range of natural prompts and user feedback. However, these prompts vary in sophistication and the feedback cannot be applied offline to new models. In order to ensure that benchmarks keep up with the pace of LLM development, we address how one can evaluate benchmarks on their ability to confidently separate models and their alignment with human preference. Under these principles, we developed BenchBuilder, a living benchmark that filters high-quality prompts from live data sources to enable offline evaluation on fresh, challenging prompts. BenchBuilder identifies seven indicators of a high-quality prompt, such as the requirement for domain knowledge, and utilizes an LLM annotator to select a high-quality subset of prompts from various topic clusters. The LLM evaluation process employs an LLM judge to ensure a fully automated, high-quality, and constantly updating benchmark. We apply BenchBuilder on prompts from the Chatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from a wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence intervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with human preference rankings, all at a cost of only $25 and without human labelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides a valuable tool for developers, enabling them to extract high-quality benchmarks from extensive data with minimal effort.																																	2025-08-07	PPRN:123164756		
J	Tian, Keyu; Jiang, Yi; Yuan, Zehuan; Peng, Bingyue; Wang, Liwei										Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2404.02905v2| 1;2024-04-03;https://www.arxiv.org/abs/2404.02905v1	arXiv:2404.02905			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jun 10 2024	2024	We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.																																	2024-07-04	PPRN:88389594		
J	Zheng, Boyuan; Gou, Boyu; Kil, Jihyung; Sun, Huan; Su, Yu										GPT-4V(ision) is a Generalist Web Agent, if Grounded								Arxiv											2	2;2024-03-12;https://www.arxiv.org/abs/2401.01614v2| 1;2024-01-03;https://www.arxiv.org/abs/2401.01614v1	arXiv:2401.01614			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 12 2024	2024	The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents -it can successfully complete 51.1 of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out to be not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML structure and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement.																																	2024-04-08	PPRN:86944189		
J	Kirchenbauer, John; Geiping, Jonas; Wen, Yuxin; Katz, Jonathan; Miers, Ian; Goldstein, Tom				Wen, Yuxin/AAA-4882-2019						A Watermark for Large Language Models								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2301.10226v4| 1;2023-01-24;https://www.arxiv.org/abs/2301.10226v2	arXiv:2301.10226			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient opensource algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of “green” tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an informationtheoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi -billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.																																	2024-05-19	PPRN:36025558		
J	Tang, Jiaxiang; Chen, Zhaoxi; Chen, Xiaokang; Wang, Tengfei; Zeng, Gang; Liu, Ziwei				Liu, Ziwei/AAG-6939-2021						LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation								Arxiv											1	1;2024-02-07;https://www.arxiv.org/abs/2402.05054v1	arXiv:2402.05054			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 07 2024	2024	3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.																																	2024-05-25	PPRN:87553756		
J	Arditi, Andy; Obeso, Oscar; Syed, Aaquib; Paleka, Daniel; Panickssery, Nina; Gurnee, Wes; Nanda, Neel										Refusal in Language Models Is Mediated by a Single Direction								Arxiv											3	3;2024-10-30;https://www.arxiv.org/abs/2406.11717v3| 2;2024-07-15;https://www.arxiv.org/abs/2406.11717v2| 1;2024-06-17;https://www.arxiv.org/abs/2406.11717v1	arXiv:2406.11717			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 30 2024	2024	Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.1																																	2024-12-06	PPRN:89344108		
J	Ge, Tao; Chan, Xin; Wang, Xiaoyang; Yu, Dian; Mi, Haitao; Yu, Dong				Wang, Xinyi/AAE-4342-2019						Scaling Synthetic Data Creation with 1,000,000,000 Personas								Arxiv											2	2;2024-09-24;https://www.arxiv.org/abs/2406.20094v2| 1;2024-06-28;https://www.arxiv.org/abs/2406.20094v1	arXiv:2406.20094			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Sep 24 2024	2024	We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.																																	2024-10-08	PPRN:90630169		
J	Li, Tianle; Zhang, Ge; Do, Quy Duc; Yue, Xiang; Chen, Wenhu				Zhang, Ge/N-4150-2013						Long-context LLMs Struggle with Long In-context Learning								Arxiv											3	3;2024-06-12;https://www.arxiv.org/abs/2404.02060v3| 2;2024-04-04;https://www.arxiv.org/abs/2404.02060v2| 1;2024-04-02;https://www.arxiv.org/abs/2404.02060v1	arXiv:2404.02060			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 12 2024	2024	Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs. [GRAPHICS]																																	2024-07-02	PPRN:88378303		
J	Chen, Lichang; Li, Shiyang; Yan, Jun; Wang, Hai; Gunaratna, Kalpa; Yadav, Vikas; Tang, Zheng; Srinivasan, Vijay; Zhou, Tianyi; Huang, Heng; Jin, Hongxia				YADAV, VIKAS/AGZ-4594-2022						AlpaGasus: Training A Better Alpaca with Fewer Data								Arxiv											5	5;2024-02-13;https://www.arxiv.org/abs/2307.08701v5| 4;2023-11-04;https://www.arxiv.org/abs/2307.08701v4| 3;2023-10-26;https://www.arxiv.org/abs/2307.08701v3| 2;2023-09-30;https://www.arxiv.org/abs/2307.08701v2| 1;2023-07-17;https://www.arxiv.org/abs/2307.08701v1	arXiv:2307.08701			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 13 2024	2024	Large language models (LLMs) strengthen instruction -following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., ALPACA’s 52k data) surprisingly contain many lowquality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low -quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce ALPAGASUS, which is finetuned on only 9k high -quality data filtered from the 52k ALPACA data. ALPAGASUS significantly outperforms the original ALPACA as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches > 90% performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for ALPACA) to 14 minutes 1. Moreover, the experiments prove the efficacy of our method across diverse datasets, base models, and LLM filters. Overall, ALPAGASUS demonstrates a novel data -centric IFT paradigm that can be generally applied to instruction -tuning data, leading to faster training and better instruction -following models.																																	2024-05-25	PPRN:73946311		
J	Zhuo, Terry Yue; Vu, Chien; Chim, Jenny; Hu, Han; Yu, Wenhao; Widyasari, Ratnadira; Yusuf, Imam Nur Bani; Zhan, Haolan; He, Junda; Paul, Indraneil; Brunner, Simon; Gong, Chen; Hoang, Thong; Zebaze, Armel; Hong, Xiaoheng; Li, Wen-Ding; Kaddour, Jean; Xu, Ming; Zhang, Zhihan; Yadav, Prateek; Jain, Naman; Gu, Alex; Cheng, Zhoujun; Liu, Jiawei; Liu, Qian; Wang, Zijian; Hui, Binyuan; Muennighoff, Niklas; Lo, David; Fried, Daniel; Du, Xiaoning; de Vries, Harm; Von Werra, Leandro				Liu, Jiawei/JVZ-3421-2024; Hu, Han/KMA-4343-2024; Hoang, Thong/AFQ-0983-2022; Bin/B-6414-2019; Zhan, Haolan/KEH-3324-2024; Yadav, Prateek/KFA-8307-2024; Du, Xiaoning/AAE-5334-2019						BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions								Arxiv											2	2;2024-06-26;https://www.arxiv.org/abs/2406.15877v2| 1;2024-06-22;https://www.arxiv.org/abs/2406.15877v1	arXiv:2406.15877			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 26 2024	2024	Automated software engineering has been greatly empowered by the recent advances in Large Language Models (LLMs) for programming. While current benchmarks have shown that LLMs can perform various software engineering tasks like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks. Solving challenging and practical programming tasks requires the capability of utilizing diverse function calls as tools to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding complex instructions. . Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical programming tasks, we introduce BigCodeBench, , a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained programming tasks. To evaluate LLMs rigorously, each programming task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural -language -oriented variant of BigCodeBench, , BigCodeBench-Instruct, - Instruct , that automatically transforms the original docstrings into short instructions only with essential information. Our extensive evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%. . The results underscore the need for further advancements in this area.																																	2024-10-03	PPRN:89417040		
J	Wu, Zhiyu; Chen, Xiaokang; Pan, Zizheng; Liu, Xingchao; Liu, Wen; Dai, Damai; Gao, Huazuo; Ma, Yiyang; Wu, Chengyue; Wang, Bingxuan; Xie, Zhenda; Wu, Yu; Hu, Kai; Wang, Jiawei; Sun, Yaofeng; Li, Yukun; Piao, Yishi; Guan, Kang; Liu, Aixin; Xie, Xin; You, Yuxiang; Dong, Kai; Yu, Xingkai; Zhang, Haowei; Zhao, Liang; Wang, Yisong; Ruan, Chong				Zhao, Liang/GRO-5936-2022; Dai, Damai/KEJ-3256-2024; Xie, Zhenda/O-1198-2013; Yu, Xingkai/AAO-5118-2020						DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding								Arxiv											1	1;2024-12-13;https://www.arxiv.org/abs/2412.10302v1	arXiv:2412.10302			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 13 2024	2024	We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we incorporate a dynamic tiling vision encoding strategy designed for processing high-resolution images with different aspect ratios. For the language component, we leverage DeepSeekMoE models with the Multi-head Latent Attention mechanism, which compresses Key-Value cache into latent vectors, to enable efficient inference and high throughput. Trained on an improved vision-language dataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art performance with similar or fewer activated parameters compared to existing open-source dense and MoE-based models.  [Graphics]																																	2025-01-22	PPRN:119937082		
J	Gallegos, Isabel O.; Rossi, Ryan A.; Barrow, Joe; Tanjim, Md Mehrab; Kim, Sungchul; Dernoncourt, Franck; Yu, Tong; Zhang, Ruiyi; Ahmed, Nesreen K.				Zhang, Ruiyi/JNR-1096-2023; Rossi, Ryan/C-7974-2013						Bias and Fairness in Large Language Models: A Survey								Arxiv											3	3;2024-07-12;https://www.arxiv.org/abs/2309.00770v3| 2;2024-03-12;https://www.arxiv.org/abs/2309.00770v2| 1;2023-09-02;https://www.arxiv.org/abs/2309.00770v1	arXiv:2309.00770			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.																																	2024-07-23	PPRN:84764551		
J	Zheng, Lianmin; Yin, Liangsheng; Xie, Zhiqiang; Sun, Chuyue; Huang, Jeff; Yu, Cody Hao; Cao, Shiyi; Kozyrakis, Christos; Stoica, Ion; Gonzalez, Joseph E.; Barrett, Clark; Sheng, Ying										SGLang: Efficient Execution of Structured Language Model Programs								Arxiv											3	3;2024-06-06;https://www.arxiv.org/abs/2312.07104v2| 2;2023-12-12;https://www.arxiv.org/abs/2312.07104v1| 1;2023-12-12;https://www.arxiv.org/abs/2312.07104v1	arXiv:2312.07104			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. 																																	2024-11-09	PPRN:86556648		
J	Tang, Changli; Yu, Wenyi; Sun, Guangzhi; Chen, Xianzhao; Tan, Tian; Li, Wei; Lu, Lu; Ma, Zejun; Zhang, Chao										SALMONN: Towards Generic Hearing Abilities for Large Language Models								Arxiv											2	2;2024-04-08;https://www.arxiv.org/abs/2310.13289v2| 1;2023-10-20;https://www.arxiv.org/abs/2310.13289v1	arXiv:2310.13289			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 08 2024	2024	Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. 																																	2024-04-21	PPRN:85742395		
J	Hong, Wenyi; Wang, Weihan; Lv, Qingsong; Xu, Jiazheng; Yu, Wenmeng; Ji, Junhui; Wang, Yan; Wang, Zihan; Zhang, Yuxuan; Li, Juanzi; Xu, Bin; Dong, Yuxiao; Ding, Ming; Tang, Jie										CogAgent: A Visual Language Model for GUI Agents								Arxiv											1	1;2024-12-27;https://www.arxiv.org/abs/2312.08914v3	arXiv:2312.08914			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 27 2024	2024	People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. 																																	2025-02-15	PPRN:86591433		
J	Villalobos, Pablo; Ho, Anson; Sevilla, Jaime; Besiroglu, Tamay; Heim, Lennart; Hobbhahn, Marius										Will we run out of data? Limits of LLM scaling based on human-generated data								Arxiv											1	1;2024-06-04;https://www.arxiv.org/abs/2211.04325v2	arXiv:2211.04325			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 04 2024	2024	We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress.																																	2024-06-22	PPRN:89259105		
J	Dubois, Yann; Li, Xuechen; Taori, Rohan; Gulrajani, Ishaan; Zhang, Tianyi; Ba, Jimmy; Guestrin, Carlos; Liang, Percy; Hashimoto, Tatsunori B.				Zhang, Tianyi/KTI-8843-2024						AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback								Arxiv											3	3;2024-01-08;https://www.arxiv.org/abs/2305.14387v4| 2;2023-12-06;https://www.arxiv.org/abs/2305.14387v3| 1;2023-05-22;https://www.arxiv.org/abs/2305.14387v1	arXiv:2305.14387			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 08 2024	2024	Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their strong instruction-following abilities. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, DPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.																																	2024-05-25	PPRN:72718687		
J	Du, Zhihao; Chen, Qian; Zhang, Shiliang; Hu, Kai; Lu, Heng; Yang, Yexin; Hu, Hangrui; Zheng, Siqi; Gu, Yue; Ma, Ziyang; Gao, Zhifu; Yan, Zhijie				Zhang, ShiLiang/AAA-4638-2020						CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens								Arxiv											1	1;2024-07-09;https://www.arxiv.org/abs/2407.05407v2	arXiv:2407.05407			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 09 2024	2024	Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a Co dec-based sy nthesizer for Voice generation, CosyVoice1, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.																																	2024-12-16	PPRN:119068575		
J	Dong, Xiaoyi; Zhang, Pan; Zang, Yuhang; Cao, Yuhang; Wang, Bin; Ouyang, Linke; Wei, Xilin; Zhang, Songyang; Duan, Haodong; Cao, Maosong; Zhang, Wenwei; Li, Yining; Yan, Hang; Gao, Yang; Zhang, Xinyue; Li, Wei; Li, Jingwen; Chen, Kai; He, Conghui; Zhang, Xingcheng; Qiao, Yu; Lin, Dahua; Wang, Jiaqi				He, Conghui/AAZ-3323-2021; WANG, JIAQI/KBB-8837-2024; Zang, Yuhang/AES-3018-2022; Duan, Haodong/ITV-1505-2023; Zhang, Xingcheng/AAC-6392-2019; Zhang, Wenwei/HKO-4277-2023; Qiao, Yu/ABD-5787-2021; Zhang, Songyang/GPX-5621-2022; Dong, Xiaoyi/AAC-8666-2019; Lin, Dahua/W-6576-2019						InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model								Arxiv											1	1;2024-01-29;https://www.arxiv.org/abs/2401.16420v1	arXiv:2401.16420			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension. This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent. Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments. This highlights its remarkable proficiency in the realm of multimodal understanding. The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.																																	2024-02-15	PPRN:87389379		
J	Kosinski, Michal				Kosinski, Michal/AAH-7375-2019						Evaluating Large Language Models in Theory of Mind Tasks								Arxiv											5	5;2024-11-04;https://www.arxiv.org/abs/2302.02083v7| 4;2024-02-17;https://www.arxiv.org/abs/2302.02083v6| 3;2023-11-11;https://www.arxiv.org/abs/2302.02083v5| 2;2023-08-29;https://www.arxiv.org/abs/2302.02083v4| 1;2023-02-10;https://www.arxiv.org/abs/2302.02083v2	arXiv:2302.02083			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 04 2024	2024	Eleven large language models (LLMs) were assessed using 40 bespoke false- belief tasks, considered a gold standard in testing theory of mind (ToM) in humans. Each task included a false- belief scenario, three closely matched true- belief control scenarios, and the reversed versions of all four. An LLM had to solve all eight scenarios to solve a single task. Older models solved no tasks; Generative Pre- trained Transformer (GPT)- 3- davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks; ChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of 6- y- old children observed in past studies. We explore the potential interpretation of these results, including the intriguing possibility that ToM-like ability, previously considered unique to humans, may have emerged as an unintended by- product of LLMs’ improving language skills. Regardless of how we interpret these outcomes, they signify the advent of more powerful and socially skilled AI—with profound positive and negative implications.																																	2024-12-10	PPRN:37589718		
J	Sumers, Theodore R.; Yao, Shunyu; Narasimhan, Karthik; Griffiths, Thomas L.										Cognitive Architectures for Language Agents								Arxiv											3	3;2024-03-15;https://www.arxiv.org/abs/2309.02427v3| 2;2023-09-27;https://www.arxiv.org/abs/2309.02427v2| 1;2023-09-05;https://www.arxiv.org/abs/2309.02427v1	arXiv:2309.02427			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 15 2024	2024	Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.																																	2024-04-11	PPRN:84730224		
J	Hao, Shibo; Sukhbaatar, Sainbayar; Su, Dijia; Li, Xian; Hu, Zhiting; Weston, Jason; Tian, Yuandong										Training Large Language Models to Reason in a Continuous Latent Space								Arxiv											1	1;2024-12-11;https://www.arxiv.org/abs/2412.06769v2	arXiv:2412.06769			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 11 2024	2024	Large language models (LLMs) are restricted to reason in the "language space", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed "continuous thought"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.																																	2025-01-19	PPRN:119845006		
J	Durmus, Esin; Nguyen, Karina; Liao, Thomas I.; Schiefer, Nicholas; Askell, Amanda; Bakhtin, Anton; Chen, Carol; Hatfield-Dodds, Zac; Hernandez, Danny; Joseph, Nicholas; Lovitt, Liane; McCandlish, Sam; Sikder, Orowa; Tamkin, Alex; Thamkul, Janel; Kaplan, Jared; Clark, Jack; Ganguli, Deep										Towards Measuring the Representation of Subjective Global Opinions in Language Models								Arxiv											1	1;2024-04-12;https://www.arxiv.org/abs/2306.16388v2	arXiv:2306.16388			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 12 2024	2024	Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model -generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross -national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country’s perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model’s responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on.2																																	2024-04-26	PPRN:88550710		
J	Defossez, Alexandre; Mazare, Laurent; Orsini, Manu; Royer, Amelie; Perez, Patrick; Jegou, Herve; Grave, Edouard; Zeghidour, Neil										Moshi: a speech-text foundation model for real-time dialogue								Arxiv											1	1;2024-10-02;https://www.arxiv.org/abs/2410.00037v2	arXiv:2410.00037			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 02 2024	2024	We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this "Inner Monologue" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at 																																	2024-10-16	PPRN:100822598		
J	Men, Xin; Xu, Mingyu; Zhang, Qingyu; Wang, Bingning; Lin, Hongyu; Lu, Yaojie; Han, Xianpei; Chen, Weipeng				Chen, Weipeng/ITV-5921-2023; Zhang, Qingyu/OXB-0286-2025; Lin, Hongyu/LXA-3658-2024; Han, Xianpei/MTC-8266-2025; xu, mingyu/KMX-9517-2024						ShortGPT: Layers in Large Language Models are More Redundant Than You Expect								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.03853v2	arXiv:2403.03853			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture. [GRAPHICS]																																	2024-04-05	PPRN:88061741		
J	Muennighoff, Niklas; Liu, Qian; Zebaze, Armel; Zheng, Qinkai; Hui, Binyuan; Zhuo, Terry Yue; Singh, Swayam; Tang, Xiangru; Von Werra, Leandro; Longpre, Shayne				Bin/B-6414-2019; Zheng, Qinkai/AAI-9267-2021						OctoPack: Instruction Tuning Code Large Language Models								Arxiv											2	2;2024-02-18;https://www.arxiv.org/abs/2308.07124v2| 1;2023-08-14;https://www.arxiv.org/abs/2308.07124v1	arXiv:2308.07124			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 18 2024	2024	Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.																																	2024-03-15	PPRN:77289462		
J	Zhang, Yanzhe; Zhang, Ruiyi; Gu, Jiuxiang; Zhou, Yufan; Lipka, Nedim; Yang, Diyi; Sun, Tong				Zhou, Yufan/AFR-9024-2022; zhang, yanzhe/O-7292-2015; Zhang, Ruiyi/JNR-1096-2023						LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding								Arxiv											1	1;2024-02-02;https://www.arxiv.org/abs/2306.17107v2	arXiv:2306.17107			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following evaluation also demonstrates the improvement of our model on both natural images and text-rich images. Through qualitative analysis, LLaVAR shows promising interaction (e.g., reasoning, writing, and elaboration) skills with humans based on the latest real-world online content that combines text and images. 																																	2024-02-21	PPRN:87523799		
J	Yong, Zheng-Xin; Menghini, Cristina; Bach, Stephen H.										Low-Resource Languages Jailbreak GPT-4								Arxiv											2	2;2024-01-27;https://www.arxiv.org/abs/2310.02446v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02446v1	arXiv:2310.02446			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 27 2024	2024	AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs' safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.																																	2024-05-25	PPRN:85403794		
J	Lieber, Opher; Lenz, Barak; Bata, Hofit; Cohen, Gal; Osin, Jhonathan; Dalmedigos, Itay; Safahi, Erez; Meirom, Shaked; Belinkov, Yonatan; Shalev-Shwartz, Shai; Abend, Omri; Alon, Raz; Asida, Tomer; Bergman, Amir; Glozman, Roman; Gokhman, Michael; Manevich, Avashalom; Ratner, Nir; Rozen, Noam; Shwartz, Erez; Zusman, Mor; Shoham, Yoav				Schwartz, Erez/NYS-3543-2025						Jamba: A Hybrid Transformer-Mamba Language Model								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2403.19887v2| 1;2024-03-28;https://www.arxiv.org/abs/2403.19887v1	arXiv:2403.19887			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 03 2024	2024	We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.																																	2024-07-20	PPRN:88342483		
J	Groeneveld, Dirk; Beltagy, Iz; Walsh, Pete; Bhagia, Akshita; Kinney, Rodney; Tafjord, Oyvind; Jha, Ananya Harsh; Ivison, Hamish; Magnusson, Ian; Wang, Yizhong; Arora, Shane; Atkinson, David; Authur, Russell; Chandu, Khyathi Raghavi; Cohan, Arman; Dumas, Jennifer; Elazar, Yanai; Gu, Yuling; Hessel, Jack; Khot, Tushar; Merrill, William; Morrison, Jacob; Muennighoff, Niklas; Naik, Aakanksha; Nam, Crystal; Peters, Matthew E.; Pyatkin, Valentina; Ravichander, Abhilasha; Schwenk, Dustin; Shah, Saurabh; Smith, Will; Strubell, Emma; Subramani, Nishant; Wortsman, Mitchell; Dasigi, Pradeep; Lambert, Nathan; Richardson, Kyle; Zettlemoyer, Luke; Dodge, Jesse; Lo, Kyle; Soldaini, Luca; Smith, Noah A.; Hajishirzi, Hannaneh				Merrill, William/HMV-2296-2023						OLMo: Accelerating the Science of Language Models								Arxiv											4	4;2024-06-07;https://www.arxiv.org/abs/2402.00838v4| 3;2024-02-28;https://www.arxiv.org/abs/2402.00838v3| 2;2024-02-07;https://www.arxiv.org/abs/2402.00838v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.00838v1	arXiv:2402.00838			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.																																	2024-07-15	PPRN:87456345		
J	Koh, Jing Yu; Lo, Robert; Jang, Lawrence; Duvvur, Vikram; Lim, Ming Chong; Huang, Po-Yu; Neubig, Graham; Zhou, Shuyan; Salakhutdinov, Ruslan; Fried, Daniel				Lo, Robert/OMN-0320-2025						VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks								Arxiv											2	2;2024-06-06;https://www.arxiv.org/abs/2401.13649v2| 1;2024-01-24;https://www.arxiv.org/abs/2401.13649v1	arXiv:2401.13649			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. 																																	2024-07-04	PPRN:87311365		
J	Ahn, Janice; Verma, Rishu; Lou, Renze; Liu, Di; Zhang, Rui; Yin, Wenpeng										Large Language Models for Mathematical Reasoning: Progresses and Challenges								Arxiv											4	4;2024-09-16;https://www.arxiv.org/abs/2402.00157v4| 3;2024-04-05;https://www.arxiv.org/abs/2402.00157v3| 2;2024-03-23;https://www.arxiv.org/abs/2402.00157v2| 1;2024-01-31;https://www.arxiv.org/abs/2402.00157v1	arXiv:2402.00157			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 16 2024	2024	Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.																																	2024-11-09	PPRN:87456663		
J	Wei, Xiang; Cui, Xingyu; Cheng, Ning; Wang, Xiaobin; Zhang, Xin; Huang, Shen; Xie, Pengjun; Xu, Jinan; Chen, Yufeng; Zhang, Meishan; Jiang, Yong; Han, Wenjuan				Wang, Xiaobin/KDO-4266-2024; Han, Wenjuan/ABD-7442-2020						ChatIE: Zero-Shot Information Extraction via Chatting with ChatGPT								Arxiv											2	2;2024-05-27;https://www.arxiv.org/abs/2302.10205v2| 1;2023-02-20;https://www.arxiv.org/abs/2302.10205v1	arXiv:2302.10205			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 27 2024	2024	Zero-shot Information Extraction (IE) aims to build IE systems from the unannotated text. This is a challenging task as it involves little human intervention, but it is also worthwhile, as zero-shot IE reduces the time and effort needed for data labeling. Recent research on Large Language Models (LLMs), such as GPT-3 and ChatGPT, has shown promising performance on zero-shot settings. This has inspired us to explore prompt-based methods. In this work, we are the first to quantitatively explore whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn questionanswering problem with a two-stage framework (namely, ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., e.g ., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.																																	2024-06-11	PPRN:43930107		
J	Shi, Weijia; Ajith, Anirudh; Xia, Mengzhou; Huang, Yangsibo; Liu, Daogao; Blevins, Terra; Chen, Danqi; Zettlemoyer, Luke				CHEN, DANQI/C-6441-2013						Detecting Pretraining Data from Large Language Models								Arxiv											4	4;2024-03-09;https://www.arxiv.org/abs/2310.16789v3| 3;2023-11-03;https://www.arxiv.org/abs/2310.16789v2| 2;2023-10-25;https://www.arxiv.org/abs/2310.16789v1| 1;2023-10-25;https://www.arxiv.org/abs/2310.16789v1	arXiv:2310.16789			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 09 2024	2024	Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black -box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method MIN -K% PROB based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. MIN -K% PROB can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that MIN -K% PROB achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply MIN -K% PROB to three real -world scenarios, copyrighted book detection, contaminated downstream example detection and privacy auditing of machine unlearning, and find it a consistently effective solution.																																	2024-04-07	PPRN:85792090		
J	Liu, Hao; Yan, Wilson; Zaharia, Matei; Abbeel, Pieter										World Model on Million-Length Video And Language With Blockwise RingAttention								Arxiv											3	3;2024-07-23;https://www.arxiv.org/abs/2402.08268v3| 2;2024-03-14;https://www.arxiv.org/abs/2402.08268v2| 1;2024-02-13;https://www.arxiv.org/abs/2402.08268v1	arXiv:2402.08268			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 23 2024	2024	Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the Blockwise RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, Blockwise Transformers, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.																																	2024-08-01	PPRN:87668901		
J	Cirelli, Marco; Strumia, Alessandro; Zupan, Jure				Strumia, Alessandro/AAL-3094-2020; Zupan, Jure/D-3573-2015						Dark Matter								Arxiv											1	1;2024-07-31;https://www.arxiv.org/abs/2406.01705v2	arXiv:2406.01705			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 31 2024	2024	We review observational, experimental and theoretical results related to Dark Matter.																																	2024-12-06	PPRN:91188955		
J	Behnamghader, Parishad; Adlakha, Vaibhav; Mosbach, Marius; Bahdanau, Dzmitry; Chapados, Nicolas; Reddy, Siva										LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders								Arxiv											1	1;2024-04-09;https://www.arxiv.org/abs/2404.05961v1	arXiv:2404.05961			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 09 2024	2024	Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.																																	2024-04-22	PPRN:88466143		
J	Cerezo, M.; Larocca, Martin; Garcia-Martin, Diego; Diaz, N.L.; Braccia, Paolo; Fontana, Enrico; Rudolph, Manuel S.; Bermejo, Pablo; Ijaz, Aroosa; Thanasilp, Supanut; Anschuetz, Eric R.; Holmes, Zoe				Cerezo, Marco/ABD-9254-2020						Does provable absence of barren plateaus imply classical simulability? Or, why we need to rethink variational quantum computing								Arxiv											2	2;2024-03-19;https://www.arxiv.org/abs/2312.09121v2| 1;2023-12-14;https://www.arxiv.org/abs/2312.09121v1	arXiv:2312.09121			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	A large amount of effort has recently been put into understanding the barren plateau phenomenon. In this perspective article, we face the increasingly loud elephant in the room and ask a question that has been hinted at by many but not explicitly addressed: Can the structure that allows one to avoid barren plateaus also be leveraged to efficiently simulate the loss classically? We present strong evidence that commonly used models with provable absence of barren plateaus are also classically simulable, provided that one can collect some classical data from quantum devices during an initial data acquisition phase. This follows from the observation that barren plateaus result from a curse of dimensionality, and that current approaches for solving them end up encoding the problem into some small, classically simulable, subspaces. Thus, while stressing quantum computers can be essential for collecting data, our analysis sheds serious doubt on the non-classicality of the information processing capabilities of parametrized quantum circuits for barren plateau-free landscapes. We end by discussing caveats in our arguments, the role of smart initializations and the possibility of provably superpolynomial, or simply practical, advantages from running parametrized quantum circuits.																																	2024-04-12	PPRN:86587343		
J	Tong, Alexander; Fatras, Kilian; Malkin, Nikolay; Huguet, Guillaume; Zhang, Yanlei; Rector-Brooks, Jarrid; Wolf, Guy; Bengio, Yoshua										Improving and generalizing flow-based generative models with minibatch optimal transport								Arxiv											3	3;2024-03-11;https://www.arxiv.org/abs/2302.00482v4| 2;2023-10-31;https://www.arxiv.org/abs/2302.00482v3| 1;2023-02-01;https://www.arxiv.org/abs/2302.00482v1	arXiv:2302.00482			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 11 2024	2024	Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrodinger bridge inference.																																	2024-04-07	PPRN:36109416		
J	Zheng, Lianmin; Chiang, Wei-Lin; Sheng, Ying; Li, Tianle; Zhuang, Siyuan; Wu, Zhanghao; Zhuang, Yonghao; Li, Zhuohan; Lin, Zi; Xing, Eric P.; Gonzalez, Joseph E.; Stoica, Ion; Zhang, Hao				Zhuang, Siyuan/JLK-9571-2023; li, zhuohan/IZJ-4138-2023						LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset								Arxiv											4	4;2024-03-10;https://www.arxiv.org/abs/2309.11998v4| 3;2023-09-30;https://www.arxiv.org/abs/2309.11998v3| 2;2023-09-22;https://www.arxiv.org/abs/2309.11998v2| 1;2023-09-21;https://www.arxiv.org/abs/2309.11998v1	arXiv:2309.11998			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 10 2024	2024	Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. 																																	2024-04-08	PPRN:85088478		
J	Marks, Samuel; Tegmark, Max										The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets								Arxiv											4	4;2024-08-19;https://www.arxiv.org/abs/2310.06824v3| 3;2023-12-08;https://www.arxiv.org/abs/2310.06824v2| 2;2023-10-10;https://www.arxiv.org/abs/2310.06824v1| 1;2023-10-10;https://www.arxiv.org/abs/2310.06824v1	arXiv:2310.06824			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 19 2024	2024	Large Language Models (LLMs) have impressive capabilities, but are prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we use high-quality datasets of simple true/false statements to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that at sufficient scale, LLMs linearly represent the truth or falsehood of factual statements. We also show that simple difference-in-mean probes generalize as well as other probing techniques while identifying directions which are more causally implicated in model outputs.																																	2024-08-28	PPRN:85525707		
J	Girdhar, Rohit; Singh, Mannat; Brown, Andrew; Duval, Quentin; Azadi, Samaneh; Rambhatla, Sai Saketh; Shah, Akbar; Yin, Xi; Parikh, Devi; Misra, Ishan				Singh, Mannat/GWC-3412-2022; Yin, Xi/GQO-8468-2022						Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning								Arxiv											2	2;2024-08-02;https://www.arxiv.org/abs/2311.10709v2| 1;2023-11-17;https://www.arxiv.org/abs/2311.10709v1	arXiv:2311.10709			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 02 2024	2024	We present Emu VIDEO , a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions– adjusted noise schedules for diffusion, and multi-stage training–that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work–81% 81% vs. Google’s Imagen Video, 90% vs. Nvidia’s PYOCO, and 96% vs. Meta’s Make-A-Video. Our model outperforms commercial solutions such as RunwayML’s Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user’s text prompt, where our generations are preferred 96% over prior work.																																	2024-08-09	PPRN:86200213		
J	Kim, Sehoon; Hooper, Coleman; Gholami, Amir; Dong, Zhen; Li, Xiuyu; Shen, Sheng; Mahoney, Michael W.; Keutzer, Kurt				Chen, Shengli/AAV-4029-2021						SqueezeLLM: Dense-and-Sparse Quantization								Arxiv											4	4;2024-06-05;https://www.arxiv.org/abs/2306.07629v4| 3;2024-02-05;https://www.arxiv.org/abs/2306.07629v3| 2;2023-10-04;https://www.arxiv.org/abs/2306.07629v2| 1;2023-06-13;https://www.arxiv.org/abs/2306.07629v1	arXiv:2306.07629			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 05 2024	2024	Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. 																																	2024-07-11	PPRN:73315447		
J	Huang, Dong; Zhang, Jie M.; Luck, Michael; Bu, Qingwen; Qing, Yuhao; Cui, Heming				Luck, Michael/H-5425-2011; Qing, Yuhao/AAC-5562-2022						AgentCoder: Multi-Agent Code Generation with Effective Testing and Self-optimisation								Arxiv											3	3;2024-05-24;https://www.arxiv.org/abs/2312.13010v3| 2;2024-01-23;https://www.arxiv.org/abs/2312.13010v2| 1;2023-12-20;https://www.arxiv.org/abs/2312.13010v1	arXiv:2312.13010			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder's superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder (GPT-4) achieves 96.3% and 91.8% pass@1 in HumanEval and MBPP datasets with an overall token overhead of 56.9K and 66.3K, while state-of-the-art obtains only 90.2% and 78.9% pass@1 with an overall token overhead of 138.2K and 206.5K.																																	2024-06-08	PPRN:86742860		
J	Zhou, Yiyang; Cui, Chenhang; Yoon, Jaehong; Zhang, Linjun; Deng, Zhun; Finn, Chelsea; Bansal, Mohit; Yao, Huaxiu				Yao, Huaxiu/V-3516-2019; Zhou, Yiyang/AAU-7705-2021; Bansal, Mohit/Q-9105-2016; Zhang, Linjun/IUP-2157-2023						Analyzing and Mitigating Object Hallucination in Large Vision-Language Models								Arxiv											2	2;2024-03-16;https://www.arxiv.org/abs/2310.00754v2| 1;2023-10-01;https://www.arxiv.org/abs/2310.00754v1	arXiv:2310.00754			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 16 2024	2024	Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top.																																	2024-04-11	PPRN:85349975		
J	Kumar, Aviral; Zhuang, Vincent; Agarwal, Rishabh; Su, Yi; Co-Reyes, John D; Singh, Avi; Baumli, Kate; Iqbal, Shariq; Bishop, Colton; Roelofs, Rebecca; Zhang, Lei M; Mckinney, Kay; Shrivastava, Disha; Paduraru, Cosmin; Tucker, George; Precup, Doina; Behbahani, Feryal; Faust, Aleksandra										Training Language Models to Self-Correct via Reinforcement Learning								Arxiv											1	1;2024-10-04;https://www.arxiv.org/abs/2409.12917v2	arXiv:2409.12917			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 04 2024	2024	Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.																																	2024-10-25	PPRN:103971856		
J	Beck, Maximilian; Poeppel, Korbinian; Spanring, Markus; Auer, Andreas; Prudnikova, Oleksandra; Kopp, Michael; Klambauer, Guenter; Brandstetter, Johannes; Hochreiter, Sepp				Hochreiter, Sepp/AAI-5904-2020; Beck, Maximilian/NOE-5795-2025						xLSTM: Extended Long Short-Term Memory								Arxiv											1	1;2024-05-07;https://www.arxiv.org/abs/2405.04517v1	arXiv:2405.04517			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 07 2024	2024	In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.   [GRAPHICS]																																	2024-06-04	PPRN:88975045		
J	Wang, Liyuan; Zhang, Xingxing; Su, Hang; Zhu, Jun				Wang, Liyuan/ABB-3860-2021; Zhang, Xingxing/HGE-4445-2022						A Comprehensive Survey of Continual Learning: Theory, Method and Application								Arxiv											2	2;2024-02-06;https://www.arxiv.org/abs/2302.00487v3| 1;2023-01-31;https://www.arxiv.org/abs/2302.00487v1	arXiv:2302.00487			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.																																	2024-02-21	PPRN:36115675		
J	Lin, Zhen; Trivedi, Shubhendu; Sun, Jimeng										Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models								Arxiv											3	3;2024-05-20;https://www.arxiv.org/abs/2305.19187v3| 2;2023-10-09;https://www.arxiv.org/abs/2305.19187v2| 1;2023-05-30;https://www.arxiv.org/abs/2305.19187v1	arXiv:2305.19187			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 20 2024	2024	Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white -box access to language models, which is becoming unrealistic either due to the closed -source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for black -box LLMs. We first differentiate uncertainty vs confidence: : the former refers to the “dispersion” of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to selective NLG where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question -answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicateour experiments is available at https://github.com/zlin7/UQ-NLG. .																																	2024-06-01	PPRN:72765878		
J	Xia, Mengzhou; Gao, Tianyu; Zeng, Zhiyuan; Chen, Danqi				Zeng, Zhiyuan/JAC-2446-2023						Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning								Arxiv											2	2;2024-04-11;https://www.arxiv.org/abs/2310.06694v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06694v1	arXiv:2310.06694			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 11 2024	2024	The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the ShearedLLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building competitive small-scale LLMs.																																	2024-04-24	PPRN:85521340		
J	Shao, Wenqi; Chen, Mengzhao; Zhang, Zhaoyang; Xu, Peng; Zhao, Lirui; Li, Zhiqian; Zhang, Kaipeng; Gao, Peng; Qiao, Yu; Luo, Ping				Zhang, Kaipeng/IAN-8361-2023; Li, Zhiqian/NPI-7315-2025; Gao, Peng/B-4675-2012; Zhang, Zhaoyang/HJI-9395-2023; Qiao, Yu/ABD-5787-2021; Chen, Mengzhao/JAC-5960-2023; pluo/GPG-2707-2022						OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2308.13137v3| 2;2023-10-22;https://www.arxiv.org/abs/2308.13137v2| 1;2023-08-25;https://www.arxiv.org/abs/2308.13137v1	arXiv:2308.13137			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (textbf{OmniQuant}) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. 																																	2024-04-11	PPRN:83916763		
J	Deng, Yue; Zhang, Wenxuan; Pan, Sinno Jialin; Bing, Lidong				Zhang, Wenxuan/LKN-9746-2024; PAN, Sinno Jialin/P-6696-2014						Multilingual Jailbreak Challenges in Large Language Models								Arxiv											4	4;2024-03-04;https://www.arxiv.org/abs/2310.06474v3| 3;2024-02-28;https://www.arxiv.org/abs/2310.06474v2| 2;2023-10-10;https://www.arxiv.org/abs/2310.06474v1| 1;2023-10-10;https://www.arxiv.org/abs/2310.06474v1	arXiv:2310.06474			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 04 2024	2024	While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the “jailbreak” problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low -resource languages exhibit about three times the likelihood of encountering harmful content compared to high -resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel SELF-DEFENSE framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. 																																	2024-03-31	PPRN:85522394		
J	Huang, Chengsong; Liu, Qian; Lin, Bill Yuchen; Pang, Tianyu; Du, Chao; Lin, Min				Tianyu, Pang/AAW-2653-2020; Huang, Chengsong/HKE-0328-2023						LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition								Arxiv											3	3;2024-08-19;https://www.arxiv.org/abs/2307.13269v3| 2;2024-01-18;https://www.arxiv.org/abs/2307.13269v2| 1;2023-07-25;https://www.arxiv.org/abs/2307.13269v1	arXiv:2307.13269			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 19 2024	2024	Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a simple framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a new task, LoraHub can fluidly combine multiple LoRA modules, eliminating the need for human expertise and assumptions. Notably, the composition requires neither additional model parameters nor gradients. Empirical results on the Big-Bench Hard benchmark suggest that LoraHub, while not surpassing the performance of in-context learning, offers a notable performance-efficiency trade-off in few-shot scenarios by employing a significantly reduced number of tokens per example during inference. Notably, LoraHub establishes a better upper bound compared to in-context learning when paired with different demonstration examples, demonstrating its potential for future development. Our vision is to establish a platform for LoRA modules, empowering users to share their trained LoRA modules. This collaborative approach facilitates the seamless application of LoRA modules to novel tasks, contributing to an adaptive ecosystem. 																																	2024-08-30	PPRN:74092136		
J	Li, Tianhong; Tian, Yonglong; Li, He; Deng, Mingyang; He, Kaiming										Autoregressive Image Generation without Vector Quantization								Arxiv											2	2;2024-07-28;https://www.arxiv.org/abs/2406.11838v2| 1;2024-06-17;https://www.arxiv.org/abs/2406.11838v1	arXiv:2406.11838			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 28 2024	2024	Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. 																																	2024-08-04	PPRN:89348177		
J	Tian, Xiaoyu; Gu, Junru; Li, Bailin; Liu, Yicheng; Wang, Yang; Zhao, Zhiyong; Zhan, Kun; Jia, Peng; Lang, Xianpeng; Zhao, Hang				Zhao, Zhiyong/H-7694-2016						DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models								Arxiv											4	4;2024-06-25;https://www.arxiv.org/abs/2402.12289v5| 3;2024-03-31;https://www.arxiv.org/abs/2402.12289v3| 2;2024-02-25;https://www.arxiv.org/abs/2402.12289v2| 1;2024-02-19;https://www.arxiv.org/abs/2402.12289v1	arXiv:2402.12289			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 25 2024	2024	A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of reasoning modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and unpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a production vehicle, verifying it is effective in real-world autonomous driving environments.																																	2024-07-15	PPRN:87763288		
J	Sun, Quan; Yu, Qiying; Cui, Yufeng; Zhang, Fan; Zhang, Xiaosong; Wang, Yueze; Gao, Hongcheng; Liu, Jingjing; Huang, Tiejun; Wang, Xinlong				Gao, Hongcheng/HTL-2483-2023; Wang, Xinlong/AFI-8800-2022; JINGJING, LIU/GWU-6266-2022						Emu: Generative Pretraining in Multimodality								Arxiv											2	2;2024-05-08;https://www.arxiv.org/abs/2307.05222v2| 1;2023-07-11;https://www.arxiv.org/abs/2307.05222v1	arXiv:2307.05222			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 08 2024	2024	We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.																																	2024-05-26	PPRN:73872679		
J	Khazatsky, Alexander; Pertsch, Karl; Nair, Suraj; Balakrishna, Ashwin; Dasari, Sudeep; Karamcheti, Siddharth; Nasiriany, Soroush; Srirama, Mohan Kumar; Chen, Lawrence Yunliang; Ellis, Kirsty; Fagan, Peter David; Hejna, Joey; Itkina, Masha; Lepert, Marion; Ma, Jason; Miller, Patrick Tree; Wu, Jimmy; Belkhale, Suneel; Dass, Shivin; Ha, Huy; Lee, Abraham; Lee, Youngwoon; Jain, Arhan; Memmel, Marius; Park, Sungjae; Radosavovic, Ilija; Wang, Kaiyuan; Zhan, Albert; Black, Kevin; Chi, Cheng; Hatch, Kyle; Lin, Shan; Lu, Jingpei; Mercat, Jean; Rehman, Abdul; Sanketi, Pannag R; Sharma, Archit; Simpson, Cody; Vuong, Quan; Walke, Homer; Wulfe, Blake; Xiao, Ted; Yang, Jonathan; Yavary, Arefeh; Zhao, Tony Z.; Agia, Christopher; Baijal, Rohan; Castro, Mateo Guaman; Chen, Daphne; Chen, Qiuyu; Chung, Trinity; Drake, Jaimyn; Foster, Ethan Paul; Gao, Jensen; Herrera, David Antonio; Heo, Minho; Hsu, Kyle; Hu, Jiaheng; Jackson, Donovon; Le, Charlotte; Li, Yunshuang; Lin, Kevin; Lin, Roy; Ma, Zehan; Maddukuri, Abhiram; Mirchandani, Suvir; Morton, Daniel; Nguyen, Tony; O'Neill, Abigail; Scalise, Rosario; Seale, Derick; Son, Victor; Tian, Stephen; Tran, Emi; Wang, Andrew E.; Wu, Yilin; Xie, Annie; Yang, Jingyun; Yin, Patrick; Zhang, Yunchu; Bastani, Osbert; Berseth, Glen; Bohg, Jeannette; Goldberg, Ken; Gupta, Abhinav; Gupta, Abhishek; Jayaraman, Dinesh; Lim, Joseph J; Malik, Jitendra; Martin-Martin, Roberto; Ramamoorthy, Subramanian; Sadigh, Dorsa; Song, Shuran; Wu, Jiajun; Yip, Michael C.; Zhu, Yuke; Kollar, Thomas; Levine, Sergey; Finn, Chelsea				Martin, Roberto/Y-6009-2019; Yang, Jingyun/AAS-4711-2021; Yip, Michael/AAU-1997-2020; Vuong, Quan-Hoang/F-2115-2010; Wu, Jiajun/AFA-0504-2022; 张, 蕴初/JYP-7884-2024; Chen, Lawrence Yunliang/ACJ-4018-2022; Gupta, Abhinav/NBX-5099-2025; Chi, Cheng/GSE-4960-2022; Agia, Christopher/KGL-4088-2024; Ellis, Kirsty/MXL-9136-2025; Jayaraman, Dinesh/AAI-2527-2021; Bohg, Jeannette/AAD-4010-2019; Rehman, Abdul/KCZ-1929-2024						DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset								Arxiv											1	1;2024-03-19;https://www.arxiv.org/abs/2403.12945v1	arXiv:2403.12945			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 19 2024	2024	The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.																																	2024-05-07	PPRN:88242874		
J	Wang, Boxin; Chen, Weixin; Pei, Hengzhi; Xie, Chulin; Kang, Mintong; Zhang, Chenhui; Xu, Chejian; Xiong, Zidi; Dutta, Ritik; Schaeffer, Rylan; Truong, Sang T.; Arora, Simran; Mazeika, Mantas; Hendrycks, Dan; Lin, Zinan; Cheng, Yu; Koyejo, Sanmi; Song, Dawn; Li, Bo				Lin, Zinan/NGS-1685-2025; Xiong, Zidi/KBB-8747-2024; Zhang, Chenhui/C-2370-2015; 陈, 炜鑫/GZM-3682-2022						DECODING TRUST: A Comprehensive Assessment of Trustworthiness in GPT Models								Arxiv											5	5;2024-02-26;https://www.arxiv.org/abs/2306.11698v5| 4;2024-01-05;https://www.arxiv.org/abs/2306.11698v4| 3;2023-12-19;https://www.arxiv.org/abs/2306.11698v3| 2;2023-12-11;https://www.arxiv.org/abs/2306.11698v2| 1;2023-06-20;https://www.arxiv.org/abs/2306.11698v1	arXiv:2306.11698			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Feb 26 2024	2024	Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/ ; our dataset can be previewed at https://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of this work is at https://openreview.net/pdf?id=kaHpo8OZw2 .																																	2024-03-27	PPRN:73443254		
J	Ge, Suyu; Zhang, Yunan; Liu, Liyuan; Zhang, Minjia; Han, Jiawei; Gao, Jianfeng				han, jiawei/GVT-3012-2022; Liyuan, Liu/GRJ-6161-2022; Zhang, Minjia/MFH-5718-2025; Gao, Jianfeng/AAP-8200-2021						Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs								Arxiv											3	3;2024-01-29;https://www.arxiv.org/abs/2310.01801v3| 2;2023-10-07;https://www.arxiv.org/abs/2310.01801v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.01801v1	arXiv:2310.01801			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 29 2024	2024	In this study, we introduce adaptive KV cache compression, a plug -and -play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we propose FastGen, which constructs the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non -special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource -intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.																																	2024-05-25	PPRN:85378514		
J	Khanam, Rahima; Hussain, Muhammad				Khanam, Rahima/MVX-9881-2025						YOLOv11: An Overview of the Key Architectural Enhancements								Arxiv											1	1;2024-10-23;https://www.arxiv.org/abs/2410.17725v1	arXiv:2410.17725			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 23 2024	2024	This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.																																	2024-11-24	PPRN:118788157		
J	Zhu, Xunyu; Li, Jian; Liu, Yong; Ma, Can; Wang, Weiping				Li, Yongkai/HPI-0077-2023; Zhu, Xunyu/JVO-7183-2024						A Survey on Model Compression for Large Language Models								Arxiv											4	4;2024-07-30;https://www.arxiv.org/abs/2308.07633v4| 3;2023-09-17;https://www.arxiv.org/abs/2308.07633v3| 2;2023-08-17;https://www.arxiv.org/abs/2308.07633v2| 1;2023-08-15;https://www.arxiv.org/abs/2308.07633v1	arXiv:2308.07633			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 30 2024	2024	Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.																																	2024-08-06	PPRN:77977898		
J	Gu, Shangding; Yang, Long; Du, Yali; Chen, Guang; Walter, Florian; Wang, Jun; Knoll, Alois				Walter, Florian/AAM-7102-2021; CHen, Guang/LFU-8636-2024; Gu, Shangding/JVZ-9266-2024; Knoll, Alois/AAN-8417-2021						A Review of Safe Reinforcement Learning: Methods, Theory and Applications								Arxiv											2	2;2024-05-24;https://www.arxiv.org/abs/2205.10330v5| 1;2022-05-20;https://www.arxiv.org/abs/2205.10330v4	arXiv:2205.10330			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 24 2024	2024	Reinforcement Learning (RL) has achieved tremendous success in many complex decision-making tasks. However, safety concerns are raised during deploying RL in real-world applications, leading to a growing demand for safe RL algorithms, such as in autonomous driving and robotics scenarios. While safe control has a long history, the study of safe RL algorithms is still in the early stages. To establish a good foundation for future safe RL research, in this paper, we provide a review of safe RL from the perspectives of methods, theories, and applications. Firstly, we review the progress of safe RL from five dimensions and come up with five crucial problems for safe RL being deployed in real-world applications, coined as "2H3W". Secondly, we analyze the algorithm and theory progress from the perspectives of answering the "2H3W" problems. Particularly, the sample complexity of safe RL algorithms is reviewed and discussed, followed by an introduction to the applications and benchmarks of safe RL algorithms. Finally, we open the discussion of the challenging problems in safe RL, hoping to inspire future research on this thread. To advance the study of safe RL algorithms, we release an open-sourced repository containing the implementations of major safe RL algorithms at the link: https://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines.git.																																	2024-06-09	PPRN:43662517		
J	Roettger, Paul; Kirk, Hannah Rose; Vidgen, Bertie; Attanasio, Giuseppe; Bianchi, Federico; Hovy, Dirk				Attanasio, Giuseppe/NKE-0343-2025; Hovy, Dirk/MVX-7752-2025						XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models								Arxiv											3	3;2024-04-01;https://www.arxiv.org/abs/2308.01263v3| 2;2023-10-17;https://www.arxiv.org/abs/2308.01263v2| 1;2023-08-02;https://www.arxiv.org/abs/2308.01263v1	arXiv:2308.01263			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTEST to identify such eXaggerated Safety behaviours in a systematic way. XSTEST comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTEST’s creation and composition, and then use the test suite to highlight systematic failure modes in state -ofthe -art language models as well as more general challenges in building safer language models.																																	2024-04-17	PPRN:74219347		
J	Xia, Chunqiu Steven; Deng, Yinlin; Dunn, Soren; Zhang, Lingming				Deng, Yinlin/KBQ-0846-2024						Agentless: Demystifying LLM-based Software Engineering Agents								Arxiv											2	2;2024-10-29;https://www.arxiv.org/abs/2407.01489v2| 1;2024-07-01;https://www.arxiv.org/abs/2407.01489v1	arXiv:2407.01489			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 29 2024	2024	Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build A GENTLESS – an agentless approach to automatically resolve software development issues. Compared to the verbose and complex setup of agent-based approaches, A GENTLESS employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic A GENTLESS is able to achieve both the highest performance (32.00%, 96 correct fixes) and low cost ($0.70) compared with all existing open-source software agents! In fact, A GENTLESS has already been adopted by OpenAI as the go-to approach to showcase the real-world coding performance of both GPT-4o and the new OpenAI o1 models . Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patches or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite- S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the currently overlooked potential of a simplistic, cost-effective technique in autonomous software development. We hope A GENTLESS will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction. 																																	2024-12-03	PPRN:90657301		
J	Yuan, Youliang; Jiao, Wenxiang; Wang, Wenxuan; Huang, Jen-tse; He, Pinjia; Shi, Shuming; Tu, Zhaopeng				Huang, Jen-Tse/IRZ-7526-2023; Tu, Zhaopeng/AAS-4259-2021; Wang, Wenxuan/AAW-9073-2020						GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher								Arxiv											2	2;2023-08-12;https://www.arxiv.org/abs/2308.06463v1| 1;2024-03-01;	arXiv:2308.06463			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 01 2024	2024	Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.																																	2025-11-07	PPRN:77402728		
J	Wang, Chien-Yao; Yeh, I-Hau; Liao, Hong-Yuan Mark										YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information								Arxiv											2	2;2024-02-29;https://www.arxiv.org/abs/2402.13616v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.13616v1	arXiv:2402.13616			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 29 2024	2024	Today’s deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer -by -layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture – Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN’s architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth -wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train -fromscratch models can achieve better results than state-of-theart models pre -trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.																																	2024-03-28	PPRN:87795559		
J	Panickssery, Nina; Gabrieli, Nick; Schulz, Julian; Tong, Meg; Hubinger, Evan; Turner, Alexander Matt										Steering Llama 2 via Contrastive Activation Addition								Arxiv											4	4;2024-07-05;https://www.arxiv.org/abs/2312.06681v4| 3;2024-03-07;https://www.arxiv.org/abs/2312.06681v3| 2;2023-12-27;https://www.arxiv.org/abs/2312.06681v2| 1;2023-12-09;https://www.arxiv.org/abs/2312.06681v1	arXiv:2312.06681			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 05 2024	2024	We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).																																	2024-07-20	PPRN:86555329		
J	Xu, Yuzhuang; Wang, Shuo; Li, Peng; Luo, Fuwen; Wang, Xiaolong; Liu, Weidong; Liu, Yang				Liu, Weidong/AAE-9153-2019						Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf								Arxiv											2	2;2024-05-11;https://www.arxiv.org/abs/2309.04658v2| 1;2023-09-09;https://www.arxiv.org/abs/2309.04658v1	arXiv:2309.04658			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 11 2024	2024	Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.																																	2024-06-08	PPRN:84950514		
J	Li, Xian; Yu, Ping; Zhou, Chunting; Schick, Timo; Levy, Omer; Zettlemoyer, Luke; Weston, Jason; Lewis, Mike										SELF-ALIGNMENT WITH INSTRUCTION BACKTRANSLATION								Arxiv											2	2;2024-03-12;https://www.arxiv.org/abs/2308.06259v3| 1;2023-08-14;https://www.arxiv.org/abs/2308.06259v2	arXiv:2308.06259			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 12 2024	2024	We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.																																	2024-04-08	PPRN:77255431		
J	Chen, Guiming Hardy; Chen, Shunian; Zhang, Ruifei; Chen, Junying; Wu, Xiangbo; Zhang, Zhiyi; Chen, Zhihong; Li, Jianquan; Wan, Xiang; Wang, Benyou				Zhang, Zhiyi/AAS-2102-2021; Wang, Benyou/Y-5146-2019						ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2402.11684v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11684v1	arXiv:2402.11684			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	Large vision-language models (LVLMs) have shown premise in a broad range of vision-language tasks with their strong reasoning and generalization capabilities. However, they require considerable computational resources for training and deployment. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To this end, we propose a comprehensive pipeline for generating a synthetic dataset. The key idea is to leverage strong proprietary models to generate (i) fine-grained image annotations for vision-language alignment and (ii) complex reasoning visual question-answering pairs for visual instruction fine-tuning, yielding 1.3M samples in total. We train a series of lite VLMs on the synthetic dataset and experimental results demonstrate the effectiveness of the proposed scheme, where they achieve competitive performance on 17 benchmarks among 4B LVLMs, and even perform on par with 7B/13B-scale models on various benchmarks. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. We name our dataset textit{ALLaVA}, and open-source it to research community for developing better resource-efficient LVLMs for wider usage.																																	2024-07-04	PPRN:87761105		
J	Xu, Lin; Zhao, Yilin; Zhou, Daquan; Lin, Zhijie; Ng, See Kiong; Feng, Jiashi				Feng, Jiashi/AGX-6209-2022; Zhou, Daquan/ACT-7390-2022; Lin, Zhijie/AAA-3254-2022						PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning								Arxiv											1	1;2024-04-29;https://www.arxiv.org/abs/2404.16994v2	arXiv:2404.16994			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 29 2024	2024	Vision-language pre-training has significantly elevated performance across a wide range of image-language applications. Yet, the pre-training process for video-related tasks demands exceptionally large computational and data resources, which hinders the progress of video-language models. This paper investigates a straight-forward, highly efficient, and resource-light approach to adapting an existing image-language pre-trained model for dense video understanding. Our preliminary experiments reveal that directly fine-tuning pre-trained image-language models with multiple frames as inputs on video datasets leads to performance saturation or even a drop. Our further investigation reveals that it is largely attributed to the bias of learned high-norm visual features. Motivated by this finding, we propose a simple but effective pooling strategy to smooth the feature distribution along the temporal dimension and thus reduce the dominant impacts from the extreme features. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVA achieves new state-of-the-art performance on modern benchmark datasets for both video question-answer and captioning tasks. Notably, on the recent popular VideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average of five evaluated dimensions, exceeding the previous SOTA results from GPT4V (IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves 58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V (IG-VLM). Code is available at https://pllava.github.io/																																	2024-05-14	PPRN:88695570		
J	Guo, Shangmin; Zhang, Biao; Liu, Tianlin; Liu, Tianqi; Khalman, Misha; Llinares, Felipe; Rame, Alexandre; Mesnard, Thomas; Zhao, Yao; Piot, Bilal; Ferret, Johan; Blondel, Mathieu				Liu, Tianlin/HHM-4699-2022; Guo, Shangmin/OXC-3050-2025						Direct Language Model Alignment from Online AI Feedback								Arxiv											2	2;2024-02-29;https://www.arxiv.org/abs/2402.04792v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.04792v1	arXiv:2402.04792			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 29 2024	2024	Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.																																	2024-11-09	PPRN:87553260		
J	Qin, Zhen; Jagerman, Rolf; Hui, Kai; Zhuang, Honglei; Wu, Junru; Yan, Le; Shen, Jiaming; Liu, Tianqi; Liu, Jialu; Metzler, Donald; Wang, Xuanhui; Bendersky, Michael				LIU, JIALU/HPC-0403-2023; Hui, Kai/AAM-2600-2020; Yan, Le/G-1370-2019						Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting								Arxiv											2	2;2024-03-28;https://www.arxiv.org/abs/2306.17563v2| 1;2023-06-30;https://www.arxiv.org/abs/2306.17563v1	arXiv:2306.17563			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 28 2024	2024	Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.																																	2024-04-14	PPRN:73727885		
J	Guan, Tianrui; Liu, Fuxiao; Wu, Xiyang; Xian, Ruiqi; Li, Zongxia; Liu, Xiaoyu; Wang, Xijun; Chen, Lichang; Huang, Furong; Yacoob, Yaser; Manocha, Dinesh; Zhou, Tianyi				Guan, Tianrui/HKF-4639-2023						HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2310.14566v4| 2;2024-02-29;https://www.arxiv.org/abs/2310.14566v3| 1;2023-10-23;https://www.arxiv.org/abs/2310.14566v1	arXiv:2310.14566			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only highlights the observed failure modes, including language hallucination and visual illusion, but also deepens an understanding of these pitfalls. Our comprehensive case studies within HallusionBench shed light on the challenges of hallucination and illusion in LVLMs. Based on these insights, we suggest potential pathways for their future improvement. 																																	2024-04-11	PPRN:85758465		
J	Cheng, Kanzhi; Sun, Qiushi; Chu, Yougang; Xu, Fangzhi; Li, Yantao; Zhang, Jianbing; Wu, Zhiyong				Sun, Qiushi/LIH-5484-2024						SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/2401.10935v2| 1;2024-01-17;https://www.arxiv.org/abs/2401.10935v1	arXiv:2401.10935			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent – SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding – the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks.																																	2024-03-23	PPRN:87278336		
J	Chao, Patrick; Debenedetti, Edoardo; Robey, Alexander; Andriushchenko, Maksym; Croce, Francesco; Sehwag, Vikash; Dobriban, Edgar; Flammarion, Nicolas; Pappas, George J.; Tramer, Florian; Hassani, Hamed; Wong, Eric				Sehwag, Vikash/JAC-0941-2023						JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models								Arxiv											5	5;2024-10-31;https://www.arxiv.org/abs/2404.01318v5| 4;2024-07-16;https://www.arxiv.org/abs/2404.01318v4| 3;2024-06-16;https://www.arxiv.org/abs/2404.01318v3| 2;2024-04-23;https://www.arxiv.org/abs/2404.01318v2| 1;2024-03-28;https://www.arxiv.org/abs/2404.01318v1	arXiv:2404.01318			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.																																	2024-12-06	PPRN:88378137		
J	Wu, Likang; Zheng, Zhi; Qiu, Zhaopeng; Wang, Hao; Gu, Hongchao; Shen, Tingjia; Qin, Chuan; Zhu, Chen; Zhu, Hengshu; Liu, Qi; Xiong, Hui; Chen, Enhong				Zheng, Yefeng/ABG-7053-2020; Zhu, Hengshu/AAB-2816-2021; Zheng, Zhigang/JEM-5585-2023						A Survey on Large Language Models for Recommendation								Arxiv											2	2;2024-06-18;https://www.arxiv.org/abs/2305.19860v5| 1;2023-05-31;https://www.arxiv.org/abs/2305.19860v2	arXiv:2305.19860			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation.*																																	2024-07-04	PPRN:72812131		
J	Kim, Seungone; Suk, Juyoung; Longpre, Shayne; Lin, Bill Yuchen; Shin, Jamin; Welleck, Sean; Neubig, Graham; Lee, Moontae; Lee, Kyungjae; Seo, Minjoon										Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models								Arxiv											1	1;2024-05-02;https://www.arxiv.org/abs/2405.01535v1	arXiv:2405.01535			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 02 2024	2024	Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of opensource LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria , focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than it’s predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, P ROMETHEUS 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available 1 .																																	2024-05-19	PPRN:88722344		
J	Kaufmann, Timo; Weng, Paul; Bengs, Viktor; Huellermeier, Eyke				Weng, Paul/AAE-4430-2020						A Survey of Reinforcement Learning from Human Feedback								Arxiv											2	2;2024-04-30;https://www.arxiv.org/abs/2312.14925v2| 1;2023-12-22;https://www.arxiv.org/abs/2312.14925v1	arXiv:2312.14925			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 30 2024	2024	Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.																																	2025-09-11	PPRN:86782653		
J	Wang, Guan; Cheng, Sijie; Zhan, Xianyuan; Li, Xiangang; Song, Sen; Liu, Yang				Zhan, Xianyuan/AAG-4773-2021; Li, Xiang/B-8954-2012						OpenChat: Advancing Open-source Language Models with Mixed-Quality Data								Arxiv											2	2;2024-03-16;https://www.arxiv.org/abs/2309.11235v2| 1;2023-09-20;https://www.arxiv.org/abs/2309.11235v1	arXiv:2309.11235			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 16 2024	2024	Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.																																	2024-04-11	PPRN:85072197		
J	Fargues, Laurent; Scholze, Peter										Geometrization of the local Langlands correspondence								Arxiv											1	1;2024-01-03;https://www.arxiv.org/abs/2102.13459v3	arXiv:2102.13459			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 03 2024	2024	Following the idea of [Far16], we develop the foundations of the geometric Langlands program on the Fargues–Fontaine curve. In particular, we define a category of ℓ-adic sheaves on the stack BunG of G-bundles on the Fargues–Fontaine curve, prove a geometric Satake equivalence over the Fargues–Fontaine curve, and study the stack of L-parameters. As applications, we prove finiteness results for the cohomology of local Shimura varieties and general moduli spaces of local shtukas, and define L-parameters associated with irreducible smooth representations of G(E), a map from the spectral Bernstein center to the Bernstein center, and the spectral action of the category of perfect complexes on the stack of L-parameters on the category of ℓ-adic sheaves on BunG.																																	2024-01-13	PPRN:86970861		
J	Wang, Zekun Moore; Peng, Zhongyuan; Que, Haoran; Liu, Jiaheng; Zhou, Wangchunshu; Wu, Yuhan; Guo, Hongcheng; Gan, Ruitong; Ni, Zehao; Yang, Jian; Zhang, Man; Zhang, Zhaoxiang; Ouyang, Wanli; Xu, Ke; Huang, Stephen W.; Fu, Jie; Peng, Junran				Yang, Jianyu/KBD-3342-2024; Zhang, Zhao-xiang/R-2819-2018						RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models								Arxiv											3	3;2024-06-18;https://www.arxiv.org/abs/2310.00746v3| 2;2024-04-24;https://www.arxiv.org/abs/2310.00746v2| 1;2023-10-01;https://www.arxiv.org/abs/2310.00746v1	arXiv:2310.00746			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).																																	2024-07-06	PPRN:85355364		
J	Yao, Yuanshun; Xu, Xiaojun; Liu, Yang				Xu, Xiaojun/KAL-6882-2024						Large Language Model Unlearning								Arxiv											2	2;2024-02-16;https://www.arxiv.org/abs/2310.10683v2| 1;2023-10-14;https://www.arxiv.org/abs/2310.10683v1	arXiv:2310.10683			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 16 2024	2024	We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright -protected content, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human -written) examples required in RLHF (reinforcement learning from human feedback). (2) It is computationally efficient; the cost is comparable to light supervised finetuning. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.																																	2024-03-27	PPRN:85672234		
J	Zhu, Wenhao; Liu, Hongyi; Dong, Qingxiu; Xu, Jingjing; Huang, Shujian; Kong, Lingpeng; Chen, Jiajun; Li, Lei				kong, lingpeng/NHQ-3170-2025; zhu, wenhao/IJR-0851-2023; Xu, Jingjing/ACJ-3010-2022; El-Ashram, Saeed/AAC-6060-2021; Liu, Hongyi/AAE-9481-2020						Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis								Arxiv											3	3;2024-06-14;https://www.arxiv.org/abs/2304.04675v4| 2;2023-10-29;https://www.arxiv.org/abs/2304.04675v3| 1;2023-04-10;https://www.arxiv.org/abs/2304.04675v1	arXiv:2304.04675			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs1.																																	2024-07-02	PPRN:57493189		
J	Ma, Shuming; Wang, Hongyu; Ma, Lingxiao; Wang, Lei; Wenhui, Wang; Huang, Shaohan; Dong, Li; Wang, Ruiping; Xue, Jilong; Wei, Furu				Wang, Ruiping/LUW-7972-2024; Wang, Hongyu/OIU-9637-2025; Huang, Shaohan/LDF-3300-2024						The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17764v1	arXiv:2402.17764			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	Recent research, such as BitNet [WMD+23], is paving the way for a new era of 1bit Large Language Models (LLMs). In this work, we introduce a 1 -bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full -precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end -task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58 -bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1 -bit LLMs. {GRAPHICS]																																	2024-03-27	PPRN:87922022		
J	Xu, Zhangchen; Jiang, Fengqing; Niu, Luyao; Deng, Yuntian; Poovendran, Radha; Choi, Yejin; Lin, Bill Yuchen				Xu, Zhangchen/IYJ-6907-2023; Deng, yuntian/KIB-9835-2024; Niu, Luyao/AEN-7350-2022						Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing								Arxiv											2	2;2024-10-07;https://www.arxiv.org/abs/2406.08464v2| 1;2024-06-12;https://www.arxiv.org/abs/2406.08464v1	arXiv:2406.08464			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 07 2024	2024	High-quality instruction datais critical for aligning large language models (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI. High human labor costs and a limited, predefined scope for prompting prevent existing opensource data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets. Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned LLM? We present a self-synthesis method for generating large-scale alignment data named M AGPIE . Our key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when we input only the pre-query templates up to the position reserved for user messages, thanks to their auto-regressive nature. We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses. We further introduce extensions of M AG- PIE for filtering, generating multi-turn, preference optimization, domain-specific and multilingual datasets. We perform a comprehensive analysis of the M AGPIE- generated data. To compare M AGPIE-generated data with other public instruction datasets (e.g., ShareGPT, WildChat, Evol-Instruct, UltraChat, OpenHermes, TuluV2-Mix, GenQA), we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models. Our results indicate that using M AGPIE for supervised fine-tuning (SFT) solely can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback. We also show that in some tasks, models supervised fine-tuned with M AGPIE perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through SFT and subsequent preference optimization. This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.																																	2024-10-27	PPRN:89288873		
J	Gur, Izzeddin; Furuta, Hiroki; Huang, Austin; Safdari, Mustafa; Matsuo, Yutaka; Eck, Douglas; Faust, Aleksandra										A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis								Arxiv											3	3;2024-02-25;https://www.arxiv.org/abs/2307.12856v4| 2;2023-10-03;https://www.arxiv.org/abs/2307.12856v3| 1;2023-07-24;https://www.arxiv.org/abs/2307.12856v1	arXiv:2307.12856			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 25 2024	2024	Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.																																	2024-03-28	PPRN:74086315		
J	Zhao, Jiawei; Zhang, Zhenyu; Chen, Beidi; Wang, Zhangyang; Anandkumar, Anima; Tian, Yuandong				Zhihua, Wang/AFO-5263-2022; Zhang, Zhenyu/ISS-1688-2023; Zhao, Jiawei/ABG-6285-2021						GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection								Arxiv											2	2;2024-06-02;https://www.arxiv.org/abs/2403.03507v2| 1;2024-03-06;https://www.arxiv.org/abs/2403.03507v1	arXiv:2403.03507			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 02 2024	2024	Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.																																	2024-06-19	PPRN:88047649		
J	Xu, Zhi-Qin John; Zhang, Yaoyu; Luo, Tao; Xiao, Yanyang; Ma, Zheng										Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks								Arxiv											2	2;2024-05-17;https://www.arxiv.org/abs/1901.06523v7| 1;2024-05-15;https://www.arxiv.org/abs/1901.06523v6	arXiv:1901.06523			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 17 2024	2024	We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) - DNNs often fit target functions from low to high frequencies - on high-dimensional benchmark datasets such as MNIST/CIFAR10 and deep neural networks such as VGG16. This F-Principle of DNNs is opposite to the behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibit faster convergence for higher frequencies for various scientific computing problems. With a simple theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.																																	2024-06-12	PPRN:88929621		
J	Lin, Xinqi; He, Jingwen; Chen, Ziyan; Lyu, Zhaoyang; Dai, Bo; Yu, Fanghua; Ouyang, Wanli; Qiao, Yu; Dong, Chao				Qiao, Yu/ABD-5787-2021; He, Jingwen/HTM-5599-2023; Ouyang, Wanli/I-7135-2018						DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior								Arxiv											3	3;2024-04-12;https://www.arxiv.org/abs/2308.15070v3| 2;2024-03-31;https://www.arxiv.org/abs/2308.15070v2| 1;2023-08-29;https://www.arxiv.org/abs/2308.15070v1	arXiv:2308.15070			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Apr 12 2024	2024	We present DiffBIR, a general restoration pipeline that could handle different blind image restoration tasks in a unified framework. DiffBIR decouples blind image restoration problem into two stages: 1) degradation removal: removing image-independent content; 2) information regeneration: generating the lost image content. Each stage is developed independently but they work seamlessly in a cascaded manner. In the first stage, we use restoration modules to remove degradations and obtain high-fidelity restored results. For the second stage, we propose IRControlNet that leverages the generative ability of latent diffusion models to generate realistic details. Specifically, IRControlNet is trained based on specially produced condition images without distracting noisy content for stable generation performance. Moreover, we design a region-adaptive restoration guidance that can modify the denoising process during inference without model re-training, allowing users to balance realness and fidelity through a tunable guidance scale. Extensive experiments have demonstrated DiffBIR's superiority over state-of-the-art approaches for blind image super-resolution, blind face restoration and blind image denoising tasks on both synthetic and real-world datasets. 																																	2024-04-26	PPRN:84520927		
J	Hou, Yupeng; Li, Jiacheng; He, Zhankui; Yan, An; Chen, Xiusi; McAuley, Julian				Hou, Yupeng/GNP-3072-2022; Yan, An/GLT-4045-2022						Bridging Language and Items for Retrieval and Recommendation								Arxiv											1	1;2024-03-06;https://www.arxiv.org/abs/2403.03952v1	arXiv:2403.03952			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 06 2024	2024	This paper introduces BLAIR, a series of pretrained sentence embedding models specialized for recommendation scenarios. BLAIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items. To pretrain BLAIR, we collect AMA- ZON REVIEWS 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions. We evaluate the generalization ability of BLAIR across multiple domains and tasks, including a new task named complex product search, referring to retrieving relevant items given long, complex natural language contexts. Leveraging large language models like ChatGPT, we correspondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical results on the new task, as well as conventional retrieval and recommendation tasks, demonstrate that BLAIR exhibit strong text and item representation capacity. 																																	2024-05-01	PPRN:88047575		
J	Escriva, Albert; Kuehnel, Florian; Tada, Yuichiro				Tada, Yuichiro/C-2894-2013						Primordial Black Holes								Arxiv											2	2;2024-02-10;https://www.arxiv.org/abs/2211.05767v4| 1;2022-11-10;https://www.arxiv.org/abs/2211.05767v2	arXiv:2211.05767			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 10 2024	2024	Aspects of primordial black holes, i.e. black holes formed in the early Universe, are reviewed. Special emphasis is put on their formation, their role as dark matter candidates and their manifold signatures, particularly through gravitational waves.																																	2024-04-07	PPRN:36115025		
J	Li, Yuanchun; Wen, Hao; Wang, Weijun; Li, Xiangyu; Yuan, Yizhen; Liu, Guohong; Liu, Jiacheng; Xu, Wenxing; Wang, Xiang; Sun, Yi; Kong, Rui; Wang, Yile; Geng, Hanfei; Luan, Jian; Jin, Xuefeng; Ye, Zilong; Xiong, Guanjing; Zhang, Fan; Li, Xiang; Xu, Mengwei; Li, Zhijun; Li, Peng; Liu, Yang; Zhang, Ya-Qin; Liu, Yunxin				LI, Xiangyu/AAS-2293-2020; Xu, Mengwei/AAE-5567-2020; Wang, Weijun/IUM-6548-2023; Liu, Guohong/GZA-9206-2022; Li, Yuanchun/LTD-1972-2024; Liu, Yunxin/LCD-3829-2024; Xu, Wenxing/HGC-3794-2022; Wen, Hao/MEO-8739-2025						PERSONAL LLM AGENTS : INSIGHTS AND SURVEY ABOUT THE CAPABILITY , EFFICIENCY AND SECURITY								Arxiv											2	2;2024-05-08;https://www.arxiv.org/abs/2401.05459v2| 1;2024-01-10;https://www.arxiv.org/abs/2401.05459v1	arXiv:2401.05459			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 08 2024	2024	Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.																																	2024-05-27	PPRN:87124761		
J	Zhou, Zixuan; Ning, Xuefei; Hong, Ke; Fu, Tianyu; Xu, Jiaming; Li, Shiyao; Lou, Yuming; Wang, Luning; Yuan, Zhihang; Li, Xiuhong; Yan, Shengen; Dai, Guohao; Zhang, Xiao-Ping; Dong, Yuhan; Wang, Yu				Wang, Luning/B-2491-2012; Yuan, Zhihang/HDN-8259-2022; Li, Shiyao/OYE-4903-2025; Hong, Ke/KIB-3489-2024; Dong, Yuhan/H-6499-2011; Zhou, Zixuan/KEI-0421-2024; Zhang, Xiao-Ping (Steven)/B-1436-2016; Li, Xiuhong/ABB-8030-2020; Fu, Tianyu/KHY-8364-2024; WANG, Yu/B-7985-2011						A Survey on Efficient Inference for Large Language Models								Arxiv											1	1;2024-04-22;https://www.arxiv.org/abs/2404.14294v1	arXiv:2404.14294			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 22 2024	2024	Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.																																	2024-04-30	PPRN:88600956		
J	Mckinzie, Brandon; Gan, Zhe; Fauconnier, Jean-Philippe; Dodge, Sam; Zhang, Bowen; Dufter, Philipp; Shah, Dhruti; Du, Xianzhi; Peng, Futang; Weers, Floris; Belyi, Anton; Zhang, Haotian; Singh, Karanjeet; Kang, Doug; Jain, Ankur; He, Hongyu; Schwarzer, Max; Gunter, Tom; Kong, Xiang; Zhang, Aonan; Wang, Jianyu; Wang, Chong; Du, Nan; Lei, Tao; Wiseman, Sam; Yin, Guoli; Lee, Mark; Wang, Zirui; Pang, Ruoming; Grasch, Peter; Toshev, Alexander; Yang, Yinfei				Dodge, Somayeh/D-3293-2013; Kanel-Belov, Alexey/K-7999-2012; McKinzie, Brandon/JAC-3490-2023						MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training								Arxiv											4	4;2024-04-18;https://www.arxiv.org/abs/2403.09611v4| 3;2024-03-22;https://www.arxiv.org/abs/2403.09611v3| 2;2024-03-19;https://www.arxiv.org/abs/2403.09611v2| 1;2024-03-14;https://www.arxiv.org/abs/2403.09611v1	arXiv:2403.09611			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 18 2024	2024	In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.																																	2024-04-30	PPRN:88145199		
J	Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Xu, Tong; Wang, Hao; Sui, Dianbo; Shen, Yunhang; Li, Ke; Sun, Xing; Chen, Enhong				Li, Ke/KSM-7426-2024; Yin, Shukang/JCP-4961-2023; Shen, Yunhang/ADW-0834-2022; Sui, Dianbo/IXN-7153-2023; Zheng, Yefeng/ABG-7053-2020; Zhao, Sirui/OYE-9902-2025						Woodpecker: Hallucination Correction for Multimodal Large Language Models								Arxiv											2	2;2024-12-11;https://www.arxiv.org/abs/2310.16045v2| 1;2023-10-24;https://www.arxiv.org/abs/2310.16045v1	arXiv:2310.16045			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 11 2024	2024	Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. 																																	2025-01-19	PPRN:85771577		
J	Shang, Yuzhang; Cai, Mu; Xu, Bingxin; Lee, Yong Jae; Yan, Yan				Cai, Mu/AAD-8827-2022; Shang, Yuzhang/HTO-5198-2023						LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models								Arxiv											4	4;2024-05-22;https://www.arxiv.org/abs/2403.15388v5| 3;2024-04-12;https://www.arxiv.org/abs/2403.15388v4| 2;2024-04-01;https://www.arxiv.org/abs/2403.15388v3| 1;2024-03-25;https://www.arxiv.org/abs/2403.15388v2	arXiv:2403.15388			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select the unpruned visual tokens based on their similarity to class tokens and spatial tokens. We then cluster the pruned tokens based on key similarity and merge the clustered tokens with the unpruned tokens to supplement their information. Empirically, when applied to LLaVA-1.5, our approach can compress the visual tokens by 18 times on average, and achieve comparable performance across diverse visual question-answering and reasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.																																	2025-08-07	PPRN:88279334		
J	He, Hongliang; Yao, Wenlin; Ma, Kaixin; Yu, Wenhao; Dai, Yong; Zhang, Hongming; Lan, Zhenzhong; Yu, Dong				Zhang, Hongming/ABF-8690-2021; He, Hongliang He/Y-9779-2019						WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models								Arxiv											4	4;2024-06-06;https://www.arxiv.org/abs/2401.13919v4| 3;2024-02-29;https://www.arxiv.org/abs/2401.13919v3| 2;2024-01-28;https://www.arxiv.org/abs/2401.13919v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.13919v1	arXiv:2401.13919			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.																																	2024-06-22	PPRN:87337433		
J	Garza, Azul; Challu, Cristian; Mergenthaler-Canseco, Max										A TimeGPT-1								Arxiv											3	3;2024-05-27;https://www.arxiv.org/abs/2310.03589v3| 2;2023-10-05;https://www.arxiv.org/abs/2310.03589v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03589v1	arXiv:2310.03589			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 27 2024	2024	In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.																																	2024-06-13	PPRN:85435139		
J	Zhu, Bin; Lin, Bin; Ning, Munan; Yan, Yang; Cui, Jiaxi; Wang, Hongfa; Pang, Yatian; Jiang, Wenhao; Zhang, Junwu; Li, Zongwei; Zhang, Wancai; Li, Zhifeng; Liu, Wei; Yuan, Li				Yuan, Li/AET-1324-2022; Lin, Bin/E-9871-2010; li, zongwei/GWU-5748-2022; Wang, Yu-Zhong/S-5172-2019; jiang, wenhao/IXN-4023-2023; cui, jiaxi/H-8790-2015; zhang, junwu/IWM-6534-2023; Liu, Wei/L-1951-2019						LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment								Arxiv											6	6;2024-01-22;https://www.arxiv.org/abs/2310.01852v7| 5;2023-11-27;https://www.arxiv.org/abs/2310.01852v6| 4;2023-11-04;https://www.arxiv.org/abs/2310.01852v5| 3;2023-10-23;https://www.arxiv.org/abs/2310.01852v4| 2;2023-10-14;https://www.arxiv.org/abs/2310.01852v3| 1;2023-10-04;https://www.arxiv.org/abs/2310.01852v2	arXiv:2310.01852			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 22 2024	2024	The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N>=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with complete semantics rather than truncated segments from long videos, and all the video, depth, infrared, and audio modalities are aligned to their textual descriptions. LanguageBind has achieved superior performance on a wide range of 15 benchmarks covering video, audio, depth, and infrared. Moreover, multiple experiments have provided evidence for the effectiveness of LanguageBind in achieving indirect alignment and complementarity among diverse modalities. 																																	2024-02-06	PPRN:85399719		
J	Dong, Hanze; Xiong, Wei; Pang, Bo; Wang, Haoxiang; Zhao, Han; Zhou, Yingbo; Jiang, Nan; Sahoo, Doyen; Xiong, Caiming; Zhang, Tong				Zhang, Tong/HGC-1090-2022; Zhou, Yingbo/MSX-1381-2025						RLHF Workflow: From Reward Modeling to Online RLHF								Arxiv											3	3;2024-11-12;https://www.arxiv.org/abs/2405.07863v3| 2;2024-06-12;https://www.arxiv.org/abs/2405.07863v2| 1;2024-05-13;https://www.arxiv.org/abs/2405.07863v1	arXiv:2405.07863			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 12 2024	2024	We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. 																																	2024-12-18	PPRN:89033116		
J	Xu, Haoran; Sharaf, Amr; Chen, Yunmo; Tan, Weiting; Shen, Lingfeng; Van Durme, Benjamin; Murray, Kenton; Kim, Young Jin				Chen, Yunmo/KRP-9297-2024; Xu, Haoran/AEW-7367-2022						Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation								Arxiv											4	4;2024-06-03;https://www.arxiv.org/abs/2401.08417v4| 3;2024-02-02;https://www.arxiv.org/abs/2401.08417v3| 2;2024-01-18;https://www.arxiv.org/abs/2401.08417v2| 1;2024-01-16;https://www.arxiv.org/abs/2401.08417v1	arXiv:2401.08417			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	Moderate -sized large language models (LLMs) – those with 7B or 13B parameters – exhibit promising machine translation (MT) performance. However, they do not match the performance of state-of-the-art conventional encoder -decoder translation models or larger -scale LLMs such as GPT-4 (OpenAI, 2023). In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human -generated. Then, in contrast to supervised fine-tuning which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), , a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA (Xu et al., 2023) models with only 22K parallel sentences and tuning only 0.1% parameters yields significant improvements. The resulting model, called ALMA -R , can match or exceed the performance of the WMT competition winners and GPT-4 on WMT’21, WMT’22 and WMT’23 test datasets.																																	2024-06-22	PPRN:87190258		
J	Yi, Taoran; Fang, Jiemin; Wang, Junjie; Wu, Guanjun; Xie, Lingxi; Zhang, Xiaopeng; Liu, Wenyu; Tian, Qi; Wang, Xinggang				Xie, Lingxi/ABF-6996-2020; Yi, Tao/MHP-8239-2025; Wang, Xinggang/LSL-0946-2024; Wang, Junjie/PBU-8326-2025; Wenyu, Liu/GRS-3009-2022; wu, guanjun/LEN-0332-2024						GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models								Arxiv											3	3;2024-05-13;https://www.arxiv.org/abs/2310.08529v3| 2;2023-12-05;https://www.arxiv.org/abs/2310.08529v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08529v1	arXiv:2310.08529			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 13 2024	2024	In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. 																																	2024-06-08	PPRN:85603001		
J	Hu, Jian; Wu, Xibin; Zhu, Zilin; Xianyu; Wang, Weixun; Zhang, Dehao; Cao, Yu										OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework								Arxiv											4	4;2024-11-24;https://www.arxiv.org/abs/2405.11143v4| 3;2024-07-17;https://www.arxiv.org/abs/2405.11143v3| 2;2024-06-03;https://www.arxiv.org/abs/2405.11143v2| 1;2024-05-20;https://www.arxiv.org/abs/2405.11143v1	arXiv:2405.11143			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 24 2024	2024	As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance. However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models. We present OpenRLHF, an open-source framework enabling efficient RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques.																																	2025-01-08	PPRN:89093624		
J	Huh, Minyoung; Cheung, Brian; Wang, Tongzhou; Isola, Phillip				HUH, MINYOUNG/GWN-1313-2022; Wang, Tongzhou/AAT-5332-2021						The Platonic Representation Hypothesis								Arxiv											1	1;2024-05-13;https://www.arxiv.org/abs/2405.07987v1	arXiv:2405.07987			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 13 2024	2024	We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato’s concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis. Project Page: phillipi.github.io/prh Code: github.com/minyoungg/platonic-rep																																	2024-06-08	PPRN:89036658		
J	Packer, Charles; Wooders, Sarah; Lin, Kevin; Fang, Vivian; Patil, Shishir G.; Stoica, Ion; Gonzalez, Joseph E.										MemGPT: Towards LLMs as Operating Systems								Arxiv											2	2;2024-02-12;https://www.arxiv.org/abs/2310.08560v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08560v1	arXiv:2310.08560			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 12 2024	2024	Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems which provide the illusion of an extended virtual memory via paging between physical memory and disk. Using this technique, we introduce MemGPT (MemoryGPT), a system that intelligently manages different storage tiers in order to effectively provide extended context within the LLM’s limited context window. We evaluate our OS -inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM’s context window, and multi -session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://research.memgpt.ai.																																	2024-05-25	PPRN:85602679		
J	Cai, Weilin; Jiang, Juyong; Wang, Fan; Tang, Jing; Kim, Sunghun; Huang, Jiayi										A Survey on Mixture of Experts								Arxiv											2	2;2024-08-08;https://www.arxiv.org/abs/2407.06204v2| 1;2024-06-26;https://www.arxiv.org/abs/2407.06204v1	arXiv:2407.06204			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 08 2024	2024	Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. 																																	2024-08-18	PPRN:90752767		
J	Park, Kiho; Choe, Yo Joong; Veitch, Victor				Choe, Yo Joong/AFS-4090-2022						The Linear Representation Hypothesis and the Geometry of Large Language Models								Arxiv											2	2;2024-07-17;https://www.arxiv.org/abs/2311.03658v2| 1;2023-11-07;https://www.arxiv.org/abs/2311.03658v1	arXiv:2311.03658			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 17 2024	2024	Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.																																	2024-07-26	PPRN:86066968		
J	Xia, Mengzhou; Malladi, Sadhika; Gururangan, Suchin; Arora, Sanjeev; Chen, Danqi				CHEN, DANQI/C-6441-2013						LESS: Selecting Influential Data for Targeted Instruction Tuning								Arxiv											3	3;2024-06-13;https://www.arxiv.org/abs/2402.04333v3| 2;2024-02-20;https://www.arxiv.org/abs/2402.04333v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.04333v1	arXiv:2402.04333			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 13 2024	2024	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application.																																	2024-07-02	PPRN:87562006		
J	Gurnee, Wes; Tegmark, Max										Language Models Represent Space and Time								Arxiv											3	3;2024-03-04;https://www.arxiv.org/abs/2310.02207v3| 2;2023-12-14;https://www.arxiv.org/abs/2310.02207v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02207v1	arXiv:2310.02207			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 04 2024	2024	The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.																																	2024-03-31	PPRN:85377949		
J	Tochilkin, Dmitry; Pankratz, David; Liu, Zexiang; Huang, Zixuan; Letts, Adam; Li, Yangguang; Liang, Ding; Laforte, Christian; Jampani, Varun; Cao, Yan-Pei				Jain, Varun/HHN-1250-2022; Huang, Zixuan/AID-4221-2022						TripoSR: Fast 3D Object Reconstruction from a Single Image								Arxiv											1	1;2024-03-04;https://www.arxiv.org/abs/2403.02151v1	arXiv:2403.02151			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 04 2024	2024	This technical report introduces TripoSR, a 3D reconstruction model leveraging transformer architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the LRM network architecture, TripoSR integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that TripoSR exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the MIT license, TripoSR is intended to empower researchers, developers, and creatives with the latest advancements in 3D generative AI.																																	2024-03-30	PPRN:88020486		
J	Gou, Zhibin; Shao, Zhihong; Gong, Yeyun; Shen, Yelong; Yang, Yujiu; Huang, Minlie; Duan, Nan; Chen, Weizhu				Yang, Yujiu/JGM-0303-2023; Duan, Nan/AAR-2231-2020						ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving								Arxiv											4	4;2024-02-21;https://www.arxiv.org/abs/2309.17452v4| 3;2024-02-16;https://www.arxiv.org/abs/2309.17452v3| 2;2023-10-04;https://www.arxiv.org/abs/2309.17452v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17452v1	arXiv:2309.17452			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.																																	2024-03-20	PPRN:85331504		
J	Park, Joon Sung; Zou, Carolyn Q.; Shaw, Aaron; Hill, Benjamin Mako; Cai, Carrie; Morris, Meredith Ringel; Willer, Robb; Liang, Percy; Bernstein, Michael S.				Hill, Benjamin/AAE-8825-2019						Generative Agent Simulations of 1,000 People								Arxiv											1	1;2024-11-15;https://www.arxiv.org/abs/2411.10109v1	arXiv:2411.10109			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 15 2024	2024	The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.																																	2024-12-27	PPRN:119243954		
J	Berglund, Lukas; Tong, Meg; Kaufmann, Max; Balesni, Mikita; Stickland, Asa Cooper; Korbak, Tomasz; Evans, Owain										The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"								Arxiv											3	3;2024-05-26;https://www.arxiv.org/abs/2309.12288v4| 2;2024-04-04;https://www.arxiv.org/abs/2309.12288v3| 1;2023-09-22;https://www.arxiv.org/abs/2309.12288v2	arXiv:2309.12288			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 26 2024	2024	We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Valentina Tereshkova was the first woman to travel to space", it will not automatically be able to answer the question, "Who was the first woman to travel to space?". Moreover, the likelihood of the correct answer ("Valentina Tershkova") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if "A is B" occurs, "B is A" is more likely to occur. It is worth noting, however, that if "A is B" appears in-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of Abyssal Melodies" and showing that they fail to correctly answer "Who composed Abyssal Melodies?". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter. 																																	2024-06-11	PPRN:85203552		
J	Yan, Shi-Qi; Gu, Jia-Chen; Zhu, Yun; Ling, Zhen-Hua										Corrective Retrieval Augmented Generation								Arxiv											3	3;2024-10-07;https://www.arxiv.org/abs/2401.15884v3| 2;2024-02-16;https://www.arxiv.org/abs/2401.15884v2| 1;2024-01-29;https://www.arxiv.org/abs/2401.15884v1	arXiv:2401.15884			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 07 2024	2024	Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the C orrective R etrieval A ugmented G eneration (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return suboptimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-thenrecompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.																																	2024-11-09	PPRN:87392794		
J	Zou, Wei; Geng, Runpeng; Wang, Binghui; Jia, Jinyuan				Jia, Jinyuan/AAQ-5278-2020; Wang, Binghui/Y-8424-2019						PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models								Arxiv											2	2;2024-08-13;https://www.arxiv.org/abs/2402.07867v3| 1;2024-02-12;https://www.arxiv.org/abs/2402.07867v1	arXiv:2402.07867			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 13 2024	2024	Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.1																																	2024-08-22	PPRN:87631650		
J	Jiang, Huiqiang; Wu, Qianhui; Luo, Xufang; Li, Dongsheng; Lin, Chin-Yew; Yang, Yuqing; Qiu, Lili				Jiang, Huiqiang/KHX-2210-2024; yang, yuqing/GRJ-8747-2022						<italic>LongLLMLingua</italic>: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression								Arxiv											2	2;2024-08-12;https://www.arxiv.org/abs/2310.06839v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06839v1	arXiv:2310.06839			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 12 2024	2024	In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs’ perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x. 																																	2024-08-22	PPRN:85523415		
J	Yao, Jia-Yu; Ning, Kun-Peng; Liu, Zhen-Hui; Ning, Mu-Nan; Liu, Yu-Yang; Yuan, Li				Yuan, Li/AET-1324-2022; yao, jiayu/GZK-4322-2022						LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples								Arxiv											2	2;2024-08-04;https://www.arxiv.org/abs/2310.01469v3| 1;2023-10-04;https://www.arxiv.org/abs/2310.01469v2	arXiv:2310.01469			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 04 2024	2024	Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still cannot completely trust their answers, since LLMs suffer from hallucination—fabricating non-existent facts, deceiving users with or without their awareness. However, the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsensical prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. Moreover, we provide both theoretical and experimental evidence that transformers can be manipulated to produce specific pre-define tokens by perturbing its input sequence. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, , and it shares similar characteristics with conventional adversarial examples as a basic property of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore the basic properties of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub3. 																																	2024-08-09	PPRN:85398769		
J	Labrak, Yanis; Bazoge, Adrien; Morin, Emmanuel; Gourraud, Pierre-Antoine; Rouvier, Mickael; Dufour, Richard				Gourraud, Pierre-Antoine/O-3024-2015						BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains								Arxiv											3	3;2024-07-17;https://www.arxiv.org/abs/2402.10373v3| 2;1800-01-01;https://www.arxiv.org/abs/2402.10373v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.10373v1	arXiv:2402.10373			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jul 17 2024	2024	Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.																																	2024-07-25	PPRN:87729292		
J	Pal, Arka; Karkhanis, Deep; Dooley, Samuel; Roberts, Manley; Naidu, Siddartha; White, Colin										Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2402.13228v2| 1;2024-02-20;https://www.arxiv.org/abs/2402.13228v1	arXiv:2402.13228			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.																																	2024-07-20	PPRN:87776609		
J	Xiao, Guangxuan; Lin, Ji; Seznec, Mickael; Wu, Hao; Demouth, Julien; Han, Song										SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models								Arxiv											3	3;2024-03-29;https://www.arxiv.org/abs/2211.10438v7| 2;2024-03-26;https://www.arxiv.org/abs/2211.10438v6| 1;2022-11-18;https://www.arxiv.org/abs/2211.10438v2	arXiv:2211.10438			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. 																																	2024-04-18	PPRN:23220846		
J	Liu, Tianqi; Zhao, Yao; Joshi, Rishabh; Khalman, Misha; Saleh, Mohammad; Liu, Peter J.; Liu, Jialu				LIU, JIALU/HPC-0403-2023						Statistical Rejection Sampling Improves Preference Optimization								Arxiv											2	2;2024-01-23;https://www.arxiv.org/abs/2309.06657v2| 1;2023-09-13;https://www.arxiv.org/abs/2309.06657v1	arXiv:2309.06657			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 23 2024	2024	Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted to sampling preference pairs only from the SFT policy. To address these limitations, we introduce a novel approach called Statistical Rejection Sampling Optimization (RSO) that aims to source preference data from the target optimal policy using rejection sampling, enabling a more accurate estimation of the optimal policy. We also propose a unified framework that enhances the loss functions used in both SLiC and DPO from a preference modeling standpoint. Through extensive experiments across three diverse tasks, we demonstrate that RSO consistently outperforms both SLiC and DPO on evaluations from both Large Language Model (LLM) and human raters.																																	2024-02-10	PPRN:85005970		
J	Li, Yuhong; Huang, Yingbing; Yang, Bowen; Venkitesh, Bharat; Locatelli, Acyr; Ye, Hanchen; Cai, Tianle; Lewis, Patrick; Chen, Deming				Ye, Hanchen/AEU-7219-2022						SnapKV: LLM Knows What You are Looking for Before Generation								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2404.14469v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.14469v1	arXiv:2404.14469			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.																																	2024-07-04	PPRN:88626844		
J	Zhang, Zeyu; Bo, Xiaohe; Ma, Chen; Li, Rui; Chen, Xu; Dai, Quanyu; Zhu, Jieming; Dong, Zhenhua; Wen, Ji-Rong				dong, zhenhua/HOC-9104-2023; Chen, Xu/MTC-1347-2025; Zhu, Youwen/Q-7799-2019						A Survey on the Memory Mechanism of Large Language Model based Agents								Arxiv											1	1;2024-04-21;https://www.arxiv.org/abs/2404.13501v1	arXiv:2404.13501			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 21 2024	2024	Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.																																	2024-05-01	PPRN:88607205		
J	Qin, Yujia; Hu, Shengding; Lin, Yankai; Chen, Weize; Ding, Ning; Cui, Ganqu; Zeng, Zheni; Zhou, Xuanhe; Huang, Yufei; Xiao, Chaojun; Han, Chi; Fung, Yi Ren; Su, Yusheng; Wang, Huadong; Qian, Cheng; Tian, Runchu; Zhu, Kunlun; Liang, Shihao; Shen, Xingyu; Xu, Bokai; Zhang, Zhen; Ye, Yining; Li, Bowen; Tang, Ziwei; Yi, Jing; Zhu, Yuzhang; Dai, Zhenning; Yan, Lan; Cong, Xin; Lu, Yaxi; Zhao, Weilin; Huang, Yuxiang; Yan, Junxi; Han, Xu; Sun, Xian; Li, Dahai; Phang, Jason; Yang, Cheng; Wu, Tongshuang; Ji, Heng; Li, Guoliang; Liu, Zhiyuan; Sun, Maosong				xu, bokai/JRY-3788-2023; Hu, Shengding/JRY-6064-2023; Li, Guoliang/E-3481-2012; Liang, Shihao/IIW-5207-2023; Ye, Yining/JPL-4050-2023; han, chi/LPR-1781-2024; Li, Dahai/HCH-7167-2022; Ouyang, Zhengxiao/Q-1461-2019; Liu, Zhiyuan/I-2233-2014; zhao, weilin/JMC-5864-2023; Su, Yusheng/ITU-1189-2023						Tool Learning with Foundation Models								Arxiv											2	2;2024-08-06;https://www.arxiv.org/abs/2304.08354v3| 1;2023-04-17;https://www.arxiv.org/abs/2304.08354v1	arXiv:2304.08354			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 06 2024	2024	Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of recent powerful foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as tool learning with foundation models, , combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation and comprehensive review of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. We recapitulate existing tool learning research and formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning, such as ensuring safe and trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this paper could inspire future research in integrating tools with foundation models. Relevant codes and datasets are publicly available for further research exploration1.																																	2024-08-13	PPRN:64088880		
J	Lai, Xin; Tian, Zhuotao; Chen, Yukang; Yang, Senqiao; Peng, Xiangru; Jia, Jiaya				Tian, Zhuotao/HJP-1597-2023; Jia, Jiaya/I-3251-2012; Chen, Yukang/HKW-0344-2023						Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs								Arxiv											1	1;2024-06-26;https://www.arxiv.org/abs/2406.18629v1	arXiv:2406.18629			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 26 2024	2024	Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro.  [GRAPHICS]																																	2024-07-17	PPRN:90138146		
J	Larocca, Martin; Thanasilp, Supanut; Wang, Samson; Sharma, Kunal; Biamonte, Jacob; Coles, Patrick J.; Cincio, Lukasz; McClean, Jarrod R.; Holmes, Zoe; Cerezo, M.				Cerezo, Marco/ABD-9254-2020						A Review of Barren Plateaus in Variational Quantum Computing								Arxiv											1	1;2024-05-01;https://www.arxiv.org/abs/2405.00781v1	arXiv:2405.00781			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	Variational quantum computing offers a flexible computational paradigm with applications in diverse areas. However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm -- choices of ansatz, initial state, observable, loss function and hardware noise -- can lead to BPs when ill-suited. Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory. This article provides a comprehensive review of the current understanding of the BP phenomenon.																																	2024-05-19	PPRN:88722770		
J	Gu, Yuxian; Dong, Li; Wei, Furu; Huang, Minlie										MiniLLM: Knowledge Distillation of Large Language Models								Arxiv											4	4;2024-04-10;https://www.arxiv.org/abs/2306.08543v4| 3;2024-03-12;https://www.arxiv.org/abs/2306.08543v3| 2;2024-02-28;https://www.arxiv.org/abs/2306.08543v2| 1;2023-06-14;https://www.arxiv.org/abs/2306.08543v1	arXiv:2306.08543			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 10 2024	2024	Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.																																	2024-04-24	PPRN:73360492		
J	Chi, Cheng; Xu, Zhenjia; Pan, Chuer; Cousineau, Eric; Burchfiel, Benjamin; Feng, Siyuan; Tedrake, Russ; Song, Shuran				Chi, Cheng/GSE-4960-2022; Feng, Siyuan/AAA-1320-2022						Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots								Arxiv											3	3;2024-03-06;https://www.arxiv.org/abs/2402.10329v3| 2;2024-02-19;https://www.arxiv.org/abs/2402.10329v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.10329v1	arXiv:2402.10329			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 06 2024	2024	We present Universal Manipulation Interface (UMI) -- a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI's versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI's hardware and software system is open-sourced at https://umi-gripper.github.io.																																	2024-05-01	PPRN:87754229		
J	Wei, Jason; Karina, Nguyen; Chung, Hyung Won; Jiao, Yunxin Joy; Papay, Spencer; Glaese, Amelia; Schulman, John; Fedus, William										Measuring short-form factuality in large language models								Arxiv											1	1;2024-11-07;https://www.arxiv.org/abs/2411.04368v1	arXiv:2411.04368			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 07 2024	2024	We present SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. We prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models "know what they know," and our hope is that this benchmark will remain relevant for the next few generations of frontier models. 																																	2024-12-16	PPRN:119074336		
J	Ballet, J.; Bruel, P.; Burnett, T.H.; Lott, B.		Fermi-LAT Collaborat								Fermi Large Area Telescope Fourth Source Catalog Data Release 4 (4FGL-DR4)								Arxiv											4	4;2024-07-24;https://www.arxiv.org/abs/2307.12546v4| 3;2024-02-22;https://www.arxiv.org/abs/2307.12546v3| 2;2023-11-20;https://www.arxiv.org/abs/2307.12546v2| 1;2023-07-24;https://www.arxiv.org/abs/2307.12546v1	arXiv:2307.12546			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 24 2024	2024	We present an incremental version (4FGL-DR4, for Data Release 4) of the fourth Fermi-LAT catalog containing 7194 γ-ray sources. Based on the first 14 years of science data in the energy range from 50 MeV to 1 TeV, it uses the same analysis methods as the 4FGL-DR3 catalog did for 12 years of data, with only a few improvements. The spectral parameters, spectral energy distributions, light curves and associations are updated for all sources. We add four new extended sources and modify two existing ones. Among the 6658 4FGL-DR3 sources, we delete 14 and change the localization of 10, while 32 are newly associated, eleven associations are changed and three associations are discarded. We add 546 point sources, among which 8 are considered identified and 229 have a plausible counterpart at other wavelengths. Most are just above the detection threshold, and 14 are transient sources below the detection threshold that can affect the light curves of nearby sources.																																	2024-08-08	PPRN:74085670		
J	Wallace, Eric; Xiao, Kai; Leike, Reimar; Weng, Lilian; Heidecke, Johannes; Beutel, Alex										The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions								Arxiv											1	1;2024-04-19;https://www.arxiv.org/abs/2404.13208v1	arXiv:2404.13208			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 19 2024	2024	Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training - while imposing minimal degradations on standard capabilities.																																	2024-05-01	PPRN:88604698		
J	Das, Abhimanyu; Kong, Weihao; Leach, Andrew; Mathur, Shaan; Sen, Rajat; Yu, Rose										Long-term Forecasting with TiDE: Time-series Dense Encoder								Arxiv											3	3;2024-04-04;https://www.arxiv.org/abs/2304.08424v5| 2;2023-12-02;https://www.arxiv.org/abs/2304.08424v4| 1;2023-04-17;https://www.arxiv.org/abs/2304.08424v1	arXiv:2304.08424			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 04 2024	2024	Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time -series forecasting. Motivated by this, we propose a Multi -layer Perceptron (MLP) based encoder -decoder model, Time -series Dense Encoder (TiDE), for long-term time -series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time -series forecasting benchmarks while being 5-10x faster than the best Transformer based model.																																	2024-04-19	PPRN:63647560		
J	Liu, Zirui; Yuan, Jiayi; Jin, Hongye; Zhong, Shaochen; Xu, Zhaozhuo; Braverman, Vladimir; Chen, Beidi; Hu, Xia				braverman, vladimir/H-4565-2011; Jin, Hongye/HKO-3543-2023						KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache								Arxiv											1	1;2024-02-05;https://www.arxiv.org/abs/2402.02750v1	arXiv:2402.02750			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 05 2024	2024	Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU’s SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly implementation, KIVIcan enable Llama (Llama-2), Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory usage (including the model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼ 3.47× throughput on real LLM inference workload. 																																	2024-02-19	PPRN:87517658		
J	Ashkboos, Saleh; Mohtashami, Amirkeivan; Croci, Maximilian L.; Li, Bo; Cameron, Pashmina; Jaggi, Martin; Alistarh, Dan; Hoefler, Torsten; Hensman, James				Hoefler, Torsten/HKF-3023-2023						QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs								Arxiv											2	2;2024-10-29;https://www.arxiv.org/abs/2404.00456v2| 1;2024-03-30;https://www.arxiv.org/abs/2404.00456v1	arXiv:2404.00456			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 29 2024	2024	We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism, and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4 bits, without any channels identified for retention in higher precision. Our 4-bit quantized LLaMa2-70B model has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the zero-shot performance. We also show that QuaRot can provide lossless 6 and 8 bit LLaMa2 models without any calibration data using round-to-nearest quantization. 																																	2024-11-30	PPRN:88360913		
J	Yang, Songlin; Wang, Bailin; Shen, Yikang; Panda, Rameswar; Kim, Yoon				Panda, Rameswar/AAY-9834-2020						Gated Linear Attention Transformers with Hardware-Efficient Training								Arxiv											5	5;2024-08-27;https://www.arxiv.org/abs/2312.06635v6| 4;2024-06-05;https://www.arxiv.org/abs/2312.06635v5| 3;2024-02-15;https://www.arxiv.org/abs/2312.06635v4| 2;2023-12-24;https://www.arxiv.org/abs/2312.06635v3| 1;2023-12-12;https://www.arxiv.org/abs/2312.06635v2	arXiv:2312.06635			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 27 2024	2024	Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.																																	2024-09-06	PPRN:86556758		
J	Hubinger, Evan; Denison, Carson; Mu, Jesse; Lambert, Mike; Tong, Meg; MacDiarmid, Monte; Lanham, Tamera; Ziegler, Daniel M.; Maxwell, Tim; Cheng, Newton; Jermyn, Adam; Askell, Amanda; Radhakrishnan, Ansh; Anil, Cem; Duvenaud, David; Ganguli, Deep; Barez, Fazl; Clark, Jack; Ndousse, Kamal; Sachan, Kshitij; Sellitto, Michael; Sharma, Mrinank; DasSarma, Nova; Grosse, Roger; Kravec, Shauna; Bai, Yuntao; Witten, Zachary; Favaro, Marina; Brauner, Jan; Karnofsky, Holden; Christiano, Paul; Bowman, Samuel R.; Graham, Logan; Kaplan, Jared; Mindermann, Soren; Greenblatt, Ryan; Shlegeris, Buck; Schiefer, Nicholas; Perez, Ethan				Bai, Yuntao/IYJ-4381-2023						Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training								Arxiv											3	3;2024-01-17;https://www.arxiv.org/abs/2401.05566v3| 2;2024-01-12;https://www.arxiv.org/abs/2401.05566v2| 1;2024-01-10;https://www.arxiv.org/abs/2401.05566v1	arXiv:2401.05566			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 17 2024	2024	Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.																																	2024-02-03	PPRN:87124314		
J	Saab, Khaled; Tu, Tao; Weng, Wei-Hung; Tanno, Ryutaro; Stutz, David; Wulczyn, Ellery; Zhang, Fan; Strother, Tim; Park, Chunjong; Vedadi, Elahe; Chaves, Juanma Zambrano; Hu, Szu-Yeu; Schaekermann, Mike; Kamath, Aishwarya; Cheng, Yong; Barrett, David G.T.; Cheung, Cathy; Mustafa, Basil; Palepu, Anil; McDuff, Daniel; Hou, Le; Golany, Tomer; Liu, Luyang; Alayrac, Jean-baptiste; Houlsby, Neil; Tomasev, Nenad; Freyberg, Jan; Lau, Charles; Kemp, Jonas; Lai, Jeremy; Azizi, Shekoofeh; Kanada, Kimberly; Man, Siwai; Kulkarni, Kavita; Sun, Ruoxi; Shakeri, Siamak; He, Luheng; Caine, Ben; Webson, Albert; Latysheva, Natasha; Johnson, Melvin; Mansfield, Philip; Lu, Jian; Rivlin, Ehud; Anderson, Jesper; Green, Bradley; Wong, Renee; Krause, Jonathan; Shlens, Jonathon; Dominowska, Ewa; Eslami, S.M. Ali; Chou, Katherine; Cui, Claire; Vinyals, Oriol; Kavukcuoglu, Koray; Manyika, James; Dean, Jeff; Hassabis, Demis; Matias, Yossi; Webster, Dale; Barral, Joelle; Corrado, Greg; Semturs, Christopher; Mahdavi, S. Sara; Gottweis, Juraj; Karthikesalingam, Alan; Natarajan, Vivek				LIU, Luyang/AAB-1675-2021; tu, tao/KVB-7209-2024; Natarajan, Vivek/AAG-4944-2021; Vedadi, Elahe/HKP-0041-2023						Capabilities of Gemini Models in Medicine								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2404.18416v2| 1;2024-04-29;https://www.arxiv.org/abs/2404.18416v1	arXiv:2404.18416			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 01 2024	2024	Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health & medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.																																	2024-05-19	PPRN:88697288		
J	Zhou, Zhihan; Ji, Yanrong; Li, Weijian; Dutta, Pratik; Davuluri, Ramana; Liu, Han				Davuluri, Ramana/OYE-0310-2025						DNABERT-2: EFFICIENT FOUNDATION MODEL AND BENCHMARK FOR MULTI-SPECIES GENOMES								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2306.15006v2	arXiv:2306.15006			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $36$ distinct datasets across $9$ tasks, with input lengths ranging from $70$ to $10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with 21× fewer parameters and approximately 92×  less GPU time in pre-training.																																	2024-04-12	PPRN:88237428		
J	Hu, Li; Gao, Xin; Zhang, Peng; Sun, Ke; Zhang, Bang; Bo, Liefeng										Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation								Arxiv											2	2;2024-06-13;https://www.arxiv.org/abs/2311.17117v3| 1;2023-12-07;https://www.arxiv.org/abs/2311.17117v2	arXiv:2311.17117			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 13 2024	2024	Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results.																																	2024-07-02	PPRN:86518912		
J	Anwar, Usman; Saparov, Abulhair; Rando, Javier; Paleka, Daniel; Turpin, Miles; Hase, Peter; Lubana, Ekdeep Singh; Jenner, Erik; Casper, Stephen; Sourbut, Oliver; Edelman, Benjamin L.; Zhang, Zhaowei; Guenther, Mario; Korinek, Anton; Hernandez-Orallo, Jose; Hammond, Lewis; Bigelow, Eric; Pan, Alexander; Langosco, Lauro; Korbak, Tomasz; Zhang, Heidi; Zhong, Ruiqi; hEigeartaigh, Sean O; Recchia, Gabriel; Corsi, Giulio; Chan, Alan; Anderljung, Markus; Edwards, Lilian; Bengio, Yoshua; Chen, Danqi; Albanie, Samuel; Maharaj, Tegan; Foerster, Jakob; Tramer, Florian; He, He; Kasirzadeh, Atoosa; Choi, Yejin; Krueger, David				CHEN, DANQI/C-6441-2013; Zhong, Ruiqi/HGA-7351-2022; Hernandez-Orallo, Jose/H-9166-2015; Albanie, Samuel/AAC-9729-2020						Foundational Challenges in Assuring Alignment and Safety of Large Language Models								Arxiv											1	1;2024-04-15;https://www.arxiv.org/abs/2404.09932v1	arXiv:2404.09932			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose 200+ concrete research questions.																																	2024-05-09	PPRN:88529488		
J	Panickssery, Arjun; Bowman, Samuel R.; Feng, Shi				Feng, Shi/HKN-8278-2023						LLM Evaluators Recognize and Favor Their Own Generations								Arxiv											1	1;2024-04-15;https://www.arxiv.org/abs/2404.13076v1	arXiv:2404.13076			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others' while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By fine-tuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.																																	2024-05-01	PPRN:88607658		
J	Zhang, Dan; Zhoubian, Sining; Hu, Ziniu; Yue, Yisong; Dong, Yuxiao; Tang, Jie				Hu, Ziniu/HJI-4899-2023						ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search								Arxiv											2	2;2024-11-18;https://www.arxiv.org/abs/2406.03816v3| 1;2024-06-06;https://www.arxiv.org/abs/2406.03816v1	arXiv:2406.03816			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 18 2024	2024	Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST^text{EM} and Self-Rewarding LM. 																																	2024-12-28	PPRN:89260205		
J	Chen, Lin; Wei, Xilin; Li, Jinsong; Dong, Xiaoyi; Zhang, Pan; Zang, Yuhang; Chen, Zehui; Duan, Haodong; Lin, Bin; Tang, Zhenyu; Yuan, Li; Qiao, Yu; Lin, Dahua; Zhao, Feng; Wang, Jiaqi				Lin, Dahua/W-6576-2019; WANG, JIAQI/KBB-8837-2024; Tang, Zhenyu/AAO-6764-2021; Dong, Xiaoyi/AAC-8666-2019; Zang, Yuhang/AES-3018-2022; Duan, Haodong/ITV-1505-2023; Lin, Bin/V-3431-2019						ShareGPT4Video: Improving Video Understanding and Generation with Better Captions								Arxiv											1	1;2024-06-06;https://www.arxiv.org/abs/2406.04325v1	arXiv:2406.04325			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 06 2024	2024	We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, , 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) ShareCaptioner-Video, , an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, , a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations2 will be open-sourced and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.																																	2024-06-22	PPRN:89249307		
J	Karaev, Nikita; Rocco, Ignacio; Graham, Benjamin; Neverova, Natalia; Vedaldi, Andrea; Rupprecht, Christian				Неверова, Наталья/A-8316-2014; Rocco, Ignacio/AAH-5435-2019; Rupprecht, Christian/ABF-7744-2021						CoTracker: It is Better to Track Together								Arxiv											3	3;2024-10-01;https://www.arxiv.org/abs/2307.07635v3| 2;2023-12-26;https://www.arxiv.org/abs/2307.07635v2| 1;2023-07-14;https://www.arxiv.org/abs/2307.07635v1	arXiv:2307.07635			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 01 2024	2024	We introduce CoTracker, a transformer-based model that tracks a large number of 2D points in long video sequences. Differently from most existing approaches that track points independently, CoTracker tracks them jointly, accounting for their dependencies. We show that joint tracking significantly improves tracking accuracy and robustness, and allows CoTracker to track occluded points and points outside of the camera view. We also introduce several innovations for this class of trackers, including using token proxies that significantly improve memory efficiency and allow CoTracker to track 70k points jointly and simultaneously at inference on a single GPU. CoTracker is an online algorithm that operates causally on short windows. However, it is trained utilizing unrolled windows as a recurrent network, maintaining tracks for long periods of time even when points are occluded or leave the field of view. Quantitatively, CoTracker substantially outperforms prior trackers on standard point-tracking benchmarks. Code and model weights are available at https://co-tracker.github.io/																																	2024-10-13	PPRN:73951179		
J	Yu, Wangbo; Xing, Jinbo; Yuan, Li; Hu, Wenbo; Li, Xiaoyu; Huang, Zhipeng; Gao, Xiangjun; Wong, Tien-Tsin; Shan, Ying; Tian, Yonghong				Hu, Wenbo/JKH-5582-2023; TIAN, Yonghong/M-4937-2013; XING, Jinbo/JHT-1415-2023						ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis								Arxiv											1	1;2024-09-03;https://www.arxiv.org/abs/2409.02048v1	arXiv:2409.02048			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 03 2024	2024	Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. In this work, we propose ViewCrafter, a novel method for synthesizing high-fidelity novel views of generic scenes from single or sparse images with the prior of video diffusion model. Our method takes advantage of the powerful generation capabilities of video diffusion model and the coarse 3D clues offered by point-based representation to generate high-quality video frames with precise camera pose control. To further enlarge the generation range of novel views, we tailored an iterative view synthesis strategy together with a camera trajectory planning algorithm to progressively extend the 3D clues and the areas covered by the novel views. With ViewCrafter, we can facilitate various applications, such as immersive experiences with real-time rendering by efficiently optimizing a 3D-GS representation using the reconstructed 3D points and the generated novel views, and scene-level text-to-3D generation for more imaginative content creation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in synthesizing high-fidelity and consistent novel views. 																																	2024-09-12	PPRN:91717762		
J	Maini, Pratyush; Feng, Zhili; Schwarzschild, Avi; Lipton, Zachary C.; Zico Kolter, J.										TOFU: A Task of Fictitious Unlearning for LLMs								Arxiv											1	1;2024-01-11;https://www.arxiv.org/abs/2401.06121v1	arXiv:2401.06121			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.																																	2024-01-26	PPRN:87124383		
J	Xiang, Jianfeng; Lv, Zelong; Xu, Sicheng; Deng, Yu; Wang, Ruicheng; Zhang, Bowen; Chen, Dong; Tong, Xin; Yang, Jiaolong				Xiang, Jian-Feng/ABH-1934-2020						Structured 3D Latents for Scalable and Versatile 3D Generation								Arxiv											1	1;2024-12-02;https://www.arxiv.org/abs/2412.01506v1	arXiv:2412.01506			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. 																																	2025-01-11	PPRN:119664049		
J	Zhang, Pan; Dong, Xiaoyi; Zang, Yuhang; Cao, Yuhang; Qian, Rui; Chen, Lin; Guo, Qipeng; Duan, Haodong; Wang, Bin; Ouyang, Linke; Zhang, Songyang; Zhang, Wenwei; Li, Yining; Gao, Yang; Sun, Peng; Zhang, Xinyue; Li, Wei; Li, Jingwen; Wang, Wenhai; Yan, Hang; He, Conghui; Zhang, Xingcheng; Chen, Kai; Dai, Jifeng; Qiao, Yu; Lin, Dahua; Wang, Jiaqi				Zang, Yuhang/AES-3018-2022; Wang, Bin/MVU-8917-2025; Qiao, Yu/ABD-5787-2021; Zhang, Songyang/GPX-5621-2022; Dong, Xiaoyi/AAC-8666-2019; Zhang, Xingcheng/AAC-6392-2019; Zhang, Wenwei/HKO-4277-2023; He, Conghui/AAZ-3323-2021; Lin, Dahua/W-6576-2019; Dai, Jifeng/HGU-8741-2022; WANG, JIAQI/KBB-8837-2024; Duan, Haodong/ITV-1505-2023						InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output								Arxiv											1	1;2024-07-03;https://www.arxiv.org/abs/2407.03320v1	arXiv:2407.03320			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks.																																	2024-07-20	PPRN:90692179		
J	McClenny, Levi D; Braga-Neto, Ulisses				Braga-Neto, Ulisses/ABI-2677-2020						Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism								Arxiv											1	1;2024-06-18;https://www.arxiv.org/abs/2009.04544v5	arXiv:2009.04544			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Physics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, it has been recognized that adaptive procedures are needed to force the neural network to fit accurately the stubborn spots in the solution of "stiff" PDEs. In this paper, we propose a fundamentally new way to train PINNs adaptively, where the adaptation weights are fully trainable and applied to each training point individually, so the neural network learns autonomously which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, which is reminiscent of similar mechanisms used in computer vision. The basic idea behind these SA-PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights. In addition, we show how to build a continuous map of self-adaptive weights using Gaussian Process regression, which allows the use of stochastic gradient descent in problems where conventional gradient descent is not enough to produce accurate solutions. Finally, we derive the Neural Tangent Kernel matrix for SA-PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a smooth equalization of the eigenvalues of the NTK matrix corresponding to the different loss terms. In numerical experiments with several linear and nonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art PINN algorithm in L2 error, while using a smaller number of training epochs.																																	2024-07-06	PPRN:89375443		
J	Xu, Yinghao; Shi, Zifan; Yifan, Wang; Chen, Hansheng; Yang, Ceyuan; Peng, Sida; Shen, Yujun; Wetzstein, Gordon				Xu, Yinghao/NEU-2017-2025; yifan, wang/IAR-9011-2023						GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation								Arxiv											1	1;2024-03-21;https://www.arxiv.org/abs/2403.14621v1	arXiv:2403.14621			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. 																																	2024-04-13	PPRN:88259912		
J	Cai, Tianle; Wang, Xuezhi; Ma, Tengyu; Chen, Xinyun; Zhou, Denny				Ma, Tengyu/D-9086-2017; Chen, Xinyun/ABZ-9877-2022						Large Language Models as Tool Makers								Arxiv											2	2;2024-03-11;https://www.arxiv.org/abs/2305.17126v2| 1;2023-05-26;https://www.arxiv.org/abs/2305.17126v1	arXiv:2305.17126			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 11 2024	2024	Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost.																																	2024-04-08	PPRN:72730001		
J	Lian, Long; Li, Boyi; Yala, Adam; Darrell, Trevor										LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models								Arxiv											3	3;2024-03-04;https://www.arxiv.org/abs/2305.13655v3| 2;2023-10-10;https://www.arxiv.org/abs/2305.13655v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.13655v1	arXiv:2305.13655			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 04 2024	2024	Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io																																	2024-04-02	PPRN:71747442		
J	Yang, Zeyu; Yang, Hongye; Pan, Zijie; Zhang, Li				Pan, Zijie/MEQ-2078-2025; Yang, Zeyu/OVX-7271-2025						Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting								Arxiv											3	3;2024-02-22;https://www.arxiv.org/abs/2310.10642v3| 2;2024-01-18;https://www.arxiv.org/abs/2310.10642v2| 1;2023-10-16;https://www.arxiv.org/abs/2310.10642v1	arXiv:2310.10642			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial and temporal structure of dynamic scenes from directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as an entirety and propose to approximate the underlying spatiotemporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view -dependent and time -evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable -length video and end -to -end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi -view scenarios, demonstrate our 4DGS model’s superior visual quality and efficiency.																																	2024-03-22	PPRN:85662364		
J	Zhang, Zhexin; Lei, Leqi; Wu, Lindong; Sun, Rui; Huang, Yongkang; Long, Chong; Liu, Xiao; Lei, Xuanyu; Tang, Jie; Huang, Minlie										SafetyBench: Evaluating the Safety of Large Language Models								Arxiv											2	2;2024-06-24;https://www.arxiv.org/abs/2309.07045v2| 1;2023-09-13;https://www.arxiv.org/abs/2309.07045v1	arXiv:2309.07045			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 24 2024	2024	With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. 																																	2024-07-15	PPRN:85005634		
J	Chen, Xi; Huang, Lianghua; Liu, Yu; Shen, Yujun; Zhao, Deli; Zhao, Hengshuang				Zhao, Dan/D-1031-2013						AnyDoor: Zero-shot Object-level Image Customization								Arxiv											1	1;2024-05-08;https://www.arxiv.org/abs/2307.09481v2	arXiv:2307.09481			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 08 2024	2024	This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations in a harmonious way. Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage. Such a challenging zero-shot setting requires an adequate characterization of a certain object. To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain texture details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings. We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness. Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on and object moving. 																																	2024-05-27	PPRN:88811395		
J	Liu, Chris Yuhao; Zeng, Liang; Liu, Jiacai; Yan, Rui; He, Jujie; Wang, Chaojie; Yan, Shuicheng; Liu, Yang; Zhou, Yahui				yan, shuicheng/HCH-9860-2022						Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs								Arxiv											1	1;2024-10-24;https://www.arxiv.org/abs/2410.18451v1	arXiv:2410.18451			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 24 2024	2024	In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs -significantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model series - Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B -with the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications.1																																	2024-11-27	PPRN:118807701		
J	Wu, Chengyue; Chen, Xiaokang; Wu, Zhiyu; Ma, Yiyang; Liu, Xingchao; Pan, Zizheng; Liu, Wen; Xie, Zhenda; Yu, Xingkai; Ruan, Chong; Luo, Ping				Xie, Zhenda/O-1198-2013; pluo/GPG-2707-2022; Yu, Xingkai/AAO-5118-2020						Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation								Arxiv											1	1;2024-10-17;https://www.arxiv.org/abs/2410.13848v1	arXiv:2410.13848			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.																																	2024-11-12	PPRN:115444991		
J	Hong, Wenyi; Wang, Weihan; Ding, Ming; Yu, Wenmeng; Lv, Qingsong; Wang, Yan; Cheng, Yean; Huang, Shiyu; Ji, Junhui; Xue, Zhao; Zhao, Lei; Yang, Zhuoyi; Gu, Xiaotao; Zhang, Xiaohan; Feng, Guanyu; Yin, Da; Wang, Zihan; Qi, Ji; Song, Xixuan; Zhang, Peng; Liu, Debing; Xu, Bin; Li, Juanzi; Dong, Yuxiao; Tang, Jie				zhang, shibo/JDQ-7881-2023; Huang, Shiyu/LRU-2222-2024; Zhao, Lei/F-4258-2018; 航航, 张/KBC-0720-2024; qingsong, lv/GNM-9566-2022						CogVLM2: Visual Language Models for Image and Video Understanding								Arxiv											1	1;2024-08-29;https://www.arxiv.org/abs/2408.16500v1	arXiv:2408.16500			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 29 2024	2024	Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in pursuit of enhanced vision-language fusion, efficient higher-resolution architecture, and broader modalities and applications. Here we propose the CogVLM2 family, a new generation of visual language models for image and video understanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image understanding model, CogVLM2 inherits the visual expert architecture with improved training recipes in both pre-training and post-training stages, supporting input resolution up to $1344 times 1344$ pixels. As a video understanding model, CogVLM2-Video integrates multi-frame input with timestamps and proposes automated temporal grounding data construction. Notably, CogVLM2 family has achieved state-of-the-art results on benchmarks like MMBench, MM-Vet, TextVQA, MVBench and VCGBench. 																																	2024-09-23	PPRN:91782886		
J	Tan, Zhen; Li, Dawei; Wang, Song; Beigi, Alimohammad; Jiang, Bohan; Bhattacharjee, Amrita; Karami, Mansooreh; Li, Jundong; Cheng, Lu; Liu, Huan				Li, Dawei/IZD-9687-2023; Tan, Zhen/JCE-9258-2023						Large Language Models for Data Annotation and Synthesis: A Survey								Arxiv											3	3;2024-12-02;https://www.arxiv.org/abs/2402.13446v3| 2;2024-06-23;https://www.arxiv.org/abs/2402.13446v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.13446v1	arXiv:2402.13446			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	Data annotation and synthesis generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation and synthesis. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation and synthesis. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field1.																																	2025-01-15	PPRN:87787759		
J	Schneuing, Arne; Harris, Charles; Du, Yuanqi; Didi, Kieran; Jamasb, Arian; Igashov, Ilia; Du, Weitao; Gomes, Carla; Blundell, Tom L; Lio, Pietro; Welling, Max; Bronstein, Michael; Correia, Bruno				Gomes, Carla/K-8690-2013; P. Lió, Pietro/AAV-3358-2021						Structure-based Drug Design with Equivariant Diffusion Models								Arxiv											2	2;2024-09-23;https://www.arxiv.org/abs/2210.13695v3| 1;2023-06-30;https://www.arxiv.org/abs/2210.13695v2	arXiv:2210.13695			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 23 2024	2024	Structure-based drug design (SBDD) aims to design small-molecule ligands that bind with high affinity and specificity to pre-determined protein targets. Generative SBDD methods leverage structural data of drugs in complex with their protein targets to propose new drug candidates. These approaches typically place one atom at a time in an autoregressive fashion using the binding pocket as well as previously added ligand atoms as context in each step. Recently a surge of diffusion generative models has entered this domain which hold promise to capture the statistical properties of natural ligands more faithfully. However, most existing methods focus exclusively on bottom-up de novo design of compounds or tackle other drug development challenges with task-specific models. The latter requires curation of suitable datasets, careful engineering of the models and retraining from scratch for each task. Here we show how a single pre-trained diffusion model can be applied to a broader range of problems, such as off-the-shelf property optimization, explicit negative design, and partial molecular design with inpainting. We formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an SE(3)-equivariant diffusion model that generates novel ligands conditioned on protein pockets. Our in silico experiments demonstrate that DiffSBDD captures the statistics of the ground truth data effectively. Furthermore, we show how additional constraints can be used to improve the generated drug candidates according to a variety of computational metrics. These results support the assumption that diffusion models represent the complex distribution of structural data more accurately than previous methods, and are able to incorporate additional design objectives and constraints changing nothing but the sampling strategy.																																	2024-10-08	PPRN:73727107		
J	Zhang, Tianjun; Patil, Shishir G.; Jain, Naman; Shen, Sheng; Zaharia, Matei; Stoica, Ion; Gonzalez, Joseph E.										RAFT: Adapting Language Model to Domain Specific RAG								Arxiv											2	2;2024-06-05;https://www.arxiv.org/abs/2403.10131v2| 1;2024-03-15;https://www.arxiv.org/abs/2403.10131v1	arXiv:2403.10131			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 05 2024	2024	Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG.																																	2024-06-22	PPRN:88167570		
J	Kocevski, Dale D.; Finkelstein, Steven L.; Barro, Guillermo; Taylor, Anthony J.; Calabro, Antonello; Laloux, Brivael; Buchner, Johannes; Trump, Jonathan R.; Leung, Gene C.K.; Yang, Guang; Dickinson, Mark; Perez-Gonzalez, Pablo G.; Pacucci, Fabio; Inayoshi, Kohei; Somerville, Rachel S.; Mcgrath, Elizabeth J.; Akins, Hollis B.; Bagley, Micaela B.; Bisigello, Laura; Bowler, Rebecca A.A.; Carnall, Adam; Casey, Caitlin M.; Cheng, Yingjie; Cleri, Nikko J.; Costantin, Luca; Cullen, Fergus; Davis, Kelcey; Donnan, Callum T.; Dunlop, James S.; Ellis, Richard S.; Ferguson, Henry C.; Fujimoto, Seiji; Fontana, Adriano; Giavalisco, Mauro; Grazian, Andrea; Grogin, Norman A.; Hathi, Nimish P.; Hirschmann, Michaela; Huertas-Company, Marc; Holwerda, Benne W.; Illingworth, Garth; Juneau, Stephanie; Kartaltepe, Jeyhan S.; Koekemoer, Anton M.; Li, Wenxiu; Lucas, Ray A.; Magee, Dan; Mason, Charlotte; McLeod, Derek J.; McLure, Ross J.; Napolitano, Lorenzo; Papovich, Casey; Pirzkal, Nor; Rodighiero, Giulia; Santini, Paola; Wilkins, Stephen M.; Yung, L. Y. Aaron				Kartaltepe, Jeyhan/ABI-1561-2020; Yung, L. Y. Aaron/AAT-6862-2021; Cheng, Yingjie/JPA-5703-2023; Bowler, Rebecca/HIK-2775-2022; Ellis, Richard/ABL-1310-2022; Dunlop, James/ADB-7947-2022; Taylor, Anthony/NXC-5403-2025; wenxiu, li/MTF-5824-2025; Calabrò, Antonello/AAX-1028-2020; COSTANTIN, LUCA/AAJ-9715-2021; Mason, Charlotte/IYJ-2820-2023; Yang, Guang/KRP-9720-2024; Trump, Jonathan/NPI-4613-2025; Inayoshi, Kohei/AAW-2098-2020; Giavalisco, Mauro/AEV-5974-2022						The Rise of Faint, Red AGN at z>4: A Sample of Little Red Dots in the JWST Extragalactic Legacy Fields								Arxiv											2	2;2024-04-19;https://www.arxiv.org/abs/2404.03576v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.03576v1	arXiv:2404.03576			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 19 2024	2024	We present a sample of 341 "little red dots" (LRDs) spanning the redshift range z ∼ 2−11 using data from the CEERS, PRIMER, JADES, UNCOVER and NGDEEP surveys. These sources are likely heavily-reddened AGN that trace a previously-hidden phase of dust-obscured black hole growth in the early Universe. Unlike past use of color indices to identify LRDs, we employ continuum slope fitting using shifting bandpasses to sample the same rest-frame emission blueward and redward of the Balmer break. This approach allows us to identify LRDs over a wider redshift range and is less susceptible to contamination from galaxies with strong breaks that otherwise lack a rising red continuum. The redshift distribution of our sample increases at z<8 and then undergoes a rapid decline at z∼4.5, which may tie the emergence, and obscuration, of these sources to the inside-out growth that galaxies experience during this epoch. We find that LRDs are 2-3 dex more numerous than bright quasars at z∼5−7, but their number density is only 0.6-1 dex higher than X-ray and UV selected AGN at these redshifts. Within our sample, we have identified the first X-ray detected LRDs at z=3.1 and z=4.66. An X-ray spectral analysis confirms that these AGN are moderately obscured with log(NH/cm2) of 23.3−1.3+0.4 and 22.72−0.16+0.13. Our analysis reveals that reddened AGN emission dominates their rest-optical light, while the rest-UV originates from their host galaxies. We also present NIRSpec follow-up spectroscopy of 17 LRDs that show broad emission lines consistent with AGN activity. The confirmed AGN fraction of our sample is 71% for sources with F444W<26.5. In addition, we find three LRDs with narrow blue-shifted Balmer absorption features in their spectra, suggesting an outflow of high-density, low ionization gas from near the central engine of these faint, red AGN.																																	2024-05-11	PPRN:88405670		
J	Das, Abhimanyu; Kong, Weihao; Sen, Rajat; Zhou, Yichen										A decoder-only foundation model for time-series forecasting								Arxiv											4	4;2024-04-17;https://www.arxiv.org/abs/2310.10688v4| 3;2024-02-04;https://www.arxiv.org/abs/2310.10688v3| 2;2024-01-31;https://www.arxiv.org/abs/2310.10688v2| 1;2023-10-14;https://www.arxiv.org/abs/2310.10688v1	arXiv:2310.10688			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 17 2024	2024	Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.																																	2024-04-28	PPRN:85680107		
J	Zhang, Hongxin; Du, Weihua; Shan, Jiaming; Zhou, Qinhong; Du, Yilun; Tenenbaum, Joshua B.; Shu, Tianmin; Gan, Chuang				Shu, Tianmin/AAT-6085-2021; Du, Weihua/JGM-9058-2023						Building Cooperative Embodied Agents Modularly with Large Language Models								Arxiv											2	2;2024-02-17;https://www.arxiv.org/abs/2307.02485v2| 1;2023-07-05;https://www.arxiv.org/abs/2307.02485v1	arXiv:2307.02485			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 17 2024	2024	In this work, we address challenging multi -agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multiobjective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive -inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long -horizon tasks efficiently. Our experiments on CWAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning -based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA -2 still underperform, we fine-tune a CoLLAMA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human -agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi -agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.																																	2024-03-15	PPRN:73800374		
J	Ma, Nanye; Goldstein, Mark; Albergo, Michael S.; Boffi, Nicholas M.; Vanden-Eijnden, Eric; Xie, Saining										SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers								Arxiv											2	2;2024-09-23;https://www.arxiv.org/abs/2401.08740v2| 1;2024-01-16;https://www.arxiv.org/abs/2401.08740v1	arXiv:2401.08740			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 23 2024	2024	We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: learning in discrete or continuous time, the objective function, the interpolant that connects the distributions, and deterministic or stochastic sampling. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 and 512x512 benchmark using the exact same model structure, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06 and 2.62, respectively.																																	2024-10-07	PPRN:87208878		
J	Wolf, Yotam; Wies, Noam; Avnery, Oshri; Levine, Yoav; Shashua, Amnon										Fundamental Limitations of Alignment in Large Language Models								Arxiv											4	4;2024-06-03;https://www.arxiv.org/abs/2304.11082v6| 3;2024-02-05;https://www.arxiv.org/abs/2304.11082v5| 2;2023-10-11;https://www.arxiv.org/abs/2304.11082v4| 1;2023-04-19;https://www.arxiv.org/abs/2304.11082v1	arXiv:2304.11082			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.																																	2024-06-22	PPRN:65040651		
J	Yu, Tianyu; Zhang, Haoye; Yao, Yuan; Dang, Yunkai; Chen, Da; Lu, Xiaoman; Cui, Ganqu; He, Taiwen; Liu, Zhiyuan; Chua, Tat-Seng; Sun, Maosong				Liu, Zhiyuan/I-2233-2014; Wang, Meng/AEZ-9059-2022; Yu, Tianyu/OFO-0197-2025						RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness								Arxiv											1	1;2024-05-27;https://www.arxiv.org/abs/2405.17220v1	arXiv:2405.17220			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 27 2024	2024	Learning from feedback reduces the hallucination of multimodal large language models (MLLMs) by aligning them with human preferences. While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention. However, these methods heavily rely on costly proprietary models like GPT-4V, resulting in scalability issues. Moreover, this paradigm essentially distills the proprietary models to provide a temporary solution to quickly bridge the performance gap. As this gap continues to shrink, the community is soon facing the essential challenge of aligning MLLMs using labeler models of comparable capability. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm for super GPT-4V trustworthiness. RLAIF-V maximally exploits the open-source feedback from two perspectives, including high-quality feedback data and online feedback learning algorithm. In order to better expose the difference in trustworthiness, we propose a novel deconfounded candidate response generation strategy. To enhance the accuracy of pairwise feedback data from open-source MLLMs, we employ a divide-and-conquer approach that breaks down the response into atomic claims, simplifying the task to obtain more reliable results. For the feedback learning algorithm, RLAIF-V mitigates the distribution shift problem of vanilla direct preference optimization in an online learning fashion, improving both the learning performance and efficiency. Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models without sacrificing performance on other tasks. Using a 34B model as labeler, RLAIF-V 7B model reduces object hallucination by 82.9% and overall hallucination by 42.1%, outperforming the labeler model. Remarkably, RLAIF-V also reveals the selfalignment potential of open-source MLLMs, where a 12B model can learn from the feedback of itself to achieve less than 29.5% overall hallucination rate, surpassing GPT-4V (45.9%) by a large margin. The results shed light on a promising route to enhance the efficacy of leading-edge MLLMs.																																	2024-06-11	PPRN:89071464		
J	Chu, Xiangxiang; Qiao, Limeng; Zhang, Xinyu; Xu, Shuang; Wei, Fei; Yang, Yang; Sun, Xiaofei; Hu, Yiming; Lin, Xinyang; Zhang, Bo; Shen, Chunhua				Wei, Fei/ITZ-4717-2023						MobileVLM V2: Faster and Stronger Baseline for Vision Language Model								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2402.03766v1	arXiv:2402.03766			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Feb 06 2024	2024	We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .																																	2024-02-21	PPRN:87533979		
J	Bereska, Leonard; Gavves, Efstratios				Gavves, Efstratios/AAA-6992-2019						Mechanistic Interpretability for AI Safety- A Review								Arxiv											3	3;2024-08-23;https://www.arxiv.org/abs/2404.14082v3| 2;2024-07-25;https://www.arxiv.org/abs/2404.14082v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.14082v1	arXiv:2404.14082			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 23 2024	2024	Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.																																	2024-09-04	PPRN:88611997		
J	Feng, Xidong; Wan, Ziyu; Wen, Muning; Mcaleer, Stephen Marcus; Wen, Ying; Zhang, Weinan; Wang, Jun				Wen, Muning/OOK-5749-2025						Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training								Arxiv											2	2;2024-02-09;https://www.arxiv.org/abs/2309.17179v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17179v1	arXiv:2309.17179			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 09 2024	2024	Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by using tree-search algorithms to guide multi-step reasoning. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLM. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.																																	2024-05-25	PPRN:85337891		
J	Sorensen, Taylor; Moore, Jared; Fisher, Jillian; Gordon, Mitchell; Mireshghallah, Niloofar; Rytting, Christopher Michael; Ye, Andre; Jiang, Liwei; Lu, Ximing; Dziri, Nouha; Althoff, Tim; Choi, Yejin				Lu, Ximing/LLL-7542-2024; Jiang, Liwei/IYK-0150-2023						A Roadmap to Pluralistic Alignment								Arxiv											1	1;2024-02-07;https://www.arxiv.org/abs/2402.05070v1	arXiv:2402.05070			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 07 2024	2024	With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.																																	2024-05-25	PPRN:87560103		
J	Li, Xinghang; Liu, Minghuan; Zhang, Hanbo; Yu, Cunjun; Xu, Jie; Wu, Hongtao; Cheang, Chilam; Jing, Ya; Zhang, Weinan; Liu, Huaping; Li, Hang; Kong, Tao				jing, ya/MIQ-9362-2025; Yu, Cunjun/GPG-1154-2022; Zhang, Hanbo/LGZ-2975-2024						Vision-Language Foundation Models as Effective Robot Imitators								Arxiv											3	3;2024-02-05;https://www.arxiv.org/abs/2311.01378v3| 2;2023-11-06;https://www.arxiv.org/abs/2311.01378v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01378v1	arXiv:2311.01378			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 05 2024	2024	Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy. Codes and models will be public.																																	2024-02-21	PPRN:85984310		
J	Ze, Yanjie; Zhang, Gu; Zhang, Kangning; Hu, Chenyuan; Wang, Muhan; Xu, Huazhe				Ze, Yanjie/MBV-8320-2025						3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations								Arxiv											6	6;2024-09-27;https://www.arxiv.org/abs/2403.03954v7| 5;2024-06-08;https://www.arxiv.org/abs/2403.03954v6| 4;2024-05-27;https://www.arxiv.org/abs/2403.03954v5| 3;2024-04-08;https://www.arxiv.org/abs/2403.03954v3| 2;2024-04-02;https://www.arxiv.org/abs/2403.03954v2| 1;2024-03-06;https://www.arxiv.org/abs/2403.03954v1	arXiv:2403.03954			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 27 2024	2024	Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. 																																	2024-10-09	PPRN:88048788		
J	Yang, Enneng; Shen, Li; Guo, Guibing; Wang, Xingwei; Cao, Xiaochun; Zhang, Jie; Tao, Dacheng				Shen, Li/AEZ-9528-2022; Zhang, Jiehuang/MGV-4525-2025						Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities								Arxiv											2	2;2024-09-05;https://www.arxiv.org/abs/2408.07666v4| 1;2024-08-21;https://www.arxiv.org/abs/2408.07666v3	arXiv:2408.07666			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 05 2024	2024	Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. 																																	2024-09-18	PPRN:91503636		
J	Lieberum, Tom; Rajamanoharan, Senthooran; Conmy, Arthur; Smith, Lewis; Sonnerat, Nicolas; Varma, Vikrant; Kramar, Janos; Dragan, Anca; Shah, Rohin; Nanda, Neel										Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2								Arxiv											2	2;2024-08-19;https://www.arxiv.org/abs/2408.05147v2| 1;2024-08-09;https://www.arxiv.org/abs/2408.05147v1	arXiv:2408.05147			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 19 2024	2024	Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse decomposition of a neural network's latent representations into seemingly interpretable features. Despite recent excitement about their potential, research applications outside of industry are limited by the high cost of training a comprehensive suite of SAEs. In this work, we introduce Gemma Scope, an open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2 2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs on the Gemma 2 pre-trained models, but additionally release SAEs trained on instruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each SAE on standard metrics and release these results. We hope that by releasing these SAE weights, we can help make more ambitious safety and interpretability research easier for the community. 																																	2024-08-30	PPRN:91319028		
J	Ye, Jiabo; Xu, Haiyang; Liu, Haowei; Hu, Anwen; Yan, Ming; Qian, Qi; Zhang, Ji; Huang, Fei; Zhou, Jingren				Xu, Haiyang/AAC-2095-2021; Zhou, Mingyuan/AAE-8717-2021						mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models								Arxiv											2	2;2024-08-13;https://www.arxiv.org/abs/2408.04840v2| 1;2024-08-09;https://www.arxiv.org/abs/2408.04840v1	arXiv:2408.04840			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 13 2024	2024	Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in executing instructions for a variety of single-image tasks. Despite this progress, significant challenges remain in modeling long image sequences. In this work, we introduce the versatile multi-modal large language model, mPLUG-Owl3, which enhances the capability for long image-sequence understanding in scenarios that incorporate retrieved image-text knowledge, interleaved image-text, and lengthy videos. Specifically, we propose novel hyper attention blocks to efficiently integrate vision and language into a common language-guided semantic space, thereby facilitating the processing of extended multi-image scenarios. Extensive experimental results suggest that mPLUG-Owl3 achieves state-of-the-art performance among models with a similar size on single-image, multi-image, and video benchmarks. Moreover, we propose a challenging long visual sequence evaluation named Distractor Resistance to assess the ability of models to maintain focus amidst distractions. Finally, with the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance on ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to the development of more efficient and powerful multimodal large language models.																																	2024-08-22	PPRN:91320047		
J	Yi, Sibo; Liu, Yule; Sun, Zhen; Cong, Tianshuo; He, Xinlei; Song, Jiaxing; Xu, Ke; Li, Qi				宋, 甲行/HGC-9152-2022; Li, Qixian/ITV-3609-2023						Jailbreak Attacks and Defenses Against Large Language Models: A Survey								Arxiv											1	1;2024-07-05;https://www.arxiv.org/abs/2407.04295v1	arXiv:2407.04295			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 05 2024	2024	Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of "jailbreaking", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.																																	2024-07-20	PPRN:90728388		
J	Hayou, Soufiane; Ghosh, Nikhil; Yu, Bin				Hayou, Soufiane/JJD-9155-2023						LoRA+: Efficient Low Rank Adaptation of Large Models								Arxiv											2	2;2024-07-04;https://www.arxiv.org/abs/2402.12354v2| 1;2024-02-19;https://www.arxiv.org/abs/2402.12354v1	arXiv:2402.12354			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in (Hu et al., 2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen fixed ratio. We call this proposed algorithm LoRA+. In our extensive experiments, LoRA+ improves performance (1% − 2% improvements) and finetuning speed (up to ∼ 2X SpeedUp), at the same computational cost as LoRA.																																	2024-07-20	PPRN:87759414		
J	Diao, Shizhe; Wang, Pengcheng; Lin, Yong; Pan, Rui; Liu, Xiang; Zhang, Tong				Lin, Yong/JXX-1882-2024; Zhang, Tongyu/KDM-8780-2024						Active Prompting with Chain-of-Thought for Large Language Models								Arxiv											2	2;2024-06-07;https://www.arxiv.org/abs/2302.12246v4| 1;2023-02-23;https://www.arxiv.org/abs/2302.12246v2	arXiv:2302.12246			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method.1																																	2024-07-04	PPRN:46119872		
J	Zhao, Zihuai; Fan, Wenqi; Li, Jiatong; Liu, Yunqing; Mei, Xiaowei; Wang, Yiqi; Wen, Zhen; Wang, Fei; Zhao, Xiangyu; Tang, Jiliang; Li, Qing				Zhao, Xiangyu/AAO-2203-2020						Recommender Systems in the Era of Large Language Models (LLMs)								Arxiv											5	5;2024-04-29;https://www.arxiv.org/abs/2307.02046v6| 4;2024-04-22;https://www.arxiv.org/abs/2307.02046v5| 3;2024-04-17;https://www.arxiv.org/abs/2307.02046v4| 2;2024-04-15;https://www.arxiv.org/abs/2307.02046v3| 1;2023-07-05;https://www.arxiv.org/abs/2307.02046v1	arXiv:2307.02046			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 29 2024	2024	With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems, to provide researchers in relevant fields with an in-depth understanding. Therefore, in this paper, we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training, Fine-tuning, and Prompting. More specifically, we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then, we review recent techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss future directions in this emerging field.																																	2024-05-15	PPRN:73802603		
J	Chen, Junsong; Ge, Chongjian; Xie, Enze; Wu, Yue; Yao, Lewei; Ren, Xiaozhe; Wang, Zhongdao; Luo, Ping; Lu, Huchuan; Li, Zhenguo				Chen, Xiangyu/P-7839-2018; Jun, Cheng/AFO-0740-2022; Lv, Zhengtong/AAW-9611-2020						PixArt-Sigma: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation								Arxiv											2	2;2024-03-17;https://www.arxiv.org/abs/2403.04692v2| 1;2024-03-07;https://www.arxiv.org/abs/2403.04692v1	arXiv:2403.04692			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 17 2024	2024	In this paper, we introduce PixArt-Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-Sigma represents a significant advancement over its predecessor, PixArt-alpha, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term "weak-to-strong training". The advancements in PixArt-Sigma are twofold: (1) High-Quality Training Data: PixArt-Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.																																	2024-04-11	PPRN:88062967		
J	Xia, Chunqiu Steven; Zhang, Lingming										Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT								Arxiv											2	2;2024-12-09;https://www.arxiv.org/abs/2304.00385v2| 1;2023-04-01;https://www.arxiv.org/abs/2304.00385v1	arXiv:2304.00385			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 09 2024	2024	Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (clozestyle APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate (G&V) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial and yet previously ignored information in test failures as well as in plausible patches. To address these aforementioned limitations, we propose CHATREPAIR, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. CHATREPAIR first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement CHATREPAIR using state-of-the-art dialogue-based LLM – ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that CHATREPAIR is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!																																	2025-01-19	PPRN:53676148		
J	Fan, Zhiwen; Wang, Kevin; Wen, Kairun; Zhu, Zehao; Xu, Dejia; Wang, Zhangyang				Zhihua, Wang/AFO-5263-2022; Wen, Kairun/HCI-0027-2022						LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS								Arxiv											5	5;2024-11-12;https://www.arxiv.org/abs/2311.17245v6| 4;2024-03-29;https://www.arxiv.org/abs/2311.17245v5| 3;2024-02-06;https://www.arxiv.org/abs/2311.17245v4| 2;2023-12-07;https://www.arxiv.org/abs/2311.17245v3| 1;2023-12-04;https://www.arxiv.org/abs/2311.17245v2	arXiv:2311.17245			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 12 2024	2024	Recent advances in real-time neural rendering using point-based techniques have enabled broader adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting impose substantial storage overhead, as Structure-from-Motion (SfM) points can grow to millions, often requiring gigabyte-level disk space for a single unbounded scene. This growth presents scalability challenges and hinders splatting efficiency. To address this, we introduce LightGaussian, a method for transforming 3D Gaussians into a more compact format. Inspired by Network Pruning, LightGaussian identifies Gaussians with minimal global significance on scene reconstruction, and applies a pruning and recovery process to reduce redundancy while preserving visual quality. Knowledge distillation and pseudo-view augmentation then transfer spherical harmonic coefficients to a lower degree, yielding compact representations. Gaussian Vector Quantization, based on each Gaussian's global significance, further lowers bitwidth with minimal accuracy loss. LightGaussian achieves an average 15x compression rate while boosting FPS from 144 to 237 within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank & Temple datasets. The proposed Gaussian pruning approach is also adaptable to other 3D representations (e.g., Scaffold-GS), demonstrating strong generalization capabilities.																																	2024-12-18	PPRN:86368073		
J	Wang, Yi; Li, Kunchang; Li, Xinhao; Yu, Jiashuo; He, Yinan; Wang, Chenting; Chen, Guo; Pei, Baoqi; Yan, Ziang; Zheng, Rongkun; Xu, Jilan; Wang, Zun; Shi, Yansong; Jiang, Tianxiang; Li, Songze; Zhang, Hongjie; Huang, Yifei; Qiao, Yu; Wang, Yali; Wang, Limin				Wang, Limin/AAE-3419-2019; Huang, Yifei/ABE-4692-2020; He, Yinan/AAQ-6332-2020; Li, Kunchang/KFA-4043-2024; Jiang, Tianxiang/IYS-4435-2023; Zheng, Rongkun/A-7923-2008						InternVideo2: Scaling Foundation Models for Multimodal Video Understanding								Arxiv											2	2;2024-07-28;https://www.arxiv.org/abs/2403.15377v3| 1;2024-03-22;https://www.arxiv.org/abs/2403.15377v1	arXiv:2403.15377			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 28 2024	2024	We introduce InternVideo2, a new family of video foundation models (ViFM) that achieve the state-of-the-art results in video recognition, video-text tasks, and video-centric dialogue. Our core design is a progressive training approach that unifies the masked video modeling, crossmodal contrastive learning, and next token prediction, scaling up the video encoder size to 6B parameters. At the data level, we prioritize spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. Through extensive experiments, we validate our designs and demonstrate superior performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related dialogue and long video understanding benchmarks, highlighting its ability to reason and comprehend longer contexts. 																																	2024-08-04	PPRN:88261713		
J	Wang, Jianyi; Yue, Zongsheng; Zhou, Shangchen; Chan, Kelvin C.K.; Loy, Chen Change				Wang, Jianyi/ADL-2553-2022; Yue, Zongsheng/ADZ-0457-2022						Exploiting Diffusion Prior for Real-World Image Super-Resolution								Arxiv											4	4;2024-06-28;https://www.arxiv.org/abs/2305.07015v4| 3;2023-11-05;https://www.arxiv.org/abs/2305.07015v3| 2;2023-07-03;https://www.arxiv.org/abs/2305.07015v2| 1;2023-05-11;https://www.arxiv.org/abs/2305.07015v1	arXiv:2305.07015			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 28 2024	2024	We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution (SR). Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we employ a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches. 																																	2024-07-17	PPRN:69104855		
J	Pang, Richard Yuanzhe; Yuan, Weizhe; Cho, Kyunghyun; He, He; Sukhbaatar, Sainbayar; Weston, Jason										Iterative Reasoning Preference Optimization								Arxiv											2	2;2024-06-26;https://www.arxiv.org/abs/2404.19733v3| 1;2024-05-07;https://www.arxiv.org/abs/2404.19733v2	arXiv:2404.19733			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 26 2024	2024	Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer. We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy on GSM8K, MATH, and ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based models not relying on additionally sourced datasets. For example, we see a large improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with majority voting out of 32 samples.																																	2024-07-15	PPRN:88979202		
J	Liu, Fang; Liu, Yang; Shi, Lin; Huang, Houkun; Wang, Ruifeng; Yang, Zhen; Zhang, Li; Li, Zhongqi; Ma, Yuchi				Shi, Lin/LEM-4882-2024; Yang, Zhen/LWK-5010-2024; Ma, Yuchi/C-8792-2018; Zhang, Li/AAA-9787-2020						Exploring and Evaluating Hallucinations in LLM-Powered Code Generation								Arxiv											1	1;2024-05-11;https://www.arxiv.org/abs/2404.00971v2	arXiv:2404.00971			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 11 2024	2024	The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.																																	2024-06-08	PPRN:89034305		
J	Price, Ilan; Sanchez-Gonzalez, Alvaro; Alet, Ferran; Andersson, Tom R.; El-Kadi, Andrew; Masters, Dominic; Ewalds, Timo; Stott, Jacklynn; Mohamed, Shakir; Battaglia, Peter; Lam, Remi; Willson, Matthew				Andersson, Tom/ISU-6762-2023						GenCast: Diffusion-based ensemble forecasting for medium-range weather								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2312.15796v2| 1;2023-12-25;https://www.arxiv.org/abs/2312.15796v1	arXiv:2312.15796			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 01 2024	2024	Weather forecasts are fundamentally uncertain, so predicting the range of probable weather scenarios is crucial for important decisions, from warning the public about hazardous weather, to planning renewable energy use. Here, we introduce GenCast, a probabilistic weather model with greater skill and speed than the top operational medium-range weather forecast in the world, the European Centre for Medium-Range Forecasts (ECMWF)’s ensemble forecast, ENS. Unlike traditional approaches, which are based on numerical weather prediction (NWP), GenCast is a machine learning weather prediction (MLWP) method, trained on decades of reanalysis data. GenCast generates an ensemble of stochastic 15-day global forecasts, at 12-hour steps and 0 . 25 ◦ latitude-longitude resolution, for over 80 surface and atmospheric variables, in 8 minutes. It has greater skill than ENS on 97.4% of 1320 targets we evaluated, and better predicts extreme weather, tropical cyclones, and wind power production. This work helps open the next chapter in operational weather forecasting, where critical weather-dependent decisions are made with greater accuracy and efficiency.																																	2024-05-19	PPRN:86821159		
J	Wang, Zhenhailong; Mao, Shaoguang; Wu, Wenshan; Ge, Tao; Wei, Furu; Ji, Heng										Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration								Arxiv											3	3;2024-01-04;https://www.arxiv.org/abs/2307.05300v3| 2;2023-07-11;https://www.arxiv.org/abs/2307.05300v1| 1;2024-03-01;	arXiv:2307.05300			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 01 2024	2024	Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.																																	2025-11-07	PPRN:73870978		
J	Ding, Yiran; Zhang, Li Lyna; Zhang, Chengruidong; Xu, Yuanyuan; Shang, Ning; Xu, Jiahang; Yang, Fan; Yang, Mao				Ding, Yiran/JVP-2305-2024; 徐, 媛媛/JEF-9781-2023						LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens								Arxiv											1	1;2024-02-21;https://www.arxiv.org/abs/2402.13753v1	arXiv:2402.13753			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 21 2024	2024	Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.																																	2024-11-09	PPRN:87793739		
J	Ustun, Ahmet; Aryabumi, Viraat; Yong, Zheng-Xin; Ko, Wei-Yin; D'souza, Daniel; Onilude, Gbemileke; Bhandari, Neel; Singh, Shivalika; Ooi, Hui-Lee; Kayid, Amr; Vargus, Freddie; Blunsom, Phil; Longpre, Shayne; Muennighoff, Niklas; Fadaee, Marzieh; Kreutzer, Julia; Hooker, Sara				Kayid, Amr/ABA-4445-2022						Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model								Arxiv											1	1;2024-02-12;https://www.arxiv.org/abs/2402.07827v1	arXiv:2402.07827			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 12 2024	2024	Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101																																	2024-02-27	PPRN:87631108		
J	Zhang, Ningyu; Yao, Yunzhi; Tian, Bozhong; Wang, Peng; Deng, Shumin; Wang, Mengru; Xi, Zekun; Mao, Shengyu; Zhang, Jintian; Ni, Yuansheng; Cheng, Siyuan; Xu, Ziwen; Xu, Xin; Gu, Jia-Chen; Jiang, Yong; Xie, Pengjun; Huang, Fei; Liang, Lei; Zhang, Zhiqiang; Zhu, Xiaowei; Zhou, Jun; Chen, Huajun				Huajun, Chen/B-6340-2013; Deng, Shumin/AAP-7003-2021; Cheng, Siyuan/HIR-2767-2022; Zhang, Ningyu/AAQ-7391-2021; Wang, Pengcheng/JYQ-2527-2024						A Comprehensive Study of Knowledge Editing for Large Language Models								Arxiv											3	3;2024-11-17;https://www.arxiv.org/abs/2401.01286v5| 2;2024-03-28;https://www.arxiv.org/abs/2401.01286v4| 1;2024-01-02;https://www.arxiv.org/abs/2401.01286v1	arXiv:2401.01286			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 17 2024	2024	Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on- the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs’ behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the knowledge editing problem and then provide a comprehensive review of cutting-edge approaches. Drawing inspiration from educational and cognitive research theories [1–3], we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge. Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches. Additionally, we provide an in-depth analysis of knowledge location, which can give a deeper understanding of the knowledge structures inherent within LLMs. Initially conceived as a means to steer LLMs efficiently, we hope that insights gained from knowledge editing research could shed light on the underlying knowledge mechanisms of LLMs. To facilitate future research, we have released an open-source framework, EasyEdit1, which will enable practitioners to efficiently and flexibly implement knowledge editing for LLMs. Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.																																	2024-12-28	PPRN:86914188		
J	Li, Kenneth; Patel, Oam; Viegas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin				Pfister, Hanspeter/NJR-9877-2025						Inference-Time Intervention: Eliciting Truthful Answers from a Language Model								Arxiv											4	4;2024-06-26;https://www.arxiv.org/abs/2306.03341v6| 3;2023-10-20;https://www.arxiv.org/abs/2306.03341v5| 2;2023-09-21;https://www.arxiv.org/abs/2306.03341v4| 1;2023-06-07;https://www.arxiv.org/abs/2306.03341v2	arXiv:2306.03341			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 26 2024	2024	We introduce Inference-Time Intervention (ITI), a technique designed to enhance the "truthfulness" of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.																																	2024-07-15	PPRN:73042194		
J	Gao, Ruiqi; Holynski, Aleksander; Henzler, Philipp; Brussee, Arthur; Martin-Brualla, Ricardo; Srinivasan, Pratul; Barron, Jonathan T.; Poole, Ben				Gao, Rui-Qi/GRF-3082-2022						CAT3D: Create Anything in 3D with Multi-View Diffusion Models								Arxiv											1	1;2024-05-16;https://www.arxiv.org/abs/2405.10314v1	arXiv:2405.10314			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 16 2024	2024	Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation. 																																	2024-06-12	PPRN:89086360		
J	Dong, Runpei; Han, Chunrui; Peng, Yuang; Qi, Zekun; Ge, Zheng; Yang, Jinrong; Zhao, Liang; Sun, Jianjian; Zhou, Hongyu; Wei, Haoran; Kong, Xiangwen; Zhang, Xiangyu; Ma, Kaisheng; Yi, Li				Qi, Zekun/OJV-3861-2025; Kong, Xiangwen/AGE-1424-2022						DreamLLM: Synergistic Multimodal Comprehension and Creation								Arxiv											2	2;2024-03-15;https://www.arxiv.org/abs/2309.11499v2| 1;2023-09-20;https://www.arxiv.org/abs/2309.11499v1	arXiv:2309.11499			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 15 2024	2024	This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy.																																	2024-04-11	PPRN:85076554		
J	Pan, Shirui; Luo, Linhao; Wang, Yufei; Chen, Chen; Wang, Jiapu; Wu, Xindong				Wu, Xindong/AAB-6713-2022; LUO, LINHAO/IAM-3162-2023; Wang, Jiapu/JXN-2694-2024						Unifying Large Language Models and Knowledge Graphs: A Roadmap								Arxiv											2	2;2024-01-25;https://www.arxiv.org/abs/2306.08302v3| 1;2023-06-14;https://www.arxiv.org/abs/2306.08302v2	arXiv:2306.08302			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.																																	2024-05-25	PPRN:73358665		
J	Chen, Weifeng; Ji, Yatai; Wu, Jie; Wu, Hefeng; Xie, Pan; Li, Jiashi; Xia, Xin; Xiao, Xuefeng; Lin, Liang										Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning								Arxiv											3	3;2024-08-12;https://www.arxiv.org/abs/2305.13840v3| 2;2023-12-06;https://www.arxiv.org/abs/2305.13840v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.13840v1	arXiv:2305.13840			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 12 2024	2024	Recent advances in text-to-image (T2I) diffusion models have enabled impressive image generation capabilities guided by text prompts. However, extending these techniques to video generation remains challenging, with existing text-to-video (T2V) methods often struggling to produce high-quality and motion-consistent videos. In this work, we introduce Control-A-Video, a controllable T2V diffusion model that can generate videos conditioned on text prompts and reference control maps like edge and depth maps. To tackle video quality and motion consistency issues, we propose novel strategies to incorporate content prior and motion prior into the diffusion-based generation process. Specifically, we employ a first-frame condition scheme to transfer video generation from the image domain. Additionally, we introduce residual-based and optical flow-based noise initialization to infuse motion priors from reference videos, promoting relevance among frame latents for reduced flickering. Furthermore, we present a Spatio-Temporal Reward Feedback Learning (ST-ReFL) algorithm that optimizes the video diffusion model using multiple reward models for video quality and motion consistency, leading to superior outputs. Comprehensive experiments demonstrate that our framework generates higher-quality, more consistent videos compared to existing state-of-the-art methods in controllable text-to-video generation																																	2024-08-22	PPRN:71714908		
J	Lapid, Raz; Langberg, Ron; Sipper, Moshe										OPEN SESAME ! UNIVERSAL BLACK-BOX JAILBREAKING OF LARGE LANGUAGE MODELS								Arxiv											4	4;2024-08-05;https://www.arxiv.org/abs/2309.01446v4| 3;2023-11-21;https://www.arxiv.org/abs/2309.01446v3| 2;2023-09-17;https://www.arxiv.org/abs/2309.01446v2| 1;2023-09-04;https://www.arxiv.org/abs/2309.01446v1	arXiv:2309.01446			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 05 2024	2024	We introduce a novel approach that employs a Genetic Algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. . The GA attack works by optimizing a universal adversarial prompt that—when combined with a user’s query—disrupts the attacked model’s alignment, resulting in unintended and potentially harmful outputs. To our knowledge this is the first automated universal black-box jailbreak attack.																																	2024-08-11	PPRN:84730289		
J	Salemi, Alireza; Mysore, Sheshera; Bendersky, Michael; Zamani, Hamed				Salemi, Alireza/IWM-0238-2023						LaMP: When Large Language Models Meet Personalization								Arxiv											3	3;2024-06-05;https://www.arxiv.org/abs/2304.11406v4| 2;2024-01-09;https://www.arxiv.org/abs/2304.11406v3| 1;2023-04-22;https://www.arxiv.org/abs/2304.11406v1	arXiv:2304.11406			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 05 2024	2024	This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.																																	2024-07-11	PPRN:65246978		
J	Zelikman, Eric; Harik, Georges; Shao, Yijia; Jayasiri, Varuna; Haber, Nick; Goodman, Noah D.										Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking								Arxiv											2	2;2024-03-18;https://www.arxiv.org/abs/2403.09629v2| 1;2024-03-14;https://www.arxiv.org/abs/2403.09629v1	arXiv:2403.09629			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	When writing and talking, people sometimes pause to think. Although reasoning -focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self -Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few -shot examples in question -answering and learning from those that lead to a correct answer. This is a highly constrained setting – ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought’s start and end, and an extended teacher -forcing technique. Encouragingly, generated rationales disproportionately help model difficult -to -predict tokens and improve the LM’s ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero -shot improvements on GSM8K (5.9% -*10.9%) and CommonsenseQA (36.3% -*47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.																																	2024-04-11	PPRN:88140942		
J	Liu, Sijia; Yao, Yuanshun; Jia, Jinghan; Casper, Stephen; Baracaldo, Nathalie; Hase, Peter; Yao, Yuguang; Liu, Chris Yuhao; Xu, Xiaojun; Li, Hang; Varshney, Kush R.; Bansal, Mohit; Koyejo, Sanmi; Liu, Yang				Liu, Sijia/HOC-2459-2023; Xu, Xiaojun/KAL-6882-2024						Rethinking Machine Unlearning for Large Language Models								Arxiv											6	6;2024-12-06;https://www.arxiv.org/abs/2402.08787v6| 5;2024-07-15;https://www.arxiv.org/abs/2402.08787v5| 4;1800-01-01;https://www.arxiv.org/abs/2402.08787v4| 3;2024-04-05;https://www.arxiv.org/abs/2402.08787v3| 2;2024-02-15;https://www.arxiv.org/abs/2402.08787v2| 1;2024-02-01;	arXiv:2402.08787			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 06 2024	2024	We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.																																	2025-01-17	PPRN:87729343		
J	Zhang, Renrui; Jiang, Dongzhi; Zhang, Yichi; Lin, Haokun; Guo, Ziyu; Qiu, Pengshuo; Zhou, Aojun; Lu, Pan; Chang, Kai-Wei; Gao, Peng; Li, Hongsheng				Li, Hongsheng/AES-5328-2022; Chang, Kai-Wei/AAJ-7874-2020; Zhang, Zhuosheng/AAF-4919-2020						MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?								Arxiv											2	2;2024-08-18;https://www.arxiv.org/abs/2403.14624v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14624v1	arXiv:2403.14624			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 18 2024	2024	The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark may provide unique insights to guide the future development of MLLMs. 																																	2024-08-28	PPRN:88261039		
J	Wang, Haoxiang; Xiong, Wei; Xie, Tengyang; Zhao, Han; Zhang, Tong				Zhang, tong/IAP-2587-2023; Xie, Tengyang/ABM-6089-2022						Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts								Arxiv											1	1;2024-06-18;https://www.arxiv.org/abs/2406.12845v1	arXiv:2406.12845			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. The trained RM serves as a proxy for human preferences. However, due to the black -box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, it is desirable for them to be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a twostage approach: i) train an Absolute-Rating Multi -Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture -of -Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B , obtains stateof -the -art performance on RewardBench, a benchmark evaluating RMs for language modeling. Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model. 																																	2024-07-04	PPRN:89356441		
J	Anastassiou, Philip; Chen, Jiawei; Chen, Jitong; Chen, Yuanzhe; Chen, Zhuo; Chen, Ziyi; Cong, Jian; Deng, Lelai; Ding, Chuang; Gao, Lu; Gong, Mingqing; Huang, Peisong; Huang, Qingqing; Huang, Zhiying; Huo, Yuanyuan; Jia, Dongya; Li, Chumin; Li, Feiya; Li, Hui; Li, Jiaxin; Li, Xiaoyang; Li, Xingxing; Liu, Lin; Liu, Shouda; Liu, Sichao; Liu, Xudong; Liu, Yuchen; Liu, Zhengxi; Lu, Lu; Pan, Junjie; Wang, Xin; Wang, Yuping; Wang, Yuxuan; Wei, Zhen; Wu, Jian; Yao, Chao; Yang, Yifeng; Yi, Yuanhao; Zhang, Junteng; Zhang, Qidi; Zhang, Shuo; Zhang, Wenjie; Zhang, Yang; Zhao, Zilin; Zhong, Dejian; Zhuang, Xiaobin		ByteDance		Chen, Jitong/P-4806-2017; huo, YUANYUAN/IZE-7736-2023; Anastassiou, Philip/LKJ-4653-2024; Chen, Yuanzhe/ACU-7395-2022; ZHANG, Qidi/GLT-3979-2022; LI, xingxing/JTV-5937-2023; pan, junjie/GXM-4977-2022						Seed-TTS: A Family of High-Quality Versatile Speech Generation Models								Arxiv											1	1;2024-06-04;https://www.arxiv.org/abs/2406.02430v1	arXiv:2406.02430			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 04 2024	2024	We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named Seed-TTSDiT, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, Seed-TTSDiT does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. 																																	2024-06-22	PPRN:89259162		
J	He, Xiaoxin; Tian, Yijun; Sun, Yifei; Chawla, Nitesh V.; Laurent, Thomas; LeCun, Yann; Bresson, Xavier; Hooi, Bryan				Sun, Yifei/AAO-8801-2021						G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering								Arxiv											3	3;2024-05-27;https://www.arxiv.org/abs/2402.07630v3| 2;2024-03-14;https://www.arxiv.org/abs/2402.07630v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.07630v1	arXiv:2402.07630			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 27 2024	2024	Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.1																																	2024-06-09	PPRN:87631123		
J	Wang, Yidong; Yu, Zhuohao; Yao, Wenjin; Zeng, Zhengran; Yang, Linyi; Wang, Cunxiang; Chen, Hao; Jiang, Chaoya; Xie, Rui; Wang, Jindong; Xie, Xing; Ye, Wei; Zhang, Shikun; Zhang, Yue				Yu, Zhuohao/KFR-0312-2024; wang, jindong/ACD-8485-2022						PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization								Arxiv											2	2;2024-05-24;https://www.arxiv.org/abs/2306.05087v2| 1;2023-06-08;https://www.arxiv.org/abs/2306.05087v1	arXiv:2306.05087			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. 																																	2024-06-12	PPRN:73234104		
J	Wang, Junyang; Xu, Haiyang; Ye, Jiabo; Yan, Ming; Shen, Weizhou; Zhang, Ji; Huang, Fei; Sang, Jitao				wang, junyang/HMP-6590-2023; Xu, Haiyang/AAC-2095-2021						Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception								Arxiv											2	2;2024-04-18;https://www.arxiv.org/abs/2401.16158v2| 1;2024-01-29;https://www.arxiv.org/abs/2401.16158v1	arXiv:2401.16158			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 18 2024	2024	Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.																																	2024-04-28	PPRN:87391114		
J	Togo, Atsushi; Shinohara, Kohei; Tanaka, Isao				TOGO, Atsushi/D-1080-2017						Spglib: a software library for crystal symmetry search								Arxiv											2	2;2024-03-13;https://www.arxiv.org/abs/1808.01590v2| 1;2018-08-05;https://www.arxiv.org/abs/1808.01590v1	arXiv:1808.01590			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 13 2024	2024	A computer algorithm to search symmetries of crystal structures as implemented in the spglib code is described. An iterative algorithm is employed to robustly identify space group types tolerating a certain amount of distortion in the crystal structures. The source code is distributed under the 3-Clause BSD License, a permissive open-source software license. This paper focuses on the algorithm for identifying the space group symmetry of the crystal structures.																																	2024-04-11	PPRN:12885980		
J	Zhao, Zhiyuan; Wang, Bin; Ouyang, Linke; Dong, Xiaoyi; Wang, Jiaqi; He, Conghui				Zhao, Zhiyuan/AED-3875-2022; He, Conghui/AAZ-3323-2021; WANG, JIAQI/KBB-8837-2024; Dong, Xiaoyi/AAC-8666-2019; Wang, Bin/MVU-8917-2025						Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization								Arxiv											2	2;2024-02-06;https://www.arxiv.org/abs/2311.16839v2| 1;2023-11-28;https://www.arxiv.org/abs/2311.16839v1	arXiv:2311.16839			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	Multimodal large language models have made significant advancements in recent years, yet they still suffer from a common issue known as the "hallucination problem", in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images. This paper introduces a novel solution, Hallucination-Aware Direct Preference Optimization (HA-DPO), which reframes the hallucination problem as a preference selection task. The model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinatory). Furthermore, this paper proposes an efficient pipeline for constructing positive~(non-hallucinatory) and negative~(hallucinatory) sample pairs, ensuring a high-quality, style-consistent dataset for robust preference learning. When applied to three mainstream multimodal models, HA-DPO significantly reduced hallucination issues and amplified the models' generalization capabilities. Notably, the MiniGPT-4 model, when enhanced with HA-DPO, demonstrated a substantial improvement: POPE accuracy rose from 51.13% to 86.13% (an absolute improvement of 35%), and the MME score surged from 932.00 to 1326.46 (a relative improvement of 42.32%). 																																	2024-02-21	PPRN:86309361		
J	Abbott, T.M.C.; Acevedo, M.; Aguena, M.; Alarcon, A.; Allam, S.; Alves, O.; Amon, A.; Andrade-Oliveira, F.; Annis, J.; Armstrong, P.; Asorey, J.; Avila, S.; Bacon, D.; Bassett, B.A.; Bechtol, K.; Bernardinelli, P.H.; Bernstein, G.M.; Bertin, E.; Blazek, J.; Bocquet, S.; Brooks, D.; Brout, D.; Buckley-Geer, E.; Burke, D.L.; Camacho, H.; Camilleri, R.; Campos, A.; Carnero Rosell, A.; Carollo, D.; Carr, A.; Carretero, J.; Castander, F.J.; Cawthon, R.; Chang, C.; Chen, R.; Choi, A.; Conselice, C.; Costanzi, M.; da Costa, L.N.; Crocce, M.; Davis, T.M.; DePoy, D.L.; Desai, S.; Diehl, H.T.; Dixon, M.; Dodelson, S.; Doel, P.; Doux, C.; Drlica-Wagner, A.; Elvin-Poole, J.; Everett, S.; Ferrero, I.; Ferte, A.; Flaugher, B.; Foley, R.J.; Fosalba, P.; Friedel, D.; Frieman, J.; Frohmaier, C.; Galbany, L.; Garcia-Bellido, J.; Gatti, M.; Gaztanaga, E.; Giannini, G.; Glazebrook, K.; Graur, O.; Gruen, D.; Gruendl, R.A.; Gutierrez, G.; Hartley, W.G.; Herner, K.; Hinton, S.R.; Hollowood, D.L.; Honscheid, K.; Huterer, D.; Jain, B.; James, D.J.; Jeffrey, N.; Kelsey, L.; Kent, S.; Kessler, R.; Kim, A.G.; Kirshner, R.P.; Kovacs, E.; Kuehn, K.; Lahav, O.; Lee, J.; Lee, S.; Lewis, G.F.; Li, T.S.; Lidman, C.; Lin, H.; Marshall, J.L.; Martini, P.; Mena-Fernandez, J.; Menanteau, F.; Miquel, R.; Mohr, J.J.; Mould, J.; Muir, J.; Moeller, A.; Neilsen, E.; Nichol, R.C.; Nugent, P.; Ogando, R.L.C.; Palmese, A.; Pan, Y.-C.; Paterno, M.; Percival, W.J.; Pereira, M.E.S.; Pieres, A.; Malagon, A.A. Plazas; Popovic, B.; Porredon, A.; Prat, J.; Qu, H.; Raveri, M.; Rodriguez-Monroy, M.; Romer, A.K.; Roodman, A.; Rose, B.; Sako, M.; Sanchez, E.; Sanchez Cid, D.; Schubnell, M.; Scolnic, D.; Sevilla-Noarbe, I.; Shah, P.; Smith, J.Allyn.; Smith, M.; Soares-Santos, M.; Suchyta, E.; Sullivan, M.; Suntzeff, N.; Swanson, M.E.C.; Sanchez, B.O.; Tarle, G.; Taylor, G.; Thomas, D.; To, C.; Toy, M.; Troxel, M.A.; Tucker, B.E.; Tucker, D.L.; Uddin, S.A.; Vincenzi, M.; Walker, A.R.; Weaverdyck, N.; Wechsler, R.H.; Weller, J.; Wester, W.; Wiseman, P.; Yamamoto, M.; Yuan, F.; Zhang, B.; Zhang, Y.		DES Collaboration		Pereira, Moisés/JCE-2886-2023; Wechsler, Risa/JOZ-3146-2023; Frieman, Josh/AED-8202-2022; Armstrong, Patrick/MIK-4482-2025						The Dark Energy Survey: Cosmology Results With ~1500 New High-redshift Type Ia Supernovae Using The Full 5-year Dataset								Arxiv											1	1;2024-01-08;https://www.arxiv.org/abs/2401.02929v2	arXiv:2401.02929			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 08 2024	2024	We present cosmological constraints from the sample of Type Ia supernovae (SN Ia) discovered and measured during the full five years of the Dark Energy Survey (DES) Supernova Program. In contrast to most previous cosmological samples, in which supernovae are classified based on their spectra, we classify the DES supernovae using a machine learning algorithm applied to their light curves in four photometric bands. Spectroscopic redshifts are acquired from a dedicated follow-up survey of the host galaxies. After accounting for the likelihood of each supernova being a SN Ia, we find 1635 DES SNe in the redshift-range 0.10 < z < 1.13 that pass quality selection criteria and can be used to constrain cosmological parameters. This quintuples the number of high-quality z > 0.5 SNe compared to the previous leading compilation of Pantheon+, and results in the tightest cosmological constraints achieved by any supernova data set to date. To derive cosmological constraints we combine the DES supernova data with a high-quality external low-redshift sample consisting of 194 SNe Ia spanning 0.025 < z < 0.10. Using supernova data alone and including systematic uncertainties we find ΩM = 0.352 + 0.017 in flat-ΛCDM, and (ΩM, w) = (0.264+0.074−0.096, −0.80+0.14 −0.16) in flat-wCDM. For flat-w0waCDM, we find (ΩM, w0, wa) = (0.495+0.033−0.043, −0.36+0.36−0.30, −8.8+3.7 −4.5), consistent with a constant equation of state to within ∼ 2o-. Including Planck CMB data, SDSS BAO data, and DES 3 × 2-point data gives (ΩM, w) = (0.321 + 0.007, −0.941 + 0.026). In all cases dark energy is consistent with a cosmological constant to within ∼ 2σ. In our analysis, systematic errors on cosmological parameters are subdominant compared to statistical errors; these results thus pave the way for future photometrically classified supernova analyses.																																	2024-01-25	PPRN:87047022		
J	Zhang, Ruiqi; Lin, Licong; Bai, Yu; Mei, Song				Mei, Song/AFQ-2667-2022; Zhang, Ruiqi/JRY-0376-2023; Bai, Yu/AAG-7494-2020						Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning								Arxiv											2	2;2024-10-10;https://www.arxiv.org/abs/2404.05868v2| 1;2024-04-08;https://www.arxiv.org/abs/2404.05868v1	arXiv:2404.05868			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 10 2024	2024	Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data.																																	2024-11-03	PPRN:88467575		
J	Lin, Jianghao; Dai, Xinyi; Xi, Yunjia; Liu, Weiwen; Chen, Bo; Zhang, Hao; Liu, Yong; Wu, Chuhan; Li, Xiangyang; Zhu, Chenxu; Guo, Huifeng; Yu, Yong; Tang, Ruiming; Zhang, Weinan				Xi, Yunjia/LUW-8681-2024; Liu, Weiwen/LMQ-1488-2024						How Can Recommender Systems Benefit from Large Language Models: A Survey								Arxiv											3	3;2024-07-09;https://www.arxiv.org/abs/2306.05817v6| 2;2024-02-02;https://www.arxiv.org/abs/2306.05817v5| 1;2023-06-12;https://www.arxiv.org/abs/2306.05817v2	arXiv:2306.05817			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 09 2024	2024	With the rapid development of online services, recommender systems (RS) have become increasingly indispensable for mitigating information overload. Despite remarkable progress, conventional recommendation models (CRM) still have some limitations, e.g., lacking open-world knowledge, and difficulties in comprehending users' underlying preferences and motivations. Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities, which mainly stem from their extensive open-world knowledge, reasoning ability, as well as their comprehension of human culture and society. Consequently, the emergence of LLM is inspiring the design of recommender systems and pointing out a promising research direction, i.e., whether we can incorporate LLM and benefit from their knowledge and capabilities to compensate for the limitations of CRM. In this paper, we conduct a comprehensive survey on this research direction from the perspective of the whole pipeline in real-world recommender systems. Specifically, we summarize existing works from two orthogonal aspects: where and how to adapt LLM to RS. For the WHERE question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, user interaction, and pipeline controller. For the HOW question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLM or not, and whether to involve conventional recommendation models for inference. Then, we highlight key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects. 																																	2024-07-21	PPRN:73301088		
J	Lin, Shanchuan; Wang, Anran; Yang, Xiao				Wang, Anran/L-5829-2017						SDXL-Lightning: Progressive Adversarial Diffusion Distillation								Arxiv											3	3;2024-03-02;https://www.arxiv.org/abs/2402.13929v3| 2;2024-02-26;https://www.arxiv.org/abs/2402.13929v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.13929v1	arXiv:2402.13929			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 02 2024	2024	We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.																																	2024-03-30	PPRN:87793712		
J	Li, Daiqing; Kamko, Aleks; Akhgari, Ehsan; Sabet, Ali; Xu, Linmiao; Doshi, Suhail				Doshi, Smit/IQS-3468-2023						Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17245v1	arXiv:2402.17245			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models.																																	2024-11-09	PPRN:87923026		
J	Li, Xianming; Li, Jing				Li, Xianming/AAL-7797-2020; Li, Jing/IQU-9459-2023						AnglE-optimized Text Embeddings								Arxiv											9	9;2024-12-31;https://www.arxiv.org/abs/2309.12871v9| 8;2024-07-17;https://www.arxiv.org/abs/2309.12871v8| 7;2024-05-16;https://www.arxiv.org/abs/2309.12871v7| 6;2023-11-08;https://www.arxiv.org/abs/2309.12871v6| 5;2023-10-24;https://www.arxiv.org/abs/2309.12871v5| 4;2023-10-19;https://www.arxiv.org/abs/2309.12871v4| 3;2023-10-17;https://www.arxiv.org/abs/2309.12871v3| 2;2023-10-05;https://www.arxiv.org/abs/2309.12871v2| 1;2023-09-22;https://www.arxiv.org/abs/2309.12871v1	arXiv:2309.12871			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 31 2024	2024	High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data. Extensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks. The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.																																	2025-02-15	PPRN:85175784		
J	Yang, Dongchao; Tian, Jinchuan; Tan, Xu; Huang, Rongjie; Liu, Songxiang; Chang, Xuankai; Shi, Jiatong; Zhao, Sheng; Bian, Jiang; Zhao, Zhou; Wu, Xixin; Meng, Helen				Yang, Dongchao/Y-8059-2018; Shi, Jiatong/HCI-6201-2022; zhao, sheng/JWO-6127-2024						UniAudio: An Audio Foundation Model Toward Universal Audio Generation								Arxiv											5	5;2024-12-10;https://www.arxiv.org/abs/2310.00704v6| 4;2023-12-11;https://www.arxiv.org/abs/2310.00704v5| 3;2023-10-11;https://www.arxiv.org/abs/2310.00704v4| 2;2023-10-05;https://www.arxiv.org/abs/2310.00704v2| 1;2023-10-01;https://www.arxiv.org/abs/2310.00704v1	arXiv:2310.00704			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 10 2024	2024	Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization-based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 audio generation tasks. Demo and code are released.																																	2025-01-19	PPRN:85355761		
J	Yue, Xiang; Zheng, Tianyu; Ni, Yuansheng; Wang, Yubo; Zhang, Kai; Tong, Shengbang; Sun, Yuxuan; Yin, Ming; Yu, Botao; Zhang, Ge; Sun, Huan; Su, Yu; Chen, Wenhu; Neubig, Graham				Yu, Botao/LIH-7233-2024; Zheng, Tianyu/JXM-4664-2024						MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark								Arxiv											1	1;2024-09-04;https://www.arxiv.org/abs/2409.02813v1	arXiv:2409.02813			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 04 2024	2024	This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly "see" and "read" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.																																	2024-09-13	PPRN:91735364		
J	Xie, Tianbao; Zhang, Danyang; Chen, Jixuan; Li, Xiaochuan; Zhao, Siheng; Cao, Ruisheng; Hua, Toh Jing; Cheng, Zhoujun; Shin, Dongchan; Lei, Fangyu; Liu, Yitao; Xu, Yiheng; Zhou, Shuyan; Savarese, Silvio; Xiong, Caiming; Zhong, Victor; Yu, Tao				zhou, shuyan/GYR-0523-2022; Liu, Yitao/KSM-2343-2024; Cao, Ruisheng/NFT-9188-2025						OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2404.07972v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.07972v1	arXiv:2404.07972			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 30 2024	2024	Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OS W ORLD , the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OS W ORLD can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OS W ORLD , we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OS W ORLD reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OS W ORLD provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at https://os-world.github.io. .																																	2024-06-16	PPRN:88500278		
J	Miao, Xupeng; Oliaro, Gabriele; Zhang, Zhihao; Cheng, Xinhao; Wang, Zeyu; Zhang, Zhengxin; Wong, Rae Ying Yee; Zhu, Alan; Yang, Lijie; Shi, Xiaoxiang; Shi, Chunan; Chen, Zhuoming; Arfeen, Daiyaan; Abhyankar, Reyna; Jia, Zhihao				ZHIHAO, ZHANG/KMG-3126-2024; Zhang, Zhengxin/OEN-4949-2025; Wang, Zeyu/AAB-1311-2022; Shi, Xiaoxiang/JQV-6315-2023						SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification								Arxiv											3	3;2024-04-01;https://www.arxiv.org/abs/2305.09781v4| 2;2024-01-23;https://www.arxiv.org/abs/2305.09781v3| 1;2023-05-16;https://www.arxiv.org/abs/2305.09781v1	arXiv:2305.09781			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8x for distributed LLM inference and by 2.6-3.5x for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/																																	2024-04-17	PPRN:70037532		
J	Zheng, Xin; Wang, Yi; Liu, Yixin; Li, Ming; Zhang, Miao; Jin, Di; Yu, Philip S.; Pan, Shirui				ZHENG, XIN/MVX-3834-2025; Wang, Yi/KYQ-2026-2024; Liu, Yixin/HPD-6922-2023; Jin, Di/AAC-3716-2019; Li, Ming/GYQ-7804-2022						Graph Neural Networks for Graphs with Heterophily: A Survey								Arxiv											2	2;2024-02-25;https://www.arxiv.org/abs/2202.07082v3| 1;2024-02-22;https://www.arxiv.org/abs/2202.07082v2	arXiv:2202.07082			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 25 2024	2024	Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real -world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. Furthermore, we discuss the correlation between graph heterophily and various graph research domains, aiming to facilitate the development of more effective GNNs across a spectrum of practical applications and learning tasks in the graph research community. In the end, we point out the potential directions to advance and stimulate more future research and applications on heterophilic graph learning with GNNs.																																	2024-03-28	PPRN:87804595		
J	Peng, Boci; Zhu, Yun; Liu, Yongchao; Bo, Xiaohe; Shi, Haizhou; Hong, Chuntao; Zhang, Yan; Tang, Siliang				Yongchao Liu/AAW-4770-2021; Peng, Bo/ADV-5422-2022						Graph Retrieval-Augmented Generation: A Survey								Arxiv											2	2;2024-09-10;https://www.arxiv.org/abs/2408.08921v2| 1;2024-08-15;https://www.arxiv.org/abs/2408.08921v1	arXiv:2408.08921			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 10 2024	2024	Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as “hallucination”, lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at https://github.com/pengboci/GraphRAG-Survey.																																	2024-09-26	PPRN:91491634		
J	Lu, Shiyin; Li, Yang; Chen, Qing-Guo; Xu, Zhao; Luo, Weihua; Zhang, Kaifu; Ye, Han-Jia				Chen, Qingguo/E-4346-2014; Zhang, Kaifu/NJR-1612-2025						Ovis: Structural Embedding Alignment for Multimodal Large Language Model								Arxiv											1	1;2024-05-31;https://www.arxiv.org/abs/2405.20797v1	arXiv:2405.20797			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 31 2024	2024	Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities. However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information. We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings. This structural approach mirrors the method used for generating textual embeddings. Empirical evaluations on various multimodal benchmarks demonstrate that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall. These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning. Both the source code and the training dataset of Ovis will be made publicly available.																																	2024-06-19	PPRN:89127122		
J	Zhao, Wenting; Ren, Xiang; Hessel, Jack; Cardie, Claire; Choi, Yejin; Deng, Yuntian				Deng, yuntian/KIB-9835-2024						WildChat: 1M ChatGPT Interaction Logs in the Wild								Arxiv											1	1;2024-05-02;https://www.arxiv.org/abs/2405.01470v1	arXiv:2405.01470			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual opt -in to anonymously collect their chat transcripts and request headers. From this, we compiled W ILD C HAT , a corpus of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns. We compare W ILD C HAT with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use -cases for researchers to study. In addition to timestamped chat transcripts, we enrich the dataset with demographic data, including state, country, and hashed IP addresses, alongside request headers. This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions. Finally, because it captures a broad range of use cases, we demonstrate the dataset’s potential utility in fine-tuning instruction -following models. W ILD C HAT is released at https://wildchat.allen.ai under AI2 ImpACT Licenses1. 																																	2024-07-04	PPRN:88766871		
J	Xu, Jing; Lee, Andrew; Sukhbaatar, Sainbayar; Weston, Jason										<italic>Some things are more</italic> CRINGE <italic>than others</italic>: Iterative Preference Optimization with the Pairwise Cringe Loss								Arxiv											2	2;2024-04-22;https://www.arxiv.org/abs/2312.16682v2| 1;2023-12-27;https://www.arxiv.org/abs/2312.16682v1	arXiv:2312.16682			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 22 2024	2024	Practitioners commonly align large language models using pairwise preferences, i.e., given labels of the type response A is preferred to response B for a given input. Perhaps less commonly, methods have also been developed for binary feedback, i.e. training models given labels of type response A is good or bad. We show how an existing performant binary feedback method, the Cringe Loss (Adolphs et al., 2022), can be generalized to the pairwise preference setting using a simple soft margin extension. Pairwise Cringe Loss is straightforward to implement and efficient to train, and we find it outperforms state-of-the-art preference optimization algorithms such as PPO and DPO on the AlpacaFarm benchmark. We show that iterations of training of our model are important for improved results, and that we can generalize DPO to Iterative DPO in the same way.																																	2024-05-02	PPRN:86852054		
J	Bianchi, Federico; Suzgun, Mirac; Attanasio, Giuseppe; Rottger, Paul; Jurafsky, Dan; Hashimoto, Tatsunori; Zou, James				Bianchi, Federico/JZT-6891-2024; Attanasio, Giuseppe/NKE-0343-2025; Zhang, James/HHS-8616-2022						Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions								Arxiv											3	3;2024-03-19;https://www.arxiv.org/abs/2309.07875v3| 2;2023-09-25;https://www.arxiv.org/abs/2309.07875v2| 1;2023-09-14;https://www.arxiv.org/abs/2309.07875v1	arXiv:2309.07875			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3% safety examples (a few hundred demonstrations) when fine-tuning a model like LLaMA can substantially improve its safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones. As a whole, our results illustrate trade-offs in training LLMs to be helpful and training them to be safe.																																	2024-04-12	PPRN:85016606		
J	Ziems, Caleb; Held, William; Shaikh, Omar; Chen, Jiaao; Zhang, Zhehao; Yang, Diyi										Can Large Language Models Transform Computational Social Science?								Arxiv											3	3;2024-02-26;https://www.arxiv.org/abs/2305.03514v3| 2;2023-12-07;https://www.arxiv.org/abs/2305.03514v2| 1;2023-04-12;https://www.arxiv.org/abs/2305.03514v1	arXiv:2305.03514			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	Large Language Models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the Computational Social Science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zeroshot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.																																	2024-11-09	PPRN:68145255		
J	Xiao, Shitao; Wang, Yueze; Zhou, Junjie; Yuan, Huaying; Xing, Xingrun; Yan, Ruiran; Li, Chaofan; Wang, Shuting; Huang, Tiejun; Liu, Zheng				Liu, Zheng/AHI-3660-2022; Z, J/GZG-3471-2022; Huang, Tiejun/D-6161-2011						OmniGen: Unified Image Generation								Arxiv											2	2;2024-11-21;https://www.arxiv.org/abs/2409.11340v2| 1;2024-09-17;https://www.arxiv.org/abs/2409.11340v1	arXiv:2409.11340			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 21 2024	2024	The emergence of Large Language Models (LLMs) has unified language generation tasks and revolutionized human-machine interaction. However, in the realm of image generation, a unified model capable of handling various tasks within a single framework remains largely unexplored. In this work, we introduce OmniGen, a new diffusion model for unified image generation. OmniGen is characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports various downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional plugins. Moreover, compared to existing diffusion models, it is more user-friendly and can complete complex tasks end-to-end through instructions without the need for extra intermediate steps, greatly simplifying the image generation workflow. 3) Knowledge Transfer: Benefit from learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of the chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model.																																	2024-12-31	PPRN:91941562		
J	Liu, Zijun; Zhang, Yanzhe; Li, Peng; Liu, Yang; Yang, Diyi				zhang, yanzhe/O-7292-2015; liu, zijun/HJO-9912-2023						A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration								Arxiv											2	2;2024-11-15;https://www.arxiv.org/abs/2310.02170v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02170v1	arXiv:2310.02170			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 15 2024	2024	Recent studies show that collaborating multiple large language model (LLM) powered agents is a promising way for task solving. However, current approaches are constrained by using a fixed number of agents and static communication structures. In this work, we propose automatically selecting a team of agents from candidates to collaborate in a dynamic communication structure toward different tasks and domains. Specifically, we build a framework named Dynamic LLM-Powered Agent Network ( DyLAN ) for LLM-powered agent collaboration, operating a two-stage paradigm: (1) Team Optimization and (2) Task Solving. During the first stage, we utilize an agent selection algorithm, based on an unsupervised metric called Agent Importance Score, enabling the selection of best agents according to their contributions in a preliminary trial, oriented to the given task. Then, in the second stage, the selected agents collaborate dynamically according to the query. Empirically, we demonstrate that DyLAN outperforms strong baselines in code generation, decision-making, general reasoning, and arithmetic reasoning tasks with moderate computational cost. On specific subjects in MMLU, selecting a team of agents in the team optimization stage improves accuracy by up to 25.0% in DyLAN.																																	2024-12-25	PPRN:85378338		
J	Agarwal, Rishabh; Singh, Avi; Zhang, Lei M.; Bohnet, Bernd; Rosias, Luis; Chan, Stephanie C.Y; Zhang, Biao; Anand, Ankesh; Abbas, Zaheer; Nova, Azade; Co-Reyes, John D.; Chu, Eric; Behbahani, Feryal; Faust, Aleksandra; Larochelle, Hugo										Many-Shot In-Context Learning								Arxiv											3	3;2024-10-17;https://www.arxiv.org/abs/2404.11018v3| 2;2024-05-22;https://www.arxiv.org/abs/2404.11018v2| 1;2024-04-17;https://www.arxiv.org/abs/2404.11018v1	arXiv:2404.11018			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. We also find that inference cost increases linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL to varying degrees. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.																																	2024-11-15	PPRN:88556979		
J	Agrawal, Pravesh; Antoniak, Szymon; Hanna, Emma Bou; Bout, Baptiste; Chaplot, Devendra; Chudnovsky, Jessica; Costa, Diogo; De Monicault, Baudouin; Garg, Saurabh; Gervet, Theophile; Ghosh, Soham; Heliou, Amelie; Jacob, Paul; Jiang, Albert Q.; Khandelwal, Kartik; Lacroix, Timothee; Lample, Guillaume; Casas, Diego Las; Lavril, Thibaut; Le Scao, Teven; Lo, Andy; Marshall, William; Martin, Louis; Mensch, Arthur; Muddireddy, Pavankumar; Nemychnikova, Valera; Pellat, Marie; Von Platen, Patrick; Raghuraman, Nikhil; Roziere, Baptiste; Sablayrolles, Alexandre; Saulnier, Lucile; Sauvestre, Romain; Shang, Wendy; Soletskyi, Roman; Stewart, Lawrence; Stock, Pierre; Studnia, Joachim; Subramanian, Sandeep; Vaze, Sagar; Wang, Thomas; Yang, Sophia		Mistral AI Sci Team								Pixtral 12B								Arxiv											2	2;2024-10-10;https://www.arxiv.org/abs/2410.07073v2| 1;2024-10-09;https://www.arxiv.org/abs/2410.07073v1	arXiv:2410.07073			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 10 2024	2024	We introduce Pixtral 12B, a 12–billion-parameter multimodal language model. Pixtral 12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B & Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral 12B is released under Apache 2.0 license.																																	2024-11-06	PPRN:105761160		
J	Yang, Sherry; Du, Yilun; Ghasemipour, Seyed Kamyar Seyed; Tompson, Jonathan; Kaelbling, Leslie; Schuurmans, Dale; Abbeel, Pieter										Learning Interactive Real-World Simulators								Arxiv											3	3;2024-09-26;https://www.arxiv.org/abs/2310.06114v3| 2;2024-01-13;https://www.arxiv.org/abs/2310.06114v2| 1;2023-10-09;https://www.arxiv.org/abs/2310.06114v1	arXiv:2310.06114			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 26 2024	2024	Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by ∆x, ∆y” from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications.																																	2024-10-09	PPRN:85526962		
J	Liu, Zhijian; Tang, Haotian; Amini, Alexander; Yang, Xinyu; Mao, Huizi; Rus, Daniela; Han, Song				Rodriguez, Daniela/HNP-6345-2023						BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation								Arxiv											2	2;2024-09-01;https://www.arxiv.org/abs/2205.13542v3| 1;2022-05-26;https://www.arxiv.org/abs/2205.13542v2	arXiv:2205.13542			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 01 2024	2024	Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. 																																	2024-09-11	PPRN:12220958		
J	Rafailov, Rafael; Hejna, Joey; Park, Ryan; Finn, Chelsea										From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function								Arxiv											2	2;2024-08-12;https://www.arxiv.org/abs/2404.12358v2| 1;2024-04-18;https://www.arxiv.org/abs/2404.12358v1	arXiv:2404.12358			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 12 2024	2024	Reinforcement Learning From Human Feedback (RLHF) has been critical to the success of the latest generation of generative AI models. In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches. Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm. In this work we rectify this difference. We theoretically show that we can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation. Using our theoretical results, we provide three concrete empirical insights. First, we show that because of its token level interpretation, DPO is able to perform some type of credit assignment. Next, we prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy. Empirically we show that a simple beam search yields meaningful improvement over the base DPO policy. Finally, we show how the choice of reference policy causes implicit rewards to decline during training. We conclude by discussing applications of our work, including information elicitation in multi-turn dialogue, reasoning, agentic applications and end-to-end training of multi-model systems.																																	2024-08-22	PPRN:88564604		
J	Xie, Yuxi; Goyal, Anirudh; Zheng, Wenyue; Kan, Min-Yen; Lillicrap, Timothy; Kawaguchi, Kenji; Shieh, Michael				Xie, Yuxi/GXG-7686-2022; Kan, Min-Yen/AAI-8029-2020; Kawaguchi, Kenji/AAR-8297-2020						Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2405.00451v2| 1;2024-05-01;https://www.arxiv.org/abs/2405.00451v1	arXiv:2405.00451			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to 81.8% (+5.9%), 34.7% (+5.8%), and 76.4% (+15.8%), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. 																																	2024-07-04	PPRN:88712087		
J	Dong, Yihong; Jiang, Xue; Jin, Zhi; Li, Ge				Dong, Yihong/LCE-6194-2024; Jin, Zhi/E-1288-2013						Self-collaboration Code Generation via ChatGPT								Arxiv											2	2;2024-05-11;https://www.arxiv.org/abs/2304.07590v3| 1;2023-04-15;https://www.arxiv.org/abs/2304.07590v1	arXiv:2304.07590			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 11 2024	2024	Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct ‘experts’, each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9%-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that selfcollaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.																																	2024-05-30	PPRN:63426752		
J	Liu, Mingjie; Ene, Teodor-Dumitru; Kirby, Robert; Cheng, Chris; Pinckney, Nathaniel; Liang, Rongjian; Alben, Jonah; Anand, Himyanshu; Banerjee, Sanmitra; Bayraktaroglu, Ismet; Bhaskaran, Bonita; Catanzaro, Bryan; Chaudhuri, Arjun; Clay, Sharon; Dally, Bill; Dang, Laura; Deshpande, Parikshit; Dhodhi, Siddhanth; Halepete, Sameer; Hill, Eric; Hu, Jiashang; Jain, Sumit; Jindal, Ankit; Khailany, Brucek; Kokai, George; Kunal, Kishor; Li, Xiaowei; Lind, Charley; Liu, Hao; Oberman, Stuart; Omar, Sujeet; Pasandi, Ghasem; Pratty, Sreedhar; Raiman, Jonathan; Sarkar, Ambar; Shao, Zhengjiang; Sun, Hanfei; Suthar, Pratik P; Tej, Varun; Turner, Walker; Xu, Kaizhe; Ren, Haoxing				Liang, Rongjian/ABE-3318-2022; Hill, Eric/B-1674-2012; Kunal, Kishor/MIO-3330-2025						ChipNeMo: Domain-Adapted LLMs for Chip Design								Arxiv											4	4;2024-04-04;https://www.arxiv.org/abs/2311.00176v5| 3;2024-03-07;https://www.arxiv.org/abs/2311.00176v4| 2;2023-12-03;https://www.arxiv.org/abs/2311.00176v3| 1;2023-10-31;https://www.arxiv.org/abs/2311.00176v1	arXiv:2311.00176			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 04 2024	2024	ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: domain-adaptive tokenization, domain-adaptive continued pretraining, model alignment with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our evaluations demonstrate that domain-adaptive pretraining of language models, can lead to superior performance in domain related downstream tasks compared to their base LLaMA2 counterparts, without degradations in generic capabilities. In particular, our largest model, ChipNeMo-70B, outperforms the highly capable GPT-4 on two of our use cases, namely engineering assistant chatbot and EDA scripts generation, while exhibiting competitive performance on bug summarization and analysis. These results underscore the potential of domain-specific customization for enhancing the effectiveness of large language models in specialized applications.																																	2024-04-20	PPRN:85927556		
J	Jiang, Dongfu; He, Xuan; Zeng, Huaye; Wei, Cong; Ku, Max; Liu, Qian; Chen, Wenhu				Jiang, Dongfu/JTV-4943-2023						MANTIS: Interleaved Multi-Image Instruction Tuning								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2405.01483v2| 1;2024-05-02;https://www.arxiv.org/abs/2405.01483v1	arXiv:2405.01483			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Large multimodal models (LMMs) have shown great results in single-image vision language tasks. However, their abilities to solve multi-image visual language tasks is yet to be improved. The existing LMMs like OpenFlamingo, Emu2, Idefics gain their multi-image ability through pre-training on hundreds of millions of noisy interleaved image-text data from the web, which is neither efficient nor effective. In this paper, we aim to build strong multi-image LMMs via instruction tuning with academic-level resources. Therefore, we meticulously construct Mantis-Instruct containing 721K multi-image instruction data to train a family of models Mantis. The instruction tuning empowers Mantis with different multi-image skills like co-reference, comparison, reasoning, and temporal understanding. We evaluate Mantis on five multi-image benchmarks and seven single-image benchmarks. Mantis-SigLIP can achieve SoTA results on all the multi-image benchmarks and beat the strongest multi-image baseline, Idefics2-8B by an average of 11 absolute points. Notably, Idefics2-8B was pre-trained on 140M interleaved multi-image data, which is 200x larger than Mantis-Instruct. We observe that Mantis performs equivalently well on the held-in and held-out benchmarks, which shows its generalization ability. Notably, we found that Mantis can even match the performance of GPT-4V on multi-image benchmarks. We further evaluate Mantis on single-image benchmarks and demonstrate that Mantis also maintains a strong single-image performance on par with CogVLM and Emu2. Our results show that multi-image abilities are not necessarily gained through massive pre-training, instead, it can be gained by the low-cost instruction tuning. Our work provides new perspectives on how to improve LMMs' multi-image abilities.   [GRAPHIC]																																	2024-06-06	PPRN:88722817		
J	Yang, Lihe; Kang, Bingyi; Huang, Zilong; Xu, Xiaogang; Feng, Jiashi; Zhao, Hengshuang				Kang, Bingyi/AAR-2697-2020; Huang, Zilong/AAW-6071-2021; Feng, Jiashi/AGX-6209-2022						Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data								Arxiv											2	2;2024-04-07;https://www.arxiv.org/abs/2401.10891v2| 1;2024-01-19;https://www.arxiv.org/abs/2401.10891v1	arXiv:2401.10891			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 07 2024	2024	This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. 																																	2024-04-21	PPRN:87309460		
J	Sun, Jiashuo; Xu, Chengjin; Tang, Lumingyuan; Wang, Saizhuo; Lin, Chen; Gong, Yeyun; Ni, Lionel M.; Shum, Heung-Yeung; Guo, Jian										Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph								Arxiv											6	6;2023-11-24;https://www.arxiv.org/abs/2307.07697v5| 5;2023-11-07;https://www.arxiv.org/abs/2307.07697v4| 4;2023-09-30;https://www.arxiv.org/abs/2307.07697v3| 3;2023-09-19;https://www.arxiv.org/abs/2307.07697v2| 2;2023-07-15;https://www.arxiv.org/abs/2307.07697v1| 1;2024-03-01;	arXiv:2307.07697			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 01 2024	2024	Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$hbox{LLM}otimeshbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.																																	2025-11-07	PPRN:73950927		
J	Wang, Junyang; Wang, Yuhang; Xu, Guohai; Zhang, Jing; Gu, Yukai; Jia, Haitao; Wang, Jiaqi; Xu, Haiyang; Yan, Ming; Zhang, Ji; Sang, Jitao				wang, junyang/HMP-6590-2023; Yan, Ming/LDT-2692-2024						AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/2311.07397v2| 1;2023-11-13;https://www.arxiv.org/abs/2311.07397v1	arXiv:2311.07397			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Despite making significant progress in multi-modal tasks, current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences. Therefore, evaluating MLLMs' hallucinations is becoming increasingly important in model improvement and practical application deployment. Previous works are limited in high evaluation costs (e.g., relying on humans or advanced LLMs) and insufficient evaluation dimensions (e.g., types of tasks and hallucinations). In this paper, we propose an LLM-free multi-dimensional benchmark AMBER, which can be used to evaluate both generative task and discriminative task including existence, attribute and relation hallucination. Based on AMBER, we design a low-cost and efficient evaluation pipeline. Additionally, we conduct a comprehensive evaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision), and also give guideline suggestions for mitigating hallucinations. The data and code of AMBER are available at https://github.com/junyangwang0410/AMBER.																																	2024-03-23	PPRN:86133400		
J	Ashkboos, Saleh; Croci, Maximilian L.; do Nascimento, Marcelo Gennari; Hoefler, Torsten; Hensman, James				Hoefler, Torsten/HKF-3023-2023						SliceGPT: Compress Large Language Models by Deleting Rows and Columns								Arxiv											2	2;2024-02-09;https://www.arxiv.org/abs/2401.15024v2| 1;2024-01-26;https://www.arxiv.org/abs/2401.15024v1	arXiv:2401.15024			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 09 2024	2024	Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post -hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post -training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA -2 70B, OPT 66B and Phi -2 models while maintaining 99%, 99% and 90% zero -shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA -2 70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre -trained models. Code is available at: https://github.com/microsoft/TransformerCompression.																																	2024-05-25	PPRN:87368093		
J	Chen, Yukang; Xue, Fuzhao; Li, Dacheng; Hu, Qinghao; Zhu, Ligeng; Li, Xiuyu; Fang, Yunhao; Tang, Haotian; Yang, Shang; Liu, Zhijian; He, Ethan; Yin, Hongxu; Molchanov, Pavlo; Kautz, Jan; Fan, Linxi; Zhu, Yuke; Lu, Yao; Han, Song				Hu, Qinghao/IWD-6615-2023; Yin, Hongxu/AAZ-3328-2020; Xue, Fuzhao/GLT-5176-2022						LongVILA: Scaling Long-Context Visual Language Models for Long Videos								Arxiv											3	3;2024-12-13;https://www.arxiv.org/abs/2408.10188v6| 2;2024-11-01;https://www.arxiv.org/abs/2408.10188v5| 1;2024-10-21;https://www.arxiv.org/abs/2408.10188v4	arXiv:2408.10188			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 13 2024	2024	Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g. 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.																																	2025-01-22	PPRN:118756503		
J	Singhal, Prasann; Goyal, Tanya; Xu, Jiacheng; Durrett, Greg				Xu, Jiacheng/AAS-1409-2021						A Long Way to Go: Investigating Length Correlations in RLHF								Arxiv											3	3;2024-07-10;https://www.arxiv.org/abs/2310.03716v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03716v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03716v1	arXiv:2310.03716			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 10 2024	2024	Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for “helpfulness” in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a purely length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data.																																	2024-07-23	PPRN:85435117		
J	White, Colin; Dooley, Samuel; Roberts, Manley; Pal, Arka; Feuer, Benjamin; Jain, Siddhartha; Shwartz-Ziv, Ravid; Jain, Neel; Saifullah, Khalid; Naidu, Siddartha; Hegde, Chinmay; Lecun, Yann; Goldstein, Tom; Neiswanger, Willie; Goldblum, Micah										LiveBench: A Challenging, Contamination-Free LLM Benchmark								Arxiv											1	1;2024-06-27;https://www.arxiv.org/abs/2406.19314v1	arXiv:2406.19314			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 27 2024	2024	Test set contamination, wherein test data from a benchmark ends up in a newer model’s training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench , the first benchmark that (1) contains frequentlyupdated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65% accuracy. We release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and we will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.																																	2024-07-17	PPRN:90143799		
J	Yao, Shunyu; Shinn, Noah; Razavi, Pedram; Narasimhan, Karthik										τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.12045v1	arXiv:2406.12045			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	Existing benchmarks do not test language agents on their interaction with human users or ability to follow domain-specific rules, both of which are vital for deploying them in real world applications. We propose τ-bench, a benchmark emulating dynamic conversations between a user (simulated by language models) and a language agent provided with domain-specific API tools and policy guidelines. We employ an efficient and faithful evaluation process that compares the database state at the end of a conversation with the annotated goal state. We also propose a new metric (pass^k) to evaluate the reliability of agent behavior over multiple trials. Our experiments show that even state-of-the-art function calling agents (like gpt-4o ) succeed on < 50% of the tasks, and are quite inconsistent (pass^8 < 25% in retail). Our findings point to the need for methods that can improve the ability of agents to act consistently and follow rules reliably.																																	2024-07-04	PPRN:89358020		
J	Wang, Xuezhi; Zhou, Denny										Chain-of-Thought Reasoning Without Prompting								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2402.10200v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.10200v1	arXiv:2402.10200			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few -shot or zero -shot chain -of -thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre -trained LLMs by simply altering the decoding process. Rather than conventional greedy decoding, we investigate the top-k alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs’ intrinsic reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model’s decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.																																	2024-06-06	PPRN:87698977		
J	Ju, Zeqian; Wang, Yuancheng; Shen, Kai; Tan, Xu; Xin, Detai; Yang, Dongchao; Liu, Yanqing; Leng, Yichong; Song, Kaitao; Tang, Siliang; Wu, Zhizheng; Qin, Tao; Li, Xiang-Yang; Ye, Wei; Zhang, Shikun; Bian, Jiang; He, Lei; Li, Jinyu; Zhao, Sheng				Wang, Yuancheng/GLR-2067-2022; Song, Kaitao/JKJ-5832-2023; jinyu, Li/JQV-7729-2023; Yang, Dongchao/Y-8059-2018; Zhang, Shikun/ITU-3545-2023						NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models								Arxiv											3	3;2024-04-23;https://www.arxiv.org/abs/2403.03100v3| 2;2024-03-27;https://www.arxiv.org/abs/2403.03100v2| 1;2024-03-05;https://www.arxiv.org/abs/2403.03100v1	arXiv:2403.03100			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 23 2024	2024	While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.																																	2024-05-02	PPRN:88028344		
J	He, Xingwei; Lin, Zhenghao; Gong, Yeyun; Jin, A-Long; Zhang, Hang; Lin, Chen; Jiao, Jian; Yiu, Siu Ming; Duan, Nan; Chen, Weizhu				JIN, ALONG/JEP-3284-2023; Duan, Nan/AAR-2231-2020; He, XW/JXN-0620-2024						AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators								Arxiv											2	2;2024-04-05;https://www.arxiv.org/abs/2303.16854v2| 1;2023-03-29;https://www.arxiv.org/abs/2303.16854v1	arXiv:2303.16854			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 05 2024	2024	Many natural language processing (NLP) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt LLMs to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with LLMs. Our experiment results on three tasks, including user input and keyword relevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing AnnoLLM. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality.																																	2024-04-20	PPRN:56494864		
J	De, Soham; Smith, Samuel L.; Fernando, Anushan; Botev, Aleksandar; Cristian-Muraru, George; Gu, Albert; Haroun, Ruba; Berrada, Leonard; Chen, Yutian; Srinivasan, Srivatsan; Desjardins, Guillaume; Doucet, Arnaud; Budden, David; Teh, Yee Whye; Pascanu, Razvan; De Freitas, Nando; Gulcehre, Caglar				Teh, Yee/C-3400-2008; Budden, David/D-7106-2014; Desjardins, Guillaume/AFM-0561-2022						Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19427v1	arXiv:2402.19427			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.																																	2024-03-28	PPRN:87989321		
J	Zhou, Baichuan; Hu, Ying; Weng, Xi; Jia, Junlong; Luo, Jie; Liu, Xien; Wu, Ji; Huang, Lei				Liu, Xien/KFA-3468-2024; Luo, Jie/MGU-9977-2025						TinyLLaVA: A Framework of Small-scale Large Multimodal Models								Arxiv											1	1;2024-02-22;https://www.arxiv.org/abs/2402.14289v1	arXiv:2402.14289			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 22 2024	2024	We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.																																	2024-03-21	PPRN:87799622		
J	Ren, Jiawei; Pan, Liang; Tang, Jiaxiang; Zhang, Chi; Cao, Ang; Zeng, Gang; Liu, Ziwei				Liu, Ziwei/AAG-6939-2021; Zhang, Cheng/GXM-4582-2022						DreamGaussian4D: Generative 4D Gaussian Splatting								Arxiv											3	3;2024-06-10;https://www.arxiv.org/abs/2312.17142v3| 2;2023-12-29;https://www.arxiv.org/abs/2312.17142v2| 1;2023-12-28;https://www.arxiv.org/abs/2312.17142v1	arXiv:2312.17142			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	4D content generation has achieved remarkable progress recently. However, existing methods suffer from long optimization times, a lack of motion controllability, and a low quality of details. In this paper, we introduce DreamGaussian4D (DG4D), an efficient 4D generation framework that builds on Gaussian Splatting (GS). Our key insight is that combining explicit modeling of spatial transformations with static GS makes an efficient and powerful representation for 4D generation. Moreover, video generation methods have the potential to offer valuable spatialtemporal priors, enhancing the high-quality 4D generation. Specifically, we propose an integral framework with two major modules: 1) Image-to-4D GS- we initially generate static GS with DreamGaussianHD, followed by HexPlane-based dynamic generation with Gaussian deformation; and 2) Video-to-Video Texture Refinement- we refine the generated UV-space texture maps and meanwhile enhance their temporal consistency by utilizing a pre-trained image-to-video diffusion model. Notably, DG4D reduces the optimization time from several hours to just a few minutes, allows the generated 3D motion to be visually controlled, and produces animated meshes that can be realistically rendered in 3D engines.																																	2024-07-04	PPRN:86852503		
J	Singh, Avi; Co-Reyes, John D.; Agarwal, Rishabh; Anand, Ankesh; Patil, Piyush; Garcia, Xavier; Liu, Peter J.; Harrison, James; Lee, Jaehoon; Xu, Kelvin; Parisi, Aaron; Kumar, Abhishek; Alemi, Alex; Rizkowsky, Alex; Nova, Azade; Adlam, Ben; Bohnet, Bernd; Elsayed, Gamaleldin; Sedghi, Hanie; Mordatch, Igor; Simpson, Isabelle; Gur, Izzeddin; Snoek, Jasper; Pennington, Jeffrey; Hron, Jiri; Kenealy, Kathleen; Swersky, Kevin; Mahajan, Kshiteej; Culp, Laura; Xiao, Lechao; Bileschi, Maxwell L.; Constant, Noah; Novak, Roman; Liu, Rosanne; Warkentin, Tris; Qian, Yundi; Bansal, Yamini; Dyer, Ethan; Neyshabur, Behnam; Sohl-Dickstein, Jascha; Fiedel, Noah				Mahajan, Kshiteej/JXY-1195-2024; Harrison, Judith/AAC-2289-2020						Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models								Arxiv											4	4;2024-04-18;https://www.arxiv.org/abs/2312.06585v4| 3;2023-12-22;https://www.arxiv.org/abs/2312.06585v3| 2;2023-12-12;https://www.arxiv.org/abs/2312.06585v2| 1;2023-12-11;https://www.arxiv.org/abs/2312.06585v1	arXiv:2312.06585			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 18 2024	2024	Fine-tuning language models (LMs) on human -generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high -quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self -training method based on expectation -maximization, which we call ReSTEM, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReSTEM scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self -training with feedback can reduce dependence on human -generated data.																																	2024-05-03	PPRN:86538295		
J	Liu, Wei; Zeng, Weihao; He, Keqing; Jiang, Yong; He, Junxian				HE, Junxian/OHV-2278-2025						What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning								Arxiv											2	2;2024-04-16;https://www.arxiv.org/abs/2312.15685v2| 1;2023-12-25;https://www.arxiv.org/abs/2312.15685v1	arXiv:2312.15685			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning – when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present DEITA (short for Data -Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach. Empirically, DEITA performs better or on par with the state-of-the-art open -source alignment models with only 6K SFT training data samples – over 10x less than the data used in the baselines. When further trained with direct preference optimization (DPO), DEITA-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55 MT -Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools on automatic data selection, facilitating data -efficient alignment. We release our models as well as the selected datasets for future researches to effectively align models more efficiently.1																																	2024-04-26	PPRN:86826401		
J	Copet, Jade; Kreuk, Felix; Gat, Itai; Remez, Tal; Kant, David; Synnaeve, Gabriel; Adi, Yossi; Defossez, Alexandre				Adi, Yossi/HLG-5748-2023						Simple and Controllable Music Generation								Arxiv											3	3;2024-01-30;https://www.arxiv.org/abs/2306.05284v3| 2;2023-11-07;https://www.arxiv.org/abs/2306.05284v2| 1;2023-06-08;https://www.arxiv.org/abs/2306.05284v1	arXiv:2306.05284			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 30 2024	2024	We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft																																	2024-05-25	PPRN:73235095		
J	Wu, Yecheng; Zhang, Zhuoyang; Chen, Junyu; Tang, Haotian; Li, Dacheng; Fang, Yunhao; Zhu, Ligeng; Xie, Enze; Yin, Hongxu; Yi, Li; Han, Song; Lu, Yao				Yin, Hongxu/AAZ-3328-2020; Wu, Yecheng/OGN-0265-2025						VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation								Arxiv											1	1;2024-10-23;https://www.arxiv.org/abs/2409.04429v2	arXiv:2409.04429			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 23 2024	2024	VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.																																	2024-11-26	PPRN:118788864		
J	Xing, Zhaohu; Ye, Tian; Yang, Yijun; Liu, Guang; Zhu, Lei				Zhu, Lei/KUD-1330-2024						SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation								Arxiv											3	3;2024-09-15;https://www.arxiv.org/abs/2401.13560v4| 2;2024-02-25;https://www.arxiv.org/abs/2401.13560v3| 1;2024-01-25;https://www.arxiv.org/abs/2401.13560v2	arXiv:2401.13560			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 15 2024	2024	The Transformer architecture has demonstrated remarkable results in 3D medical image segmentation due to its capability of modeling global relationships. However, it poses a significant computational burden when processing high-dimensional medical images. Mamba, as a State Space Model (SSM), has recently emerged as a notable approach for modeling long-range dependencies in sequential data, and has excelled in the field of natural language processing with its remarkable memory efficiency and computational speed. Inspired by this, we devise SegMamba, a novel 3D medical image Seg mentation Mamba model, to effectively capture long-range dependencies within whole-volume features at every scale. Our SegMamba outperforms Transformer-based methods in whole- volume feature modeling, maintaining high efficiency even at a resolution of 64 × 64 × 64, where the sequential length is approximately 260k. Moreover, we collect and annotate a novel large-scale dataset (named CRC-500) to facilitate benchmarking evaluation in 3D colorectal cancer (CRC) segmentation. Experimental results on our CRC-500 and two public benchmark datasets further demonstrate the effectiveness and universality of our method. The code for SegMamba is publicly available at: https://github.com/ge-xing/SegMamba.																																	2024-12-24	PPRN:87336154		
J	Wang, Junlin; Wang, Jue; Athiwaratkun, Ben; Zhang, Ce; Zou, James				Wang, Junlin/LOS-3617-2024						Mixture-of-Agents Enhances Large Language Model Capabilities								Arxiv											1	1;2024-06-07;https://www.arxiv.org/abs/2406.04692v1	arXiv:2406.04692			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.1																																	2024-06-22	PPRN:89244051		
J	Li, Nathaniel; Pan, Alexander; Gopal, Anjali; Yue, Summer; Berrios, Daniel; Gatti, Alice; Li, Justin D.; Dombrowski, Ann-Kathrin; Goel, Shashwat; Phan, Long; Mukobi, Gabriel; Helm-Burger, Nathan; Lababidi, Rassin; Justen, Lennart; Liu, Andrew B.; Chen, Michael; Barrass, Isabelle; Zhang, Oliver; Zhu, Xiaoyuan; Tamirisa, Rishub; Bharathi, Bhrugu; Khoja, Adam; Zhao, Zhenqi; Herbert-Voss, Ariel; Breuer, Cort B.; Marks, Samuel; Patel, Oam; Zou, Andy; Mazeika, Mantas; Wang, Zifan; Oswal, Palash; Lin, Weiran; Hunt, Adam A.; Tienken-Harder, Justin; Shih, Kevin Y.; Talley, Kemper; Guan, John; Kaplan, Russell; Steneker, Ian; Campbell, David; Jokubaitis, Brad; Levinson, Alex; Wang, Jean; Qian, William; Karmakar, Kallol Krishna; Basart, Steven; Fitz, Stephen; Levine, Mindy; Kumaraguru, Ponnurangam; Tupakula, Uday; Varadharajan, Vijay; Wang, Ruoyu; Shoshitaishvili, Yan; Ba, Jimmy; Esvelt, Kevin M.; Wang, Alexandr; Hendrycks, Dan				wang, zifan/HHS-5709-2022; Zhu, Xiaoyuan/G-2611-2019; Lin, Weiran/KZZ-6197-2024; B., Bharathi/GXV-8824-2022; Zou, Andy/MGU-4410-2025; Hunt, Adam/IZE-1082-2023; Karmakar, Kallol/U-8808-2019; Esvelt, Kevin/AFT-8004-2022; Justen, Lennart/GWZ-3765-2022						The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning								Arxiv											7	7;2024-05-15;https://www.arxiv.org/abs/2403.03218v7| 6;2024-05-03;https://www.arxiv.org/abs/2403.03218v6| 5;2024-04-28;https://www.arxiv.org/abs/2403.03218v5| 4;2024-04-24;https://www.arxiv.org/abs/2403.03218v4| 3;2024-04-23;https://www.arxiv.org/abs/2403.03218v3| 2;2024-03-06;https://www.arxiv.org/abs/2403.03218v2| 1;2024-03-05;https://www.arxiv.org/abs/2403.03218v1	arXiv:2403.03218			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 15 2024	2024	The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. 																																	2024-08-28	PPRN:88028895		
J	Xu, Zhenhua; Zhang, Yujia; Xie, Enze; Zhao, Zhen; Guo, Yong; Wong, Kwan-Yee.K.; Li, Zhenguo; Zhao, Hengshuang				Lv, Zhengtong/AAW-9611-2020; Wong, Kwan-Yee/C-1577-2009						DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model								Arxiv											4	4;2024-03-14;https://www.arxiv.org/abs/2310.01412v4| 3;2024-02-13;https://www.arxiv.org/abs/2310.01412v3| 2;2023-10-08;https://www.arxiv.org/abs/2310.01412v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01412v1	arXiv:2310.01412			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 14 2024	2024	Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non -textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end -to -end autonomous driving system based on LLMs. Capable of processing multi -frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end -to -end fashion. These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end -to -end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain -specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V. The code and dataset will be publicly available.																																	2024-04-11	PPRN:85350309		
J	Albalak, Alon; Elazar, Yanai; Xie, Sang Michael; Longpre, Shayne; Lambert, Nathan; Wang, Xinyi; Muennighoff, Niklas; Hou, Bairu; Pan, Liangming; Jeong, Haewon; Raffel, Colin; Chang, Shiyu; Hashimoto, Tatsunori; Wang, William Yang				Pan, Liangming/LIF-2753-2024						A Survey on Data Selection for Language Models								Arxiv											2	2;2024-08-02;https://www.arxiv.org/abs/2402.16827v3| 1;2024-03-08;https://www.arxiv.org/abs/2402.16827v2	arXiv:2402.16827			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 02 2024	2024	A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required. Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies. To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.																																	2024-08-25	PPRN:88106341		
J	Allen-Zhu, Zeyuan; Li, Yuanzhi										Physics of Language Models: Part 3.1, Knowledge Storage and Extraction								Arxiv											3	3;2024-07-16;https://www.arxiv.org/abs/2309.14316v3| 2;2023-12-26;https://www.arxiv.org/abs/2309.14316v2| 1;2023-09-25;https://www.arxiv.org/abs/2309.14316v1	arXiv:2309.14316			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 16 2024	2024	Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g., “What is Abraham Lincoln’s birthday?”). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model’s ability to extract knowledge and various diversity measures of the training data. Essentially, , for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling, translations) during pre- training. . Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge — whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. This paper provides several key recommendations for LLM pretraining in the industry: (1) rewrite the pretraining data — using small, auxiliary models — to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.																																	2024-07-25	PPRN:85195850		
J	Wang, Xiaoxuan; Hu, Ziniu; Lu, Pan; Zhu, Yanqiao; Zhang, Jieyu; Subramaniam, Satyen; Loomba, Arjun R.; Zhang, Shichang; Sun, Yizhou; Wang, Wei				Zhang, Jieyu/IQS-6560-2023; Zhang, Shichang/K-1715-2019; Hu, Ziniu/HJI-4899-2023						SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models								Arxiv											3	3;2024-06-28;https://www.arxiv.org/abs/2307.10635v3| 2;2024-02-08;https://www.arxiv.org/abs/2307.10635v2| 1;2023-07-20;https://www.arxiv.org/abs/2307.10635v1	arXiv:2307.10635			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 28 2024	2024	Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.																																	2024-07-17	PPRN:74077875		
J	Li, Lijun; Dong, Bowen; Wang, Ruohui; Hu, Xuhao; Zuo, Wangmeng; Lin, Dahua; Qiao, Yu; Shao, Jing				Lin, Dahua/W-6576-2019; Zuo, Wangmeng/B-3701-2008; Dong, Bowen/LDG-1077-2024						SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2402.05044v4| 2;2024-03-04;https://www.arxiv.org/abs/2402.05044v3| 1;2024-02-08;https://www.arxiv.org/abs/2402.05044v2	arXiv:2402.05044			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. 																																	2024-06-22	PPRN:87572329		
J	Han, Chi; Wang, Qifan; Peng, Hao; Xiong, Wenhan; Chen, Yu; Ji, Heng; Wang, Sinong				Wang, Sinong/AAQ-6664-2020; han, chi/LPR-1781-2024						LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models								Arxiv											6	6;2024-03-09;https://www.arxiv.org/abs/2308.16137v6| 5;2023-11-16;https://www.arxiv.org/abs/2308.16137v5| 4;2023-10-03;https://www.arxiv.org/abs/2308.16137v4| 3;2023-09-07;https://www.arxiv.org/abs/2308.16137v3| 2;2023-09-05;https://www.arxiv.org/abs/2308.16137v2| 1;2023-08-30;https://www.arxiv.org/abs/2308.16137v1	arXiv:2308.16137			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 09 2024	2024	Today's large language models (LLMs) typically train on short text segments (e.g., <4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues. Through theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis further reveals that commonly used techniques like truncating the attention window or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7x decoding speed up and 7.5x memory saving over the original model. Our code will be publicly available upon publication.																																	2024-04-07	PPRN:84584415		
J	Du, Zhihao; Wang, Yuxuan; Chen, Qian; Shi, Xian; Lv, Xiang; Zhao, Tianyu; Gao, Zhifu; Yang, Yexin; Gao, Changfeng; Wang, Hui; Yu, Fan; Liu, Huadai; Sheng, Zhengyan; Gu, Yue; Deng, Chong; Wang, Wen; Zhang, Shiliang; Yan, Zhijie; Zhou, Jingren				Zhou, Mingyuan/AAE-8717-2021; shi, xian/AAL-3936-2021; lyu, xiangguang/HGE-5005-2022; Zhang, ShiLiang/AAA-4638-2020; Zhao, Tianyu/KGM-1194-2024; Liu, Huadai/OUK-1428-2025						CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models								Arxiv											3	3;2024-12-25;https://www.arxiv.org/abs/2412.10117v3| 2;2024-12-18;https://www.arxiv.org/abs/2412.10117v2| 1;2024-12-13;https://www.arxiv.org/abs/2412.10117v1	arXiv:2412.10117			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 25 2024	2024	In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. 																																	2025-02-05	PPRN:119937513		
J	Xu, Shusheng; Fu, Wei; Gao, Jiaxuan; Ye, Wenjie; Liu, Weilin; Mei, Zhiyu; Wang, Guangju; Yu, Chao; Wu, Yi				Yu, Chao/HTS-5623-2023						Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study								Arxiv											3	3;2024-10-10;https://www.arxiv.org/abs/2404.10719v3| 2;2024-04-21;https://www.arxiv.org/abs/2404.10719v2| 1;2024-04-16;https://www.arxiv.org/abs/2404.10719v1	arXiv:2404.10719			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 10 2024	2024	Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions. Our code is publicly available at https://github.com/openpsi-project/ReaLHF.																																	2024-11-01	PPRN:88544252		
J	Park, Ryan; Rafailov, Rafael; Ermon, Stefano; Finn, Chelsea										Disentangling Length from Quality in Direct Preference Optimization								Arxiv											2	2;2024-09-09;https://www.arxiv.org/abs/2403.19159v2| 1;2024-03-28;https://www.arxiv.org/abs/2403.19159v1	arXiv:2403.19159			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 09 2024	2024	Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstrapping. We then develop a principled but simple regularization strategy that prevents length exploitation, while still maintaining improvements in model quality. We demonstrate these effects across datasets on summarization and dialogue, where we achieve up to 20% improvement in win rates when controlling for length, despite the GPT4 judge's well-known verbosity bias.																																	2024-09-26	PPRN:88334987		
J	Wang, Xingyao; Chen, Yangyi; Yuan, Lifan; Zhang, Yizhe; Li, Yunzhu; Peng, Hao; Ji, Heng				Chen, Yangyi/LSL-4051-2024						Executable Code Actions Elicit Better LLM Agents								Arxiv											4	4;2024-06-07;https://www.arxiv.org/abs/2402.01030v4| 3;2024-05-24;https://www.arxiv.org/abs/2402.01030v3| 2;2024-03-18;https://www.arxiv.org/abs/2402.01030v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.01030v1	arXiv:2402.01030			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.																																	2024-06-22	PPRN:87509288		
J	Li, Zongyi; Huang, Daniel Zhengyu; Liu, Burigede; Anandkumar, Anima				Li, Zongyi/AAY-3602-2020; Huang, Daniel/LSJ-9214-2024						Fourier Neural Operator with Learned Deformations for PDEs on General Geometries								Arxiv											2	2;2024-05-02;https://www.arxiv.org/abs/2207.05209v2| 1;2022-07-11;https://www.arxiv.org/abs/2207.05209v1	arXiv:2207.05209			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	Deep learning surrogate models have shown promise in solving partial differential equations (PDEs). Among them, the Fourier neural operator (FNO) achieves good accuracy, and is significantly faster compared to numerical solvers, on a variety of PDEs, such as fluid flows. However, the FNO uses the Fast Fourier transform (FFT), which is limited to rectangular domains with uniform grids. In this work, we propose a new framework, viz., geo-FNO, to solve PDEs on arbitrary geometries. Geo-FNO learns to deform the input (physical) domain, which may be irregular, into a latent space with a uniform grid. The FNO model with the FFT is applied in the latent space. The resulting geo-FNO model has both the computation efficiency of FFT and the flexibility of handling arbitrary geometries. Our geo-FNO is also flexible in terms of its input formats, viz., point clouds, meshes, and design parameters are all valid inputs. We consider a variety of PDEs such as the Elasticity, Plasticity, Euler's, and Navier-Stokes equations, and both forward modeling and inverse design problems. Geo-FNO is $10^5$ times faster than the standard numerical solvers and twice more accurate compared to direct interpolation on existing ML-based PDE solvers such as the standard FNO.																																	2024-05-20	PPRN:10464626		
J	Li, Ming; Zhang, Yong; Li, Zhitao; Chen, Jiuhai; Chen, Lichang; Cheng, Ning; Wang, Jianzong; Zhou, Tianyi; Xiao, Jing				xiao, jing/HRB-7391-2023; Li, Zhitao/KHV-6882-2024; WANG, JIANZONG/GZL-9336-2022						From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning								Arxiv											5	5;2024-04-06;https://www.arxiv.org/abs/2308.12032v5| 4;2024-02-20;https://www.arxiv.org/abs/2308.12032v4| 3;2023-09-15;https://www.arxiv.org/abs/2308.12032v3| 2;2023-09-08;https://www.arxiv.org/abs/2308.12032v2| 1;2023-08-23;https://www.arxiv.org/abs/2308.12032v1	arXiv:2308.12032			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 06 2024	2024	In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model's expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere $10%$ of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements																																	2024-04-21	PPRN:83105671		
J	Luo, Linhao; Li, Yuan-Fang; Haffari, Gholamreza; Pan, Shirui				LUO, LINHAO/IAM-3162-2023; Li, Yuan-Fang/T-7532-2019						Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning								Arxiv											2	2;2024-02-24;https://www.arxiv.org/abs/2310.01061v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01061v1	arXiv:2310.01061			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 24 2024	2024	Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results1.																																	2024-11-20	PPRN:85350235		
J	Tang, Xiangru; Zou, Anni; Zhang, Zhuosheng; Li, Ziming; Zhao, Yilun; Zhang, Xingyao; Cohan, Arman; Gerstein, Mark				Zhao, Ziang/IAR-5845-2023; Gerstein, Mark/HSC-3904-2023; Zhang, Zhuosheng/AAF-4919-2020						MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning								Arxiv											2	2;2024-02-20;https://www.arxiv.org/abs/2311.10537v3| 1;2023-11-16;https://www.arxiv.org/abs/2311.10537v1	arXiv:2311.10537			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 20 2024	2024	Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities.																																	2024-03-19	PPRN:86199572		
J	Duan, Michael; Suri, Anshuman; Mireshghallah, Niloofar; Min, Sewon; Shi, Weijia; Zettlemoyer, Luke; Tsvetkov, Yulia; Choi, Yejin; Evans, David; Hajishirzi, Hannaneh										Do Membership Inference Attacks Work on Large Language Models?								Arxiv											1	1;2024-02-12;https://www.arxiv.org/abs/2402.07841v1	arXiv:2402.07841			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 12 2024	2024	Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges. We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work.																																	2024-05-25	PPRN:87631654		
J	Paul, Debjit; Paul, Mete; Peyrard, Maxime; Borges, Beatriz; Bosselut, Antoine; West, Robert; Faltings, Boi				West, Robert/B-5414-2009						REFINER: Reasoning Feedback on Intermediate Representations								Arxiv											2	2;2024-02-04;https://www.arxiv.org/abs/2304.01904v2| 1;2023-04-04;https://www.arxiv.org/abs/2304.01904v1	arXiv:2304.01904			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 04 2024	2024	Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT-3.5 or ChatGPT as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human -in -the -loop data but can be substituted with humans at inference time.																																	2024-02-21	PPRN:54005213		
J	Qi, Xiangyu; Panda, Ashwinee; Lyu, Kaifeng; Ma, Xiao; Roy, Subhrajit; Beirami, Ahmad; Mittal, Prateek; Henderson, Peter				Qi, Xiangyu/GZM-8733-2022						Safety Alignment Should Be Made More Than Just a Few Tokens Deep								Arxiv											1	1;2024-06-10;https://www.arxiv.org/abs/2406.05946v1	arXiv:2406.05946			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.1																																	2024-07-04	PPRN:89268659		
J	Li, Zhimin; Zhang, Jianwei; Lin, Qin; Xiong, Jiangfeng; Long, Yanxin; Deng, Xinchi; Zhang, Yingfang; Liu, Xingchao; Huang, Minbin; Xiao, Zedong; Chen, Dayou; He, Jiajun; Li, Jiahao; Li, Wenyue; Zhang, Chen; Quan, Rongwei; Lu, Jianxiang; Huang, Jiabin; Yuan, Xiaoyan; Zheng, Xiaoxiao; Li, Yixuan; Zhang, Jihong; Zhang, Chao; Chen, Meng; Liu, Jie; Fang, Zheng; Wang, Weiyan; Xue, Jinbao; Tao, Yangyu; Zhu, Jianchen; Liu, Kai; Lin, Sihuan; Sun, Yifu; Li, Yun; Wang, Dongdong; Chen, Mingtao; Hu, Zhichao; Xiao, Xiao; Chen, Yan; Liu, Yuhong; Liu, Wei; Wang, Di; Yang, Yong; Jiang, Jie; Lu, Qinglin				he, jiajun/IVV-6339-2023; Fang, Zheng/HOC-9097-2023; Li, Xiaoran/OGO-0799-2025; Li, Wenyue/HGU-4340-2022; Li, Jiahao/KCY-9880-2024; Zheng, Xiaoxiao/JXM-5655-2024						Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding								Arxiv											1	1;2024-05-14;https://www.arxiv.org/abs/2405.08748v1	arXiv:2405.08748			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 14 2024	2024	We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/ Tencent/HunyuanDiT																																	2024-06-08	PPRN:89047592		
J	Yoran, Ori; Wolfson, Tomer; Ram, Ori; Berant, Jonathan										Making Retrieval-Augmented Language Models Robust to Irrelevant Context								Arxiv											2	2;2024-05-05;https://www.arxiv.org/abs/2310.01558v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01558v1	arXiv:2310.01558			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 05 2024	2024	Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.																																	2024-05-24	PPRN:85377831		
J	Mitra, Arindam; Khanpour, Hamed; Rosset, Corby; Awadallah, Ahmed										Orca-Math: Unlocking the potential of SLMs in Grade School Math								Arxiv											1	1;2024-02-16;https://www.arxiv.org/abs/2402.14830v1	arXiv:2402.14830			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 16 2024	2024	We show that an SLM can reach ∼ 87% pass@1 on GSM8K while trained on only 200K synthetic math problems. Mathematical word problem -solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi -GSM uses top -48 to boost the performance from 68.2 to 81.5, [38] uses top -100 to boost LLAMA -2’s performance from 38.6% to 71.9%). In this work, we present Orca-Math, a 7 -billion -parameter SLM based on the Mistral -7B, which achieves 86.81% on GSM8k without the need for multiple model calls or the use of verifiers, code execution or any other external tools. Our approach has the following key elements: (1) A high quality synthetic dataset of 200K math problems created using a multiagent setup where agents collaborate to create the data, (2) An iterative learning techniques that enables the SLM to practice solving problems, receive feedback on its solutions and learn from preference pairs incorporating the SLM solutions and the feedback. When trained with Supervised Fine -Tuning alone, Orca-Math achieves 81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math achieves 86.81% pass@1. Orca-Math surpasses the performance of significantly larger models such as LLAMA -2-70B, WizardMath-70B, Gemini -Pro, ChatGPT-3.5. It also significantly outperforms other smaller models while using much smaller data (hundreds of thousands vs. millions of problems).																																	2024-11-09	PPRN:87868248		
J	Huang, Wenlong; Wang, Chen; Li, Yunzhu; Zhang, Ruohan; Fei-Fei, Li				Li, Feifei/C-3476-2017						ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation								Arxiv											2	2;2024-11-12;https://www.arxiv.org/abs/2409.01652v2| 1;2024-09-03;https://www.arxiv.org/abs/2409.01652v1	arXiv:2409.01652			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 12 2024	2024	Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models.																																	2024-12-18	PPRN:91718936		
J	Liu, Ruibo; Wei, Jerry; Liu, Fangyu; Si, Chenglei; Zhang, Yanzhe; Rao, Jinmeng; Zheng, Steven; Peng, Daiyi; Yang, Diyi; Zhou, Denny; Dai, Andrew M.				Liu, Fangyu/KIC-4734-2024; zhang, yanzhe/O-7292-2015						Best Practices and Lessons Learned on Synthetic Data								Arxiv											2	2;2024-08-10;https://www.arxiv.org/abs/2404.07503v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.07503v1	arXiv:2404.07503			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 10 2024	2024	The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.																																	2024-08-21	PPRN:88501195		
J	Shah, Jay; Bikshandi, Ganesh; Zhang, Ying; Thakkar, Vijay; Ramani, Pradeep; Dao, Tri										FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision								Arxiv											2	2;2024-07-11;https://www.arxiv.org/abs/2407.08608v1| 1;2024-07-01;	arXiv:2407.08608			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0× with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6× lower numerical error than a baseline FP8 attention.																																	2024-11-18	PPRN:90770936		
J	Soldaini, Luca; Kinney, Rodney; Bhagia, Akshita; Schwenk, Dustin; Atkinson, David; Authur, Russell; Bogin, Ben; Chandu, Khyathi; Dumas, Jennifer; Elazar, Yanai; Hofmann, Valentin; Jha, Ananya Harsh; Kumar, Sachin; Lucy, Li; Lyu, Xinxi; Lambert, Nathan; Magnusson, Ian; Morrison, Jacob; Muennighoff, Niklas; Naik, Aakanksha; Nam, Crystal; Peters, Matthew E.; Ravichander, Abhilasha; Richardson, Kyle; Shen, Zejiang; Strubell, Emma; Subramani, Nishant; Tafjord, Oyvind; Walsh, Pete; Zettlemoyer, Luke; Smith, Noah A.; Hajishirzi, Hannaneh; Beltagy, Iz; Groeneveld, Dirk; Dodge, Jesse; Lo, Kyle				Lucy, Li/JPK-7598-2023						Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research								Arxiv											1	1;2024-01-31;https://www.arxiv.org/abs/2402.00159v1	arXiv:2402.00159			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 31 2024	2024	Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.																																	2024-02-21	PPRN:87456326		
J	Greenblatt, Ryan; Denison, Carson; Wright, Benjamin; Roger, Fabien; Macdiarmid, Monte; Marks, Sam; Treutlein, Johannes; Belonax, Tim; Chen, Jack; Duvenaud, David; Khan, Akbir; Michael, Julian; Mindermann, Soren; Perez, Ethan; Petrini, Linda; Uesato, Jonathan; Kaplan, Jared; Shlegeris, Buck; Bowman, Samuel R.; Hubinger, Evan										Alignment faking in large language models								Arxiv											1	1;2024-12-20;https://www.arxiv.org/abs/2412.14093v2	arXiv:2412.14093			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 20 2024	2024	We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference—as in this case—or not.																																	2025-01-29	PPRN:120040570		
J	Lin, Bin; Ge, Yunyang; Cheng, Xinhua; Li, Zongjian; Zhu, Bin; Wang, Shaodong; He, Xianyi; Ye, Yang; Yuan, Shenghai; Chen, Liuhan; Jia, Tanghui; Zhang, Junwu; Tang, Zhenyu; Pang, Yatian; She, Bin; Yan, Cen; Hu, Zhiheng; Dong, Xiaoyi; Chen, Lin; Pan, Zhang; Zhou, Xing; Dong, Shaoling; Tian, Yonghong; Yuan, Li; Open Sora Plan Team				Lin, Bin/V-3431-2019; Yuan, Shenghai/JDM-6241-2023; Dong, Xiaoyi/AAC-8666-2019; Tang, Zhenyu/ACY-2335-2022; zhang, junwu/IWM-6534-2023; TIAN, Yonghong/M-4937-2013; Li, Yongjiang/AAF-9313-2020						Open-Sora Plan: Open-Source Large Video Generation Model								Arxiv											1	1;2024-11-28;https://www.arxiv.org/abs/2412.00131v1	arXiv:2412.00131			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 28 2024	2024	We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. 																																	2025-01-11	PPRN:119628536		
J	Wu, Yue; Sun, Zhiqing; Yuan, Huizhuo; Ji, Kaixuan; Yang, Yiming; Gu, Quanquan				Ji, Kaixuan/HZH-5940-2023						Self-Play Preference Optimization for Language Model Alignment								Arxiv											5	5;2024-10-04;https://www.arxiv.org/abs/2405.00675v5| 4;2024-06-14;https://www.arxiv.org/abs/2405.00675v4| 3;2024-05-26;https://www.arxiv.org/abs/2405.00675v3| 2;2024-05-23;https://www.arxiv.org/abs/2405.00675v2| 1;2024-05-01;https://www.arxiv.org/abs/2405.00675v1	arXiv:2405.00675			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	Standard reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy updates to provably approximate the Nash equilibrium. Additionally, we propose a new SPPO objective which is both strongly motivated by theory and is simple and effective in practice. In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench, Arena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of 38.77%. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models. 																																	2024-10-27	PPRN:88712514		
J	Li, Kenneth; Hopkins, Aspen K.; Bau, David; Viegas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin				Pfister, Hanspeter/NJR-9877-2025; Bau, David/KGM-5427-2024						Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task								Arxiv											2	2;2024-06-26;https://www.arxiv.org/abs/2210.13382v5| 1;2022-10-24;https://www.arxiv.org/abs/2210.13382v3	arXiv:2210.13382			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 26 2024	2024	Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question in a synthetic setting by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network. By leveraging these intervention techniques, we produce “latent saliency maps” that help explain predictions. 1																																	2024-07-15	PPRN:35976844		
J	Xu, Hu; Xie, Saining; Tan, Xiaoqing Ellen; Huang, Po-Yao; Howes, Russell; Sharma, Vasu; Li, Shang-Wen; Ghosh, Gargi; Zettlemoyer, Luke; Feichtenhofer, Christoph				Huang, Poyao/IAQ-3889-2023; Li, Shangwen/OYE-2845-2025						Demystifying CLIP Data								Arxiv											3	3;2024-04-07;https://www.arxiv.org/abs/2309.16671v4| 2;2023-10-02;https://www.arxiv.org/abs/2309.16671v3| 1;2023-09-28;https://www.arxiv.org/abs/2309.16671v1	arXiv:2309.16671			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 07 2024	2024	Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP’s data by filtering with its model parameters. In this work, we intend to reveal CLIP’s data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP’s concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP’s data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP’s 68.3% on ViT-B models. Scaling to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-bigG producing 82.1%. 																																	2024-04-21	PPRN:85323761		
J	Zheng, Huaixiu Steven; Mishra, Swaroop; Chen, Xinyun; Cheng, Heng-Tze; Chi, Ed H.; Le, Quoc V; Zhou, Denny				Chen, Xinyun/ABZ-9877-2022						Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models								Arxiv											2	2;2024-03-12;https://www.arxiv.org/abs/2310.06117v2| 1;2023-10-09;https://www.arxiv.org/abs/2310.06117v1	arXiv:2310.06117			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 12 2024	2024	We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.																																	2024-04-08	PPRN:85521927		
J	Sun, Quan; Cui, Yufeng; Zhang, Xiaosong; Zhang, Fan; Yu, Qiying; Luo, Zhengxiong; Wang, Yueze; Rao, Yongming; Liu, Jingjing; Huang, Tiejun; Wang, Xinlong				张, 帆/IST-0185-2023; Rao, Yongming/U-8310-2019; Huang, Tiejun/D-6161-2011; Luo, Zhengxiong/HLP-8344-2023; Wang, Xinlong/AFI-8800-2022						Generative Multimodal Models are In-Context Learners								Arxiv											2	2;2024-05-08;https://www.arxiv.org/abs/2312.13286v2| 1;2023-12-20;https://www.arxiv.org/abs/2312.13286v1	arXiv:2312.13286			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 08 2024	2024	The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.																																	2024-05-27	PPRN:86741686		
J	Rosset, Corby; Cheng, Ching-An; Mitra, Arindam; Santacroce, Michael; Awadallah, Ahmed; Xie, Tengyang				Xie, Tengyang/ABM-6089-2022; Cheng, Ching-An/AAZ-1802-2020						Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences								Arxiv											1	1;2024-04-04;https://www.arxiv.org/abs/2404.03715v1	arXiv:2404.03715			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 04 2024	2024	This paper studies post -training large language models (LLMs) using preference feedback from a powerful oracle to help a model iteratively improve over itself. The typical approach for post -training LLMs involves Reinforcement Learning from Human Feedback (RLHF), which traditionally separates reward learning and subsequent policy optimization. However, such a reward maximization approach is limited by the nature of “point -wise” rewards (such as that of the Bradley -Terry model), which fails to express complex intransitive or cyclic preference relations. While advances on RLHF show reward learning and policy optimization can be merged into a single contrastive objective for stability, they yet still remain tethered to the reward maximization framework. Recently, a new wave of research sidesteps the reward maximization presumptions in favor of directly optimizing over “pair -wise” or general preferences. In this paper, we introduce Direct Nash Optimization (DNO), a provable and scalable algorithm that marries the simplicity and stability of contrastive learning with theoretical generality from optimizing general preferences. Because DNO is a batched on -policy algorithm using a regression -based objective, its implementation is straightforward and efficient. Moreover, DNO enjoys monotonic improvement across iterations which helps it improve even over a strong teacher (such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model aligned by DNO achieves the state-of-the-art win -rate against GPT-4-Turbo of 33% on AlpacaEval 2.0 (even after controlling for response length), an absolute gain of 26% (7% → 33%) over the initializing model. It outperforms models with far more parameters, including Mistral Large, Self -Rewarding LM (70B parameters), and older versions of GPT-4. Our ablation studies analyze critical design decisions surrounding the choice of preference pairs, and the use of LLMs-as-preference-annotators. These results underscore the promise of DNO in the LLMs post -training, as well as offer actionable insights for the AI research community.																																	2024-04-19	PPRN:88429373		
J	Hu, Xiwei; Wang, Rui; Fang, Yixiao; Fu, Bin; Cheng, Pei; Yu, Gang										ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment								Arxiv											1	1;2024-03-08;https://www.arxiv.org/abs/2403.05135v1	arXiv:2403.05135			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 08 2024	2024	Diffusion models have demonstrated remarkable performance in the domain of text-to-image generation. However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense prompts, encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc. In this paper, we introduce an Efficient Large Language Model Adapter, termed ELLA, which equips text-to-image diffusion models with powerful Large Language Models (LLM) to enhance text alignment without training of either U-Net or LLM. To seamlessly bridge two pre-trained models, we investigate a range of semantic alignment connector designs and propose a novel module, the Timestep-Aware Semantic Connector (TSC), which dynamically extracts timestep-dependent conditions from LLM. Our approach adapts semantic features at different stages of the denoising process, assisting diffusion models in interpreting lengthy and intricate prompts over sampling timesteps. Additionally, ELLA can be readily incorporated with community models and tools to improve their prompt-following capabilities. To assess text-to-image models in dense prompt following, we introduce Dense Prompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K dense prompts. Extensive experiments demonstrate the superiority of ELLA in dense prompt following compared to state-of-the-art methods, particularly in multiple object compositions involving diverse attributes and relationships.																																	2024-04-07	PPRN:88087226		
J	Wu, Tianhao; Yuan, Weizhe; Golovneva, Olga; Xu, Jing; Tian, Yuandong; Jiao, Jiantao; Weston, Jason; Sukhbaatar, Sainbayar				Wu, Tianhao/AAX-8318-2021						Meta-Rewarding Language Models: Self-<italic>Improving Alignment with</italic> LLM-as-a-Meta-Judge								Arxiv											1	1;2024-07-30;https://www.arxiv.org/abs/2407.19594v2	arXiv:2407.19594			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 30 2024	2024	Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024c) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model’s ability to judge and follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision.																																	2024-08-06	PPRN:91157323		
J	Ke, Tsung-Wei; Gkanatsios, Nikolaos; Fragkiadaki, Katerina										3D Diffuser Actor: Policy Diffusion with 3D Scene Representations								Arxiv											1	1;2024-03-11;https://www.arxiv.org/abs/2402.10885v2	arXiv:2402.10885			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 11 2024	2024	We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts the 3D translation and rotation error for each of them, by featurizing them using 3D relative attention to other 3D visual and language tokens. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 16.3% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in the setting of zero-shot unseen scene generalization by being able to successfully run 0.2 more tasks, a 7% relative increase. It also works in the real world from a handful of demonstrations. We ablate our model's architectural design choices, such as 3D scene featurization and 3D relative attentions, and show they all help generalization. Our results suggest that 3D scene representations and powerful generative modeling are keys to efficient robot learning from demonstrations.																																	2024-04-08	PPRN:88119930		
J	Gao, Bofei; Song, Feifan; Yang, Zhe; Cai, Zefan; Miao, Yibo; Dong, Qingxiu; Li, Lei; Ma, Chenghao; Chen, Liang; Xu, Runxin; Tang, Zhengyang; Wang, Benyou; Zan, Daoguang; Quan, Shanghaoran; Zhang, Ge; Sha, Lei; Zhang, Yichang; Ren, Xuancheng; Liu, Tianyu; Chang, Baobao				Sha, Lei/HGB-4806-2022; Liu, Tianyu/ABD-7412-2020; Zan, Daoguang/KLY-4874-2024; Wang, Benyou/Y-5146-2019; Li, Lei/LMN-0940-2024						Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models								Arxiv											3	3;2024-12-24;https://www.arxiv.org/abs/2410.07985v3| 2;2024-10-11;https://www.arxiv.org/abs/2410.07985v2| 1;2024-10-10;https://www.arxiv.org/abs/2410.07985v1	arXiv:2410.07985			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Dec 24 2024	2024	Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.																																	2025-02-02	PPRN:105777740		
J	Renze, Matthew; Guven, Erhan										Self-Reflection in LLM Agents: Effects on Problem-Solving Performance								Arxiv											2	2;2024-10-16;https://www.arxiv.org/abs/2405.06682v3| 1;2024-05-05;https://www.arxiv.org/abs/2405.06682v1	arXiv:2405.06682			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 16 2024	2024	In this study, we investigated the effects of self-reflection in large language models (LLMs) on problem-solving performance. We instructed nine popular LLMs to answer a series of multiple-choice questions to provide a performance baseline. For each incorrectly answered question, we instructed eight types of self-reflecting LLM agents to reflect on their mistakes and provide themselves with guidance to improve problem-solving. Then, using this guidance, each self-reflecting agent attempted to re-answer the same questions. Our results indicate that LLM agents are able to significantly improve their problem-solving performance through self-reflection (p < 0.001). .001). In addition, we compared the various types of self-reflection to determine their individual contribution to performance. All code and data are available on GitHub at https://github.com/matthewrenze/self-reflection																																	2024-11-07	PPRN:89029854		
J	Bochkovskii, Aleksei; Delaunoy, Amael; Germain, Hugo; Santos, Marcel; Zhou, Yichao; Richter, Stephan R.; Koltun, Vladlen				Zou, Yi-Chao/JFA-3676-2023						Depth Pro: Sharp Monocular Metric Depth in Less Than a Second								Arxiv											1	1;2024-10-02;https://www.arxiv.org/abs/2410.02073v1	arXiv:2410.02073			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 02 2024	2024	We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro																																	2024-10-18	PPRN:102598734		
J	Souly, Alexandra; Lu, Qingyuan; Bowen, Dillon; Trinh, Tu; Hsieh, Elvis; Pandey, Sana; Abbeel, Pieter; Svegliato, Justin; Emmons, Scott; Watkins, Olivia; Toyer, Sam										A StrongREJECT for Empty Jailbreaks								Arxiv											2	2;2024-08-27;https://www.arxiv.org/abs/2402.10260v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.10260v1	arXiv:2402.10260			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 27 2024	2024	Most jailbreak papers claim the jailbreaks they propose are highly effective, often boasting near-100% attack success rates. However, it is perhaps more common than not for jailbreak developers to substantially exaggerate the effectiveness of their jailbreaks. We suggest this problem arises because jailbreak researchers lack a standard, high-quality benchmark for evaluating jailbreak performance, leaving researchers to create their own. To create a benchmark, researchers must choose a dataset of forbidden prompts to which a victim model will respond, along with an evaluation method that scores the harmfulness of the victim model’s responses. We show that existing benchmarks suffer from significant shortcomings and introduce the StrongREJECT benchmark to address these issues. StrongREJECT’s dataset contains prompts that victim models must answer with specific, harmful information, while its automated evaluator measures the extent to which a response gives useful information to forbidden prompts. In doing so, the StrongREJECT evaluator achieves state-of-the-art agreement with human judgments of jailbreak effectiveness. Notably, we find that existing evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and the StrongREJECT evaluator. We describe a surprising and novel phenomenon that explains this discrepancy: jailbreaks bypassing a victim model’s safety fine-tuning tend to reduce its capabilities. Together, our findings underscore the need for researchers to use a high-quality benchmark, such as StrongREJECT, when developing new jailbreak attacks. 																																	2024-09-06	PPRN:87729578		
J	Beck, Jacob; Vuorio, Risto; Liu, Evan Zheran; Xiong, Zheng; Zintgraf, Luisa; Finn, Chelsea; Whiteson, Shimon										A Survey of Meta-Reinforcement Learning								Arxiv											1	1;2024-08-16;https://www.arxiv.org/abs/2301.08028v3	arXiv:2301.08028			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 16 2024	2024	While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.																																	2024-08-25	PPRN:35900972		
J	Kim, Dongjun; Lai, Chieh-Hsin; Liao, Wei-Hsiang; Murata, Naoki; Takida, Yuhta; Uesaka, Toshimitsu; He, Yutong; Mitsufuji, Yuki; Ermon, Stefano				He, Yutong/HII-5140-2022; Lai, Chieh-Hsin/JXN-7654-2024						Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion								Arxiv											3	3;2024-03-30;https://www.arxiv.org/abs/2310.02279v3| 2;2024-03-13;https://www.arxiv.org/abs/2310.02279v2| 1;2023-10-01;https://www.arxiv.org/abs/2310.02279v1	arXiv:2310.02279			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 30 2024	2024	Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64x64 resolution (FID 1.92). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's access to the score function can streamline the adoption of established controllable/conditional generation methods from the diffusion community. This access also enables the computation of likelihood. The code is available at https://github.com/sony/ctm.																																	2024-04-17	PPRN:85398125		
J	Wen, Licheng; Fu, Daocheng; Li, Xin; Cai, Xinyu; Ma, Tao; Cai, Pinlong; Dou, Min; Shi, Botian; He, Liang; Qiao, Yu				CAI, XINYU/IWV-0949-2023; Shi, Botian/HTT-0363-2023; ma, tao/JTS-7892-2023; He, Liang/CAF-0477-2022; Cai, Pinlong/P-6490-2017; Qiao, Yu/ABD-5787-2021; Fu, Daocheng/LCE-6917-2024						DILU: A KNOWLEDGE-DRIVEN APPROACH TO AUTONOMOUS DRIVING WITH LARGE LANGUAGE MODELS								Arxiv											3	3;2024-02-22;https://www.arxiv.org/abs/2309.16292v3| 2;2023-10-12;https://www.arxiv.org/abs/2309.16292v2| 1;2023-09-28;https://www.arxiv.org/abs/2309.16292v1	arXiv:2309.16292			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 22 2024	2024	Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge -driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu’s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning -based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge -driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain.																																	2024-03-21	PPRN:85321866		
J	Kirk, Robert; Mediratta, Ishita; Nalmpantis, Christoforos; Luketina, Jelena; Hambro, Eric; Grefenstette, Edward; Raileanu, Roberta				Nalmpantis, Christoforos/AAS-2727-2020						Understanding the Effects of RLHF on LLM Generalisation and Diversity								Arxiv											3	3;2024-02-19;https://www.arxiv.org/abs/2310.06452v3| 2;2024-01-03;https://www.arxiv.org/abs/2310.06452v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06452v1	arXiv:2310.06452			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 19 2024	2024	Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.																																	2024-03-21	PPRN:85521002		
J	Wu, Zhiyong; Han, Chengcheng; Ding, Zichen; Weng, Zhenmin; Liu, Zhoumianze; Yao, Shunyu; Yu, Tao; Kong, Lingpeng				kong, lingpeng/NHQ-3170-2025						OS-Copilot: Towards Generalist Computer Agents with Self-Improvement								Arxiv											2	2;2024-02-15;https://www.arxiv.org/abs/2402.07456v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.07456v1	arXiv:2402.07456			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 15 2024	2024	Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents.																																	2024-03-13	PPRN:87638359		
J	Yuan, Lifan; Cui, Ganqu; Wang, Hanbin; Ding, Ning; Wang, Xingyao; Deng, Jia; Shan, Boji; Chen, Huimin; Xie, Ruobing; Lin, Yankai; Liu, Zhenghao; Zhou, Bowen; Peng, Hao; Liu, Zhiyuan; Sun, Maosong				Zhou, Bowen/AAH-1042-2020; LIU, ZHENGHAO/GRY-2799-2022; Liu, Zhiyuan/I-2233-2014						Advancing LLM Reasoning Generalists with Preference Trees								Arxiv											1	1;2024-04-02;https://www.arxiv.org/abs/2404.02078v1	arXiv:2404.02078			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 02 2024	2024	We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.																																	2024-04-18	PPRN:88377531		
J	Zhou, Xuhui; Zhu, Hao; Mathur, Leena; Zhang, Ruohong; Qi, Zhengyang; Yu, Haofei; Morency, Louis-Philippe; Bisk, Yonatan; Fried, Daniel; Neubig, Graham; Sap, Maarten				Morency, Louis-Philippe/B-2006-2008						SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents								Arxiv											2	2;2024-03-22;https://www.arxiv.org/abs/2310.11667v2| 1;2023-10-18;https://www.arxiv.org/abs/2310.11667v1	arXiv:2310.11667			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 22 2024	2024	Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.																																	2024-04-13	PPRN:85698992		
J	Song, Enxin; Chai, Wenhao; Wang, Guanhong; Zhang, Yucheng; Zhou, Haoyang; Wu, Feiyang; Chi, Haozhe; Guo, Xun; Ye, Tian; Zhang, Yanting; Lu, Yan; Hwang, Jenq-Neng; Wang, Gaoang				Chai, Wenhao/LIG-6923-2024; Ye, Tianyong/MSX-9881-2025; Wang, Guan-Hong/AEM-1705-2022; Zhang, Yucheng/HGB-4718-2022; Zhou, Haoyang/KTI-1671-2024; Song, EnXin/LRB-4201-2024						MovieChat: From Dense Token to Sparse Memory for Long Video Understanding								Arxiv											4	4;2024-03-09;https://www.arxiv.org/abs/2307.16449v4| 3;2023-12-03;https://www.arxiv.org/abs/2307.16449v3| 2;2023-11-24;https://www.arxiv.org/abs/2307.16449v2| 1;2023-07-31;https://www.arxiv.org/abs/2307.16449v1	arXiv:2307.16449			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 09 2024	2024	[cs.CV] Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre -defined vision tasks. Yet, existing systems can only handle videos with very few frames. For long videos, the computation complexity, memory cost, and long-term temporal connection impose additional challenges. Taking advantage of the AtkinsonShiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose the MovieChat to overcome these challenges. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video and 14K manual annotations for validation of the effectiveness of our method.																																	2024-04-08	PPRN:74181105		
J	Wang, Ke; Pan, Junting; Shi, Weikang; Lu, Zimu; Zhan, Mingjie; Li, Hongsheng				Li, Hongsheng/AES-5328-2022; Shi, WeiKang/LIG-8896-2024; Zhan, Mingjie/MGW-5518-2025						Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset								Arxiv											1	1;2024-02-22;https://www.arxiv.org/abs/2402.14804v1	arXiv:2402.14804			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 22 2024	2024	Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development. The project is available at https://mathvision-cuhk.github.io																																	2024-03-21	PPRN:87804465		
J	Gong, Yuan; Luo, Hongyin; Liu, Alexander H.; Karlinsky, Leonid; Glass, James				Gong, Yuan/AAO-9009-2020						Listen, Think, and Understand								Arxiv											3	3;2024-02-19;https://www.arxiv.org/abs/2305.10790v3| 2;2023-10-02;https://www.arxiv.org/abs/2305.10790v2| 1;2023-05-18;https://www.arxiv.org/abs/2305.10790v1	arXiv:2305.10790			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 19 2024	2024	The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.																																	2024-03-19	PPRN:70531542		
J	Durante, Zane; Huang, Qiuyuan; Wake, Naoki; Gong, Ran; Park, Jae Sung; Sarkar, Bidipta; Taori, Rohan; Noda, Yusuke; Terzopoulos, Demetri; Choi, Yejin; Ikeuchi, Katsushi; Vo, Hoi; Li, Fei-Fei; Gao, Jianfeng				Gao, Jianfeng/AAP-8200-2021; Gong, Ran/KCK-4652-2024; Vo, Ngoc/AAR-8948-2021; Li, Feifei/JTT-8011-2023						Agent AI: Surveying the Horizons of Multimodal Interaction								Arxiv											2	2;2024-01-25;https://www.arxiv.org/abs/2401.03568v2| 1;2024-01-07;https://www.arxiv.org/abs/2401.03568v1	arXiv:2401.03568			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 25 2024	2024	Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define "Agent AI" as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, and can produce meaningful embodied actions. In particular, we explore systems that aim to improve agents based on next-embodied action prediction by incorporating external knowledge, multi-sensory inputs, and human feedback. We argue that by developing agentic AI systems in grounded environments, one can also mitigate the hallucinations of large foundation models and their tendency to generate environmentally incorrect outputs. The emerging field of Agent AI subsumes the broader embodied and agentic aspects of multimodal interactions. Beyond agents acting and interacting in the physical world, we envision a future where people can easily create any virtual reality or simulated scene and interact with agents embodied within the virtual environment.																																	2024-01-31	PPRN:87070570		
J	Gu, Alex; Roziere, Baptiste; Leather, Hugh; Solar-Lezama, Armando; Synnaeve, Gabriel; Wang, Sida I.										CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution								Arxiv											1	1;2024-01-05;https://www.arxiv.org/abs/2401.03065v1	arXiv:2401.03065			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 05 2024	2024	We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to acing CRUXEval, we provide examples of consistent GPT-4 failures on simple programs as a lens into its code reasoning capabilities and areas for improvement.																																	2024-05-25	PPRN:87040898		
J	Xie, Jian; Zhang, Kai; Chen, Jiangjie; Zhu, Tinghui; Lou, Renze; Tian, Yuandong; Xiao, Yanghua; Su, Yu				Chen, Jiangjie/JCE-5486-2023; Zhu, Ting/LXW-0633-2024; Zhang, Kai/KOD-2592-2024						TravelPlanner: A Benchmark for Real-World Planning with Language Agents								Arxiv											3	3;2024-10-23;https://www.arxiv.org/abs/2402.01622v4| 2;2024-06-23;https://www.arxiv.org/abs/2402.01622v3| 1;2024-02-05;https://www.arxiv.org/abs/2402.01622v2	arXiv:2402.01622			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 23 2024	2024	Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.																																	2024-11-24	PPRN:87523593		
J	Adler, Bo; Agarwal, Niket; Aithal, Ashwath; Anh, Dong H.; Bhattacharya, Pallab; Brundyn, Annika; Casper, Jared; Catanzaro, Bryan; Clay, Sharon; Cohen, Jonathan; Das, Sirshak; Dattagupta, Ayush; Delalleau, Olivier; Derczynski, Leon; Dong, Yi; Egert, Daniel; Evans, Ellie; Ficek, Aleksander; Fridman, Denys; Ghosh, Shaona; Ginsburg, Boris; Gitman, Igor; Grzegorzek, Tomasz; Hero, Robert; Huang, Jining; Jawa, Vibhu; Jennings, Joseph; Jhunjhunwala, Aastha; Kamalu, John; Khan, Sadaf; Kuchaiev, Oleksii; Legresley, Patrick; Li, Hui; Liu, Jiwei; Liu, Zihan; Long, Eileen; Mahabaleshwarkar, Ameya Sunil; Majumdar, Somshubra; Maki, James; Martinez, Miguel; de Melo, Maer Rodrigues; Moshkov, Ivan; Narayanan, Deepak; Narenthiran, Sean; Navarro, Jesus; Nguyen, Phong; Nitski, Osvald; Noroozi, Vahid; Nutheti, Guruprasad; Parisien, Christopher; Parmar, Jupinder; Patwary, Mostofa; Pawelec, Krzysztof; Ping, Wei; Prabhumoye, Shrimai; Roy, Rajarshi; Saar, Trisha; Sabavat, Vasanth Rao Naik; Satheesh, Sanjeev; Scowcroft, Jane Polak; Sewall, Jason; Shamis, Pavel; Shen, Gerald; Shoeybi, Mohammad; Sizer, Dave; Smelyanskiy, Misha; Soares, Felipe; Sreedhar, Makesh Narsimhan; Su, Dan; Subramanian, Sandeep; Sun, Shengyang; Toshniwal, Shubham; Wang, Hao; Wang, Zhilin; You, Jiaxuan; Zeng, Jiaqi; Zhang, Jimmy; Zhang, Jing; Zhang, Vivienne; Zhang, Yian; Zhu, Chen				Jawa, Vibhu/NFT-6031-2025; You, Jiaxuan/ABC-7506-2020; Nguyen, Phong H/HHN-2723-2022						Nemotron-4 340B Technical Report								Arxiv											2	2;2024-08-06;https://www.arxiv.org/abs/2406.11704v2| 1;2024-06-17;https://www.arxiv.org/abs/2406.11704v1	arXiv:2406.11704			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 06 2024	2024	We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open Model License Agreement, a permissive model license that allows distribution, modification, and use of the models and its outputs. These models perform competitively to open access models on a wide range of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. We believe that the community can benefit from these models in various research studies and commercial applications, especially for generating synthetic data to train smaller language models. Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing the effectiveness of these models in generating synthetic data. To further support open research and facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in our model alignment process.																																	2024-08-17	PPRN:89348428		
J	Plaat, Aske; Wong, Annie; Verberne, Suzan; Broekens, Joost; van Stein, Niki; Back, Thomas				van Stein, Niki/JHU-4597-2023; Verberne, Suzan/K-3993-2019; Plaat, Aske/HIR-9410-2022; Neukart, Florian/KVC-7626-2024						Reasoning with Large Language Models, a Survey								Arxiv											1	1;2024-07-16;https://www.arxiv.org/abs/2407.11511v1	arXiv:2407.11511			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 16 2024	2024	   Scaling up language models to billions of parameters has opened up possibilities for in-context learning, allowing instruction tuning and few-shot learning on tasks that the model was not specifically trained for. This has achieved breakthrough performance on language tasks such as translation, summarization, and question-answering. Furthermore, in addition to these associative "System 1" tasks, recent advances in Chain-of-thought prompt learning have demonstrated strong "System 2" reasoning abilities, answering a question in the field of artificial general intelligence whether LLMs can reason.    The field started with the question whether LLMs can solve grade school math word problems. This paper reviews the rapidly expanding field of prompt-based reasoning with LLMs. Our taxonomy identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. Finally, we highlight the relation between reasoning and prompt-based learning, and we discuss the relation between reasoning, sequential decision processes, and reinforcement learning. We find that self-improvement, self-reflection, and some metacognitive abilities of the reasoning processes are possible through the judicious use of prompts. True self-improvement and self-reasoning, to go from reasoning with LLMs to reasoning by LLMs, remains future work.																																	2024-07-25	PPRN:90851897		
J	Kambhampati, Subbarao; Valmeekam, Karthik; Guan, Lin; Verma, Mudit; Stechly, Kaya; Bhambri, Siddhant; Saldyt, Lucas; Murthy, Anil										LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks								Arxiv											2	2;2024-06-12;https://www.arxiv.org/abs/2402.01817v3| 1;2024-02-06;https://www.arxiv.org/abs/2402.01817v2	arXiv:2402.01817			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 12 2024	2024	There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of {bf LLM-Modulo Frameworks} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.																																	2024-07-10	PPRN:87529163		
J	Todd, Eric; Li, Millicent L.; Sen Sharma, Arnab; Mueller, Aaron; Wallace, Byron C.; Bau, David				Bau, David/KGM-5427-2024						Function Vectors in Large Language Models								Arxiv											2	2;2024-02-25;https://www.arxiv.org/abs/2310.15213v2| 1;2023-10-23;https://www.arxiv.org/abs/2310.15213v1	arXiv:2310.15213			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 25 2024	2024	We report the presence of a simple neural mechanism that represents an inputoutput function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some extent they can be summed to create vectors that trigger new complex tasks. Our findings show that compact, causal internal vector representations of function abstractions can be explicitly extracted from LLMs.																																	2024-03-24	PPRN:85769174		
J	Matthee, Jorryt; Naidu, Rohan P.; Brammer, Gabriel; Chisholm, John; Eilers, Anna-Christina; Goulding, Andy; Greene, Jenny; Kashino, Daichi; Labbe, Ivo; Lilly, Simon J.; Mackenzie, Ruari; Oesch, Pascal A.; Weibel, Andrea; Wuyts, Stijn; Xiao, Mengyuan; Bordoloi, Rongmon; Bouwens, Rychard; van Dokkum, Pieter; Illingworth, Garth; Kramarenko, Ivan; Maseda, Michael V.; Mason, Charlotte; Meyer, Romain A.; Nelson, Erica J.; Reddy, Naveen A.; Shivaei, Irene; Simcoe, Robert A.; Yue, Minghao				Matthee, Jorryt/KHD-9384-2024; Kashino, Daichi/AAN-9011-2021; Oesch, Pascal/AFN-4775-2022; Meyer, Romain/LRT-7485-2024; Mason, Charlotte/IYJ-2820-2023; Brammer, Gabriel/AAB-4859-2020; Nelson, Erica/OUI-1817-2025; Wuyts, Stijn/ADQ-9308-2022; Labbe, Ivo/B-1408-2016						Little Red Dots: an abundant population of faint AGN at z ∼ 5 revealed by the EIGER and FRESCO JWST surveys								Arxiv											2	2;2024-01-23;https://www.arxiv.org/abs/2306.05448v2| 1;2023-06-08;https://www.arxiv.org/abs/2306.05448v1	arXiv:2306.05448			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 23 2024	2024	Characterising the prevalence and properties of faint active galactic nuclei (AGN) in the early Universe is key for understanding the formation of supermassive black holes (SMBHs) and determining their role in cosmic reionization. We perform a spectroscopic search for broad Hα emitters at z ≈ 4 − 6 using deep JWST/NIRCam imaging and wide field slitless spectroscopy from the EIGER and FRESCO surveys. We identify 20 Hα lines at z = 4.2 − 5.5 that have broad components with line widths from ∼ 1200 − 3700 km s−1, contributing ∼ 30 − 90 % of the total line flux. We interpret these broad components as being powered by accretion onto SMBHs with implied masses ∼ 107−8 M⊙. In the UV luminosity range MUV,AGN+host = −21 to −18, we measure number densities of≈ 10−5 cMpc−3. This is an order of magnitude higher than expected from extrapolating quasar UV luminosity functions. Yet, such AGN are found in only < 1 % of star-forming galaxies at z ∼ 5. The number density discrepancy is much lower when compared to the broad Hα luminosity function. The SMBH mass function agrees with large cosmological simulations. In two objects we detect complex Hα profiles that we tentatively interpret as caused by absorption signatures from dense gas fueling SMBH growth and outflows. We may be witnessing early AGN feedback that will clear dust-free pathways through which more massive blue quasars are seen. We uncover a strong correlation between reddening and the fraction of total galaxy luminosity arising from faint AGN. This implies that early SMBH growth is highly obscured and that faint AGN are only minor contributors to cosmic reionization.																																	2024-02-10	PPRN:73265860		
J	Rawles, Christopher; Clinckemaillie, Sarah; Chang, Yifan; Waltz, Jonathan; Lau, Gabrielle; Fair, Marybeth; Li, Alice; Bishop, William; Li, Wei; Campbell-Ajala, Folawiyo; Toyama, Daniel; Berry, Robert; Tyamagundlu, Divya; Lillicrap, Timothy; Riva, Oriana										AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents								Arxiv											4	4;2024-12-19;https://www.arxiv.org/abs/2405.14573v4| 3;2024-10-22;https://www.arxiv.org/abs/2405.14573v3| 2;2024-06-10;https://www.arxiv.org/abs/2405.14573v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14573v1	arXiv:2405.14573			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 19 2024	2024	Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present A NDROID WORLD , a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, A NDROID WORLD dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device’s system state. We experiment with baseline agents to test A NDROID WORLD and provide initial results on the benchmark. Our best agent can complete 30.6% of A NDROID- WORLD ’s tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. 																																	2025-01-27	PPRN:88989344		
J	Yang, Jihan; Yang, Shusheng; Gupta, Anjali W.; Han, Rilyn; Fei-Fei, Li; Xie, Saining				Yang, Jihan/JQI-4498-2023						Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces								Arxiv											1	1;2024-12-18;https://www.arxiv.org/abs/2412.14171v1	arXiv:2412.14171			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 18 2024	2024	Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also “think in space” from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive—though subhuman—visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs’ spatial distance ability.																																	2025-01-24	PPRN:120040642		
J	Zhou, Hongjian; Liu, Fenglin; Gu, Boyang; Zou, Xinyu; Huang, Jinfa; Wu, Jinge; Li, Yiru; Chen, Sam; Zhou, Peilin; Liu, Junling; Hua, Yining; Mao, Chengfeng; You, Chenyu; Wu, Xian; Zheng, Yefeng; Clifton, Lei; Li, Zheng; Luo, Jiebo; Clifton, David A.				You, Chenyu/AFK-5803-2022; Wu, Xian/JRW-5738-2023; Wu, Jinge/IAP-3578-2023; Zheng, Yefeng/ABG-7053-2020						The Development and Deployment of Large Language Models in Medicine								Arxiv											4	4;2024-07-10;https://www.arxiv.org/abs/2311.05112v6| 3;2024-02-02;https://www.arxiv.org/abs/2311.05112v3| 2;2023-12-11;https://www.arxiv.org/abs/2311.05112v2| 1;2023-11-09;https://www.arxiv.org/abs/2311.05112v1	arXiv:2311.05112			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 10 2024	2024	Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a comprehensive review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide a clear understanding of the distinct advantages and limitations of LLMs in medicine. Overall, in this review, we address the following study questions: 1) What are the practices for developing medical LLMs? 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities and challenges of LLMs in medicine and serve as a practical resource for constructing effective medical LLMs. We also maintain a regularly updated list of practical guides on medical LLMs at: https://github.com/AI-in-Health/MedLLMsPracticalGuide. .																																	2024-08-03	PPRN:86108898		
J	Verga, Pat; Hofstatter, Sebastian; Althammer, Sophia; Su, Yixuan; Piktus, Aleksandra; Arkhangorodsky, Arkady; Xu, Minjie; White, Naomi; Lewis, Patrick				Xu, Minjie/GWC-3059-2022						Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2404.18796v2| 1;2024-04-29;https://www.arxiv.org/abs/2404.18796v1	arXiv:2404.18796			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model’s free -form generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intra-model bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.																																	2024-05-21	PPRN:88696066		
J	Park, Gunho; Park, Baeseong; Kim, Minsub; Lee, Sungjae; Kim, Jeonghoon; Kwon, Beomseok; Kwon, Se Jung; Kim, Byeongwook; Lee, Youngjoo; Lee, Dongsoo				Lee, Youngjoo/AAO-5467-2020						LUT-GEMM: QUANTIZED MATRIX MULTIPLICATION BASED ON LUTS FOR EFFICIENT INFERENCE IN LARGE-SCALE GENERATIVE LANGUAGE MODELS								Arxiv											2	2;2024-04-01;https://www.arxiv.org/abs/2206.09557v4| 1;2022-06-20;https://www.arxiv.org/abs/2206.09557v1	arXiv:2206.09557			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	Recent advances in self-supervised learning and the Transformer architecture have significantly improved natural language processing (NLP), achieving remarkably low perplexity. However, the growing size of NLP models introduces a memory wall problem during the generation phase. To mitigate this issue, recent efforts have focused on quantizing model weights to sub-4-bit precision while preserving full precision for activations, resulting in practical speed-ups during inference on a single GPU. However, these improvements primarily stem from reduced memory movement, which necessitates a resource-intensive dequantization process rather than actual computational reduction. In this paper, we introduce LUT-GEMM, an efficient kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization. Furthermore, we proposed group-wise quantization to offer a flexible trade-off between compression ratio and accuracy. The impact of LUT-GEMM is facilitated by implementing high compression ratios through low-bit quantization and efficient LUT-based operations. We show experimentally that when applied to the OPT-175B model with 3-bit quantization, LUT-GEMM substantially accelerates token generation latency, achieving a remarkable 2.1× improvement on a single GPU when compared to OPTQ, which relies on the costly dequantization process.																																	2024-05-03	PPRN:12683651		
J	Hu, Anwen; Xu, Haiyang; Ye, Jiabo; Yan, Ming; Zhang, Liang; Zhang, Bo; Li, Chen; Zhang, Ji; Jin, Qin; Huang, Fei; Zhou, Jingren				Zhang, Bo/ABF-8476-2021						mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding								Arxiv											1	1;2024-03-19;https://www.arxiv.org/abs/2403.12895v1	arXiv:2403.12895			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.																																	2025-08-07	PPRN:123158588		
J	Biderman, Dan; Portes, Jacob; Ortiz, Jose Javier Gonzalez; Paul, Mansheej; Greengard, Philip; Jennings, Connor; King, Daniel; Havens, Sam; Chiley, Vitaliy; Frankle, Jonathan; Blakeney, Cody; Cunningham, John P.										LoRA Learns Less and Forgets Less								Arxiv											2	2;2024-09-20;https://www.arxiv.org/abs/2405.09673v2| 1;2024-05-15;https://www.arxiv.org/abs/2405.09673v1	arXiv:2405.09673			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 20 2024	2024	Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (approximately 100K prompt-response pairs) and continued pretraining (20B unstructured tokens) data regimes. Our results show that, in the standard low-rank settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA better maintains the base model's performance on tasks outside the target domain. We show that LoRA mitigates forgetting more than common regularization techniques such as weight decay and dropout; it also helps maintain more diverse generations. Finally, we show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.																																	2024-10-07	PPRN:89077154		
J	Rajamanoharan, Senthooran; Lieberum, Tom; Sonnerat, Nicolas; Conmy, Arthur; Varma, Vikrant; Kramar, Janos; Nanda, Neel										Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders								Arxiv											3	3;2024-08-01;https://www.arxiv.org/abs/2407.14435v3| 2;2024-07-29;https://www.arxiv.org/abs/2407.14435v2| 1;2024-07-19;https://www.arxiv.org/abs/2407.14435v1	arXiv:2407.14435			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 01 2024	2024	Sparse autoencoders (SAEs) are a promising unsupervised approach for identifying causally relevant and interpretable linear features in a language model's (LM) activations. To be useful for downstream tasks, SAEs need to decompose LM activations faithfully; yet to be interpretable the decomposition must be sparse -- two objectives that are in tension. In this paper, we introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity at a given sparsity level on Gemma 2 9B activations, compared to other recent advances such as Gated and TopK SAEs. We also show that this improvement does not come at the cost of interpretability through manual and automated interpretability studies. JumpReLU SAEs are a simple modification of vanilla (ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU activation function -- and are similarly efficient to train and run. By utilising straight-through-estimators (STEs) in a principled manner, we show how it is possible to train JumpReLU SAEs effectively despite the discontinuous JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs to directly train L0 to be sparse, instead of training on proxies such as L1, avoiding problems like shrinkage.																																	2024-08-08	PPRN:91010534		
J	Prabhudesai, Mihir; Goyal, Anirudh; Pathak, Deepak; Fragkiadaki, Katerina										Aligning Text-to-Image Diffusion Models with Reward Backpropagation								Arxiv											3	3;2024-06-22;https://www.arxiv.org/abs/2310.03739v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03739v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03739v1	arXiv:2310.03739			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 22 2024	2024	Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets. Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations. We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest.  [GRAPHICS]																																	2024-07-15	PPRN:85435344		
J	Clark, Kevin; Vicol, Paul; Swersky, Kevin; Fleet, David J										Directly Fine-Tuning Diffusion Models on Differentiable Rewards								Arxiv											2	2;2024-06-21;https://www.arxiv.org/abs/2309.17400v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17400v1	arXiv:2309.17400			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 21 2024	2024	We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT- K , which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K = 1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient -based fine-tuning algorithms.																																	2024-07-11	PPRN:85461561		
J	Lermen, Simon; Rogers-Smith, Charlie; Ladish, Jeffrey										LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B								Arxiv											2	2;2024-05-22;https://www.arxiv.org/abs/2310.20624v2| 1;2023-10-31;https://www.arxiv.org/abs/2310.20624v1	arXiv:2310.20624			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 22 2024	2024	Content Warning: This document contains content that some may find disturbing or offensive, including content that is hateful or violent in nature. AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat—a collection of instruction fine-tuned large language models—they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. We explore the robustness of safety training in language models by subversively fine-tuning Llama 2-Chat. We employ quantized low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B and on the Mixtral instruct model. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve refusal rates of about 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Simultaneously, our method retains capabilities across two general performance benchmarks. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights. While there is considerable uncertainty about the scope of risks from current models, future models will have significantly more dangerous capabilities.																																	2024-06-04	PPRN:85908098		
J	Huang, Jiangyong; Yong, Silong; Ma, Xiaojian; Linghu, Xiongkun; Li, Puhao; Wang, Yan; Li, Qing; Zhu, Song-Chun; Jia, Baoxiong; Huang, Siyuan										An Embodied Generalist Agent in 3D World								Arxiv											3	3;2024-05-09;https://www.arxiv.org/abs/2311.12871v3| 2;2024-04-19;https://www.arxiv.org/abs/2311.12871v2| 1;2023-11-18;https://www.arxiv.org/abs/2311.12871v1	arXiv:2311.12871			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 09 2024	2024	Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. 																																	2024-05-28	PPRN:86244268		
J	Liao, Yi-Lun; Wood, Brandon; Das, Abhishek; Smidt, Tess										EQUIFORMERV2: IMPROVED EQUIVARIANT TRANSFORMER FOR SCALING TO HIGHER-DEGREE REPRESENTATIONS								Arxiv											3	3;2024-03-06;https://www.arxiv.org/abs/2306.12059v3| 2;2023-12-02;https://www.arxiv.org/abs/2306.12059v2| 1;2023-06-21;https://www.arxiv.org/abs/2306.12059v1	arXiv:2306.12059			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 06 2024	2024	Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace SOp3q convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements – attention re-normalization, separable S2 activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state -of -the -art methods on large-scale OC20 dataset by up to 9% on forces, 4% on energies, offers better speed-accuracy trade-offs, and 2× reduction in DFT calculations needed for computing adsorption energies. Additionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC trained on both OC20 and OC22 datasets, achieving much better data efficiency. Finally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M datasets to better understand the performance gain brought by higher degrees.																																	2024-04-05	PPRN:73459124		
J	Zheng, Chujie; Zhou, Hao; Meng, Fandong; Zhou, Jie; Huang, Minlie				Zheng, Chujie/CAG-9031-2022						LARGE LANGUAGE MODELS ARE NOT ROBUST MULTIPLE CHOICE SELECTORS								Arxiv											3	3;2024-02-22;https://www.arxiv.org/abs/2309.03882v4| 2;2023-10-06;https://www.arxiv.org/abs/2309.03882v3| 1;2023-09-07;https://www.arxiv.org/abs/2309.03882v1	arXiv:2309.03882			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 22 2024	2024	Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias", namely, they prefer to select specific option IDs as answers (like "Option A"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.																																	2024-11-09	PPRN:84856319		
J	Wu, Tongtong; Luo, Linhao; Li, Yuan-Fang; Pan, Shirui; Vu, Thuy-Trang; Haffari, Gholamreza				LUO, LINHAO/IAM-3162-2023; Vu, Thuy-Trang/MBW-2710-2025; Li, Yuan-Fang/T-7532-2019						Continual Learning for Large Language Models: A Survey								Arxiv											2	2;2024-02-07;https://www.arxiv.org/abs/2402.01364v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01364v1	arXiv:2402.01364			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 07 2024	2024	Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.																																	2024-02-23	PPRN:87509650		
J	Hua, Wenyue; Fan, Lizhou; Li, Lingyao; Mei, Kai; Ji, Jianchao; Ge, Yingqiang; Hemphill, Libby; Zhang, Yongfeng				MEI, KAIYUAN/LFV-3617-2024						War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2311.17227v2	arXiv:2311.17227			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 30 2024	2024	Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose WarAgent, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems’ abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts.																																	2024-05-25	PPRN:87419873		
J	Zhao, Yu; Yin, Huifeng; Zeng, Bo; Wang, Hao; Shi, Tianqi; Lyu, Chenyang; Wang, Longyue; Luo, Weihua; Zhang, Kaifu				SHI, TIANQI/AGH-7938-2022; Luo, Weihua/LIF-4790-2024						Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions								Arxiv											2	2;2024-11-25;https://www.arxiv.org/abs/2411.14405v2| 1;2024-11-21;https://www.arxiv.org/abs/2411.14405v1	arXiv:2411.14405			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 25 2024	2024	Currently OpenAI o1 sparks a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding—which are well-suited for reinforcement learning (RL)—but also places greater emphasis on open-ended resolutions. We aim to address the question: “Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?” Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies—optimized for complex real-world problem-solving tasks.  [Graphics]																																	2025-01-08	PPRN:119316988		
J	Ouyang, Shuyin; Zhang, Jie M.; Harman, Mark; Wang, Meng										An Empirical Study of the Non-determinism of ChatGPT in Code Generation								Arxiv											2	2;2024-10-17;https://www.arxiv.org/abs/2308.02828v2| 1;2023-08-05;https://www.arxiv.org/abs/2308.02828v1	arXiv:2308.02828			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Oct 17 2024	2024	There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different codes for the same prompt. Non-determinism is a potential menace to scientific conclusion validity. When non-determinism is high, scientific conclusions simply cannot be relied upon unless researchers change their behaviour to control for it in their empirical analyses. This paper conducts an empirical study to demonstrate that non-determinism is, indeed, high, thereby underlining the need for this behavioural change. We choose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems from three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval). Our results reveal high degrees of non-determinism: the ratio of coding tasks with zero equal test output across different requests is 75.76%, 51.00%, and 47.56% for CodeContests, APPS, and HumanEval, respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature=1). These results confirm that there is, currently, a significant threat to scientific conclusion validity. In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.																																	2024-11-13	PPRN:74308255		
J	Hosseini, Arian; Yuan, Xingdi; Malkin, Nikolay; Courville, Aaron; Sordoni, Alessandro; Agarwal, Rishabh										V-STaR: Training Verifiers for Self-Taught Reasoners								Arxiv											2	2;2024-08-14;https://www.arxiv.org/abs/2402.06457v2| 1;2024-02-09;https://www.arxiv.org/abs/2402.06457v1	arXiv:2402.06457			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 14 2024	2024	Common self-improvement approaches for large language models (LLMs), such as STaR, iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.																																	2024-08-22	PPRN:87607929		
J	Colgain, Eoin O; Dainotti, Maria Giovanna; Capozziello, Salvatore; Pourojaghi, Saeed; Sheikh-Jabbari, M.M.; Stojkovic, Dejan				Pourojaghi, Saeed/GWZ-5118-2022; Sheikh-Jabbari, Mohammad/AAM-7848-2020; Capozziello, Salvatore/AAA-6859-2019; Dainotti, Maria/AAD-3896-2022						Does DESI 2024 Confirm ΛCDM?								Arxiv											2	2;2024-04-26;https://www.arxiv.org/abs/2404.08633v2| 1;2024-04-12;https://www.arxiv.org/abs/2404.08633v1	arXiv:2404.08633			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	We demonstrate that a ∼ 2σ discrepancy with the Planck-ΛCDM cosmology in DESI Luminous Red Galaxy (LRG) data at zeff = 0.51 translates into an unexpectedly large Ωm value, Ωm = 0 . 668− 0.169 +0 .180  . We independently confirm that this anomaly drives the preference for w0 > − 1 in DESI data confronted to the w0wa CDM model. We show that redshift bins of DESI constraints allow Ωm to wiggle at the ∼ 2σ level with increasing effective redshift in the ΛCDM model. Given that LRG data at zeff = 0.51 is at odds with Type Ia supernovae in overlapping redshifts, we expect that this anomaly will decrease in statistical significance with future DESI data releases leaving an increasing Ωm trend with effective redshift at higher redshifts. We estimate the significance of the latter in DESI data at ∼ 1.8σ and comment on how it dovetails with independent observations. It is imperative to understand what makes DESI LRG data at zeff = 0.51 . 51 an outlier when it comes to Ωm <span style="font-size:10px"> </span>determinations.																																	2024-05-16	PPRN:88550023		
J	Tong, Shengbang; Liu, Zhuang; Zhai, Yuexiang; Ma, Yi; LeCun, Yann; Xie, Saining										Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs								Arxiv											2	2;2024-04-25;https://www.arxiv.org/abs/2401.06209v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.06209v1	arXiv:2401.06209			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 25 2024	2024	Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.																																	2024-05-04	PPRN:87153645		
J	Gromov, Andrey; Tirumala, Kushal; Shapourian, Hassan; Glorioso, Paolo; Roberts, Daniel A.										The Unreasonable Ineffectiveness of the Deeper Layers								Arxiv											2	2;2025-03-03;https://www.arxiv.org/abs/2403.17887v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17887v1	arXiv:2403.17887			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 26 2024	2024	We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.																																	2025-08-07	PPRN:88291761		
J	Naous, Tarek; Ryan, Michael J.; Ritter, Alan; Xu, Wei				xu, wei/HHD-2891-2022						Having Beer after Prayer? Measuring Cultural Bias in Large Language Models								Arxiv											4	4;2024-03-20;https://www.arxiv.org/abs/2305.14456v4| 3;2024-02-19;https://www.arxiv.org/abs/2305.14456v3| 2;2023-11-16;https://www.arxiv.org/abs/2305.14456v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.14456v1	arXiv:2305.14456			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 20 2024	2024	As the reach of large language models (LMs) expands globally, their ability to cater to diverse cultural contexts becomes crucial. Despite advancements in multilingual capabilities, models are not designed with appropriate cultural nuances. In this paper, we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. We introduce CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities spanning eight types that contrast Arab and Western cultures. CAMeL provides a foundation for measuring cultural biases in LMs through both extrinsic and intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance in Arabic of 16 different LMs on tasks such as story generation, NER, and sentiment analysis, where we find concerning cases of stereotyping and cultural unfairness. We further test their text-infilling performance, revealing the incapability of appropriate adaptation to Arab cultural contexts. Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment. 																																	2024-04-12	PPRN:72713207		
J	Liu, Yuliang; Yang, Biao; Liu, Qiang; Li, Zhang; Ma, Zhiyin; Zhang, Shuo; Bai, Xiang										TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document								Arxiv											2	2;2024-03-15;https://www.arxiv.org/abs/2403.04473v2| 1;2024-03-07;https://www.arxiv.org/abs/2403.04473v1	arXiv:2403.04473			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 15 2024	2024	We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. It also learns to perform screenshot tasks through finetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.																																	2024-04-11	PPRN:88061234		
J	Ren, Ruiyang; Wang, Yuhao; Qu, Yingqi; Zhao, Wayne Xin; Liu, Jing; Tian, Hao; Wu, Hua; Wen, Ji-Rong; Wang, Haifeng				Liu, Jingming/GPG-2366-2022; Xia, Lianghao/IWV-0954-2023; Wang, Yuhao/JMD-0355-2023						Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation								Arxiv											2	2;2024-11-19;https://www.arxiv.org/abs/2307.11019v3| 1;2023-07-20;https://www.arxiv.org/abs/2307.11019v1	arXiv:2307.11019			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 19 2024	2024	Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs’ awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs’ QA and judgemental capabilities. 																																	2024-12-28	PPRN:74078485		
J	Zhong, Tianyang; Liu, Zhengliang; Pan, Yi; Zhang, Yutong; Zhou, Yifan; Liang, Shizhe; Wu, Zihao; Lyu, Yanjun; Shu, Peng; Yu, Xiaowei; Cao, Chao; Jiang, Hanqi; Chen, Hanxu; Li, Yiwei; Chen, Junhao; Hu, Huawen; Liu, Yihen; Zhao, Huaqin; Xu, Shaochen; Dai, Haixing; Zhao, Lin; Zhang, Ruidong; Zhao, Wei; Yang, Zhenyuan; Chen, Jingyuan; Wang, Peilong; Ruan, Wei; Wang, Hui; Zhao, Huan; Zhang, Jing; Ren, Yiming; Qin, Shihuan; Chen, Tong; Li, Jiaxi; Zidan, Arif Hassan; Jahin, Afrar; Chen, Minheng; Xia, Sichen; Holmes, Jason; Zhuang, Yan; Wang, Jiaqi; Xu, Bochen; Xia, Weiran; Yu, Jichao; Tang, Kaibo; Yang, Yaxuan; Sun, Bolun; Yang, Tao; Lu, Guoyu; Wang, Xianqiao; Chai, Lilong; Li, He; Lu, Jin; Sun, Lichao; Zhang, Xin; Ge, Bao; Hu, Xintao; Zhang, Lian; Zhou, Hua; Zhang, Lu; Zhang, Shu; Liu, Ninghao; Jiang, Bei; Kong, Linglong; Xiang, Zhen; Ren, Yudan; Liu, Jun; Jiang, Xi; Bao, Yu; Zhang, Wei; Li, Xiang; Li, Gang; Liu, Wei; Shen, Dinggang; Sikora, Andrea; Zhai, Xiaoming; Zhu, Dajiang; Liu, Tianming				Chen, Minheng/OTH-9039-2025; chen, jingyuan/KVC-1118-2024; Chen, Junhao/LCD-2351-2024; Yu, Xiaowei/AAC-6158-2021; Li, Jiaxi/GXM-8915-2022; Zhao, Lin/ABM-7665-2022; SHU, PENG/LDF-4318-2024; yuan, yixuan/KLZ-6092-2024; Chen, Hanxu/KVB-4765-2024; Wang, Peilong/R-4412-2016; Liu, Tianming/GLS-1211-2022; ren, yiming/ODI-9217-2025; Wu, Zihao/KDP-2552-2024; Zhai, Xiaoming/AAB-7129-2021; zhang, yutong/GXV-2287-2022; SUN, BOLUN/LWK-4180-2024; Yang, ZhenYuan/LXV-5784-2024; Newsome, Andrea/AAC-6365-2020; zhao, wei/IQS-1144-2023						Evaluation of OpenAI o1: Opportunities and Challenges of AGI								Arxiv											1	1;2024-09-27;https://www.arxiv.org/abs/2409.18486v1	arXiv:2409.18486			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 27 2024	2024	This comprehensive study evaluates the performance of OpenAI’s o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include: 83.3% success rate in solving complex competitive programming problems, surpassing many human experts. Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models. 100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions. Advanced natural language inference capabilities across general and specialized domains like medicine. Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis. Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields. Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills. Effective performance in social media analysis, including sentiment analysis and emotion recognition. The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence. This evaluation not only highlights o1-preview’s current strengths and limitations but also identifies crucial areas for future development, including multi-mo dal integration, domain-specific validation, and ethical considerations for real-world applications. The findings provide valuable insights into the potential of large language models in numerous fields and pave the way for further advancements in AI research and application.																																	2025-01-24	PPRN:100706604		
J	Chen, Guiming Hardy; Chen, Shunian; Liu, Ziche; Jiang, Feng; Wang, Benyou				Wang, Benyou/Y-5146-2019						Humans or LLMs as the Judge? A Study on Judgement Biases								Arxiv											5	5;2024-09-26;https://www.arxiv.org/abs/2402.10669v5| 4;2024-06-16;https://www.arxiv.org/abs/2402.10669v4| 3;2024-04-17;https://www.arxiv.org/abs/2402.10669v3| 2;2024-02-20;https://www.arxiv.org/abs/2402.10669v2| 1;2024-02-16;https://www.arxiv.org/abs/2402.10669v1	arXiv:2402.10669			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 26 2024	2024	Adopting human and large language models (LLM) as judges (a.k.a human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating Misinformation Oversight Bias, Gender Bias, Authority Bias and Beauty Bias on LLM and human judges. We curate a dataset referring to the revised Bloom's Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems.																																	2024-10-08	PPRN:87729977		
J	Munkhdalai, Tsendsuren; Faruqui, Manaal; Gopal, Siddharth										Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention								Arxiv											2	2;2024-08-09;https://www.arxiv.org/abs/2404.07143v2| 1;2024-04-10;https://www.arxiv.org/abs/2404.07143v1	arXiv:2404.07143			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 09 2024	2024	This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.																																	2024-08-21	PPRN:88478606		
J	Fu, Xingyu; Hu, Yushi; Li, Bangzheng; Feng, Yu; Wang, Haoyu; Lin, Xudong; Roth, Dan; Smith, Noah A.; Ma, Wei-Chiu; Krishna, Ranjay				Hu, Yushi/MGV-6188-2025; Fu, Xingyu/GZM-3129-2022; lin, Xudong/ISA-0071-2023; Wang, Haoyu/IUO-7393-2023						BLINK: Multimodal Large Language Models Can See but Not Perceive								Arxiv											4	4;2024-07-03;https://www.arxiv.org/abs/2404.12390v4| 3;2024-05-04;https://www.arxiv.org/abs/2404.12390v3| 2;2024-04-25;https://www.arxiv.org/abs/2404.12390v2| 1;2024-04-18;https://www.arxiv.org/abs/2404.12390v1	arXiv:2404.12390			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 03 2024	2024	We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans "within a blink" (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not "emerged" yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.																																	2024-07-20	PPRN:88565224		
J	Zhang, Yuang; Gu, Jiaxi; Wang, Li-Wen; Wang, Han; Cheng, Junqi; Zhu, Yuefeng; Zou, Fangyuan										MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance								Arxiv											1	1;2024-06-28;https://www.arxiv.org/abs/2406.19680v1	arXiv:2406.19680			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 28 2024	2024	In recent years, generative artificial intelligence has achieved significant advancements in the field of image generation, spawning a variety of applications. However, video generation still faces considerable challenges in various aspects, such as controllability, video length, and richness of details, which hinder the application and popularization of this technology. In this work, we propose a controllable video generation framework, dubbed MimicMotion, which can generate high-quality videos of arbitrary length mimicking specific motion guidance. Compared with previous methods, our approach has several highlights. Firstly, we introduce confidence-aware pose guidance that ensures high frame quality and temporal smoothness. Secondly, we introduce regional loss amplification based on pose confidence, which significantly reduces image distortion. Lastly, for generating long and smooth videos, we propose a progressive latent fusion strategy. By this means, we can produce videos of arbitrary length with acceptable resource consumption. With extensive experiments and user studies, MimicMotion demonstrates significant improvements over previous approaches in various aspects. Detailed results and comparisons are available on our project page: https://tencent.github.io/MimicMotion .																																	2024-07-17	PPRN:90635794		
J	Cao, Bochuan; Cao, Yuanpu; Lin, Lu; Chen, Jinghui				Chen, Jinghui/AFT-5065-2022						Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM								Arxiv											3	3;2024-06-12;https://www.arxiv.org/abs/2309.14348v3| 2;2023-12-07;https://www.arxiv.org/abs/2309.14348v2| 1;2023-09-18;https://www.arxiv.org/abs/2309.14348v1	arXiv:2309.14348			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 12 2024	2024	Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less.																																	2024-07-04	PPRN:85225546		
J	Liu, Haohe; Yuan, Yi; Liu, Xubo; Mei, Xinhao; Kong, Qiuqiang; Tian, Qiao; Wang, Yuping; Wang, Wenwu; Wang, Yuxuan; Plumbley, Mark D.				liu, haohe/JBS-1030-2023; wang, wenwu/HOF-4371-2023; Wang, Yuxuan/P-4470-2014; Plumbley, Mark/A-7298-2008; Mei, Xinhao/JRZ-0800-2023; Liu, Xubo/HNR-3002-2023						AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining								Arxiv											2	2;2024-05-11;https://www.arxiv.org/abs/2308.05734v3| 1;2023-08-10;https://www.arxiv.org/abs/2308.05734v1	arXiv:2308.05734			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 11 2024	2024	Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called "language of audio" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate state-of-the-art or competitive performance against previous approaches. Our code, pretrained model, and demo are available at https://audioldm.github.io/audioldm2.																																	2024-05-30	PPRN:75399598		
J	Bahri, Yasaman; Dyer, Ethan; Kaplan, Jared; Lee, Jaehoon; Sharma, Utkarsh										Explaining Neural Scaling Laws								Arxiv											2	2;2024-04-29;https://www.arxiv.org/abs/2102.06701v2| 1;2021-02-12;https://www.arxiv.org/abs/2102.06701v1	arXiv:2102.06701			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 29 2024	2024	The population loss of trained deep neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains the origins of and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents under modifications of task and architecture aspect ratio. Our work provides a taxonomy for classifying different scaling regimes, underscores that there can be different mechanisms driving improvements in loss, and lends insight into the microscopic origins of and relationships between scaling exponents.																																	2024-05-10	PPRN:12561046		
J	Wang, Haofan; Spinelli, Matteo; Wang, Qixun; Bai, Xu; Qin, Zekui; Chen, Anthony				Wong, Howard/HZJ-8545-2023; Spinelli, Matteo/AAC-1658-2019; Wang, Qixun/ITT-4329-2023						InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation								Arxiv											2	2;2024-04-04;https://www.arxiv.org/abs/2404.02733v2| 1;2024-04-03;https://www.arxiv.org/abs/2404.02733v1	arXiv:2404.02733			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy designs.Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements. Our codes will be available at https://github.com/InstantStyle/InstantStyle.																																	2024-04-20	PPRN:88396386		
J	Chen, Zilong; Wang, Feng; Wang, Yikai; Liu, Huaping				Chen, Zilong/HCI-2162-2022						Text-to-3D using Gaussian Splatting								Arxiv											4	4;2024-04-02;https://www.arxiv.org/abs/2309.16585v4| 3;2023-10-31;https://www.arxiv.org/abs/2309.16585v3| 2;2023-09-29;https://www.arxiv.org/abs/2309.16585v2| 1;2023-09-28;https://www.arxiv.org/abs/2309.16585v1	arXiv:2309.16585			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Automatic text-to-3D generation that combines Score Distillation Sampling (SDS) with the optimization of volume rendering has achieved remarkable progress in synthesizing realistic 3D objects. Yet most existing text-to-3D methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the Janus issue, since it is hard to explicitly integrate 3D priors into implicit 3D representations. Besides, it is usually time-consuming for them to generate elaborate 3D models with rich colors. In response, this paper proposes GSGEN, a novel method that adopts Gaussian Splatting, a recent state-of-the-art representation, to text-to-3D generation. GSGEN aims at generating high-quality 3D objects and addressing existing shortcomings by exploiting the explicit nature of Gaussian Splatting that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimization, a coarse representation is established under 3D point cloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative appearance refinement to enrich texture details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D assets with delicate details and accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. 																																	2024-05-03	PPRN:85324177		
J	Jiang, Hang; Zhang, Xiajie; Cao, Xubo; Breazeal, Cynthia; Roy, Deb; Kabbara, Jad										PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits								Arxiv											4	4;2024-04-02;https://www.arxiv.org/abs/2305.02547v5| 3;2024-02-26;https://www.arxiv.org/abs/2305.02547v4| 2;2023-11-16;https://www.arxiv.org/abs/2305.02547v3| 1;2023-05-04;https://www.arxiv.org/abs/2305.02547v1	arXiv:2305.02547			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 02 2024	2024	Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44 -item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas’ self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas’ writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.																																	2024-04-18	PPRN:67338168		
J	Deletang, Gregoire; Ruoss, Anian; Duquenne, Paul-Ambroise; Catt, Elliot; Genewein, Tim; Mattern, Christopher; Grau-Moya, Jordi; Wenliang, Li Kevin; Aitchison, Matthew; Orseau, Laurent; Hutter, Marcus; Veness, Joel										Language Modeling Is Compression								Arxiv											2	2;2024-03-18;https://www.arxiv.org/abs/2309.10668v2| 1;2023-09-19;https://www.arxiv.org/abs/2309.10668v1	arXiv:2309.10668			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.																																	2024-04-12	PPRN:85050426		
J	Zhang, Biao; Liu, Zhongtao; Cherry, Colin; Firat, Orhan										WHEN SCALING MEETS LLM FINETUNING: THE EFFECT OF DATA, MODEL AND FINETUNING METHOD								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17193v1	arXiv:2402.17193			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full -model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data -limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data -dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.																																	2024-03-27	PPRN:87922446		
J	Schulhoff, Sander; Ilie, Michael; Balepur, Nishant; Kahadze, Konstantine; Liu, Amanda; Si, Chenglei; Li, Yinheng; Gupta, Aayush; Han, Hyojung; Schulhoff, Sevien; Dulepet, Pranav Sandeep; Vidyadhara, Saurav; Ki, Dayeon; Agrawal, Sweta; Pham, Chau; Kroiz, Gerson; Li, Feileen; Tao, Hudson; Srivastava, Ashay; Da Costa, Hevander; Gupta, Saloni; Rogers, Megan L.; Goncearenco, Inna; Sarli, Giuseppe; Galynker, Igor; Peskoff, Denis; Carpuat, Marine; White, Jules; Anadkat, Shyamal; Hoyle, Alexander; Resnik, Philip				Rogers, Megan/AAC-1894-2021; Galynker, Igor/AAC-9629-2021						The Prompt Report: A Systematic Survey of Prompting Techniques								Arxiv											2	2;2024-12-30;https://www.arxiv.org/abs/2406.06608v5| 1;2024-06-17;https://www.arxiv.org/abs/2406.06608v2	arXiv:2406.06608			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 30 2024	2024	Generative Artificial Intelligence (GenAI) systems are increasingly being deployed across diverse industries and research domains. Developers and end-users interact with these systems through the use of prompting and prompt engineering. Although prompt engineering is a widely adopted and extensively researched area, it suffers from conflicting terminology and a fragmented ontological understanding of what constitutes an effective prompt due to its relatively recent emergence. We establish a structured understanding of prompt engineering by assembling a taxonomy of prompting techniques and analyzing their applications. We present a detailed vocabulary of 33 vocabulary terms, a taxonomy of 58 LLM prompting techniques, and 40 techniques for other modalities. Additionally, we provide best practices and guidelines for prompt engineering, including advice for prompting state-of-the-art (SOTA) LLMs such as ChatGPT. We further present a meta-analysis of the entire literature on natural language prefix-prompting. As a culmination of these efforts, this paper presents the most comprehensive survey on prompt engineering to date.																																	2025-03-06	PPRN:89285484		
J	Fu, Chaoyou; Lin, Haojia; Long, Zuwei; Shen, Yunhang; Zhao, Meng; Zhang, Yifan; Dong, Shaoqi; Wang, Xiong; Yin, Di; Ma, Long; Zheng, Xiawu; He, Ran; Ji, Rongrong; Wu, Yunsheng; Shan, Caifeng; Sun, Xing				Shen, Yunhang/ADW-0834-2022; Shan, Caifeng/W-6178-2019						VITA: Towards Open-Source Interactive Omni Multimodal LLM								Arxiv											2	2;2024-09-10;https://www.arxiv.org/abs/2408.05211v2| 1;2024-08-09;https://www.arxiv.org/abs/2408.05211v1	arXiv:2408.05211			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 10 2024	2024	The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. 																																	2024-09-26	PPRN:91319542		
J	Hernandez, Evan; Li, Belinda Z.; Andreas, Jacob										Inspecting and Editing Knowledge Representations in Language Models								Arxiv											2	2;2024-08-09;https://www.arxiv.org/abs/2304.00740v3| 1;2023-04-03;https://www.arxiv.org/abs/2304.00740v1	arXiv:2304.00740			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 09 2024	2024	Neural language models (LMs) represent facts about the world described by text. Sometimes these facts derive from training data (in most LMs, a representation of the word "banana" encodes the fact that bananas are fruits). Sometimes facts derive from input text itself (a representation of the sentence "I poured out the bottle" encodes the fact that the bottle became empty). We describe REMEDI, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system. REMEDI encodings can be used as knowledge editors: when added to LM hidden representations, they modify downstream generation to be consistent with new facts. REMEDI encodings may also be used as probes: when compared to LM representations, they reveal which properties LMs already attribute to mentioned entities, in some cases making it possible to predict when LMs will generate outputs that conflict with background knowledge or input text. REMEDI thus links work on probing, prompting, and LM editing, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs.																																	2024-08-22	PPRN:53757881		
J	Cheng, Xuxin; Li, Jialong; Yang, Shiqi; Yang, Ge; Wang, Xiaolong				Yang, Shiqi/JFJ-2024-2023						Open-TeleVision: Teleoperation with Immersive Active Visual Feedback								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2407.01512v1| 1;2024-07-01;	arXiv:2407.01512			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Teleoperation serves as a powerful method for collecting on-robot data essential for robot learning from demonstrations. The intuitiveness and ease of use of the teleoperation system are crucial for ensuring high-quality, diverse, and scalable data. To achieve this, we propose an immersive teleoperation system Open-TeleVision that allows operators to actively perceive the robot's surroundings in a stereoscopic manner. Additionally, the system mirrors the operator's arm and hand movements on the robot, creating an immersive experience as if the operator's mind is transmitted to a robot embodiment. We validate the effectiveness of our system by collecting data and training imitation learning policies on four long-horizon, precise tasks (Can Sorting, Can Insertion, Folding, and Unloading) for 2 different humanoid robots and deploy them in the real world. 																																	2024-11-17	PPRN:90657037		
J	Waleffe, Roger; Byeon, Wonmin; Riach, Duncan; Norick, Brandon; Korthikanti, Vijay; Dao, Tri; Gu, Albert; Hatamizadeh, Ali; Singh, Sudhakar; Narayanan, Deepak; Kulshreshtha, Garvit; Singh, Vartika; Casper, Jared; Kautz, Jan; Shoeybi, Mohammad; Catanzaro, Bryan										An Empirical Study of Mamba-based Language Models								Arxiv											1	1;2024-06-12;https://www.arxiv.org/abs/2406.07887v1	arXiv:2406.07887			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 12 2024	2024	Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.																																	2024-07-04	PPRN:89288175		
J	Liu, Yuanxin; Li, Shicheng; Liu, Yi; Wang, Yuxiang; Ren, Shuhuai; Li, Lei; Chen, Sishuo; Sun, Xu; Hou, Lu				yuxiang, wang/GVU-3250-2022; Ren, Shuhuai/KDO-1734-2024; Liu, Yuanxin/OXB-7414-2025; Li, Lei/LMN-0940-2024						TempCompass: Do Video LLMs Really Understand Videos?								Arxiv											3	3;2024-06-03;https://www.arxiv.org/abs/2403.00476v3| 2;2024-03-17;https://www.arxiv.org/abs/2403.00476v2| 1;2024-03-01;https://www.arxiv.org/abs/2403.00476v1	arXiv:2403.00476			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	Recently, there is a surge in interest surrounding video large language models (Video LLMs). However, existing benchmarks fail to provide a comprehensive feedback on the temporal perception ability of Video LLMs. On the one hand, most of them are unable to distinguish between different temporal aspects (e.g., speed, direction) and thus cannot reflect the nuanced performance on these specific aspects. On the other hand, they are limited in the diversity of task formats (e.g., only multi-choice QA), which hinders the understanding of how temporal perception performance may vary across different types of tasks. Motivated by these two problems, we propose the TempCompass benchmark, which introduces a diversity of temporal aspects and task formats. To collect high-quality test data, we devise two novel strategies: (1) In video collection, we construct conflicting videos that share the same static content but differ in a specific temporal aspect, which prevents Video LLMs from leveraging single-frame bias or language priors. (2) To collect the task instructions, we propose a paradigm where humans first annotate meta-information for a video and then an LLM generates the instruction. We also design an LLM-based approach to automatically and accurately evaluate the responses from Video LLMs. Based on TempCompass, we comprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs, and reveal the discerning fact that these models exhibit notably poor temporal perception ability. 																																	2024-06-22	PPRN:87996225		
J	Bozorgasl, Zavareh; Chen, Hao				Bozorgasl, Zavareh/AGJ-5309-2022						Wav-KAN: Wavelet Kolmogorov-Arnold Networks								Arxiv											1	1;2024-05-27;https://www.arxiv.org/abs/2405.12832v2	arXiv:2405.12832			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 27 2024	2024	In this paper, we introduce Wav-KAN, an innovative neural network architecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN) framework to enhance interpretability and performance. Traditional multilayer perceptrons (MLPs) and even recent advancements like Spl-KAN face challenges related to interpretability, training speed, robustness, computational efficiency, and performance. Wav-KAN addresses these limitations by incorporating wavelet functions into the Kolmogorov-Arnold network structure, enabling the network to capture both high-frequency and low-frequency components of the input data efficiently. Wavelet-based approximations employ orthogonal or semi-orthogonal basis and maintain a balance between accurately representing the underlying data structure and avoiding overfitting to the noise. While continuous wavelet transform (CWT) has a lot of potentials, we also employed discrete wavelet transform (DWT) for multiresolution analysis, which obviated the need for recalculation of the previous steps in finding the details. Analogous to how water conforms to the shape of its container, Wav-KAN adapts to the data structure, resulting in enhanced accuracy, faster training speeds, and increased robustness compared to Spl-KAN and MLPs. Our results highlight the potential of Wav-KAN as a powerful tool for developing interpretable and high-performance neural networks, with applications spanning various fields. This work sets the stage for further exploration and implementation of Wav-KAN in frameworks such as PyTorch and TensorFlow, aiming to make wavelets in KAN as widespread as activation functions like ReLU and sigmoid in universal approximation theory (UAT). The codes to replicate the simulations are available at https://github.com/zavareh1/Wav-KAN.																																	2024-06-11	PPRN:89075241		
J	Paulus, Anselm; Zharmagambetov, Arman; Guo, Chuan; Amos, Brandon; Tian, Yuandong				Amos, Brandon/C-6312-2015						AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs								Arxiv											1	1;2024-04-21;https://www.arxiv.org/abs/2404.16873v1	arXiv:2404.16873			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 21 2024	2024	While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM , or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter , to generate human-readable adversarial prompts in seconds, ∼ 800 × faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM . This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter , LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.																																	2024-06-05	PPRN:88965426		
J	Zhou, Yiyang; Cui, Chenhang; Rafailov, Rafael; Finn, Chelsea; Yao, Huaxiu				Yao, Huaxiu/V-3516-2019						Aligning Modalities in Vision Large Language Models via Preference Fine-tuning								Arxiv											1	1;2024-02-18;https://www.arxiv.org/abs/2402.11411v1	arXiv:2402.11411			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 18 2024	2024	Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization. In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches. 																																	2024-11-09	PPRN:87761547		
J	Yu, Le; Yu, Bowen; Yu, Haiyang; Huang, Fei; Li, Yongbin				Li, Yongbin/GWM-7528-2022; YU, Haiyang/KDN-0866-2024; Bowen, Yu/MFH-7462-2025						Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch								Arxiv											2	2;2024-02-04;https://www.arxiv.org/abs/2311.03099v2| 1;2023-11-06;https://www.arxiv.org/abs/2311.03099v1	arXiv:2311.03099			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 04 2024	2024	In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 − p) to approximate the original embeddings. Then, we use DARE as a versatile plug -and -play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder -based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. For instance, the amalgamation of WizardLM and WizardMath significantly enhances the GSM8K zero -shot accuracy of WizardLM from 2.2 to 66.3, retaining the instruction-following proficiency while surpassing WizardMath’s 64.2 performance. Our merged LM also ranks first among models with 7 billion parameters on the Open LLM Leaderboard.																																	2024-05-25	PPRN:86054751		
J	Zhang, Xin; Zhang, Dong; Li, Shimin; Zhou, Yaqian; Qiu, Xipeng										SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models								Arxiv											2	2;2024-01-23;https://www.arxiv.org/abs/2308.16692v2| 1;2023-08-31;https://www.arxiv.org/abs/2308.16692v1	arXiv:2308.16692			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jan 23 2024	2024	Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks.																																	2024-05-25	PPRN:84617493		
J	Kopiczko, Dawid J.; Blankevoort, Tijmen; Asano, Yuki M.				Asano, Yuki/AAQ-1283-2021						VeRA: Vector-based Random Matrix Adaptation								Arxiv											2	2;2024-01-16;https://www.arxiv.org/abs/2310.11454v2| 1;2023-10-17;https://www.arxiv.org/abs/2310.11454v1	arXiv:2310.11454			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 16 2024	2024	Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.																																	2024-02-02	PPRN:86471988		
J	Qin, Yiwei; Li, Xuefeng; Zou, Haoyang; Liu, Yixiu; Xia, Shijie; Huang, Zhen; Ye, Yixin; Yuan, Weizhe; Liu, Hector; Li, Yuanzhi; Liu, Pengfei				liu, yixiu/HGC-6110-2022; Liu, Pengfei/JUV-0307-2023						O1 Replication Journey: A Strategic Progress Report -- Part 1								Arxiv											1	1;2024-10-08;https://www.arxiv.org/abs/2410.18982v1	arXiv:2410.18982			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 08 2024	2024	This paper introduces a pioneering approach to artificial intelligence research, embodied in our O1 Replication Journey. In response to the announcement of OpenAI's groundbreaking O1 model, we embark on a transparent, real-time exploration to replicate its capabilities while reimagining the process of conducting and communicating AI research. Our methodology addresses critical challenges in modern AI research, including the insularity of prolonged team-based projects, delayed information sharing, and the lack of recognition for diverse contributions. By providing comprehensive, real-time documentation of our replication efforts, including both successes and failures, we aim to foster open science, accelerate collective advancement, and lay the groundwork for AI-driven scientific discovery. Our research progress report diverges significantly from traditional research papers, offering continuous updates, full process transparency, and active community engagement throughout the research journey. Technologically, we proposed the journey learning paradigm, which encourages models to learn not just shortcuts, but the complete exploration process, including trial and error, reflection, and backtracking. With only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded. [GRAPHICS]																																	2024-11-30	PPRN:118868223		
J	Zhang, Junyi; Herrmann, Charles; Hur, Junhwa; Jampani, Varun; Darrell, Trevor; Cole, Forrester; Sun, Deqing; Yang, Ming-Hsuan				Sun, Deqing/KLD-7402-2024; Zhang, Junyi/IRY-8601-2023; Jain, Varun/HHN-1250-2022; Yang, Ming-Hsuan/T-9533-2019						MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion								Arxiv											1	1;2024-10-04;https://www.arxiv.org/abs/2410.03825v1	arXiv:2410.03825			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Mo tion DU St3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUSt3R’s representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction. Interactive 4D results, source code, and trained models will be available at: https://monst3r-project.github.io/. .																																	2024-10-27	PPRN:104377161		
J	Yan, Yunzhi; Lin, Haotong; Zhou, Chenxu; Wang, Weijie; Sun, Haiyang; Zhan, Kun; Lang, Xianpeng; Zhou, Xiaowei; Peng, Sida				zhou, chenxu/OQK-3077-2025; Sun, Licai/JZD-9768-2024; Wang, Weijie/IST-4346-2023						Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting								Arxiv											3	3;2024-08-18;https://www.arxiv.org/abs/2401.01339v3| 2;2024-07-16;https://www.arxiv.org/abs/2401.01339v2| 1;2024-01-02;https://www.arxiv.org/abs/2401.01339v1	arXiv:2401.01339			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 18 2024	2024	This paper aims to tackle the problem of modeling dynamic urban streets for autonomous driving scenes. Recent methods extend NeRF by incorporating tracked vehicle poses to animate vehicles, enabling photo-realistic view synthesis of dynamic urban street scenes. However, significant limitations are their slow training and rendering speed. We introduce Street Gaussians, a new explicit scene representation that tackles these limitations. Specifically, the dynamic urban scene is represented as a set of point clouds equipped with semantic logits and 3D Gaussians, each associated with either a foreground vehicle or the background. To model the dynamics of foreground object vehicles, each object point cloud is optimized with optimizable tracked poses, along with a 4D spherical harmonics model for the dynamic appearance. The explicit representation allows easy composition of object vehicles and background, which in turn allows for scene editing operations and rendering at 135 FPS (1066 * 1600 resolution) within half an hour of training. The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets. Experiments show that the proposed method consistently outperforms state-of-the-art methods across all datasets. The code will be released to ensure reproducibility.																																	2024-08-30	PPRN:86916640		
J	Cao, Yihan; Kang, Yanbin; Wang, Chi; Sun, Lichao				Cao, Yihan/AAA-8456-2021						Instruction Mining: Instruction Data Selection for Tuning Large Language Models								Arxiv											3	3;2024-07-26;https://www.arxiv.org/abs/2307.06290v3| 2;2023-10-27;https://www.arxiv.org/abs/2307.06290v2| 1;2023-07-12;https://www.arxiv.org/abs/2307.06290v1	arXiv:2307.06290			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 26 2024	2024	Large language models (LLMs) are initially pretrained for broad capabilities and then finetuned with instruction-following datasets to improve their performance in interacting with humans. Despite advances in finetuning, a standardized guideline for selecting high-quality datasets to optimize this process remains elusive. In this paper, we first propose InstructMining, an innovative method designed for automatically selecting premium instruction-following data for finetuning LLMs. Specifically, InstructMining utilizes natural language indicators as a measure of data quality, applying them to evaluate unseen datasets. During experimentation, we discover that double descent phenomenon exists in large language model finetuning. Based on this observation, we further leverage BlendSearch to help find the best subset among the entire dataset (i.e., 2,532 out of 100,000). Experiment results show that InstructMining-7B achieves state-of-the-art performance on two of the most popular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.																																	2024-08-04	PPRN:73885203		
J	Wang, Zhilin; Dong, Yi; Delalleau, Olivier; Zeng, Jiaqi; Shen, Gerald; Egert, Daniel; Zhang, Jimmy J.; Sreedhar, Makesh Narsimhan; Kuchaiev, Oleksii										HelpSteer2: Open-source dataset for training top-performing reward models								Arxiv											1	1;2024-06-12;https://www.arxiv.org/abs/2406.08673v1	arXiv:2406.08673			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 12 2024	2024	High -quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high -quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HHRLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC -BY -4.0). Using a powerful internal base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench’s primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. In particular, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi -attribute score predicted by our reward models. HelpSteer2 is available at 																																	2024-07-10	PPRN:89302606		
J	Zhang, Jintian; Xu, Xin; Zhang, Ningyu; Liu, Ruibo; Hooi, Bryan; Deng, Shumin				Hooi, Bryan/AAU-5707-2020						Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View								Arxiv											3	3;2024-05-27;https://www.arxiv.org/abs/2310.02124v3| 2;2024-02-26;https://www.arxiv.org/abs/2310.02124v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02124v1	arXiv:2310.02124			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	May 27 2024	2024	As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi -agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi -agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top -tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, 1 , hoping to catalyze further research in this promising avenue.																																	2024-06-11	PPRN:85376870		
J	Li, Kunchang; Wang, Yali; He, Yinan; Li, Yizhuo; Wang, Yi; Liu, Yi; Wang, Zun; Xu, Jilan; Chen, Guo; Luo, Ping; Wang, Limin; Qiao, Yu				Li, Kunchang/KFA-4043-2024; Li, Yizhuo/AAL-5705-2021; Wang, Limin/AAE-3419-2019; Qiao, Yu/ABD-5787-2021; pluo/GPG-2707-2022						MVBench: A Comprehensive Multi-modal Video Understanding Benchmark								Arxiv											4	4;2024-05-23;https://www.arxiv.org/abs/2311.17005v4| 3;2024-01-21;https://www.arxiv.org/abs/2311.17005v3| 2;2023-12-03;https://www.arxiv.org/abs/2311.17005v2| 1;2023-11-28;https://www.arxiv.org/abs/2311.17005v1	arXiv:2311.17005			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	With the rapid development of Multi -modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi -modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel staticto-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiplechoice QA to evaluate each task. On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs. Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi -modal training with diverse instruction-tuning data. The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench. All models and data are available at https : //github.com/OpenGVLab/Ask-Anything .																																	2024-06-05	PPRN:86310371		
J	Xin, Huajian; Guo, Daya; Shao, Zhihong; Ren, Zhizhou; Zhu, Qihao; Liu, Bo; Ruan, Chong; Li, Wenda; Liang, Xiaodan				Li, Xiaofeng/LIC-9574-2024; Zhu, Qihao/JWO-8071-2024; 任, 治洲/OEN-2892-2025; Guo, Daya/HPG-8192-2023						DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14333v1	arXiv:2405.14333			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After finetuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.																																	2024-06-05	PPRN:88990092		
J	Woo, Gerald; Liu, Chenghao; Kumar, Akshat; Xiong, Caiming; Savarese, Silvio; Sahoo, Doyen				Liu, Chenghao/M-1202-2014						Unified Training of Universal Time Series Forecasting Transformers								Arxiv											2	2;2024-05-22;https://www.arxiv.org/abs/2402.02592v2| 1;2024-02-04;https://www.arxiv.org/abs/2402.02592v1	arXiv:2402.02592			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 22 2024	2024	Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. 																																	2024-06-04	PPRN:87518105		
J	Yuan, Zhihang; Shang, Yuzhang; Zhou, Yang; Dong, Zhen; Zhou, Zhe; Xue, Chenhao; Wu, Bingzhe; Li, Zhikai; Gu, Qingyi; Lee, Yong Jae; Yan, Yan; Chen, Beidi; Sun, Guangyu; Keutzer, Kurt				Gu, Qingyi/J-6290-2016; Shang, Yuzhang/HTO-5198-2023; Sun, Guangyu/GXF-4043-2022; 吴, 秉哲/KZV-0893-2024; Yuan, Zhihang/HDN-8259-2022; Dong, Zhen/AEY-4046-2022						LLM Inference Unveiled: Survey and Roofline Model Insights								Arxiv											5	5;2024-05-01;https://www.arxiv.org/abs/2402.16363v6| 4;2024-03-15;https://www.arxiv.org/abs/2402.16363v5| 3;2024-03-11;https://www.arxiv.org/abs/2402.16363v4| 2;2024-02-29;https://www.arxiv.org/abs/2402.16363v3| 1;2024-02-26;https://www.arxiv.org/abs/2402.16363v1	arXiv:2402.16363			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn’t been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on Roofline model for systematic analysis of LLM inference techniques. This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory -bound, how much memory and computation they need, and how to choose the right hardware. We systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as model compression (e.g., quantization), algorithm improvements (e.g., speculative decoding), and both system and hardware -level enhancements (e.g., operator fusion). Our survey stands out by analyzing these methods with Roofline model, helping us understand their impact on memory access and computation. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment. The analyze tool, LLM-Viewer, is open -sourced.																																	2024-05-19	PPRN:87889471		
J	Chen, Canyu; Shu, Kai				Chen, Canyu/KFT-0519-2024						Can LLM-Generated Misinformation Be Detected?								Arxiv											5	5;2024-04-23;https://www.arxiv.org/abs/2309.13788v5| 4;2024-04-15;https://www.arxiv.org/abs/2309.13788v4| 3;2024-03-16;https://www.arxiv.org/abs/2309.13788v3| 2;2023-12-12;https://www.arxiv.org/abs/2309.13788v2| 1;2023-09-25;https://www.arxiv.org/abs/2309.13788v1	arXiv:2309.13788			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 23 2024	2024	The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.																																	2024-05-04	PPRN:85196861		
J	Singh, Chandan; Inala, Jeevana Priya; Galley, Michel; Caruana, Rich; Gao, Jianfeng				Gao, Jianfeng/AAP-8200-2021						Rethinking Interpretability in the Era of Large Language Models								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2402.01761v1	arXiv:2402.01761			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 30 2024	2024	Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.																																	2024-05-25	PPRN:87522673		
J	Wang, Wen; Jiang, Yan; Xie, Kangyang; Liu, Zide; Chen, Hao; Cao, Yue; Wang, Xinlong; Shen, Chunhua				Wang, Xinlong/AFI-8800-2022						Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models								Arxiv											2	2;2024-01-04;https://www.arxiv.org/abs/2303.17599v3| 1;2023-03-30;https://www.arxiv.org/abs/2303.17599v1	arXiv:2303.17599			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 04 2024	2024	Large-scale text-to-image diffusion models achieve unprecedented success in image generation and editing. However, how to extend such success to video editing is unclear. Recent initial attempts at video editing require significant text-to-video data and computation resources for training, which is often not accessible. In this work, we propose vid2vid-zero, a simple yet effective method for zero-shot video editing. Our vid2vid-zero leverages off-the-shelf image diffusion models, and doesn't require training on any video. At the core of our method is a null-text inversion module for text-to-video alignment, a cross-frame modeling module for temporal consistency, and a spatial regularization module for fidelity to the original video. Without any training, we leverage the dynamic nature of the attention mechanism to enable bi-directional temporal modeling at test time. Experiments and analyses show promising results in editing attributes, subjects, places, etc., in real-world videos. 																																	2024-01-13	PPRN:52098357		
J	Chen, Junying; Cai, Zhenyang; Ji, Ke; Wang, Xidong; Liu, Wanlong; Wang, Rongsheng; Hou, Jianye; Wang, Benyou				Wang, Xidong/IZD-5718-2023; JI, Ke/LHA-0772-2024; Wang, Benyou/Y-5146-2019; Wang, Rongsheng/AGM-5221-2022						HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs								Arxiv											1	1;2024-12-25;https://www.arxiv.org/abs/2412.18925v1	arXiv:2412.18925			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 25 2024	2024	The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.																																	2025-01-24	PPRN:120254755		
J	Li, Zhang; Yang, Biao; Liu, Qiang; Ma, Zhiyin; Zhang, Shuo; Yang, Jingxu; Sun, Yabo; Liu, Yuliang; Bai, Xiang										Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models								Arxiv											3	3;2024-08-26;https://www.arxiv.org/abs/2311.06607v4| 2;2023-11-24;https://www.arxiv.org/abs/2311.06607v2| 1;2023-11-11;https://www.arxiv.org/abs/2311.06607v1	arXiv:2311.06607			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Aug 26 2024	2024	Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448 × 448) used in the original training of the welltrained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344 × 896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative results validate the effectiveness of our designs. Additionally, experiments on 18 datasets further demonstrate that Monkey surpasses existing LMMs in many tasks like Image Captioning and various Visual Question Answering formats. Specially, in qualitative tests focused on dense text question answering, Monkey has exhibited encouraging results compared with GPT4V. 																																	2024-09-06	PPRN:86137574		
J	Sauer, Axel; Boesel, Frederic; Dockhorn, Tim; Blattmann, Andreas; Esser, Patrick; Rombach, Robin				Esser, Patrick/AAD-7144-2019						Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.12015v1	arXiv:2403.12015			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed. Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a novel distillation approach overcoming the limitations of ADD. In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent diffusion models. This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art text-to-image generators using only four unguided sampling steps. Moreover, we systematically investigate its scaling behavior and demonstrate LADD's effectiveness in various applications such as image editing and inpainting.																																	2024-04-11	PPRN:88198135		
J	Liu, Hong; Li, Zhiyuan; Hall, David; Liang, Percy; Ma, Tengyu				li, zhiyuan/ACF-5587-2022; Ma, Tengyu/D-9086-2017						Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training								Arxiv											4	4;2024-03-05;https://www.arxiv.org/abs/2305.14342v4| 3;2023-10-17;https://www.arxiv.org/abs/2305.14342v3| 2;2023-10-09;https://www.arxiv.org/abs/2305.14342v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.14342v1	arXiv:2305.14342			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 05 2024	2024	Given the massive cost of language model pre -training, a non -trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second -order (Hessian -based) optimizers often incur too much per -step overhead. In this paper, we propose Sophia, Second -order Clipped Stochastic Optimization, a simple scalable second -order optimizer that uses a light -weight estimate of the diagonal Hessian as the preconditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element -wise clipping. The clipping controls the worst -case update size and tames the negative impact of non -convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per -step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall -clock time, achieving the same perplexity with 50% fewer steps, less total compute, and reduced wall -clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.																																	2024-04-03	PPRN:71606997		
J	Rasul, Kashif; Ashok, Arjun; Williams, Andrew Robert; Ghonia, Hena; Bhagwatkar, Rishika; Khorasani, Arian; Bayazi, Mohammad Javad Darvishi; Adamopoulos, George; Riachi, Roland; Hassen, Nadhir; Bilos, Marin; Garg, Sahil; Schneider, Anderson; Chapados, Nicolas; Drouin, Alexandre; Zantedeschi, Valentina; Nevmyvaka, Yuriy; Rish, Irina				Bayazi, Javad/LSI-6445-2024						Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting								Arxiv											3	3;2024-02-08;https://www.arxiv.org/abs/2310.08278v3| 2;2023-11-20;https://www.arxiv.org/abs/2310.08278v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08278v1	arXiv:2310.08278			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 08 2024	2024	Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero -shot and few -shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag -Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder -only transformer architecture that uses lags as covariates. Lag -Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero -shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag -Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag -Llama serves as a strong contender to the current state -of -art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.																																	2024-05-25	PPRN:85604905		
J	Tang, Yixuan; Yang, Yi				Tang, Yixuan/GQG-9963-2022						MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries								Arxiv											1	1;2024-01-27;https://www.arxiv.org/abs/2401.15391v1	arXiv:2401.15391			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jan 27 2024	2024	Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.																																	2024-02-15	PPRN:87385236		
J	Liu, Xin; Zhu, Yichen; Gu, Jindong; Lan, Yunshi; Yang, Chao; Qiao, Yu				Liu, Xin/MCX-7244-2025; Qiao, Yu/ABD-5787-2021						MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models								Arxiv											3	3;2024-06-19;https://www.arxiv.org/abs/2311.17600v5| 2;2024-06-18;https://www.arxiv.org/abs/2311.17600v4| 1;2024-03-12;https://www.arxiv.org/abs/2311.17600v2	arXiv:2311.17600			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 18 2024	2024	The security concerns surrounding Large Language Models (LLMs) have been extensively explored, yet the safety of Multimodal Large Language Models (MLLMs) remains understudied. In this paper, we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images, as if the text query itself were malicious. To address this, we introduce MM-SafetyBench, a comprehensive framework designed for conducting safety-critical evaluations of MLLMs against such image-based manipulations. We have compiled a dataset comprising 13 scenarios, resulting in a total of 5,040 text-image pairs. Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned. In response, we propose a straightforward yet effective prompting strategy to enhance the resilience of MLLMs against these types of attacks. Our work underscores the need for a concerted effort to strengthen and enhance the safety measures of open-source MLLMs against potential malicious exploits. The resource is available at https://github.com/isXinLiu/MM-SafetyBench																																	2025-08-07	PPRN:88117825		
J	Wan, Zhongwei; Wang, Xin; Liu, Che; Alam, Samiul; Zheng, Yu; Liu, Jiachen; Qu, Zhongnan; Yan, Shen; Zhu, Yi; Zhang, Quanlu; Chowdhury, Mosharaf; Zhang, Mi				Alam, Samiul/HHS-2157-2022; Wang, Xin/MXL-4068-2025; Wan, Zhongwei/JDM-4369-2023						Efficient Large Language Models: A Survey								Arxiv											4	4;2024-05-23;https://www.arxiv.org/abs/2312.03863v4| 3;2024-01-31;https://www.arxiv.org/abs/2312.03863v3| 2;2023-12-23;https://www.arxiv.org/abs/2312.03863v2| 1;2023-12-06;https://www.arxiv.org/abs/2312.03863v1	arXiv:2312.03863			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding and language generation, and thus have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain the repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient LLMs research and inspire them to contribute to this important and exciting field.																																	2024-06-05	PPRN:86436306		
J	Cao, Defu; Jia, Furong; Arik, Sercan O; Pfister, Tomas; Zheng, Yixiang; Ye, Wen; Liu, Yan				liu, yan/HGV-1365-2022; Jia, Furong/LTC-4831-2024; Cao, Defu/ABB-9546-2021						TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting								Arxiv											2	2;2024-04-02;https://www.arxiv.org/abs/2310.04948v3| 1;2023-10-12;https://www.arxiv.org/abs/2310.04948v2	arXiv:2310.04948			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.																																	2024-04-18	PPRN:85604921		
J	Yugay, Vladimir; Li, Yue; Gevers, Theo; Oswald, Martin R.				Oswald, Martin/AAM-2779-2021						Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting								Arxiv											1	1;2024-03-22;https://www.arxiv.org/abs/2312.10070v2	arXiv:2312.10070			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 22 2024	2024	We present a dense simultaneous localization and mapping (SLAM) method that uses 3D Gaussians as a scene representation. Our approach enables interactive-time reconstruction and photo-realistic rendering from real-world single-camera RGBD videos. To this end, we propose a novel effective strategy for seeding new Gaussians for newly explored areas and their effective online optimization that is independent of the scene size and thus scalable to larger scenes. This is achieved by organizing the scene into sub-maps which are independently optimized and do not need to be kept in memory. We further accomplish frame-to-model camera tracking by minimizing photometric and geometric losses between the input and rendered frames. The Gaussian representation allows for high-quality photo-realistic real-time rendering of real-world scenes. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance in mapping, tracking, and rendering compared to existing neural dense SLAM methods.																																	2025-08-07	PPRN:123160740		
J	Li, Yingji; Du, Mengnan; Song, Rui; Wang, Xin; Wang, Ying				wang, xin/AAI-5618-2020; Du, Mengnan/MXL-9283-2025						A Survey on Fairness in Large Language Models								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2308.10149v2| 1;2023-08-20;https://www.arxiv.org/abs/2308.10149v1	arXiv:2308.10149			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.																																	2024-03-20	PPRN:81931387		
J	Jin, Hongye; Han, Xiaotian; Yang, Jingfeng; Jiang, Zhimeng; Liu, Zirui; Chang, Chia-Yuan; Chen, Huiyuan; Hu, Xia				Yang, Jingfeng/HLP-5419-2023; Jin, Hongye/HKO-3543-2023; Jiang, Zhimeng/ACT-5432-2022						LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning								Arxiv											2	2;2024-02-03;https://www.arxiv.org/abs/2401.01325v2| 1;2024-01-02;https://www.arxiv.org/abs/2401.01325v1	arXiv:2401.01325			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 03 2024	2024	It is well known that LLMs cannot generalize well to long contexts whose lengths are larger than the training sequence length. This poses challenges when employing LLMs for processing long input sequences during inference. In this work, we argue that LLMs themselves have inherent capabilities to handle long contexts without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range. The two-level attentions are computed based on the original model's self-attention mechanism during inference. With minor code modification, our SelfExtend can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments on multiple benchmarks and the results show that our SelfExtend can effectively extend existing LLMs' context window length. 																																	2024-02-21	PPRN:86915080		
J	Wu, Haoning; Zhang, Zicheng; Zhang, Erli; Chen, Chaofeng; Liao, Liang; Wang, Annan; Li, Chunyi; Sun, Wenxiu; Yan, Qiong; Zhai, Guangtao; Lin, Weisi				Chen, Chaofeng/LMN-2902-2024; Zhai, Guangtao/G-5258-2013; li, chunyi/KIG-6117-2024; Lin, Weisi/A-8011-2012						Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision								Arxiv											3	3;2024-01-01;https://www.arxiv.org/abs/2309.14181v3| 2;2023-09-28;https://www.arxiv.org/abs/2309.14181v2| 1;2023-09-25;https://www.arxiv.org/abs/2309.14181v1	arXiv:2309.14181			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 01 2024	2024	The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. 																																	2024-01-06	PPRN:85196879		
J	Han, Seungju; Rao, Kavel; Ettinger, Allyson; Jiang, Liwei; Lin, Bill Yuchen; Lambert, Nathan; Choi, Yejin; Dziri, Nouha				Jiang, Liwei/IYK-0150-2023						WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs								Arxiv											3	3;2024-12-09;https://www.arxiv.org/abs/2406.18495v3| 2;2024-07-09;https://www.arxiv.org/abs/2406.18495v2| 1;2024-06-26;https://www.arxiv.org/abs/2406.18495v1	arXiv:2406.18495			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 09 2024	2024	We introduce WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. To address these challenges, we construct WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios. Through extensive evaluations on WildGuardTest and ten existing public benchmarks, we show that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8% to 2.4%.																																	2025-01-19	PPRN:89911261		
J	Xie, Zhifei; Wu, Changqiao				XIE, Zhifei/KIG-2667-2024						Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming								Arxiv											3	3;2024-11-05;https://www.arxiv.org/abs/2408.16725v3| 2;2024-08-30;https://www.arxiv.org/abs/2408.16725v2| 1;2024-08-29;https://www.arxiv.org/abs/2408.16725v1	arXiv:2408.16725			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 05 2024	2024	Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model’s language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method "Any Model Can Talk". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.																																	2024-12-09	PPRN:91633170		
J	Li, Jiaqi; Wang, Mengmeng; Zheng, Zilong; Zhang, Muhan				Li, Jiaqi/HHN-8236-2022						LooGLE: Can Long-Context Language Models Understand Long Contexts?								Arxiv											2	2;2024-09-06;https://www.arxiv.org/abs/2311.04939v2| 1;2023-11-08;https://www.arxiv.org/abs/2311.04939v1	arXiv:2311.04939			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 06 2024	2024	Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards "true long-context understanding". [GRAPHICS]																																	2024-09-23	PPRN:86109852		
J	Zhang, Kaichen; Li, Bo; Zhang, Peiyuan; Pu, Fanyi; Cahyono, Joshua Adrian; Hu, Kairui; Liu, Shuai; Zhang, Yuanhan; Yang, Jingkang; Li, Chunyuan; Liu, Ziwei				Zhang, Peiyuan/OOK-8916-2025; Liu, Ziwei/AAG-6939-2021; Yang, Jingkang/HJZ-3689-2023						LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models								Arxiv											1	1;2024-07-17;https://www.arxiv.org/abs/2407.12772v1	arXiv:2407.12772			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 17 2024	2024	The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs. We opensource our codebase and maintain leaderboard of LIVEBENCHat Github and LiveBench.																																	2024-07-25	PPRN:90867610		
J	Long, Lin; Wang, Rui; Xiao, Ruixuan; Zhao, Junbo; Ding, Xiao; Chen, Gang; Wang, Haobo				Xiang, Xiao/IQT-2849-2023; Wang, Haobo/JCF-1064-2023						On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey								Arxiv											1	1;2024-06-14;https://www.arxiv.org/abs/2406.15126v1	arXiv:2406.15126			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Within the evolving landscape of deep learning, the dilemma of data quantity and quality has been a long-standing problem. The recent advent of Large Language Models (LLMs) offers a data-centric solution to alleviate the limitations of real-world data with synthetic data generation. However, current investigations into this field lack a unified framework and mostly stay on the surface. Therefore, this paper provides an organization of relevant studies based on a generic workflow of synthetic data generation. By doing so, we highlight the gaps within existing research and outline prospective avenues for future study. This work aims to shepherd the academic and industrial communities towards deeper, more methodical inquiries into the capabilities and applications of LLMs-driven synthetic data generation.																																	2024-07-11	PPRN:89400554		
J	Li, Xuanlin; Hsu, Kyle; Gu, Jiayuan; Pertsch, Karl; Mees, Oier; Walke, Homer Rich; Fu, Chuyuan; Lunawat, Ishikaa; Sieh, Isabel; Kirmani, Sean; Levine, Sergey; Wu, Jiajun; Finn, Chelsea; Su, Hao; Vuong, Quan; Xiao, Ted				Wu, Jiajun/AFA-0504-2022; Gu, Jiayuan/AAH-2056-2020; XUANLIN, LI/AAO-6594-2021; Vuong, Quan-Hoang/F-2115-2010; Su, Hao/HHZ-1048-2022						Evaluating Real-World Robot Manipulation Policies in Simulation								Arxiv											1	1;2024-05-09;https://www.arxiv.org/abs/2405.05941v1	arXiv:2405.05941			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 09 2024	2024	The field of robotics has made significant advances towards generalist robot manipulation policies. However, real-world evaluation of such policies is not scalable and faces reproducibility challenges, which are likely to worsen as policies broaden the spectrum of tasks they can perform. We identify control and visual disparities between real and simulated environments as key challenges for reliable simulated evaluation and propose approaches for mitigating these gaps without needing to craft full-fidelity digital twins of real-world environments. We then employ these approaches to create SIMPLER, a collection of simulated environments for manipulation policy evaluation on common real robot setups. Through paired sim-and-real evaluations of manipulation policies, we demonstrate strong correlation between policy performance in SIMPLER environments and in the real world. Additionally, we find that SIMPLER evaluations accurately reflect real-world policy behavior modes such as sensitivity to various distribution shifts. We open-source all SIMPLER environments along with our workflow for creating new environments at https://simpler-env.github.io to facilitate research on general-purpose manipulation policies and simulated evaluation frameworks.																																	2024-06-04	PPRN:88975621		
J	Shi, Yujun; Xue, Chuhui; Liew, Jun Hao; Pan, Jiachun; Yan, Hanshu; Zhang, Wenqing; Tan, Vincent Y.F.; Bai, Song				Zhang, Wenqing/F-5168-2013; Bai, Song/HSF-8925-2023						DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing								Arxiv											3	3;2024-04-07;https://www.arxiv.org/abs/2306.14435v6| 2;2023-12-11;https://www.arxiv.org/abs/2306.14435v5| 1;2023-10-06;https://www.arxiv.org/abs/2306.14435v4	arXiv:2306.14435			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 07 2024	2024	Accurate and controllable image editing is a challenging task that has attracted significant attention recently. Notably, DragGAN is an interactive point-based image editing framework that achieves impressive editing results with pixel-level precision. However, due to its reliance on generative adversarial networks (GANs), its generality is limited by the capacity of pretrained GAN models. In this work, we extend this editing framework to diffusion models and propose a novel approach DragDiffusion. By harnessing large-scale pretrained diffusion models, we greatly enhance the applicability of interactive point-based editing on both real and diffusion-generated images. Our approach involves optimizing the diffusion latents to achieve precise spatial control. The supervision signal of this optimization process is from the diffusion model's UNet features, which are known to contain rich semantic and geometric information. Moreover, we introduce two additional techniques, namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity of the original image. Lastly, we present a challenging benchmark dataset called DragBench -- the first benchmark to evaluate the performance of interactive point-based image editing methods. Experiments across a wide range of challenging cases (e.g., images with multiple objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.																																	2024-04-21	PPRN:85570725		
J	Jeong, Soyeong; Baek, Jinheon; Cho, Sukmin; Hwang, Sung Ju; Park, Jong C.				Baek, Jinheon/HLX-1814-2023; Hwang, Sung/A-8817-2018						Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity								Arxiv											2	2;2024-03-28;https://www.arxiv.org/abs/2403.14403v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14403v1	arXiv:2403.14403			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Mar 28 2024	2024	Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches.																																	2024-04-14	PPRN:88259567		
J	Xu, Ruyi; Yao, Yuan; Guo, Zonghao; Cui, Junbo; Ni, Zanlin; Ge, Chunjiang; Chua, Tat-Seng; Liu, Zhiyuan; Sun, Maosong; Huang, Gao				xu, ruyi/MAI-5241-2025; Liu, Zhiyuan/I-2233-2014; Wang, Meng/AEZ-9059-2022; Ge, Chunjiang/LOS-5681-2024						LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.11703v1	arXiv:2403.11703			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native -resolution images into smaller variable -sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336×336 supports 6 times larger (i.e., 672×1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5).																																	2024-04-11	PPRN:88197543		
J	Niu, Zhenxing; Ren, Haodong; Gao, Xinbo; Hua, Gang; Jin, Rong				gao, xin/MSV-7043-2025; Ren, Haodong/JGE-4277-2023						Jailbreaking Attack against Multimodal Large Language Model								Arxiv											2	2;2024-02-04;https://www.arxiv.org/abs/2402.02309v1| 1;2024-02-04;https://www.arxiv.org/abs/2402.02309v1	arXiv:2402.02309			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 04 2024	2024	This paper focuses on jailbreaking attacks against multi -modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an image Jailbreaking Prompt (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data -universal property). Our approach exhibits strong modeltransferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUGOwl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state -of -the -art methods. The code is available here. Warning: some content generated by language models may be offensive to some readers.																																	2024-05-25	PPRN:87518076		
J	Zeni, Claudio; Pinsler, Robert; Zugner, Daniel; Fowler, Andrew; Horton, Matthew; Fu, Xiang; Shysheya, Sasha; Crabbe, Jonathan; Sun, Lixin; Smith, Jake; Nguyen, Bichlien; Schulz, Hannes; Lewis, Sarah; Huang, Chin-Wei; Lu, Ziheng; Zhou, Yichi; Yang, Han; Hao, Hongxia; Li, Jielan; Tomioka, Ryota; Xie, Tian				LU, Ziheng/N-4349-2018; Hao, Hongxia/C-7475-2012; Zeni, Claudio/JAO-3942-2023; Sun, Lixin/AAT-1932-2020						MatterGen: a generative model for inorganic materials design								Arxiv											2	2;2024-01-29;https://www.arxiv.org/abs/2312.03687v2| 1;2023-12-06;https://www.arxiv.org/abs/2312.03687v1	arXiv:2312.03687			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture [1–3]. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints [4–13]. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen’s capabilities represent a major advancement towards creating a universal generative model for materials design.																																	2024-05-25	PPRN:86419657		
J	Wang, Yiqi; Chen, Wentao; Han, Xiaotian; Lin, Xudong; Zhao, Haiteng; Liu, Yongfei; Bohan, Zhai; Yuan, Jianbo; You, Quanzeng; Yang, Hongxia				You, Quanzeng/X-9917-2019; lin, Xudong/ISA-0071-2023; Yang, Hongxia/OIS-1418-2025; Zhang, Mengqin/JXM-3676-2024						Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning								Arxiv											2	2;2024-01-18;https://www.arxiv.org/abs/2401.06805v2| 1;2024-01-10;https://www.arxiv.org/abs/2401.06805v1	arXiv:2401.06805			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 18 2024	2024	Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.																																	2024-05-25	PPRN:87186168		
J	Ren, Kerui; Jiang, Lihan; Lu, Tao; Yu, Mulin; Xu, Linning; Ni, Zhangkai; Dai, Bo				Ou, Jie/NTR-2561-2025; NI, Zhangkai/F-3232-2019; Lu, Tao/LPR-0417-2024						Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians								Arxiv											2	2;2024-10-17;https://www.arxiv.org/abs/2403.17898v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17898v1	arXiv:2403.17898			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 17 2024	2024	The recently proposed 3D Gaussian Splatting (3DGS) demonstrates superior rendering fidelity and efficiency compared to NeRF-based scene representations. However, it struggles in large-scale scenes due to the high number of Gaussian primitives, particularly in zoomed-out views, where all primitives are rendered regardless of their projected size. This often results in inefficient use of model capacity and difficulty capturing details at varying scales. To address this, we introduce Octree-GS, a Level-of-Detail (LOD) structured approach that dynamically selects appropriate levels from a set of multi-scale Gaussian primitives, ensuring consistent rendering performance. To adapt the design of LOD, we employ an innovative grow-and-prune strategy for densification and also propose a progressive training strategy to arrange Gaussians into appropriate LOD levels. Additionally, our LOD strategy generalizes to other Gaussianbased methods, such as 2D-GS and Scaffold-GS, reducing the number of primitives needed for rendering while maintaining scene reconstruction accuracy. Experiments on diverse datasets demonstrate that our method achieves real-time speeds, with even 10 × faster than state-of-the-art methods in large-scale scenes, without compromising visual quality. 																																	2024-11-13	PPRN:88295522		
J	Cheang, Chi-Lam; Chen, Guangzeng; Jing, Ya; Kong, Tao; Li, Hang; Li, Yifeng; Liu, Yuxiao; Wu, Hongtao; Xu, Jiafeng; Yang, Yichu; Zhang, Hanbo; Zhu, Minzhao				Wu, Hongtao/MTC-3443-2025; Zhang, Hanbo/LGZ-2975-2024; jing, ya/MIQ-9362-2025; Chen, Guangzeng/ABS-5078-2022						GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation								Arxiv											1	1;2024-10-08;https://www.arxiv.org/abs/2410.06158v1	arXiv:2410.06158			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 08 2024	2024	We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. 																																	2024-10-31	PPRN:105772869		
J	Ye, Jiayi; Wang, Yanbo; Huang, Yue; Chen, Dongping; Zhang, Qihui; Moniz, Nuno; Gao, Tian; Geyer, Werner; Huang, Chao; Chen, Pin-Yu; Chawla, Nitesh V; Zhang, Xiangliang				Wang, Yanbo/HFZ-8018-2022; Chen, Pin-Yu/AAA-1059-2020; ZHANG, QIHUI/OAJ-1060-2025; Gao, Tianxiang/LSL-5577-2024; Chen, Dongping/LSL-8606-2024; Moniz, Nuno/AAG-6161-2019						Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge								Arxiv											1	1;2024-10-04;https://www.arxiv.org/abs/2410.02736v2	arXiv:2410.02736			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework—CALM—which ALM —which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications.																																	2024-10-25	PPRN:103994722		
J	Lopez-Lira, Alejandro; Tang, Yuehua				Tang, Yuehua/O-1833-2018						Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models								Arxiv											3	3;2024-09-11;https://www.arxiv.org/abs/2304.07619v5| 2;2023-09-09;https://www.arxiv.org/abs/2304.07619v4| 1;2023-04-15;https://www.arxiv.org/abs/2304.07619v1	arXiv:2304.07619			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 11 2024	2024	We document the capability of large language models (LLMs) like ChatGPT to predict stock price movements using news headlines, even without direct financial training. ChatGPT scores significantly predict out-of-sample daily stock returns, subsuming traditional methods, and predictability is stronger among smaller stocks and following negative news. To explain these findings, we develop a theoretical model incorporating information capacity constraints, underreaction, limits-to-arbitrage, and LLMs. The model generates several key predictions, which we empirically test: (i) it establishes a critical threshold in AI capabilities necessary for profitable predictions, (ii) it demonstrates that only advanced LLMs can effectively interpret complex information, and (iii) it predicts that widespread LLM adoption can enhance market efficiency. Our results suggest that sophisticated return forecasting is an emerging capability of AI systems and that these technologies can alter information diffusion and decision-making processes in financial markets. Finally, we introduce an interpretability framework to evaluate LLMs' reasoning, contributing to AI transparency and economic decision-making.																																	2024-09-27	PPRN:63560381		
J	Zhang, Di; Huang, Xiaoshui; Zhou, Dongzhan; Li, Yuqiang; Ouyang, Wanli				Zhou, Dongzhan/IXN-4421-2023; Ouyang, Wanli/I-7135-2018; Huang, Xiaoshui/HPG-0735-2023						Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B: A Technical Report								Arxiv											2	2;2024-06-13;https://www.arxiv.org/abs/2406.07394v2| 1;2024-06-11;https://www.arxiv.org/abs/2406.07394v1	arXiv:2406.07394			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 13 2024	2024	This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.																																	2024-07-02	PPRN:89286221		
J	Campbell, Andrew; Yim, Jason; Barzilay, Regina; Rainforth, Tom; Jaakkola, Tommi										Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design								Arxiv											2	2;2024-06-05;https://www.arxiv.org/abs/2402.04997v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.04997v1	arXiv:2402.04997			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 05 2024	2024	Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure.																																	2024-06-22	PPRN:87567379		
J	Gloeckle, Fabian; Idrissi, Badr Youbi; Roziere, Baptiste; Lopez-Paz, David; Synnaeve, Gabriel										Better & Faster Large Language Models via Multi-token Prediction								Arxiv											1	1;2024-04-30;https://www.arxiv.org/abs/2404.19737v1	arXiv:2404.19737			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 30 2024	2024	Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.																																	2024-05-18	PPRN:88701379		
J	Lee, Jinhyuk; Dai, Zhuyun; Ren, Xiaoqi; Chen, Blair; Cer, Daniel; Cole, Jeremy R.; Hui, Kai; Boratko, Michael; Kapadia, Rajvi; Ding, Wen; Luan, Yi; Duddu, Sai Meher Karthik; Abrego, Gustavo Hernandez; Shi, Weiqiang; Gupta, Nithi; Kusupati, Aditya; Jain, Prateek; Jonnalagadda, Siddhartha Reddy; Chang, Ming-Wei; Naim, Iftekhar				ren, xiaoqi/LKL-5860-2024; Cer, Daniel/AAW-8335-2020; Luan, Yi/HLH-8516-2023; Kusupati, Aditya/AAB-3021-2019; Hui, Kai/AAM-2600-2020						Gecko: Versatile Text Embeddings Distilled from Large Language Models								Arxiv											1	1;2024-03-29;https://www.arxiv.org/abs/2403.20327v1	arXiv:2403.20327			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.																																	2024-04-17	PPRN:88343026		
J	Henschel, Roberto; Khachatryan, Levon; Hayrapetyan, Daniil; Poghosyan, Hayk; Tadevosyan, Vahram; Wang, Zhangyang; Navasardyan, Shant; Shi, Humphrey				Zhihua, Wang/AFO-5263-2022; Shi, Honghui/V-9259-2019						StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text								Arxiv											1	1;2024-03-21;https://www.arxiv.org/abs/2403.14773v1	arXiv:2403.14773			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that enables to apply a video enhancer autoregressively for infinitely long videos without inconsistencies between chunks. Experiments show that StreamingT2V generates high motion amount. In contrast, all competing image-to-video methods are prone to video stagnation when applied naively in an autoregressive manner. Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator that outperforms competitors with consistency and motion.																																	2024-04-13	PPRN:88268392		
J	Zhu, Junzhe; Zhuang, Peiye; Koyejo, Sanmi										HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance								Arxiv											3	3;2024-03-11;https://www.arxiv.org/abs/2305.18766v4| 2;2023-11-28;https://www.arxiv.org/abs/2305.18766v3| 1;2023-05-30;https://www.arxiv.org/abs/2305.18766v2	arXiv:2305.18766			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 11 2024	2024	The advancements in automatic text-to-3D generation have been remarkable. Most existing methods use pre-trained text-to-image diffusion models to optimize 3D representations like Neural Radiance Fields (NeRFs) via latent-space denoising score matching. Yet, these methods often result in artifacts and inconsistencies across different views due to their suboptimal optimization approaches and limited understanding of 3D geometry. Moreover, the inherent constraints of NeRFs in rendering crisp geometry and stable textures usually lead to a two-stage optimization to attain high-resolution details. This work proposes holistic sampling and smoothing approaches to achieve high-quality text-to-3D generation, all in a single-stage optimization. We compute denoising scores in the text-toimage diffusion model’s latent and image spaces. Instead of randomly sampling timesteps (also referred to as noise levels in denoising score matching), we introduce a novel timestep annealing approach that progressively reduces the sampled timestep throughout optimization. To generate high-quality renderings in a singlestage optimization, we propose regularization for the variance of z-coordinates along NeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel smoothing technique that refines importance sampling weights coarse-tofine, ensuring accurate and thorough sampling in high-density regions. Extensive experiments demonstrate the superiority of our method over previous approaches, enabling the generation of highly detailed and view-consistent 3D assets through a single-stage training process.																																	2024-04-08	PPRN:72779808		
J	Kim, Seungone; Shin, Jamin; Cho, Yejin; Jang, Joel; Longpre, Shayne; Lee, Hwaran; Yun, Sangdoo; Shin, Seongjin; Kim, Sungdong; Thorne, James; Seo, Minjoon				Lee, Hwaran/GVR-7438-2022						Prometheus: Inducing Fine-grained Evaluation Capability in Language Models								Arxiv											2	2;2024-03-09;https://www.arxiv.org/abs/2310.08491v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08491v1	arXiv:2310.08491			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 09 2024	2024	Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT4) as an evaluator for long -form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child -readability), using proprietary LLMs as an evaluator is unreliable due to the closed -source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS, a fully open -source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the FEEDBACK COLLECTION, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluator LLM that can assess any given long -form text based on customized score rubric provided by the user. Experimental results show that PROMETHEUS scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering PROMETHEUS’s capability as an evaluator LLM. Lastly, PROMETHEUS achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open -sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open -source our code, dataset, and model 1.																																	2024-04-08	PPRN:85602521		
J	Havrilla, Alex; Du, Yuqing; Raparthy, Sharath Chandra; Nalmpantis, Christoforos; Dwivedi-Yu, Jane; Zhuravinskyi, Maksym; Hambro, Eric; Sukhbaatar, Sainbayar; Raileanu, Roberta				Nalmpantis, Christoforos/AAS-2727-2020						Teaching Large Language Models to Reason with Reinforcement Learning								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.04642v1	arXiv:2403.04642			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 07 2024	2024	Reinforcement Learning from Human Feedback (textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.																																	2024-04-05	PPRN:88056249		
J	Toshniwal, Shubham; Moshkov, Ivan; Narenthiran, Sean; Gitman, Daria; Jia, Fei; Gitman, Igor										OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset								Arxiv											1	1;2024-11-03;https://www.arxiv.org/abs/2402.10176v2	arXiv:2402.10176			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 03 2024	2024	Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA [1] and MAmmoTH [2] are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on our proposed prompting novelty, the recent progress in open- source LLMs, and some brute-force scaling, we construct OpenMathInstruct-1, a high-quality math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. To support the open-source efforts, we have released our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.1																																	2024-12-16	PPRN:119091308		
J	Wang, Weihan; He, Zehai; Hong, Wenyi; Cheng, Yean; Zhang, Xiaohan; Qi, Ji; Ding, Ming; Gu, Xiaotao; Huang, Shiyu; Xu, Bin; Dong, Yuxiao; Tang, Jie				Huang, Shiyu/KUC-5797-2024; 航航, 张/KBC-0720-2024						LVBench: An Extreme Long Video Understanding Benchmark								Arxiv											2	2;2024-10-23;https://www.arxiv.org/abs/2406.08035v2| 1;2024-06-12;https://www.arxiv.org/abs/2406.08035v1	arXiv:2406.08035			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 23 2024	2024	Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension. 																																	2024-11-26	PPRN:89289551		
J	Gonen, Hila; Iyer, Srini; Blevins, Terra; Smith, Noah A.; Zettlemoyer, Luke				Gonen, Hila/AAS-7384-2021						Demystifying Prompts in Language Models via Perplexity Estimation								Arxiv											2	2;2024-09-12;https://www.arxiv.org/abs/2212.04037v2| 1;2022-12-08;https://www.arxiv.org/abs/2212.04037v1	arXiv:2212.04037			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 12 2024	2024	Language models can be prompted to perform a wide variety of tasks with zero- and few-shot incontext learning. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens. In this paper, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is predicted by the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt, the better it is able to perform the task, when considering reasonable prompts that are related to it. As part of our analysis, we also devise a method to automatically extend a small seed set of manually written prompts by paraphrasing with GPT3 and backtranslation. This larger set allows us to verify that perplexity is a strong predictor of the success of a prompt and we show that the lowest perplexity prompts are consistently effective.																																	2024-09-29	PPRN:25186707		
J	Liu, Dongyang; Zhang, Renrui; Qiu, Longtian; Huang, Siyuan; Lin, Weifeng; Zhao, Shitian; Geng, Shijie; Lin, Ziyi; Jin, Peng; Zhang, Kaipeng; Shao, Wenqi; Xu, Chao; He, Conghui; He, Junjun; Shao, Hao; Lu, Pan; Qiao, Yu; Li, Hongsheng; Gao, Peng				Lin, Weifeng/GPW-6246-2022; Jin, Peng/LFT-8054-2024; He, Conghui/AAZ-3323-2021; Zhang, Zhuosheng/AAF-4919-2020; Geng, Shijie/IZE-5093-2023; Li, Hongsheng/AES-5328-2022; Qiao, Yu/ABD-5787-2021						SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models								Arxiv											2	2;2024-06-26;https://www.arxiv.org/abs/2402.05935v2| 1;2024-02-08;https://www.arxiv.org/abs/2402.05935v1	arXiv:2402.05935			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 26 2024	2024	We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 																																	2024-07-15	PPRN:87572755		
J	Pawelczyk, Martin; Neel, Seth; Lakkaraju, Himabindu										In-Context Unlearning: Language Models as Few Shot Unlearners								Arxiv											3	3;2024-06-06;https://www.arxiv.org/abs/2310.07579v4| 2;2024-06-04;https://www.arxiv.org/abs/2310.07579v3| 1;2023-10-12;https://www.arxiv.org/abs/2310.07579v2	arXiv:2310.07579			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 06 2024	2024	Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the Right to be Forgotten . Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called “In-Context Unlearning.” This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.																																	2024-06-22	PPRN:85602366		
J	Gekhman, Zorik; Yona, Gal; Aharoni, Roee; Eyal, Matan; Feder, Amir; Reichart, Roi; Herzig, Jonathan										Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?								Arxiv											1	1;2024-05-13;https://www.arxiv.org/abs/2405.05904v2	arXiv:2405.05904			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 13 2024	2024	When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.																																	2024-06-08	PPRN:89027396		
J	Yang, Han; Hu, Chenxi; Zhou, Yichi; Liu, Xixian; Shi, Yu; Li, Jielan; Li, Guanzhi; Chen, Zekun; Chen, Shuizhou; Zeni, Claudio; Horton, Matthew; Pinsler, Robert; Fowler, Andrew; Zugner, Daniel; Xie, Tian; Smith, Jake; Sun, Lixin; Wang, Qian; Kong, Lingyu; Liu, Chang; Hao, Hongxia; Lu, Ziheng				Zeni, Claudio/JAO-3942-2023; Yang, Han/JAX-7857-2023; Chen, Zekun/HNB-7364-2023; Hao, Hongxia/C-7475-2012; Hu, Chenxi/AFO-5531-2022; LU, Ziheng/N-4349-2018; Sun, Lixin/AAT-1932-2020						MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures								Arxiv											1	1;2024-05-10;https://www.arxiv.org/abs/2405.04967v2	arXiv:2405.04967			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 10 2024	2024	Accurate and fast prediction of materials properties is central to the digital transformation of materials design. However, the vast design space and diverse operating conditions pose significant challenges for accurately modeling arbitrary material candidates and forecasting their properties. We present MatterSim, a deep learning model actively learned from large-scale first-principles computations, for efficient atomistic simulations at first-principles level and accurate prediction of broad material properties across the periodic table, spanning temperatures from 0 to 5000 K and pressures up to 1000 GPa. Out-of-the-box, the model serves as a machine learning force field, and shows remarkable capabilities not only in predicting ground-state material structures and energetics, but also in simulating their behavior under realistic temperatures and pressures, signifying an up to ten-fold enhancement in precision compared to the prior best-in-class. This enables MatterSim to compute materials' lattice dynamics, mechanical and thermodynamic properties, and beyond, to an accuracy comparable with first-principles methods. Specifically, MatterSim predicts Gibbs free energies for a wide range of inorganic solids with near-first-principles accuracy and achieves a 15 meV/atom resolution for temperatures up to 1000K compared with experiments. This opens an opportunity to predict experimental phase diagrams of materials at minimal computational cost. Moreover, MatterSim also serves as a platform for continuous learning and customization by integrating domain-specific data. The model can be fine-tuned for atomistic simulations at a desired level of theory or for direct structure-to-property predictions, achieving high data efficiency with a reduction in data requirements by up to 97%.																																	2024-06-06	PPRN:88999035		
J	Raposo, David; Ritter, Sam; Richards, Blake; Lillicrap, Timothy; Humphreys, Peter Conway; Santoro, Adam										Mixture-of-Depths: Dynamically allocating compute in transformer-based language models								Arxiv											1	1;2024-04-02;https://www.arxiv.org/abs/2404.02258v1	arXiv:2404.02258			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (k) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-k routing mechanism. Since k is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the k tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50% faster to step during post -training sampling.																																	2024-04-18	PPRN:88390319		
J	Xiong, Guangzhi; Jin, Qiao; Lu, Zhiyong; Zhang, Aidong				Lu, Zhiyong/H-3622-2016						Benchmarking Retrieval-Augmented Generation for Medicine								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/2402.13178v2| 1;2024-02-20;https://www.arxiv.org/abs/2402.13178v1	arXiv:2402.13178			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrievalaugmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval -Augmented Generation Evaluation (MIRAGE), a first -of -its -kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MEDRAG toolkit introduced in this work. Overall, MEDRAG improves the accuracy of six different LLMs by up to 18% over chain -of -thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log -linear scaling property and the “lostin -the -middle” effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.																																	2024-11-09	PPRN:87776641		
J	Liu, Sheng; Ye, Haotian; Xing, Lei; Zou, James										In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering								Arxiv											2	2;2024-02-13;https://www.arxiv.org/abs/2311.06668v3| 1;2023-11-11;https://www.arxiv.org/abs/2311.06668v1	arXiv:2311.06668			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 13 2024	2024	Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.																																	2024-05-25	PPRN:86133573		
J	Gidney, Craig; Shutty, Noah; Jones, Cody										Magic state cultivation: growing T states as cheap as CNOT gates								Arxiv											1	1;2024-09-26;https://www.arxiv.org/abs/2409.17595v1	arXiv:2409.17595			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 26 2024	2024	We refine ideas from [KLZ96; JBH16; CN20; Bom+24; GJ23; Gid+23; HIF24] to efficiently prepare good |T⟩ T⟩ states. We call our construction “magic state cultivation” because it gradually grows the size and reliability of one state. Cultivation fits inside a surface code patch and uses roughly the same number of physical gates as a lattice surgery CNOT gate of equivalent reliability. We estimate the infidelity of cultivation (from injection to idling at distance 15) using a mix of state vector simulation, stabilizer simulation, error enumeration, and Monte Carlo sampling. Compared to prior work, cultivation uses an order of magnitude fewer qubit·rounds · rounds to reach logical error rates as low as 2 · 10 − 9 when subjected to 10−3   uniform depolarizing circuit noise. Halving the circuit noise to 5 · 10−4 mproves the achievable logical error rate to 4 · 10−11.  Cultivation’s efficiency and strong response to improvements in physical noise suggest that further magic state distillation may never be needed in practice.																																	2024-10-08	PPRN:100649658		
J	He, Muyang; Liu, Yexin; Wu, Boya; Yuan, Jianhao; Wang, Yueze; Huang, Tiejun; Zhao, Bo				Zhao, Bo/AGZ-0290-2022						Efficient Multimodal Learning from Data-centric Perspective								Arxiv											3	3;2024-07-22;https://www.arxiv.org/abs/2402.11530v3| 2;2024-06-13;https://www.arxiv.org/abs/2402.11530v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11530v1	arXiv:2402.11530			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 22 2024	2024	Multimodal Large Language Models (MLLMs) have demonstrated notable capabilities in general visual understanding and reasoning tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably cause significant performance drops. In this paper, we demonstrate the possibility of training a smaller but better MLLM with high-quality training data. Specifically, we introduce Bunny, a family of lightweight MLLMs with flexible vision and language backbones for efficient multimodal learning from selected training data. Experiments show that our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple benchmarks. We expect that this work can provide the community with a clean and flexible open-source tool for further research and development. 																																	2024-07-28	PPRN:87762195		
J	Wang, Chen; Shi, Haochen; Wang, Weizhuo; Zhang, Ruohan; Fei-Fei, Li; Liu, C.Karen				Zhang, Ruohan/HPH-7886-2023; Wang, Weizhuo/GPX-1517-2022						DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation								Arxiv											1	1;2024-07-04;https://www.arxiv.org/abs/2403.07788v2	arXiv:2403.07788			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 04 2024	2024	Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the complexity of translating mocap data into effective robotic policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to seamlessly replicate human actions with robot hands. Beyond direct learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism during policy rollouts to refine and further improve task performance. Through extensive evaluation across six challenging dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods in the pursuit of human-level robot dexterity.																																	2024-07-20	PPRN:90727999		
J	Tantivasadakarn, Nathanan; Thorngren, Ryan; Vishwanath, Ashvin; Verresen, Ruben										Long-range entanglement from measuring symmetry-protected topological phases								Arxiv											2	2;2024-06-08;https://www.arxiv.org/abs/2112.01519v3| 1;2022-01-06;https://www.arxiv.org/abs/2112.01519v2	arXiv:2112.01519			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 08 2024	2024	A fundamental distinction between many -body quantum states are those with short- and longrange entanglement (SRE and LRE). The latter cannot be created by finite -depth circuits, underscoring the nonlocal nature of Schro¨dinger cat states, topological order, and quantum criticality. Remarkably, examples are known where LRE is obtained by performing single -site measurements on SRE, such as the toric code from measuring a sublattice of a 2D cluster state. However, a systematic understanding of when and how measurements of SRE give rise to LRE is still lacking. Here, we establish that LRE appears upon performing measurements on symmetry -protected topological (SPT) phases—of which the cluster state is one example. For instance, we show how to implement the Kramers-Wannier transformation by adding a cluster SPT to an input state followed by measurement. This transformation naturally relates states with SRE and LRE. An application is the realization of double-semion order when the input state is the Z2 Levin-Gu SPT. Similarly, the addition of fermionic SPTs and measurement leads to an implementation of the Jordan-Wigner transformation of a general state. More generally, we argue that a large class of SPT phases protected by G × H symmetry gives rise to anomalous LRE upon measuring G -charges, and we prove that this persists for generic points in the SPT phase under certain conditions. Our work introduces a new practical tool for using SPT phases as resources for creating LRE, and uncovers the classification result that all states related by sequentially gauging Abelian groups or by Jordan-Wigner transformation are in the same equivalence class, once we augment finite -depth circuits with singlesite measurements. In particular, any topological or fracton order with a solvable finite gauge group can be obtained from a product state in this way.																																	2024-06-23	PPRN:12026106		
J	Kirchenbauer, John; Geiping, Jonas; Wen, Yuxin; Shu, Manli; Saifullah, Khalid; Kong, Kezhi; Fernando, Kasun; Saha, Aniruddha; Goldblum, Micah; Goldstein, Tom				Saha, Aniruddha/V-1837-2019; Wen, Yuxin/AAA-4882-2019						On the Reliability of Watermarks for Large Language Models								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2306.04634v4| 1;2023-06-07;https://www.arxiv.org/abs/2306.04634v2	arXiv:2306.04634			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.																																	2024-05-20	PPRN:73056057		
J	Wang, Wenjing; Yang, Huan; Tuo, Zixi; He, Huiguo; Zhu, Junchen; Fu, Jianlong; Liu, Jiaying				liu, jianyang/AAQ-4589-2020						Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation								Arxiv											3	3;2024-04-24;https://www.arxiv.org/abs/2305.10874v4| 2;2024-04-08;https://www.arxiv.org/abs/2305.10874v3| 1;2023-05-18;https://www.arxiv.org/abs/2305.10874v1	arXiv:2305.10874			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 24 2024	2024	With the explosive popularity of AI -generated content (AIGC), video generation has recently received a lot of attention. Generating videos guided by text instructions poses significant challenges, such as modeling the complex relationship between space and time, and the lack of large-scale text -video paired data. Existing text -video datasets suffer from limitations in both content quality and scale, or they are not open -source, rendering them inaccessible for study and use. For model design, previous approaches extend pretrained text -to -image generation models by adding temporal 1D convolution/attention modules for video generation. However, these approaches overlook the importance of jointly modeling space and time, inevitably leading to temporal distortions and misalignment between texts and videos. In this paper, we propose a novel approach that strengthens the interaction between spatial and temporal perceptions. In particular, we utilize a swapped cross -attention mechanism in 3D windows that alternates the “query” role between spatial and temporal blocks, enabling mutual reinforcement for each other. Moreover, to fully unlock model capabilities for high -quality video generation and promote the development of the field, we curate a large-scale and open -source video dataset called HD -VG -130M. This dataset comprises 130 million text -video pairs from the open -domain, ensuring high -definition, widescreen and watermark -free characters. A smaller -scale yet more meticulously cleaned subset further enhances the data quality, aiding models in achieving superior performance. Experimental quantitative and qualitative results demonstrate the superiority of our approach in terms of per -frame quality, temporal correlation, and text -video alignment, with clear margins.																																	2024-05-04	PPRN:70533666		
J	Mundler, Niels; He, Jingxuan; Jenko, Slobodan; Vechev, Martin										SELF-CONTRADICTORY HALLUCINATIONS OF LLMS: DETECTION AND MITIGATION								Arxiv											3	3;2024-03-15;https://www.arxiv.org/abs/2305.15852v3| 2;2023-10-01;https://www.arxiv.org/abs/2305.15852v2| 1;2023-05-25;https://www.arxiv.org/abs/2305.15852v1	arXiv:2305.15852			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 15 2024	2024	Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our primary evaluation task is open-domain text generation, but we also demonstrate the applicability of our approach to shorter question answering. Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require retrieval of external knowledge. Rather, our method complements retrieval-based methods, as a large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be verified using online text. Our approach is practically effective and has been released as a push-button tool to benefit the public at https://chatprotect.ai/.																																	2024-04-11	PPRN:72712947		
J	Song, Feifan; Yu, Bowen; Li, Minghao; Yu, Haiyang; Huang, Fei; Li, Yongbin; Wang, Houfeng				Bowen, Yu/MFH-7462-2025; XIAOJUAN, HU/GLQ-6536-2022; Li, Yongbin/GWM-7528-2022						Preference Ranking Optimization for Human Alignment								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/2306.17492v2| 1;2023-06-30;https://www.arxiv.org/abs/2306.17492v1	arXiv:2306.17492			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pairwise contrast, thus lacking contrasts from a macro perspective. In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast to accommodate preference rankings of any length. By iteratively contrasting candidates, PRO instructs the LLM to prioritize the best response while progressively ranking the rest responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automaticbased, reward-based, GPT-4, and human evaluations.																																	2024-11-09	PPRN:73728984		
J	Chen, Shaoyu; Jiang, Bo; Gao, Hao; Liao, Bencheng; Xu, Qing; Zhang, Qian; Huang, Chang; Liu, Wenyu; Wang, Xinggang				Chen, Shaoyu/MGA-4947-2025; Wenyu, Liu/GRS-3009-2022; Wang, Xinggang/LSL-0946-2024						VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning								Arxiv											1	1;2024-02-20;https://www.arxiv.org/abs/2402.13243v1	arXiv:2402.13243			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 20 2024	2024	Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.																																	2024-03-19	PPRN:87772448		
J	Wei, Boyi; Huang, Kaixuan; Huang, Yangsibo; Xie, Tinghao; Qi, Xiangyu; Xia, Mengzhou; Mittal, Prateek; Wang, Mengdi; Henderson, Peter				Mittal, Prateek/ABB-2687-2021; Qi, Xiangyu/GZM-8733-2022						Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications								Arxiv											4	4;2024-10-24;https://www.arxiv.org/abs/2402.05162v4| 3;2024-07-01;https://www.arxiv.org/abs/2402.05162v3| 2;2024-06-27;https://www.arxiv.org/abs/2402.05162v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.05162v1	arXiv:2402.05162			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 24 2024	2024	Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about 3% at the parameter level and 2.5% at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.																																	2024-11-29	PPRN:87572916		
J	Shen, Xiaoqian; Xiong, Yunyang; Zhao, Changsheng; Wu, Lemeng; Chen, Jun; Zhu, Chenchen; Liu, Zechun; Xiao, Fanyi; Varadarajan, Balakrishnan; Bordes, Florian; Liu, Zhuang; Xu, Hu; Kim, Hyunwoo J.; Soran, Bilge; Krishnamoorthi, Raghuraman; Elhoseiny, Mohamed; Chandra, Vikas				Xiong, Yunyang/HHZ-6012-2022; Elhoseiny, Mohamed/X-6406-2019; chen, liang/KCJ-7472-2024; liu, zechun/LBH-4471-2024; Zhao, Changhong/KRQ-4153-2024						LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding								Arxiv											1	1;2024-10-22;https://www.arxiv.org/abs/2410.17434v1	arXiv:2410.17434			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 22 2024	2024	Multimodal Large Language Models (MLLMs) have shown promising progress in understanding and analyzing video content. However, processing long videos remains a significant challenge constrained by LLM’s context size. To address this limitation, we propose LongVU, a spatiotemporal adaptive compression mechanism thats reduces the number of video tokens while preserving visual details of long videos. Our idea is based on leveraging cross-modal query and inter-frame dependencies to adaptively reduce temporal and spatial redundancy in videos. Specifically, we leverage DINOv2 features to remove redundant frames that exhibit high similarity. Then we utilize text-guided cross-modal query for selective frame feature reduction. Further, we perform spatial token reduction across frames based on their temporal dependencies. Our adaptive compression strategy effectively processes a large number of frames with little visual information loss within given context length. Our LongVU consistently surpass existing methods across a variety of video understanding benchmarks, especially on hour-long video understanding tasks such as VideoMME and MLVU. Given a light-weight LLM, our LongVU also scales effectively into a smaller size with state-of-the-art video understanding performance.																																	2024-11-26	PPRN:118788718		
J	Han, Simeng; Schoelkopf, Hailey; Zhao, Yilun; Qi, Zhenting; Riddell, Martin; Zhou, Wenfei; Coady, James; Peng, David; Qiao, Yujie; Benson, Luke; Sun, Lucy; Wardle-Solano, Alex; Szabo, Hannah; Zubova, Ekaterina; Burtell, Matthew; Fan, Jonathan; Liu, Yixin; Wong, Brian; Sailor, Malcolm; Ni, Ansong; Nan, Linyong; Kasai, Jungo; Yu, Tao; Zhang, Rui; Fabbri, Alexander R.; Kryscinski, Wojciech; Yavuz, Semih; Liu, Ye; Lin, Xi Victoria; Joty, Shafiq; Zhou, Yingbo; Xiong, Caiming; Ying, Rex; Cohan, Arman; Radev, Dragomir				Radev, Dragomir/E-9641-2012; Zhou, Yingbo/MSX-1381-2025; Zhao, Ziang/IAR-5845-2023						FOLIO: Natural Language Reasoning with First-Order Logic								Arxiv											3	3;2024-10-11;https://www.arxiv.org/abs/2209.00840v3| 2;2024-05-17;https://www.arxiv.org/abs/2209.00840v2| 1;2022-09-02;https://www.arxiv.org/abs/2209.00840v1	arXiv:2209.00840			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 11 2024	2024	Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NLFOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO presents a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.																																	2024-12-03	PPRN:13531208		
J	Choi, Sunjin; Kim, Joonho; Kim, Seok; Nahmgoong, June										Large AdS black holes from QFT								Arxiv											3	3;2024-09-17;https://www.arxiv.org/abs/1810.12067v3| 2;2024-09-12;https://www.arxiv.org/abs/1810.12067v2| 1;2018-10-29;https://www.arxiv.org/abs/1810.12067v1	arXiv:1810.12067			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 17 2024	2024	We study the index of N = 4 Yang-Mills theory on S3 × R at large angular momenta. A generalized Cardy limit exhibits macroscopic entropy at large N . Our result is derived using free QFT analysis, and also a background field method on S3 . The index sets a lower bound on the entropy. It saturates the Bekenstein-Hawking entropy of known supersymmetric AdS5 black holes, thus accounting for their microstates. We further analyze the so-called Macdonald index, exploring small black holes and possibly new black holes reminiscent of hairy black holes. Finally, we study aspects of large supersymmetric AdS7 black holes, using background field method on S5 and ’t Hooft anomalies.																																	2024-10-03	PPRN:13505507		
J	Gao, Jian; Gu, Chun; Lin, Youtian; Li, Zhihao; Zhu, Hao; Cao, Xun; Zhang, Li; Yao, Yao				Cao, Xun/AAM-2895-2021						Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF Decomposition and Ray Tracing								Arxiv											2	2;2024-08-08;https://www.arxiv.org/abs/2311.16043v2| 1;2023-11-27;https://www.arxiv.org/abs/2311.16043v1	arXiv:2311.16043			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 08 2024	2024	In this paper, we present a novel differentiable point-based rendering framework to achieve photo-realistic relighting. To make the reconstructed scene relightable, we enhance vanilla 3D Gaussians by associating extra properties, including normal vectors, BRDF parameters, and incident lighting from various directions. From a collection of multi-view images, the 3D scene is optimized through 3D Gaussian Splatting while BRDF and lighting are decomposed by physically based differentiable rendering. To produce plausible shadow effects in photo-realistic relighting, we introduce an innovative point-based ray tracing with the bounding volume hierarchies for efficient visibility pre-computation. Extensive experiments demonstrate our improved BRDF estimation, novel view synthesis and relighting results compared to state-of-the-art approaches. The proposed framework showcases the potential to revolutionize the mesh-based graphics pipeline with a point-based pipeline enabling editing, tracing, and relighting.																																	2024-08-17	PPRN:86295759		
J	Kapoor, Sayash; Stroebl, Benedikt; Siegel, Zachary S.; Nadgir, Nitya; Narayanan, Arvind										AI Agents That Matter								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.01502v1	arXiv:2407.01502			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.																																	2024-07-18	PPRN:90657692		
J	Zhang, Zhuosheng; Zhang, Aston				Zhang, Zhuosheng/AAF-4919-2020						You Only Look at Screens: Multimodal Chain-of-Action Agents								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2309.11436v4| 2;2024-05-20;https://www.arxiv.org/abs/2309.11436v3| 1;2023-09-21;https://www.arxiv.org/abs/2309.11436v2	arXiv:2309.11436			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	Autonomous graphical user interface (GUI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, most existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-GUI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30$K$ unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-GUI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. 																																	2024-06-22	PPRN:85083036		
J	Zhang, Chaoyun; Li, Liqun; He, Shilin; Zhang, Xu; Qiao, Bo; Qin, Si; Ma, Minghua; Kang, Yu; Lin, Qingwei; Rajmohan, Saravan; Zhang, Dongmei; Zhang, Qi				Ma, Minghua/JFA-3786-2023; Lin, Qingwei/AAZ-3604-2021; Zhang, Chi/U-9607-2019						UFO: A UI-Focused Agent for Windows OS Interaction								Arxiv											4	4;2024-05-23;https://www.arxiv.org/abs/2402.07939v5| 3;2024-03-01;https://www.arxiv.org/abs/2402.07939v4| 2;2024-02-23;https://www.arxiv.org/abs/2402.07939v3| 1;2024-02-08;https://www.arxiv.org/abs/2402.07939v1	arXiv:2402.07939			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore the superior effectiveness of UFO in fulfilling user requests. To the best of our knowledge, UFO stands as the first UI agent specifically tailored for task completion within the Windows OS environment. 																																	2024-06-05	PPRN:87673180		
J	Huang, Yukun; Wang, Jianan; Shi, Yukai; Tang, Boshi; Qi, Xianbiao; Zhang, Lei				yukun, huang/JLM-6385-2023						DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation								Arxiv											1	1;2024-05-06;https://www.arxiv.org/abs/2306.12422v2	arXiv:2306.12422			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled 3D content creation by optimizing a randomly initialized differentiable 3D representation with score distillation. However, the optimization process suffers slow convergence and the resultant 3D models often exhibit two limitations: (a) quality concerns such as missing attributes and distorted shape and texture; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between the 3D optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns the 3D optimization process with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves 3D content creation with faster convergence, better quality and diversity.																																	2024-05-25	PPRN:88791207		
J	Yuan, Zhiqiang; Liu, Mingwei; Ding, Shiji; Wang, Kaixin; Chen, Yixuan; Peng, Xin; Lou, Yiling				Yuan, Zhiqiang/ABM-8316-2022; Liu, Mingwei/AHI-4200-2022; liu, yiling/HNS-6339-2023						No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation								Arxiv											2	2;2023-05-09;https://www.arxiv.org/abs/2305.04207v2| 1;2024-05-01;	arXiv:2305.04207			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 01 2024	2024	Unit testing is essential in detecting bugs in functionally-discrete program units. Manually writing high-quality unit tests is time-consuming and laborious. Although traditional techniques can generate tests with reasonable coverage, they exhibit low readability and cannot be directly adopted by developers. Recent work has shown the large potential of large language models (LLMs) in unit test generation, which can generate more human-like and meaningful test code. ChatGPT, the latest LLM incorporating instruction tuning and reinforcement learning, has performed well in various domains. However, It remains unclear how effective ChatGPT is in unit test generation. In this work, we perform the first empirical study to evaluate ChatGPT's capability of unit test generation. Specifically, we conduct a quantitative analysis and a user study to systematically investigate the quality of its generated tests regarding the correctness, sufficiency, readability, and usability. The tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures. Still, the passing tests generated by ChatGPT resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we propose ChatTESTER, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTESTER incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTESTER by generating 34.3% more compilable tests and 18.7% more tests with correct assertions than the default ChatGPT.																																	2025-11-07	PPRN:68787641		
J	Rajamanoharan, Senthooran; Conmy, Arthur; Smith, Lewis; Lieberum, Tom; Varma, Vikrant; Kramar, Janos; Shah, Rohin; Nanda, Neel										Improving Dictionary Learning with Gated Sparse Autoencoders								Arxiv											2	2;2024-04-30;https://www.arxiv.org/abs/2404.16014v2| 1;2024-04-24;https://www.arxiv.org/abs/2404.16014v1	arXiv:2404.16014			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 30 2024	2024	Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.																																	2024-06-04	PPRN:88640336		
J	Zeng, Zhiyuan; Yu, Jiatong; Gao, Tianyu; Meng, Yu; Goyal, Tanya; Chen, Danqi				Meng, Yu/ABH-2615-2020; CHEN, DANQI/C-6441-2013; Zeng, Zhiyuan/JAC-2446-2023						Evaluating Large Language Models at Evaluating Instruction Following								Arxiv											2	2;2024-04-16;https://www.arxiv.org/abs/2310.07641v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07641v1	arXiv:2310.07641			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 16 2024	2024	As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.																																	2024-04-26	PPRN:85540129		
J	Sun, Kai; Xu, Yifan Ethan; Zha, Hanwen; Liu, Yue; Dong, Xin Luna										Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?								Arxiv											2	2;2024-04-03;https://www.arxiv.org/abs/2308.10168v2| 1;2023-08-20;https://www.arxiv.org/abs/2308.10168v1	arXiv:2308.10168			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs? To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.																																	2024-04-18	PPRN:82084095		
J	Chen, Haoxin; Zhang, Yong; Cun, Xiaodong; Xia, Menghan; Wang, Xintao; Weng, Chao; Shan, Ying				Cun, Xiaodong/AAA-4674-2022; chen, haoxin/IYS-2513-2023						VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models								Arxiv											1	1;2024-01-17;https://www.arxiv.org/abs/2401.09047v1	arXiv:2401.09047			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 17 2024	2024	Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.																																	2024-02-03	PPRN:87211335		
J	Miyake, Daiki; Iohara, Akihiro; Saito, Yu; Tanaka, Toshiyuki				Tanaka, Toshiyuki/C-2749-2011						Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models								Arxiv											2	2;2024-12-10;https://www.arxiv.org/abs/2305.16807v2| 1;2023-05-26;https://www.arxiv.org/abs/2305.16807v1	arXiv:2305.16807			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 10 2024	2024	In image editing employing diffusion models, it is crucial to preserve the reconstruction fidelity to the original image while changing its style. Although existing methods ensure reconstruction fidelity through optimization, a drawback of these is the significant amount of time required for optimization. In this paper, we propose negative-prompt inversion, a method capable of achieving equivalent reconstruction solely through forward propagation without optimization, thereby enabling ultrafast editing processes. We experimentally demonstrate that the reconstruction fidelity of our method is comparable to that of existing methods, allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds, which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction fidelity with a moderate increase in computation time.																																	2025-01-18	PPRN:72729162		
J	Liang, Dingkang; Zhou, Xin; Xu, Wei; Zhu, Xingkui; Zou, Zhikang; Ye, Xiaoqing; Tan, Xiao; Bai, Xiang				Zhu, Xingkui/LFT-7064-2024; 鑫, 周/KDP-3567-2024; liang, dingkang/ABG-6365-2021						PointMamba: A Simple State Space Model for Point Cloud Analysis								Arxiv											4	4;2024-11-25;https://www.arxiv.org/abs/2402.10739v5| 3;2024-05-29;https://www.arxiv.org/abs/2402.10739v4| 2;2024-04-02;https://www.arxiv.org/abs/2402.10739v3| 1;2024-02-19;https://www.arxiv.org/abs/2402.10739v2	arXiv:2402.10739			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 25 2024	2024	Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose PointMamba , transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our method leverages space-filling curves for effective point tokenization and adopts an extremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. 																																	2025-01-08	PPRN:87761216		
J	Barroso-Luque, Luis; Shuaibi, Muhammed; Fu, Xiang; Wood, Brandon M.; Dzamba, Misko; Gao, Meng; Rizvi, Ammar; Zitnick, C. Lawrence; Ulissi, Zachary W.										Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models								Arxiv											1	1;2024-10-16;https://www.arxiv.org/abs/2410.12771v1	arXiv:2410.12771			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 16 2024	2024	The ability to discover new materials with desirable properties is critical for numerous applications from helping mitigate climate change to advances in next generation computing hardware. AI has the potential to accelerate materials discovery and design by more effectively exploring the chemical space compared to other computational methods or by trial-and-error. While substantial progress has been made on AI for materials data, benchmarks, and models, a barrier that has emerged is the lack of publicly available training data and open pre-trained models. To address this, we present a Meta FAIR release of the Open Materials 2024 (OMat24) large-scale open dataset and an accompanying set of pre-trained models. OMat24 contains over 110 million density functional theory (DFT) calculations focused on structural and compositional diversity. Our EquiformerV2 models achieve state-of-the-art performance on the Matbench Discovery leaderboard and are capable of predicting ground-state stability and formation energies to an F1 score above 0.9 and an accuracy of 20 meV/atom, respectively. We explore the impact of model size, auxiliary denoising objectives, and fine-tuning on performance across a range of datasets including OMat24, MPtraj, and Alexandria. The open release of the OMat24 dataset and models enables the research community to build upon our efforts and drive further advancements in AI-assisted materials science.																																	2024-11-07	PPRN:114145688		
J	Lin, Yong; Lin, Hangyu; Xiong, Wei; Diao, Shizhe; Liu, Jianmeng; Zhang, Jipeng; Pan, Rui; Wang, Haoxiang; Hu, Wenbin; Zhang, Hanning; Dong, Hanze; Pi, Renjie; Zhao, Han; Jiang, Nan; Ji, Heng; Yao, Yuan; Zhang, Tong				Lin, Yong/JXX-1882-2024; Liu, Junyi/IAR-6874-2023; Yao, Yuan/A-5799-2015; Zhang, Tong/HGC-1090-2022; Diao, Shizhe/JXY-7398-2024						Mitigating the Alignment Tax of RLHF								Arxiv											3	3;2024-10-13;https://www.arxiv.org/abs/2309.06256v4| 2;2024-02-05;https://www.arxiv.org/abs/2309.06256v3| 1;2023-09-12;https://www.arxiv.org/abs/2309.06256v1	arXiv:2309.06256			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Oct 13 2024	2024	LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA’s performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here 1 .																																	2024-11-06	PPRN:84985156		
J	Li, Junyou; Zhang, Qin; Yu, Yangbin; Fu, Qiang; Ye, Deheng										More Agents Is All You Need								Arxiv											2	2;2024-10-11;https://www.arxiv.org/abs/2402.05120v2| 1;2024-02-03;https://www.arxiv.org/abs/2402.05120v1	arXiv:2402.05120			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 11 2024	2024	We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method, termed as Agent Forest, is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. 																																	2024-11-04	PPRN:87572608		
J	Liu, Ziming; Ma, Pingchuan; Wang, Yixuan; Matusik, Wojciech; Tegmark, Max				Ma, Pingchuan/AFR-0634-2022						KAN 2.0: Kolmogorov-Arnold Networks Meet Science								Arxiv											1	1;2024-08-19;https://www.arxiv.org/abs/2408.10205v1	arXiv:2408.10205			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 19 2024	2024	A major challenge of AI + Science lies in their inherent incompatibility: today's AI is primarily based on connectionism, while science depends on symbolism. To bridge the two worlds, we propose a framework to seamlessly synergize Kolmogorov-Arnold Networks (KANs) and science. The framework highlights KANs' usage for three aspects of scientific discovery: identifying relevant features, revealing modular structures, and discovering symbolic formulas. The synergy is bidirectional: science to KAN (incorporating scientific knowledge into KANs), and KAN to science (extracting scientific insights from KANs). We highlight major new functionalities in the pykan package: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN compiler that compiles symbolic formulas into KANs. (3) tree converter: convert KANs (or any neural networks) to tree graphs. Based on these tools, we demonstrate KANs' capability to discover various types of physical laws, including conserved quantities, Lagrangians, symmetries, and constitutive laws.																																	2024-08-28	PPRN:91496056		
J	Tian, Linrui; Wang, Qi; Zhang, Bang; Bo, Liefeng										EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions								Arxiv											4	4;2024-08-08;https://www.arxiv.org/abs/2402.17485v3| 3;2024-08-06;https://www.arxiv.org/abs/2402.17485v2| 2;2024-02-27;https://www.arxiv.org/abs/2402.17485v1| 1;2024-02-27;https://www.arxiv.org/abs/2402.17485v1	arXiv:2402.17485			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 08 2024	2024	In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.																																	2024-08-21	PPRN:87922509		
J	Mei, Xinhao; Meng, Chutong; Liu, Haohe; Kong, Qiuqiang; Ko, Tom; Zhao, Chengqi; Plumbley, Mark D.; Zou, Yuexian; Wang, Wenwu				Plumbley, Mark/A-7298-2008; wang, wenwu/HOF-4371-2023; Mei, Xinhao/JRZ-0800-2023; liu, haohe/JBS-1030-2023						WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research								Arxiv											2	2;2024-07-18;https://www.arxiv.org/abs/2303.17395v2| 1;2023-03-30;https://www.arxiv.org/abs/2303.17395v1	arXiv:2303.17395			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jul 18 2024	2024	The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research.																																	2024-07-27	PPRN:52086992		
J	Carlini, Nicholas; Jagielski, Matthew; Choquette-Choo, Christopher A.; Paleka, Daniel; Pearce, Will; Anderson, Hyrum; Terzis, Andreas; Thomas, Kurt; Tramer, Florian				Terzis, Andreas/A-3348-2010						Poisoning Web-Scale Training Datasets is Practical								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2302.10149v2| 1;2023-02-20;https://www.arxiv.org/abs/2302.10149v1	arXiv:2302.10149			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	Deep learning models are often trained on distributed, web-scale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01% of the LAION-400M or COYO-700M datasets for just $60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.																																	2024-05-24	PPRN:43657448		
J	Zhan, Qiusi; Fang, Richard; Bindu, Rohan; Gupta, Akul; Hashimoto, Tatsunori; Kang, Daniel										Removing RLHF Protections in GPT-4 via Fine-Tuning								Arxiv											3	3;2024-04-05;https://www.arxiv.org/abs/2311.05553v3| 2;2023-11-10;https://www.arxiv.org/abs/2311.05553v2| 1;2023-11-09;https://www.arxiv.org/abs/2311.05553v1	arXiv:2311.05553			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 05 2024	2024	As large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback (RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. We may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. We further show that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that our fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Our results show the need for further research on protections on LLMs.																																	2024-04-21	PPRN:86111767		
J	Shen, Zhenqian; Zhang, Yongqi; Wei, Lanning; Zhao, Huan; Yao, Quanming				Wei, Lanning/ISU-5803-2023; 涂, 威威/GRF-5615-2022; Yao, Quanming/Y-6095-2019						Automated Machine Learning: From Principles to Practices								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/1810.13306v5| 1;2019-12-16;https://www.arxiv.org/abs/1810.13306v4	arXiv:1810.13306			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	Machine learning (ML) methods have been developing rapidly, but configuring and selecting proper methods to achieve a desired performance is increasingly difficult and tedious. To address this challenge, automated machine learning (AutoML) has emerged, which aims to generate satisfactory ML configurations for given tasks in a data-driven way. In this paper, we provide a comprehensive survey on this topic. We begin with the formal definition of AutoML and then introduce its principles, including the bi-level learning objective, the learning strategy, and the theoretical interpretation. Then, we summarize the AutoML practices by setting up the taxonomy of existing works based on three main factors: the search space, the search algorithm, and the evaluation strategy. Each category is also explained with the representative methods. Then, we illustrate the principles and practices with exemplary applications from configuring ML pipeline, one-shot neural architecture search, and integration with foundation models. Finally, we highlight the emerging directions of AutoML and conclude the survey.																																	2024-03-24	PPRN:12926654		
J	Fu, Tsu-Jui; Hu, Wenze; Xianzhi, Du; Wang, William Yang; Yang, Yinfei; Gan, Zhe										Guiding Instruction-based Image Editing via Multimodal Large Language Models								Arxiv											2	2;2024-02-05;https://www.arxiv.org/abs/2309.17102v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17102v1	arXiv:2309.17102			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 05 2024	2024	Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.																																	2024-02-21	PPRN:85338458		
J	Fu, Yichao; Bailis, Peter; Stoica, Ion; Zhang, Hao										Break the Sequential Dependency of LLM Inference Using Lookahead Decoding								Arxiv											1	1;2024-02-03;https://www.arxiv.org/abs/2402.02057v1	arXiv:2402.02057			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 03 2024	2024	Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. 																																	2024-05-25	PPRN:87517677		
J	Ruiz, Nataniel; Li, Yuanzhen; Jampani, Varun; Wei, Wei; Hou, Tingbo; Pritch, Yael; Wadhwa, Neal; Rubinstein, Michael; Aberman, Kfir				Hou, Tingbo/H-6978-2012; Jain, Varun/HHN-1250-2022						HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models								Arxiv											2	2;2024-10-16;https://www.arxiv.org/abs/2307.06949v2| 1;2023-07-13;https://www.arxiv.org/abs/2307.06949v1	arXiv:2307.06949			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 16 2024	2024	Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth - a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10,000x smaller than a normal DreamBooth model. 																																	2024-11-12	PPRN:73909128		
J	Goswami, Mononito; Szafer, Konrad; Choudhry, Arjun; Cai, Yifu; Li, Shuo; Dubrawski, Artur										MOMENT: A Family of Open Time-series Foundation Models								Arxiv											3	3;2024-10-10;https://www.arxiv.org/abs/2402.03885v3| 2;2024-05-14;https://www.arxiv.org/abs/2402.03885v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.03885v1	arXiv:2402.03885			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 10 2024	2024	We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE).																																	2024-11-01	PPRN:87537322		
J	Wang, Hao; Piao, Yun-Song										Dark energy in light of recent DESI BAO and Hubble tension								Arxiv											2	2;2024-06-20;https://www.arxiv.org/abs/2404.18579v2| 1;2024-04-29;https://www.arxiv.org/abs/2404.18579v1	arXiv:2404.18579			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	Recently, Dark Energy Spectroscopic Instrument (DESI) collaboration based on their first year data has reported a ≳ 3σ evidence for an evolving dark energy (DE) against the cosmological constant (CC), so the standard ΛCDM model. However, it is necessary to access the impact of DESI data on the state equation w0-wa of DE in the Hubble-tension-free cosmologies, where w0 0 and wa a is the parameters of state equation of DE. In this paper, using recent DESI BAO measurements combined with Planck CMB and Pantheon Plus dataset, we perform the Monte Carlo Markov Chain (MCMC) analysis for the w0waCDM  model with possible pre-recombination resolutions of the Hubble tension. It is found that though w0  > −1 and wa  < 0 are still preferred, the CC is also ≲ 2σ consistent, while the bestfit Hubble constant H0 0 are higher than those with pre-DESI BAO data but without the further exacerbation of S8 tension. According to our results, the resolutions of Hubble tension are likely to suppress the preference of DESI for the evolving DE, thus the claim of ruling out the CC needs to be more cautious regarding not only the recent observational data but also the cosmological tensions.																																	2024-07-10	PPRN:88697054		
J	Gao, Peng; Zhuo, Le; Liu, Dongyang; Du, Ruoyi; Luo, Xu; Qiu, Longtian; Zhang, Yuhang; Lin, Chen; Huang, Rongjie; Geng, Shijie; Zhang, Renrui; Xi, Junlin; Shao, Wenqi; Jiang, Zhengkai; Yang, Tianshuo; Ye, Weicai; Tong, He; He, Jingwen; Qiao, Yu; Li, Hongsheng				Qiao, Yu/ABD-5787-2021; He, Jingwen/HTM-5599-2023; Zhang, Zhuosheng/AAF-4919-2020; Li, Hongsheng/AES-5328-2022; Gao, Peng/B-4675-2012; Yang, Tianshuo/AAI-2989-2021; Geng, Shijie/IZE-5093-2023; Du, Ruoyi/GXF-3987-2022						Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers								Arxiv											2	2;2024-06-13;https://www.arxiv.org/abs/2405.05945v3| 1;2024-05-15;https://www.arxiv.org/abs/2405.05945v2	arXiv:2405.05945			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 13 2024	2024	Sora unveils the potential of scaling Diffusion Transformer (DiT) for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this technical report, we introduce the Lumina-T2X family – a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a unified framework designed to transform noise into images, videos, multi-view 3D objects, and audio clips conditioned on text instructions. By tokenizing the latent spatialtemporal space and incorporating learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution, aspect ratio, and length during inference. Advanced techniques like RoPE, RMSNorm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT (PixArt-α), indicating that increasing the number of parameters significantly accelerates convergence of generative models without compromising visual quality. Our further comprehensive analysis underscores Lumina-T2X’s preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. Code and a series of checkpoints will be successively released to facilitate future research at https://github.com/Alpha-VLLM/Lumina-T2X . We expect that the open-sourcing of Lumina-T2X will further foster creativity, transparency, and diversity in the generative AI community. [GRAPHICS]																																	2024-07-04	PPRN:89075877		
J	Sahoo, Subham Sekhar; Arriola, Marianne; Schiff, Yair; Gokaslan, Aaron; Marroquin, Edgar; Chiu, Justin T; Rush, Alexander; Kuleshov, Volodymyr										Simple and Effective Masked Diffusion Language Models								Arxiv											1	1;2024-06-11;https://www.arxiv.org/abs/2406.07524v1	arXiv:2406.07524			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. 																																	2024-07-12	PPRN:89283801		
J	Wang, Ziyang; Zheng, Jian-Qing; Zhang, Yichi; Cui, Ge; Li, Lei				zheng, jianqing/JVN-6953-2024; Wang, Ziyang/AAI-2463-2021						Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation								Arxiv											2	2;2024-03-30;https://www.arxiv.org/abs/2402.05079v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.05079v1	arXiv:2402.05079			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 30 2024	2024	In recent advancements in medical image analysis, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks. While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms. However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation. Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba's capability. Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network. This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images. We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance. We conducted experiments on publicly available ACDC MRI Cardiac segmentation dataset, and Synapse CT Abdomen segmentation dataset. The results show that Mamba-UNet outperforms several types of UNet in medical image segmentation under the same hyper-parameter setting. The source code and baseline implementations are available.																																	2024-04-17	PPRN:87561940		
J	Kong, Aobo; Zhao, Shiwan; Chen, Hao; Li, Qicheng; Qin, Yong; Sun, Ruiqi; Zhou, Xin; Wang, Enzhi; Dong, Xiaohang				Li, Qicheng/AAH-2019-2020; Sun, Ruiqi/IWM-2668-2023						Better Zero-Shot Reasoning with Role-Play Prompting								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2308.07702v2| 1;2023-08-15;https://www.arxiv.org/abs/2308.07702v1	arXiv:2308.07702			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 14 2024	2024	Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%.Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to "think step by step", our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process. This highlights its potential to augment the reasoning capabilities of LLMs. 																																	2024-04-11	PPRN:77890491		
J	Seo, Junyoung; Jang, Wooseok; Kwak, Min-Seop; Kim, Hyeonsu; Ko, Jaehoon; Kim, Junho; Kim, Jin-Hwa; Lee, Jiyoung; Kim, Seungryong				Kim, Ha-Jung/AGU-3492-2022; Seo, Junyoung/HZM-4432-2023; Kim, Yong Joon/IQS-7291-2023; Lee, Jiyoung/KRP-3377-2024; Kim, Junho/JFJ-1013-2023						Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation								Arxiv											2	2;2024-02-06;https://www.arxiv.org/abs/2303.07937v4| 1;2023-03-14;https://www.arxiv.org/abs/2303.07937v3	arXiv:2303.07937			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 06 2024	2024	Text-to-3D generation has shown rapid progress in recent days with the advent of score distillation sampling (SDS), a methodology of using pretrained text-to2D diffusion models to optimize a neural radiance field (NeRF) in a zero-shot setting. However, the lack of 3D awareness in the 2D diffusion model often destabilizes previous methods from generating a plausible 3D scene. To address this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness into the pretrained 2D diffusion model, enhancing the robustness and 3D consistency of score distillation-based methods. Specifically, we introduce a consistency injection module which constructs a 3D point cloud from the text prompt and utilizes its projected depth map at given view as a condition for the diffusion model. The 2D diffusion model, through its generative capability, robustly infers dense structure from the sparse point cloud depth map and generates a geometrically consistent and coherent 3D scene. We also introduce a new technique called semantic coding that reduces semantic ambiguity of the text prompt for improved results. Our method can be easily adapted to various text-to-3D baselines, and we experimentally demonstrate how our method notably enhances the 3D consistency of generated scenes in comparison to previous baselines, achieving state-of-theart performance in geometric robustness and fidelity. Project page is available at https://ku-cvlab.github.io/3DFuse/																																	2024-02-22	PPRN:46904176		
J	Ovadia, Oded; Brief, Menachem; Mishaeli, Moshik; Elisha, Oren										Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs								Arxiv											3	3;2024-01-30;https://www.arxiv.org/abs/2312.05934v3| 2;2024-01-25;https://www.arxiv.org/abs/2312.05934v2| 1;2023-12-10;https://www.arxiv.org/abs/2312.05934v1	arXiv:2312.05934			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 30 2024	2024	Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.																																	2024-05-25	PPRN:86539752		
J	Min, Yingqian; Chen, Zhipeng; Jiang, Jinhao; Chen, Jie; Deng, Jia; Hu, Yiwen; Tang, Yiru; Wang, Jiapeng; Cheng, Xiaoxue; Song, Huatong; Zhao, Wayne Xin; Liu, Zheng; Wang, Zhongyuan; Wen, Ji-Rong				Xia, Lianghao/IWV-0954-2023; Wang, Jiapeng/HLG-3222-2023						Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems								Arxiv											2	2;2024-12-22;https://www.arxiv.org/abs/2412.09413v2| 1;2024-12-12;https://www.arxiv.org/abs/2412.09413v1	arXiv:2412.09413			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 22 2024	2024	Recently, slow-thinking reasoning systems, such as o1, have demonstrated remarkable capabilities in solving complex reasoning tasks. These systems typically engage in an extended thinking process before responding to a query, allowing them to generate more thorough, accurate, and well-reasoned solutions. These systems are primarily developed and maintained by industry, with their core techniques not publicly disclosed. In response, an increasing number of studies from the research community aim to explore the technical foundations underlying these powerful reasoning systems. Building on these prior efforts, this paper presents a reproduction report on implementing o1-like reasoning systems. We introduce an “imitate, explore, and self-improve” framework, denoted as STILL-2 3 , as our primary technical approach to train the reasoning model. In the initial phase, we use distilled long-form thought data to fine-tune the reasoning model, enabling it to invoke a slow-thinking mode. The model is then encouraged to explore challenging problems by generating multiple rollouts, which can result in increasingly more high-quality trajectories that lead to correct answers. Furthermore, the model undergoes self-improvement by iteratively refining its training dataset. To verify the effectiveness of this approach, we conduct extensive experiments on three challenging benchmarks. The experimental results demonstrate that our approach achieves competitive performance compared to industry-level reasoning systems on these benchmarks. 																																	2025-01-31	PPRN:119900599		
J	Kirk, Hannah Rose; Whitefield, Alexander; Roettger, Paul; Bean, Andrew; Margatina, Katerina; Ciro, Juan; Mosquera, Rafael; Bartolo, Max; Williams, Adina; He, He; Vidgen, Bertie; Hale, Scott A.				Bean, Andrew/MGB-2088-2025						The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models								Arxiv											2	2;2024-12-03;https://www.arxiv.org/abs/2404.16019v2| 1;2024-04-24;https://www.arxiv.org/abs/2404.16019v1	arXiv:2404.16019			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 03 2024	2024	Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.																																	2025-01-15	PPRN:88633874		
J	Bodnar, Cristian; Bruinsma, Wessel P.; Lucic, Ana; Stanley, Megan; Vaughan, Anna; Brandstetter, Johannes; Garvan, Patrick; Riechert, Maik; Weyn, Jonathan A.; Dong, Haiyu; Gupta, Jayesh K.; Thambiratnam, Kit; Archibald, Alexander T.; Wu, Chun-Chieh; Heider, Elizabeth; Welling, Max; Turner, Richard E.; Perdikaris, Paris				Lucic, Ana/N-9335-2019; Perdikaris, Paris/LNQ-5994-2024; Bruinsma, Wessel/OGQ-9723-2025; Archibald, Alexander/GRR-5452-2022						A Foundation Model for the Earth System								Arxiv											3	3;2024-11-21;https://www.arxiv.org/abs/2405.13063v3| 2;2024-05-28;https://www.arxiv.org/abs/2405.13063v2| 1;2024-05-20;https://www.arxiv.org/abs/2405.13063v1	arXiv:2405.13063			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 21 2024	2024	Reliable forecasts of the Earth system are crucial for human progress and safety from natural disasters. Artificial intelligence offers substantial potential to improve prediction accuracy and computational efficiency in this field, however this remains underexplored in many domains. Here we introduce Aurora, a large-scale foundation model for the Earth system trained on over a million hours of diverse data. Aurora outperforms operational forecasts for air quality, ocean waves, tropical cyclone tracks, and high-resolution weather forecasting at orders of magnitude smaller computational expense than dedicated existing systems. With the ability to fine-tune Aurora to diverse application domains at only modest computational cost, Aurora represents significant progress in making actionable Earth system predictions accessible to anyone.																																	2025-01-11	PPRN:88988393		
J	Ferrando, Javier; Sarti, Gabriele; Bisazza, Arianna; Costa-jussa, Marta R.				Costa-jussà, Marta/M-7886-2013						A Primer on the Inner Workings of Transformer-based Language Models								Arxiv											2	2;2024-10-13;https://www.arxiv.org/abs/2405.00208v3| 1;2024-05-02;https://www.arxiv.org/abs/2405.00208v2	arXiv:2405.00208			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 13 2024	2024	The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.																																	2024-10-26	PPRN:88722341		
J	Li, Yi; Wang, Hualiang; Duan, Yiqun; Zhang, Jiheng; Li, Xiaomeng				Li, Xiaomeng/AAV-6938-2020						A Closer Look at the Explainability of Contrastive Language-Image Pre-training								Arxiv											2	2;2024-09-16;https://www.arxiv.org/abs/2304.05653v2| 1;2023-04-12;https://www.arxiv.org/abs/2304.05653v1	arXiv:2304.05653			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 16 2024	2024	Contrastive language-image pre-training (CLIP) is a powerful vision-language model that has shown great benefits for various tasks. However, we have identified some issues with its explainability, which undermine its credibility and limit the capacity for related tasks. Specifically, we find that CLIP tends to focus on background regions rather than foregrounds, with noisy activations at irrelevant positions on the visualization results. These phenomena conflict with conventional explainability methods based on the class attention map (CAM), where the raw model can highlight the local foreground regions using global supervision without alignment. To address these problems, we take a closer look at its architecture and features. Based on thorough analyses, we find the raw self-attentions link to inconsistent semantic regions, resulting in the opposite visualization. Besides, the noisy activations are owing to redundant features among categories. Building on these insights, we propose the CLIP Surgery for reliable CAM, a method that allows surgery-like modifications to the inference architecture and features, without further fine-tuning as classical CAM methods. This approach significantly improves the explainability of CLIP, surpassing existing methods by large margins. Besides, it enables multimodal visualization and extends the capacity of raw CLIP on open-vocabulary tasks without extra alignment. 																																	2024-12-23	PPRN:58909362		
J	Tang, Jiaming; Zhao, Yilong; Zhu, Kan; Xiao, Guangxuan; Kasikci, Baris; Han, Song										Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference								Arxiv											2	2;2024-08-26;https://www.arxiv.org/abs/2406.10774v2| 1;2024-06-16;https://www.arxiv.org/abs/2406.10774v1	arXiv:2406.10774			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 26 2024	2024	As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 7.03× self-attention speedup, which reduces inference latency by 2.23× while performing well on tasks with long dependencies with negligible accuracy loss. 																																	2024-09-06	PPRN:89343778		
J	Sun, Mingjie; Chen, Xinlei; Kolter, J. Zico; Liu, Zhuang				SUN, MINGJIE/GQQ-0374-2022						Massive Activations in Large Language Models								Arxiv											2	2;2024-08-14;https://www.arxiv.org/abs/2402.17762v2| 1;2024-02-27;https://www.arxiv.org/abs/2402.17762v1	arXiv:2402.17762			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 14 2024	2024	We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers. 																																	2024-08-22	PPRN:87920434		
J	Wen, Chuan; Lin, Xingyu; So, John; Chen, Kai; Dou, Qi; Gao, Yang; Abbeel, Pieter				Dou, Qi/I-8175-2019; 林, 星宇/GSD-2548-2022; Wen, Chuan/LIF-6638-2024						Any-point Trajectory Modeling for Policy Learning								Arxiv											3	3;2024-07-12;https://www.arxiv.org/abs/2401.00025v3| 2;2024-02-16;https://www.arxiv.org/abs/2401.00025v2| 1;2023-12-28;https://www.arxiv.org/abs/2401.00025v1	arXiv:2401.00025			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	Learning from demonstration is a powerful method for teaching robots new skills, and having more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, Any-point Trajectory Modeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Across over 130 language-conditioned tasks we evaluated in both simulation and the real world, ATM outperforms strong video pre-training baselines by 80% on average. Furthermore, we show effective transfer learning of manipulation skills from human videos and videos from a different robot morphology. 																																	2024-07-23	PPRN:86904138		
J	Levy, Mosh; Jacoby, Alon; Goldberg, Yoav										Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models								Arxiv											2	2;2024-07-10;https://www.arxiv.org/abs/2402.14848v2| 1;2024-02-19;https://www.arxiv.org/abs/2402.14848v1	arXiv:2402.14848			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 10 2024	2024	This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.																																	2024-07-21	PPRN:87871473		
J	Fu, Zipeng; Zhao, Qingqing; Wu, Qi; Wetzstein, Gordon; Finn, Chelsea				wu, qirui/GLU-4942-2022; zhao, qingqing/AFV-7804-2022						HumanPlus: Humanoid Shadowing and Imitation from Humans								Arxiv											1	1;2024-06-15;https://www.arxiv.org/abs/2406.10454v1	arXiv:2406.10454			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 15 2024	2024	One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations.																																	2024-07-04	PPRN:89352297		
J	Yuksekgonul, Mert; Bianchi, Federico; Boen, Joseph; Liu, Sheng; Huang, Zhi; Guestrin, Carlos; Zou, James				Yuksekgonul, Mert/KHU-0698-2024; Bianchi, Federico/JZT-6891-2024						TextGrad: Automatic "Differentiation" via Text								Arxiv											1	1;2024-06-11;https://www.arxiv.org/abs/2406.07496v1	arXiv:2406.07496			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges. Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from 51% to 55%, yields 20% relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.																																	2024-07-04	PPRN:89279372		
J	Morris, Meredith Ringel; Sohl-dickstein, Jascha; Fiedel, Noah; Warkentin, Tris; Dafoe, Allan; Faust, Aleksandra; Farabet, Clement; Legg, Shane				Dafoe, Allan/G-2505-2014						Levels of AGI for Operationalizing Progress on the Path to AGI								Arxiv											3	3;2024-05-22;https://www.arxiv.org/abs/2311.02462v3| 2;2024-01-05;https://www.arxiv.org/abs/2311.02462v2| 1;2023-11-04;https://www.arxiv.org/abs/2311.02462v1	arXiv:2311.02462			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	May 22 2024	2024	We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy, providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. With these principles in mind, we propose "Levels of AGI" based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.																																	2024-06-05	PPRN:86062526		
J	Bao, Fan; Xiang, Chendong; Yue, Gang; He, Guande; Zhu, Hongzhou; Zheng, Kaiwen; Zhao, Min; Liu, Shilong; Wang, Yaole; Zhu, Jun				Zheng, Kaiwen/AAF-1029-2019; Liu, Shilong/GVS-1257-2022						Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models								Arxiv											1	1;2024-05-07;https://www.arxiv.org/abs/2405.04233v1	arXiv:2405.04233			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 07 2024	2024	We introduce Vidu, a high-performance text-to-video generator that is capable of producing 1080p videos up to 16 seconds in a single generation. Vidu is a diffusion model with U-ViT as its backbone, which unlocks the scalability and the capability for handling long videos. Vidu exhibits strong coherence and dynamism, and is capable of generating both realistic and imaginative videos, as well as understanding some professional photography techniques, on par with Sora – the most powerful reported text-to-video generator. Finally, we perform initial experiments on other controllable video generation, including canny-to-video generation, video prediction and subject-driven generation, which demonstrate promising results.																																	2024-06-04	PPRN:88980787		
J	Zhao, Yian; Lv, Wenyu; Xu, Shangliang; Wei, Jinman; Wang, Guanzhong; Dang, Qingqing; Liu, Yi; Chen, Jie				Zhao, Yang/D-1014-2009; Zhao, YiAn/JFK-5881-2023; chen, jie/HQY-7507-2023						DETRs Beat YOLOs on Real-time Object Detection								Arxiv											2	2;2024-04-03;https://www.arxiv.org/abs/2304.08069v3| 1;2023-04-17;https://www.arxiv.org/abs/2304.08069v1	arXiv:2304.08069			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP.																																	2024-04-18	PPRN:63573294		
J	Voleti, Vikram; Yao, Chun-Han; Boss, Mark; Letts, Adam; Pankratz, David; Tochilkin, Dmitry; Laforte, Christian; Rombach, Robin; Jampani, Varun				Jain, Varun/HHN-1250-2022; Boss, Mark/ABA-5591-2021						SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.12008v1	arXiv:2403.12008			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.																																	2024-04-11	PPRN:88189614		
J	Lee, Ariel N.; Hunter, Cole J.; Ruiz, Nataniel										Platypus: Quick, Cheap, and Powerful Refinement of LLMs								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2308.07317v2| 1;2023-08-14;https://www.arxiv.org/abs/2308.07317v1	arXiv:2308.07317			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 14 2024	2024	We present Platypus, a family of fine-tuned and merged Large Language Models (LLMs) that achieved the strongest performance and stood at first place in HuggingFace’s Open LLM Leaderboard * at the time of writing. In this work we describe (1) our curated dataset Open -Platypus, that is a subset of other open datasets and which we release to the public (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on a single A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open -Platypus dataset, and opens opportunities for more improvements in the field. 																																	2024-04-11	PPRN:77230825		
J	Lipman, Yaron; Havasi, Marton; Holderrieth, Peter; Shaul, Neta; Le, Matt; Karrer, Brian; Chen, Ricky T.Q.; Lopez-Paz, David; Ben-Hamu, Heli; Gat, Itai										Flow Matching Guide and Code								Arxiv											1	1;2024-12-09;https://www.arxiv.org/abs/2412.06264v1	arXiv:2412.06264			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 09 2024	2024	Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.																																	2025-01-17	PPRN:119798919		
J	Chen, Guoxin; Liao, Minpeng; Li, Chengxi; Fan, Kai										AlphaMath Almost Zero: Process Supervision without Process								Arxiv											3	3;2024-09-27;https://www.arxiv.org/abs/2405.03553v3| 2;2024-05-23;https://www.arxiv.org/abs/2405.03553v2| 1;2024-05-06;https://www.arxiv.org/abs/2405.03553v1	arXiv:2405.03553			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 27 2024	2024	Although recent advancements in large language models (LLMs) have significantly improved their performance on various tasks, they still face challenges with complex and symbolic multi-step reasoning, particularly in mathematical reasoning. To bolster the mathematical reasoning capabilities of LLMs, most existing efforts concentrate on seeking assistance from either domain experts or GPT-4 for high-quality process-supervised data, which is not only expensive but also labor-intensive. In our study, we propose an innovative framework, AlphaMath, that bypasses the need for process annotations (from humans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework focuses on unleashing the potential of a well-pretrained LLM to autonomously enhance its mathematical reasoning. Specifically, we integrate a value model with the LLM, automatically generating both process supervision and step-level evaluation signals in MCTS. Furthermore, we propose an efficient inference strategy, step-level beam search, where the value model is crafted to assist the policy model (i.e., LLM) in navigating more effective reasoning paths, rather than solely relying on prior probabilities. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, our AlphaMath framework achieves comparable or superior results to previous state-of-the-art methods.																																	2024-10-09	PPRN:88789614		
J	Dong, Guanting; Yuan, Hongyi; Lu, Keming; Li, Chengpeng; Mingfeng, Xue; Liu, Dayiheng; Wang, Wei; Yuan, Zheng; Zhou, Chang; Zhou, Jingren				李, 程鹏/HLP-9229-2023; dong, guanting/JGL-9364-2023; Zhou, Mingyuan/AAE-8717-2021						How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition								Arxiv											4	4;2024-06-07;https://www.arxiv.org/abs/2310.05492v4| 3;2024-01-19;https://www.arxiv.org/abs/2310.05492v3| 2;2023-11-01;https://www.arxiv.org/abs/2310.05492v2| 1;2023-10-09;https://www.arxiv.org/abs/2310.05492v1	arXiv:2310.05492			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.																																	2024-11-20	PPRN:85582594		
J	Tseng, Albert; Chee, Jerry; Sun, Qingyao; Kuleshov, Volodymyr; De Sa, Christopher										QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks								Arxiv											2	2;2024-06-04;https://www.arxiv.org/abs/2402.04396v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.04396v1	arXiv:2402.04396			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 04 2024	2024	Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we in-troduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme com-pression regimes (≤4 bits per weight) using three novel techniques. First, QuIP#improves QuIP’s(Chee et al., 2023) incoherence processing by using the randomized Hadamard transform, which is faster and has better theoretical properties. Sec-ond, QuIP#uses vector quantization to take ad-vantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient code books based on the highly symmetric E8lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP#uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP#outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference. Our code can be found																																	2024-11-09	PPRN:87553268		
J	Ding, Peng; Kuang, Jun; Ma, Dan; Cao, Xuezhi; Xian, Yunsen; Chen, Jiajun; Huang, Shujian				El-Ashram, Saeed/AAC-6060-2021; Ding, Peng/G-4669-2013						A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily								Arxiv											4	4;2024-04-07;https://www.arxiv.org/abs/2311.08268v4| 3;2024-03-27;https://www.arxiv.org/abs/2311.08268v3| 2;2024-02-18;https://www.arxiv.org/abs/2311.08268v2| 1;2023-11-14;https://www.arxiv.org/abs/2311.08268v1	arXiv:2311.08268			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 07 2024	2024	Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. 																																	2024-04-21	PPRN:86200864		
J	Wang, Shen; Xu, Tianlong; Li, Hang; Zhang, Chaoli; Liang, Joleen; Tang, Jiliang; Yu, Philip S.; Wen, Qingsong				Yu, Philip/A-2815-2012; Li, Hang/LOS-5863-2024; Wen, Qingsong/LTF-7625-2024						Large Language Models for Education: A Survey and Outlook								Arxiv											2	2;2024-04-01;https://www.arxiv.org/abs/2403.18105v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.18105v1	arXiv:2403.18105			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.																																	2024-04-18	PPRN:88334655		
J	Bouzenia, Islem; Devanbu, Premkumar; Pradel, Michael										RepairAgent: An Autonomous, LLM-Based Agent for Program Repair								Arxiv											2	2;2024-10-28;https://www.arxiv.org/abs/2403.17134v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.17134v1	arXiv:2403.17134			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 25 2024	2024	Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.																																	2025-08-07	PPRN:88295935		
J	Yan, Biwei; Li, Kun; Xu, Minghui; Dong, Yueyan; Zhang, Yue; Ren, Zhaochun; Cheng, Xiuzhen				Li, Kun/A-9711-2017; Xu, Ming Hui/AAC-9503-2022						On Protecting the Data Privacy of Large Language Models (LLMs): A Survey								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2403.05156v2| 1;2024-03-08;https://www.arxiv.org/abs/2403.05156v1	arXiv:2403.05156			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 14 2024	2024	Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks. When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs. Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints. Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of LLM privacy protection.																																	2024-04-11	PPRN:88082124		
J	Ye, Ruosong; Zhang, Caiqi; Wang, Runhui; Xu, Shuyuan; Zhang, Yongfeng				Xu, Shuyuan/KBP-9666-2024; Wang, Runhui/IUM-7997-2023						Language is All a Graph Needs								Arxiv											3	3;2024-02-06;https://www.arxiv.org/abs/2308.07134v5| 2;2023-08-24;https://www.arxiv.org/abs/2308.07134v3| 1;2023-08-14;https://www.arxiv.org/abs/2308.07134v1	arXiv:2308.07134			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data samples such as images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, language, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is open-sourced at https://github.com/agiresearch/InstructGLM.																																	2024-02-22	PPRN:78301614		
J	Xu, Peng; Ping, Wei; Wu, Xianchao; Mcafee, Lawrence; Zhu, Chen; Liu, Zihan; Subramanian, Sandeep; Bakhturina, Evelina; Shoeybi, Mohammad; Catanzaro, Bryan										Retrieval meets Long Context Large Language Models								Arxiv											2	2;2024-01-23;https://www.arxiv.org/abs/2310.03025v2| 1;2023-10-04;https://www.arxiv.org/abs/2310.03025v1	arXiv:2310.03025			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 23 2024	2024	Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.																																	2024-05-25	PPRN:85397752		
J	Lee, Andrew; Bai, Xiaoyan; Pres, Itamar; Wattenberg, Martin; Kummerfeld, Jonathan K.; Mihalcea, Rada										A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity								Arxiv											1	1;2024-01-03;https://www.arxiv.org/abs/2401.01967v1	arXiv:2401.01967			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 03 2024	2024	While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become "aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.																																	2024-05-25	PPRN:86964946		
J	Yao, Huanjin; Huang, Jiaxing; Wu, Wenhao; Zhang, Jingyi; Wang, Yibo; Liu, Shunyu; Wang, Yingjie; Song, Yuxin; Feng, Haocheng; Shen, Li; Tao, Dacheng				Wu, Wenhao/KDM-6203-2024; Shen, Li/AEZ-9528-2022; Wang, Yingjie/LZG-2097-2025; Huang, Jiaxing/LDG-1140-2024; Zhang, Jingyi/KPS-8963-2024; Tao, Dacheng/A-5449-2012						Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search								Arxiv											2	2;2024-12-31;https://www.arxiv.org/abs/2412.18319v2| 1;2024-12-24;https://www.arxiv.org/abs/2412.18319v1	arXiv:2412.18319			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 31 2024	2024	In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into “tree search” for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry- 260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1- like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the 7 superiority of our proposed methods on various 8 benchmarks. 																																	2025-02-22	PPRN:120147583		
J	Guo, Zirui; Xia, Lianghao; Yu, Yanhua; Ao, Tu; Huang, Chao				Yu, Yanhua/ABE-4722-2020						LightRAG: Simple and Fast Retrieval-Augmented Generation								Arxiv											2	2;2024-11-07;https://www.arxiv.org/abs/2410.05779v2| 1;2024-10-08;https://www.arxiv.org/abs/2410.05779v1	arXiv:2410.05779			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 07 2024	2024	Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG.																																	2024-12-16	PPRN:105772478		
J	Setlur, Amrith; Nagpal, Chirag; Fisch, Adam; Geng, Xinyang; Eisenstein, Jacob; Agarwal, Rishabh; Agarwal, Alekh; Berant, Jonathan; Kumar, Aviral										Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning								Arxiv											1	1;2024-10-10;https://www.arxiv.org/abs/2410.08146v1	arXiv:2410.08146			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 10 2024	2024	A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask: “How should we design process rewards?”. Our key insight is that, to be effective, the process reward for a step should measure progress : a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is > 8% more accurate, and 1 .5 − 5 × more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with 5 − 6 × gain in sample efficiency, and > 6% gain in accuracy, over ORMs.																																	2024-11-01	PPRN:105774155		
J	Hu, Mu; Yin, Wei; Zhang, Chi; Cai, Zhipeng; Long, Xiaoxiao; Chen, Hao; Wang, Kaixuan; Yu, Gang; Shen, Chunhua; Shen, Shaojie										Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation								Arxiv											2	2;2024-08-16;https://www.arxiv.org/abs/2404.15506v2| 1;2024-03-22;https://www.arxiv.org/abs/2404.15506v1	arXiv:2404.15506			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 16 2024	2024	We introduce Metric3D v2, a geometric foundation model for zero-shot metric depth and surface normal estimation from a single image, which is crucial for metric 3D recovery. While depth and normal are geometrically related and highly complimentary, they present distinct challenges. SoTA monocular depth methods achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. Meanwhile, SoTA normal estimation methods have limited zero-shot performance due to the lack of large-scale labeled data. To tackle these issues, we propose solutions for both metric depth estimation and surface normal estimation. For metric depth estimation, we show that the key to a zero-shot single-view model lies in resolving the metric ambiguity from various camera models and large-scale data training. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problem and can be effortlessly plugged into existing monocular models. For surface normal estimation, we propose a joint depth-normal optimization module to distill diverse data knowledge from metric depth, enabling normal estimators to learn beyond normal labels. Equipped with these modules, our depth-normal models can be stably trained with over 16 million of images from thousands of camera models with different-type annotations, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. 																																	2024-08-25	PPRN:88634234		
J	Chen, Sanyuan; Liu, Shujie; Zhou, Long; Liu, Yanqing; Tan, Xu; Li, Jinyu; Zhao, Sheng; Qian, Yao; Wei, Furu				jinyu, Li/JQV-7729-2023; Chen, Sanyuan/GLR-3754-2022						VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2406.05370v2| 1;2024-06-08;https://www.arxiv.org/abs/2406.05370v1	arXiv:2406.05370			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis.  [GRAPHICS]																																	2024-07-04	PPRN:89265596		
J	Aryabumi, Viraat; Dang, John; Talupuru, Dwarak; Dash, Saurabh; Cairuz, David; Lin, Hangyu; Venkitesh, Bharat; Smith, Madeline; Campos, Jon Ander; Tan, Yi Chern; Marchisio, Kelly; Bartolo, Max; Ruder, Sebastian; Locatelli, Acyr; Kreutzer, Julia; Frosst, Nick; Gomez, Aidan; Blunsom, Phil; Fadaee, Marzieh; Ustun, Ahmet; Hooker, Sara				Campos, Juan/AFU-0131-2022; Li, Wenjing/JMP-8310-2023						Aya 23: Open Weight Releases to Further Multilingual Progress								Arxiv											2	2;2024-05-31;https://www.arxiv.org/abs/2405.15032v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.15032v1	arXiv:2405.15032			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	May 31 2024	2024	This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model (Ustun et al., 2024), focusing on pairing a highly performant pre-trained model with the recently released Aya collection (Singh et al., 2024). The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world's population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. We release the open weights for both the 8B and 35B models as part of our continued commitment for expanding access to multilingual progress.																																	2024-06-19	PPRN:89017564		
J	Yu, Zehao; Sattler, Torsten; Geiger, Andreas				Sattler, Torsten/AAM-3155-2021						Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes								Arxiv											1	1;2024-04-16;https://www.arxiv.org/abs/2404.10772v1	arXiv:2404.10772			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed.																																	2024-04-26	PPRN:88540917		
J	Guo, Hang; Li, Jinmin; Dai, Tao; Ouyang, Zhihao; Ren, Xudong; Xia, Shu-Tao				Ouyang, Zhihao/ISS-3140-2023						MambaIR: A Simple Baseline for Image Restoration with State-Space Model								Arxiv											1	1;2024-03-25;https://www.arxiv.org/abs/2402.15648v2	arXiv:2402.15648			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 25 2024	2024	Recent years have seen significant advancements in image restoration, largely attributed to the development of modern deep neural networks, such as CNNs and Transformers. However, existing restoration backbones often face the dilemma between global receptive fields and efficient computation, hindering their application in practice. Recently, the Selective Structured State Space Model, especially the improved version Mamba, has shown great potential for long-range dependency modeling with linear complexity, which offers a way to resolve the above dilemma. However, the standard Mamba still faces certain challenges in low-level vision such as local pixel forgetting and channel redundancy. In this work, we introduce a simple but effective baseline, named MambaIR, which introduces both local enhancement and channel attention to improve the vanilla Mamba. In this way, our MambaIR takes advantage of the local pixel similarity and reduces the channel redundancy. Extensive experiments demonstrate the superiority of our method, for example, MambaIR outperforms SwinIR by up to 0.45dB on image SR, using similar computational cost but with a global receptive field. Code is available at url{https://github.com/csguoh/MambaIR}.																																	2025-08-07	PPRN:123160741		
J	Yao, Yao; Li, Zuchao; Zhao, Hai				Yao, Yao/OCL-4665-2025; Li, ZhiHua/GYU-9914-2022						Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models								Arxiv											2	2;2023-05-26;https://www.arxiv.org/abs/2305.16582v1| 1;2024-03-01;	arXiv:2305.16582			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 01 2024	2024	With the widespread use of language models (LMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. GoT adopts a two-stage framework with an additional GoT encoder for thought graph representation and fuses the graph representation with the original input representation through a gated fusion mechanism. We evaluate GoT's performance on a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59% using the T5-base model over the state-of-the-art Multimodal-CoT on the ScienceQA test set.																																	2025-11-07	PPRN:72728271		
J	Tu, Tao; Palepu, Anil; Schaekermann, Mike; Saab, Khaled; Freyberg, Jan; Tanno, Ryutaro; Wang, Amy; Li, Brenna; Amin, Mohamed; Tomasev, Nenad; Azizi, Shekoofeh; Singhal, Karan; Cheng, Yong; Hou, Le; Webson, Albert; Kulkarni, Kavita; Mahdavi, S Sara; Semturs, Christopher; Gottweis, Juraj; Barral, Joelle; Chou, Katherine; Corrado, Greg S; Matias, Yossi; Karthikesalingam, Alan; Natarajan, Vivek				Azizi, Shekoofeh/T-5465-2019; tu, tao/KVB-7209-2024; Natarajan, Vivek/AAG-4944-2021						Towards Conversational Diagnostic AI								Arxiv											1	1;2024-01-11;https://www.arxiv.org/abs/2401.05654v1	arXiv:2401.05654			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians’ expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue. AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE’s performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.																																	2024-01-26	PPRN:87128194		
J	Zhang, Renrui; Wei, Xinyu; Jiang, Dongzhi; Guo, Ziyu; Li, Shicheng; Zhang, Yichi; Tong, Chengzhuo; Liu, Jiaming; Zhou, Aojun; Wei, Bin; Zhang, Shanghang; Gao, Peng; Li, Chunyuan; Li, Hongsheng				Li, Hongsheng/AES-5328-2022; liu, jiaming/KVA-6603-2024; w, xy/JQW-1325-2023; Zhang, Lisa/AAW-9795-2021; guo, ziyu/IUQ-1178-2023; Zhang, Yichi/HMW-1766-2023; Zhang, Zhuosheng/AAF-4919-2020						MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine								Arxiv											2	2;2024-11-01;https://www.arxiv.org/abs/2407.08739v2| 1;2024-07-11;https://www.arxiv.org/abs/2407.08739v1	arXiv:2407.08739			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	The mathematical capabilities of Multi-modal Large Language Models (MLLMs) remain under-explored with three areas to be improved: visual encoding of math diagrams, diagram-language alignment, and chain-of-thought (CoT) reasoning. This draws forth an urgent demand for an effective training paradigm and a large-scale, comprehensive dataset with detailed CoT rationales, which is challenging to collect and costly to annotate manually. To tackle this issue, we propose MAVIS, a MAthematical VISual instruction tuning pipeline for MLLMs, featuring an automatic data engine to efficiently create mathematical visual datasets. We design the data generation process to be entirely independent of human intervention or GPT API usage, while ensuring the diagram-caption correspondence, question-answer correctness, and CoT reasoning quality. With this approach, we curate two datasets, MAVIS-Caption (558K diagram-caption pairs) and MAVIS-Instruct (834K visual math problems with CoT rationales), and propose four progressive stages for training MLLMs from scratch. First, we utilize MAVIS-Caption to fine-tune a math-specific vision encoder (CLIP-Math) through contrastive learning, tailored for improved diagram visual encoding. Second, we also leverage MAVIS-Caption to align the CLIP-Math with a large language model (LLM) by a projection layer, enhancing vision-language alignment in mathematical domains. Third, we adopt MAVIS-Instruct to perform the instruction tuning for robust problem-solving skills, and term the resulting model as MAVIS-7B. Fourth, we apply Direct Preference Optimization (DPO) to enhance the CoT capabilities of our model, further refining its step-wise reasoning performance.																																	2024-12-16	PPRN:90770165		
J	Li, Wentong; Yuan, Yuqian; Liu, Jian; Tang, Dongqi; Wang, Song; Qin, Jie; Zhu, Jianke; Zhang, Lei				Zhang, Lei/HSC-7058-2023						TokenPacker: Efficient Visual Projector for Multimodal LLM								Arxiv											4	4;2024-08-28;https://www.arxiv.org/abs/2407.02392v4| 3;2024-08-23;https://www.arxiv.org/abs/2407.02392v3| 2;2024-07-22;https://www.arxiv.org/abs/2407.02392v2| 1;2024-07-02;https://www.arxiv.org/abs/2407.02392v1	arXiv:2407.02392			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 28 2024	2024	The visual projector serves as an essential bridge between the visual encoder and the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs adopt a simple MLP to preserve all visual contexts via one-to-one transformation. However, the visual tokens are redundant and can be considerably increased when dealing with high-resolution images, impairing the efficiency of MLLMs significantly. Some recent works have introduced resampler or abstractor to reduce the number of resulting visual tokens. Unfortunately, they fail to capture finer details and undermine the visual reasoning capabilities of MLLMs. In this work, we propose a novel visual projector, which adopts a coarse-to-fine scheme to inject the enriched characteristics to generate the condensed visual tokens. In specific, we first interpolate the visual features as a low-resolution point query, providing the overall visual representation as the foundation. Then, we introduce a region-to-point injection module that utilizes high-resolution, multi-level region-based cues as fine-grained reference keys and values, allowing them to be fully absorbed within the corresponding local context region. This step effectively updates the coarse point query, transforming it into an enriched one for the subsequent LLM reasoning. Extensive experiments demonstrate that our approach compresses the visual tokens by 75%~89%, while achieves comparable or even better performance across diverse benchmarks with significantly higher efficiency. 																																	2024-09-06	PPRN:90669154		
J	Li, Chenxin; Liu, Xinyu; Li, Wuyang; Wang, Cheng; Liu, Hengyu; Liu, Yifan; Chen, Zhen; Yuan, Yixuan				Liu, hengyu/LFV-5401-2024; Yuan, Yixuan/KSL-8440-2024; Li, Chenxin/OZE-3193-2025; Liu, Xinyu/LGZ-8119-2024						U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation								Arxiv											1	1;2024-08-22;https://www.arxiv.org/abs/2406.02918v3	arXiv:2406.02918			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 22 2024	2024	U-Net has become a cornerstone in various visual applications such as image segmentation and diffusion probability models. While numerous innovative designs and improvements have been introduced by incorporating transformers or MLPs, the networks are still limited to linearly modeling patterns as well as the deficient interpretability. To address these challenges, our intuition is inspired by the impressive results of the Kolmogorov-Arnold Networks (KANs) in terms of accuracy and interpretability, which reshape the neural network learning via the stack of non-linear learnable activation functions derived from the Kolmogorov-Anold representation theorem. Specifically, in this paper, we explore the untapped potential of KANs in improving backbones for vision tasks. We investigate, modify and re-design the established U-Net pipeline by integrating the dedicated KAN layers on the tokenized intermediate representation, termed U-KAN. Rigorous medical image segmentation benchmarks verify the superiority of U-KAN by higher accuracy even with less computation cost. We further delved into the potential of U-KAN as an alternative U-Net noise predictor in diffusion models, demonstrating its applicability in generating task-oriented model architectures. These endeavours unveil valuable insights and sheds light on the prospect that with U-KAN, you can make strong backbone for medical image segmentation and generation. 																																	2024-08-31	PPRN:91505936		
J	Chen, Li; Wu, Penghao; Chitta, Kashyap; Jaeger, Bernhard; Geiger, Andreas; Li, Hongyang				Li, Hongyang/ABD-7455-2020; Jaeger, Bernhard/GQA-8874-2022; CHEN, Li/HIZ-5543-2022						End-to-end Autonomous Driving: Challenges and Frontiers								Arxiv											3	3;2024-08-15;https://www.arxiv.org/abs/2306.16927v3| 2;2024-04-22;https://www.arxiv.org/abs/2306.16927v2| 1;2023-06-29;https://www.arxiv.org/abs/2306.16927v1	arXiv:2306.16927			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 15 2024	2024	The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. We maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving. .																																	2024-08-23	PPRN:73643241		
J	Yang, Yijun; Xing, Zhaohu; Yu, Lequan; Huang, Chunwang; Fu, Huazhu; Zhu, Lei				Yu, Lequan/U-5377-2019; Huang, chunwang/AFN-9699-2022; Zhu, Lei/KUD-1330-2024; Fu, Huazhu/A-1411-2014						Vivim: a Video Vision Mamba for Medical Video Segmentation								Arxiv											4	4;2024-08-01;https://www.arxiv.org/abs/2401.14168v4| 3;2024-03-12;https://www.arxiv.org/abs/2401.14168v3| 2;2024-02-07;https://www.arxiv.org/abs/2401.14168v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.14168v1	arXiv:2401.14168			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 01 2024	2024	Medical video segmentation gains increasing attention in clinical practice due to the redundant dynamic references in video frames. However, traditional convolutional neural networks have a limited receptive field and transformer-based networks are mediocre in constructing long-term dependency from the perspective of computational complexity. This bottleneck poses a significant challenge when processing longer sequences in medical video analysis tasks using available devices with limited memory. Recently, state space models (SSMs), famous by Mamba, have exhibited impressive achievements in efficient long sequence modeling, which develops deep neural networks by expanding the receptive field on many vision tasks significantly. Unfortunately, vanilla SSMs failed to simultaneously capture causal temporal cues and preserve non-casual spatial information. To this end, this paper presents a Video Vision Mamba-based framework, dubbed as Vivim, for medical video segmentation tasks. Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales with our designed Temporal Mamba Block. We also introduce an improved boundary-aware affine constraint across frames to enhance the discriminative ability of Vivim on ambiguous lesions. Extensive experiments on thyroid segmentation, breast lesion segmentation in ultrasound videos, and polyp segmentation in colonoscopy videos demonstrate the effectiveness and efficiency of our Vivim, superior to existing methods. 																																	2024-08-08	PPRN:87330828		
J	Smith, James Seale; Hsu, Yen-Chang; Zhang, Lingyu; Hua, Ting; Kira, Zsolt; Shen, Yilin; Jin, Hongxia				Shen, Yilin/AAG-1724-2020; Hsu, Yen-Chang/AAB-7199-2022						Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA								Arxiv											2	2;2024-05-02;https://www.arxiv.org/abs/2304.06027v2| 1;2023-04-12;https://www.arxiv.org/abs/2304.06027v1	arXiv:2304.06027			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 02 2024	2024	Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., "person" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter costs and requires no storage of user data for replay. We show that C-LoRA not only outperforms several baselines for our proposed setting of text-to-image continual customization, which we refer to as Continual Diffusion, but that we achieve a new state-of-the-art in the well-established rehearsal-free continual learning setting for image classification. The high achieving performance of C-LoRA in two separate domains positions it as a compelling solution for a wide range of applications, and we believe it has significant potential for practical impact. 																																	2024-05-20	PPRN:58927225		
J	Xiong, Wei; Dong, Hanze; Ye, Chenlu; Wang, Ziqi; Zhong, Han; Ji, Heng; Jiang, Nan; Zhang, Tong				Wang, Ziqi/F-7535-2011; Ye, Chenlu/JUV-1872-2023; Zhang, Tong/HGC-1090-2022						Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint								Arxiv											4	4;2024-05-01;https://www.arxiv.org/abs/2312.11456v4| 3;2024-02-20;https://www.arxiv.org/abs/2312.11456v3| 2;2024-01-28;https://www.arxiv.org/abs/2312.11456v2| 1;2023-12-18;https://www.arxiv.org/abs/2312.11456v1	arXiv:2312.11456			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	This paper studies the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We first identify the primary challenges of existing popular methods like offline PPO and offline DPO as lacking in strategical exploration of the environment. Then, to understand the mathematical principle of RLHF, we consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid - and propose efficient algorithms with finite-sample theoretical guarantees. Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.																																	2024-05-19	PPRN:86680451		
J	Wang, Yancheng; Jiang, Ziyan; Chen, Zheng; Yang, Fan; Zhou, Yingxue; Cho, Eunah; Fan, Xing; Huang, Xiaojiang; Lu, Yanbin; Yang, Yingzhen				fan, xing/ONI-8865-2025; Yang, Yingzhen/AAU-6048-2020; Cho, Eun/AAV-3391-2020						RecMind: Large Language Model Powered Agent For Recommendation								Arxiv											3	3;2024-03-20;https://www.arxiv.org/abs/2308.14296v3| 2;2024-02-23;https://www.arxiv.org/abs/2308.14296v2| 1;2023-08-28;https://www.arxiv.org/abs/2308.14296v1	arXiv:2308.14296			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 20 2024	2024	While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.																																	2024-04-13	PPRN:84367493		
J	Zhao, Haozhe; Cai, Zefan; Si, Shuzheng; Ma, Xiaojian; An, Kaikai; Chen, Liang; Liu, Zixuan; Wang, Sheng; Han, Wenjuan; Chang, Baobao				Han, Wenjuan/ABD-7442-2020						MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning								Arxiv											2	2;2024-03-20;https://www.arxiv.org/abs/2309.07915v3| 1;2023-10-02;https://www.arxiv.org/abs/2309.07915v2	arXiv:2309.07915			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 20 2024	2024	Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing vision-language Model with Multi-Modal In-Context Learning(MMICL), a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. 																																	2024-04-12	PPRN:85349170		
J	Hong, Sirui; Lin, Yizhang; Liu, Bang; Liu, Bangbang; Wu, Binhao; Li, Danyang; Chen, Jiaqi; Zhang, Jiayi; Wang, Jinlin; Zhang, Li; Zhang, Lingyao; Yang, Min; Zhuge, Mingchen; Guo, Taicheng; Zhou, Tuo; Tao, Wei; Wang, Wenyi; Tang, Xiangru; Lu, Xiangtao; Zheng, Xiawu; Liang, Xinbing; Fei, Yaying; Cheng, Yuheng; Xu, Zongze; Wu, Chenglin				Guo, Taicheng/IUM-4515-2023; danyang, li/GLT-1067-2022; Zhang, Lingyao/KDP-2128-2024; Lin, Yizhang/KEZ-9895-2024; Tao, Wei/KGL-6588-2024; Zhou, Tuo/AAV-1246-2021; Zhang, Jia yi/HRA-8445-2023						Data Interpreter: An LLM Agent For Data Science								Arxiv											3	3;2024-03-12;https://www.arxiv.org/abs/2402.18679v3| 2;1800-01-01;https://www.arxiv.org/abs/2402.18679v2| 1;2024-02-28;https://www.arxiv.org/abs/2402.18679v1	arXiv:2402.18679			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 12 2024	2024	Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real -time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real -time data adaptability; 2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise; 3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT.																																	2024-04-12	PPRN:87991094		
J	He, Xiaoxin; Bresson, Xavier; Laurent, Thomas; Perold, Adam; LeCun, Yann; Hooi, Bryan				Hooi, Bryan/AAU-5707-2020; Laurent, Thomas/ABM-5202-2022						Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning								Arxiv											5	5;2024-03-07;https://www.arxiv.org/abs/2305.19523v5| 4;2024-02-28;https://www.arxiv.org/abs/2305.19523v4| 3;2023-10-23;https://www.arxiv.org/abs/2305.19523v3| 2;2023-10-06;https://www.arxiv.org/abs/2305.19523v2| 1;2023-05-31;https://www.arxiv.org/abs/2305.19523v1	arXiv:2305.19523			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data. 																																	2024-04-01	PPRN:72779351		
J	Alves, Duarte M.; Pombal, Jose; Guerreiro, Nuno M.; Martins, Pedro H.; Alves, Joao; Farajian, Amin; Peters, Ben; Rei, Ricardo; Fernandes, Patrick; Agrawal, Sweta; Colombo, Pierre; de Souza, Jose G.C.; Martins, Andre F.T.				Torres Martins, Andre Filipe/JXL-9782-2024; Alves, João/AAH-7435-2019						Tower: An Open Multilingual Large Language Model for Translation-Related Tasks								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17733v1	arXiv:2402.17733			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.																																	2024-03-27	PPRN:87922441		
J	Pound, Adam; Wardell, Barry				Pound, Adam/NNG-1984-2025						Black hole perturbation theory and gravitational self-force								Arxiv											1	1;2024-01-25;https://www.arxiv.org/abs/2101.04592v4	arXiv:2101.04592			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	Much of the success of gravitational-wave astronomy rests on perturbation theory. Historically, perturbative analysis of gravitational-wave sources has largely focused on post-Newtonian theory. However, strong-field perturbation theory is essential in many cases such as the quasinormal ringdown following the merger of a binary system, tidally perturbed compact objects, and extreme-mass-ratio inspirals. In this review, motivated primarily by small-mass-ratio binaries but not limited to them, we provide an overview of essential methods in (i) black hole perturbation theory, (ii) orbital mechanics in Kerr spacetime, and (iii) gravitational self-force theory. Our treatment of black hole perturbation theory covers most common methods, including the Teukolsky and Regge-Wheeler-Zerilli equations, methods of metric reconstruction, and Lorenz-gauge formulations, presenting them in a new consistent and self-contained form. Our treatment of orbital mechanics covers quasi-Keplerian and action-angle descriptions of bound geodesics and accelerated orbits, osculating geodesics, near-identity averaging transformations, multiscale expansions, and orbital resonances. Our summary of self-force theory's foundations is brief, covering the main ideas and results of matched asymptotic expansions, local expansion methods, puncture schemes, and point particle descriptions. We conclude by combining the above methods in a multiscale expansion of the perturbative Einstein equations, leading to adiabatic and post-adiabatic evolution schemes. Our presentation is intended primarily as a reference for practitioners but includes a variety of new results. In particular, we present the first complete post-adiabatic waveform-generation framework for generic (nonresonant) orbits in Kerr.																																	2024-04-03	PPRN:87335153		
J	Wang, Binghai; Zheng, Rui; Chen, Lu; Liu, Yan; Dou, Shihan; Huang, Caishuang; Shen, Wei; Jin, Senjie; Zhou, Enyu; Shi, Chenyu; Gao, Songyang; Xu, Nuo; Zhou, Yuhao; Fan, Xiaoran; Xi, Zhiheng; Zhao, Jun; Wang, Xiao; Ji, Tao; Yan, Hang; Shen, Lixing; Chen, Zhan; Gui, Tao; Zhang, Qi; Qiu, Xipeng; Huang, Xuanjing; Wu, Zuxuan; Jiang, Yu-Gang				Xi, Zhiheng/KUD-1665-2024; Ji, Tao/KIK-1554-2024; Zhou, Yuhao/ABC-4280-2022; Gui, Tao/LWI-6783-2024						Secrets of RLHF in Large Language Models Part II: Reward Modeling								Arxiv											2	2;2024-01-12;https://www.arxiv.org/abs/2401.06080v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.06080v1	arXiv:2401.06080			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 12 2024	2024	Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training. In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental results confirm that data with varying preference strengths have different impacts on reward model performance. We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data. (2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization. We have open-sourced the training code used in this report, the Anthropic’s HHRLHF dataset with preference strength information, and additionally, the validation set cleaned by GPT-4, which is used in our analysis experiments. All of these resources can be found on our project website1.																																	2024-01-31	PPRN:87124415		
J	Chen, Jiangjie; Wang, Xintao; Xu, Rui; Yuan, Siyu; Zhang, Yikai; Shi, Wei; Xie, Jian; Li, Shuang; Yang, Ruihan; Zhu, Tinghui; Chen, Aili; Li, Nianqi; Chen, Lida; Hu, Caiyu; Wu, Siye; Ren, Scott; Fu, Ziquan; Xiao, Yanghua				Zhu, Ting/LXW-0633-2024; chen, aili/AGH-2384-2022; Chen, Jiangjie/JCE-5486-2023; Yang, Ruihan/AAC-7500-2020; Xu, Rui/LRV-2470-2024						From Persona to Personalization: A Survey on Role-Playing Language Agents								Arxiv											2	2;2024-10-09;https://www.arxiv.org/abs/2404.18231v2| 1;2024-04-28;https://www.arxiv.org/abs/2404.18231v1	arXiv:2404.18231			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 09 2024	2024	Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.																																	2024-10-30	PPRN:88696492		
J	Yu, Wenhao; Zhang, Hongming; Pan, Xiaoman; Ma, Kaixin; Wang, Hongwei; Yu, Dong				Wang, Hongwei/HFT-3345-2022; Zhang, Hongming/ABF-8690-2021						Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models								Arxiv											2	2;2024-10-03;https://www.arxiv.org/abs/2311.09210v2| 1;2023-11-15;https://www.arxiv.org/abs/2311.09210v1	arXiv:2311.09210			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 03 2024	2024	Retrieval-augmented language model (RALM) represents a significant advancement in mitigating factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed, and the retrieval of irrelevant data can mislead the response generation. Moreover, standard RALMs frequently neglect their intrinsic knowledge due to the interference from retrieved information. In instances where the retrieved information is irrelevant, RALMs should ideally utilize their intrinsic knowledge or, in the absence of both intrinsic and retrieved knowledge, opt to respond with "unknown" to avoid hallucination. In this paper, we introduces C HAIN- OF-N OTE (C O N), a novel approach to improve robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of C O N is to generate sequential reading notes for each retrieved document, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. Our experimental results show that GPT-4, when equipped with C O N, outperforms the C HAIN- OF-T HOUGHT approach. Besides, we utilized GPT-4 to create 10K C O N data, subsequently trained on LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that fine-tuned RALMs equipped with C O N significantly outperform standard fine-tuned RALMs.																																	2024-10-17	PPRN:86160823		
J	Liu, Yang; Chen, Weixing; Bai, Yongjie; Liang, Xiaodan; Li, Guanbin; Gao, Wen; Lin, Liang				Chen, Weixing/LZF-3182-2025; Li, Xiaofeng/LIC-9574-2024; Li, Zhida/AFZ-6456-2022						Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI								Arxiv											6	6;2024-08-26;https://www.arxiv.org/abs/2407.06886v7| 5;2024-07-29;https://www.arxiv.org/abs/2407.06886v6| 4;2024-07-22;https://www.arxiv.org/abs/2407.06886v5| 3;2024-07-18;https://www.arxiv.org/abs/2407.06886v4| 2;2024-07-12;https://www.arxiv.org/abs/2407.06886v2| 1;2024-07-09;https://www.arxiv.org/abs/2407.06886v1	arXiv:2407.06886			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 26 2024	2024	Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. 																																	2024-09-04	PPRN:90751678		
J	Chen, Yuedong; Xu, Haofei; Zheng, Chuanxia; Zhuang, Bohan; Pollefeys, Marc; Geiger, Andreas; Cham, Tat-Jen; Cai, Jianfei				Chen, Yuedong/HNJ-1991-2023; ZHANG, JING/HKF-4837-2023						MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images								Arxiv											2	2;2024-07-18;https://www.arxiv.org/abs/2403.14627v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14627v1	arXiv:2403.14627			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 18 2024	2024	We introduce MVSplat, an efficient model that, given sparse multi-view images as input, predicts clean feed-forward 3D Gaussians. To accurately localize the Gaussian centers, we build a cost volume representation via plane sweeping, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We also learn other Gaussian primitives’ parameters jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussians via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, MVSplat achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). More impressively, compared to the latest stateof-the-art method pixelSplat, MVSplat uses 10× fewer parameters and infers more than 2× faster while providing higher appearance and geometry quality as well as better cross-dataset generalization.																																	2024-07-26	PPRN:88259985		
J	Jin, Mingyu; Yu, Qinkai; Dong, Shu; Zhao, Haiyan; Hua, Wenyue; Meng, Yanda; Zhang, Yongfeng; Du, Mengnan				Zhao, Haiyan/HGD-3793-2022; Zhang, Yongfeng/HMW-1599-2023; Meng, Yanda/IUQ-7187-2023; Yu, QINKAI/NVM-4884-2025; Du, Mengnan/MXL-9283-2025						The Impact of Reasoning Step Length on Large Language Models								Arxiv											3	3;2024-06-22;https://www.arxiv.org/abs/2401.04925v4| 2;2024-01-20;https://www.arxiv.org/abs/2401.04925v3| 1;2024-01-16;https://www.arxiv.org/abs/2401.04925v2	arXiv:2401.04925			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 22 2024	2024	Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs’ reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs’ potential in complex problem -solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task -dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences. The code is available at https://github.com/MingyuJ 666/The-Impact-of-Reasoning-Step-Len gth-on-Large-Language-Models																																	2024-07-12	PPRN:87171239		
J	Zhang, Di; Liu, Wei; Tan, Qian; Chen, Jingdan; Yan, Hang; Yan, Yuliang; Li, Jiatong; Huang, Weiran; Yue, Xiangyu; Ouyang, Wanli; Zhou, Dongzhan; Zhang, Shufei; Su, Mao; Zhong, Han-Sen; Li, Yuqiang				Zhou, Dongzhan/IXN-4421-2023; Su, Mao/LIC-8211-2024						ChemLLM: A Chemical Large Language Model								Arxiv											2	2;2024-04-25;https://www.arxiv.org/abs/2402.06852v2| 1;2024-02-10;https://www.arxiv.org/abs/2402.06852v1	arXiv:2402.06852			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 25 2024	2024	Large language models (LLMs) have made impressive progress in chemistry applications. However, the community lacks an LLM specifically designed for chemistry. The main challenges are two-fold: firstly, most chemical data and scientific knowledge are stored in structured databases, which limits the model's ability to sustain coherent dialogue when used directly. Secondly, there is an absence of objective and fair benchmark that encompass most chemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that features the first LLM dedicated to chemistry. It also includes ChemData, a dataset specifically designed for instruction tuning, and ChemBench, a robust benchmark covering nine essential chemistry tasks. ChemLLM is adept at performing various tasks across chemical disciplines with fluid dialogue interaction. Notably, ChemLLM achieves results comparable to GPT-4 on the core chemical tasks and demonstrates competitive performance with LLMs of similar size in general scenarios. ChemLLM paves a new path for exploration in chemical studies, and our method of incorporating structured chemical knowledge into dialogue systems sets a new standard for developing LLMs in various scientific fields. Codes, Datasets, and Model weights are publicly accessible at https://hf.co/AI4Chem																																	2024-05-04	PPRN:87640072		
J	Wei, Huawei; Yang, Zejun; Wang, Zhisheng				wang, zhisheng/MIK-3746-2025						AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation								Arxiv											1	1;2024-03-26;https://www.arxiv.org/abs/2403.17694v1	arXiv:2403.17694			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 26 2024	2024	In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at https://github.com/scutzzj/AniPortrait																																	2025-08-07	PPRN:123160829		
J	Nasiriany, Soroush; Xia, Fei; Yu, Wenhao; Xiao, Ted; Liang, Jacky; Dasgupta, Ishita; Xie, Annie; Driess, Danny; Wahid, Ayzaan; Xu, Zhuo; Vuong, Quan; Zhang, Tingnan; Lee, Tsang-Wei Edward; Lee, Kuang-Huei; Xu, Peng; Kirmani, Sean; Zhu, Yuke; Zeng, Andy; Hausman, Karol; Heess, Nicolas; Finn, Chelsea; Levine, Sergey; Ichter, Brian				Xia, Fei/AAW-8782-2021; Vuong, Quan-Hoang/F-2115-2010						PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs								Arxiv											1	1;2024-02-12;https://www.arxiv.org/abs/2402.07872v1	arXiv:2402.07872			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 12 2024	2024	Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task -specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real -world robotic navigation, real -world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero -shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet -Scale VLMs in robotic and spatial reasoning domains.																																	2024-05-25	PPRN:87636179		
J	Zhang, Jingyang; Yang, Jingkang; Wang, Pengyun; Wang, Haoqi; Lin, Yueqian; Zhang, Haoran; Sun, Yiyou; Du, Xuefeng; Li, Yixuan; Liu, Ziwei; Chen, Yiran; Li, Hai				Yang, Jingkang/HJZ-3689-2023; Li, Hai/L-8558-2017; Du, Xuefeng/JUV-2485-2023; Liu, Ziwei/AAG-6939-2021; Lin, Yueqian/KDH-2420-2024; Zhang, Jingyang/ACY-6379-2022						OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection								Arxiv											3	3;2024-12-16;https://www.arxiv.org/abs/2306.09301v5| 2;2024-09-24;https://www.arxiv.org/abs/2306.09301v4| 1;2023-06-15;https://www.arxiv.org/abs/2306.09301v1	arXiv:2306.09301			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 16 2024	2024	Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and scope. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate and standardized evaluation of OOD detection methodologies at large scale. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale data sets (ImageNet) and foundation models (e.g., CLIP and DINOv2), and expands its scope to investigate full-spectrum OOD detection which considers semantic and covariate distribution shifts at the same time. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool of OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims to drive advancements and offer a more robust and comprehensive evaluation benchmark for OOD detection research.																																	2025-01-25	PPRN:73358915		
J	Song, Yixin; Mi, Zeyu; Xie, Haotong; Chen, Haibo				Song, Yixin/IWV-3440-2023; Mi, Zeyu/NBY-3261-2025; Chen, Haibo/HCI-6124-2022						PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU								Arxiv											2	2;2024-12-12;https://www.arxiv.org/abs/2312.12456v2| 1;2023-12-16;https://www.arxiv.org/abs/2312.12456v1	arXiv:2312.12456			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 12 2024	2024	This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key principle underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. The evaluation shows that PowerInfer significantly outperforms llama.cpp by up to 11.69× while retaining model accuracy across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance comparable to that of a high-end server-grade A100 GPU, reaching 82% of its token generation rate on a single consumer-grade RTX 4090 GPU.																																	2025-01-20	PPRN:86737886		
J	Hagendorff, Thilo; Dasgupta, Ishita; Binz, Marcel; Chan, Stephanie C.Y.; Lampinen, Andrew; Wang, Jane X.; Akata, Zeynep; Schulz, Eric				Akata, Zeynep/I-6018-2016						MACHINE PSYCHOLOGY								Arxiv											4	4;2024-08-08;https://www.arxiv.org/abs/2303.13988v6| 3;2024-07-08;https://www.arxiv.org/abs/2303.13988v5| 2;2023-10-23;https://www.arxiv.org/abs/2303.13988v4| 1;2023-03-24;https://www.arxiv.org/abs/2303.13988v1	arXiv:2303.13988			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 08 2024	2024	Large language models (LLMs) show increasingly advanced emergent capabilities and are being incorporated across various societal domains. Understanding their behavior and reasoning abilities therefore holds significant importance. We argue that a fruitful direction for research is engaging LLMs in behavioral experiments inspired by psychology that have traditionally been aimed at understanding human cognition and behavior. In this article, we highlight and summarize theoretical perspectives, experimental paradigms, and computational analysis techniques that this approach brings to the table. It paves the way for a "machine psychology" for generative artificial intelligence (AI) that goes beyond performance benchmarks and focuses instead on computational insights that move us toward a better understanding and discovery of emergent abilities and behavioral patterns in LLMs. We review existing work taking this approach, synthesize best practices, and highlight promising future directions. We also highlight the important caveats of applying methodologies designed for understanding humans to machines. We posit that leveraging tools from experimental psychology to study AI will become increasingly valuable as models evolve to be more powerful, opaque, multi-modal, and integrated into complex real-world settings.																																	2024-08-17	PPRN:49975405		
J	Ye, Mingqiao; Danelljan, Martin; Yu, Fisher; Ke, Lei										Gaussian Grouping: Segment and Edit Anything in 3D Scenes								Arxiv											2	2;2023-12-01;https://www.arxiv.org/abs/2312.00732v1| 1;2024-07-01;	arXiv:2312.00732			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by Segment Anything Model (SAM), along with introduced 3D spatial consistency regularization. Compared to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization, style transfer and scene recomposition. 																																	2024-11-18	PPRN:86359508		
J	Xu, Rongwu; Qi, Zehan; Guo, Zhijiang; Wang, Cunxiang; Wang, Hongru; Zhang, Yue; Xu, Wei										Knowledge Conflicts for LLMs: A Survey								Arxiv											2	2;2024-06-22;https://www.arxiv.org/abs/2403.08319v2| 1;2024-03-13;https://www.arxiv.org/abs/2403.08319v1	arXiv:2403.08319			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 22 2024	2024	This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.																																	2024-07-15	PPRN:88132254		
J	Xie, Tinghao; Qi, Xiangyu; Zeng, Yi; Huang, Yangsibo; Sehwag, Udari Madhushani; Huang, Kaixuan; He, Luxi; Wei, Boyi; Li, Dacheng; Sheng, Ying; Jia, Ruoxi; Li, Bo; Li, Kai; Chen, Danqi; Henderson, Peter; Mittal, Prateek				He, Luxi/KBB-8984-2024; Huang, Kaixuan/GYR-0156-2022; Mittal, Prateek/ABB-2687-2021; Qi, Xiangyu/GZM-8733-2022						SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors								Arxiv											2	2;2025-03-01;https://www.arxiv.org/abs/2406.14598v2| 1;2024-06-20;https://www.arxiv.org/abs/2406.14598v1	arXiv:2406.14598			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with SORRY-Bench, our proposed benchmark. First, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 45 potentially unsafe topics, and 450 class-balanced unsafe instructions, compiled through human-in-the-loop methods. Second, linguistic characteristics and formatting of prompts are often overlooked, like different languages, dialects, and more -- which are only implicitly considered in many evaluations. We supplement SORRY-Bench with 20 diverse linguistic augmentations to systematically examine these effects. Third, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient manner.																																	2025-08-07	PPRN:89401015		
J	Staab, Robin; Vero, Mark; Balunovic, Mislav; Vechev, Martin										Beyond Memorization: Violating Privacy Via Inference with Large Language Models								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2310.07298v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07298v1	arXiv:2310.07298			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models’ inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals’ privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95% top-3 accuracy at a fraction of the cost (100×) and time (240×) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.																																	2024-05-25	PPRN:85540272		
J	Jin, Peng; Takanobu, Ryuichi; Zhang, Wancai; Cao, Xiaochun; Yuan, Li				Yuan, Li/AET-1324-2022; Jin, Peng/LFT-8054-2024						Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding								Arxiv											3	3;2024-04-05;https://www.arxiv.org/abs/2311.08046v3| 2;2024-03-21;https://www.arxiv.org/abs/2311.08046v2| 1;2023-11-14;https://www.arxiv.org/abs/2311.08046v1	arXiv:2311.08046			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 05 2024	2024	Large language models have demonstrated impressive universal capabilities across a wide range of open-ended tasks and have extended their utility to encompass multimodal conversations. However, existing methods encounter challenges in effectively handling both image and video understanding, particularly with limited visual tokens. In this work, we introduce Chat-UniVi, a Unified Vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation. Specifically, we employ a set of dynamic visual tokens to uniformly represent images and videos. This representation framework empowers the model to efficiently utilize a limited number of visual tokens to simultaneously capture the spatial details necessary for images and the comprehensive temporal relationship required for videos. Moreover, we leverage a multi-scale representation, enabling the model to perceive both high-level semantic concepts and low-level visual details. Notably, Chat-UniVi is trained on a mixed dataset containing both images and videos, allowing direct application to tasks involving both mediums without requiring any modifications. Extensive experimental results demonstrate that Chat-UniVi consistently outperforms even existing methods exclusively designed for either images or videos. 																																	2024-04-20	PPRN:86200591		
J	Da Silva, M.P.; Ryan-Anderson, C.; Svore, K.M.; Bello-Rivas, J. M.; Chernoguzov, A; Dreiling, J. M.; Foltz, C.; Frachon, F.; Gaebler, J. P.; Gatterman, T. M.; Grans-Samuelsson, L.; Hayes, D.; Hewitt, N.; Johansen, J.; Lucchetti, D.; Mills, M.; Moses, S. A.; Neyenhuis, B.; Paz, A.; Pino, J.; Siegfried, P.; Strabley, J.; Sundaram, A.; Tom, D.; Wernli, S. J.; Zanner, M.; Stutz, R. P; Svore, K. M.										Demonstration of logical qubits and repeated error correction with better-than-physical error rates								Arxiv											1	1;2024-04-04;https://www.arxiv.org/abs/2404.02280v2	arXiv:2404.02280			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	The promise of quantum computers hinges on the ability to scale to large system sizes, e.g., to run quantum computations consisting of more than 100 million operations fault-tolerantly. This in turn requires suppressing errors to levels inversely proportional to the size of the computation. As a step towards this ambitious goal, we present experiments on a trapped-ion QCCD processor where, through the use of fault-tolerant encoding and error correction, we are able to suppress logical error rates to levels below the physical error rates. In particular, we entangled logical qubits encoded in the [[7,1,3]] code with error rates 9.8 times to 500 times lower than at the physical level, and entangled logical qubits encoded in a [[12,2,4]] code with error rates 4.7 times to 800 times lower than at the physical level, depending on the judicious use of post-selection. Moreover, we demonstrate repeated error correction with the [[12,2,4]] code, with logical error rates below physical circuit baselines corresponding to repeated CNOTs, and show evidence that the error rate per error correction cycle, which consists of over 100 physical CNOTs, approaches the error rate of two physical CNOTs. These results signify an important transition from noisy intermediate scale quantum computing to reliable quantum computing, and demonstrate advanced capabilities toward large-scale fault-tolerant quantum computing.																																	2024-04-19	PPRN:88413982		
J	Hacohen, Yoav; Chiprut, Nisan; Brazowski, Benny; Shalem, Daniel; Moshe, Dudu; Richardson, Eitan; Levin, Eran; Shiran, Guy; Zabari, Nir; Gordon, Ori; Panet, Poriya; Weissbuch, Sapir; Kulikov, Victor; Bitterman, Yaki; Melumian, Zeev; Bibi, Ofir										LTX-Video: Realtime Video Latent Diffusion								Arxiv											1	1;2024-12-30;https://www.arxiv.org/abs/2501.00103v1	arXiv:2501.00103			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 30 2024	2024	We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer's input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.																																	2025-01-24	PPRN:120257891		
J	Yang, Yuqing; Chern, Ethan; Qiu, Xipeng; Neubig, Graham; Liu, Pengfei				Liu, Pengfei/JUV-0307-2023; Yuqing, Yang/ADJ-2720-2022						Alignment for Honesty								Arxiv											2	2;2024-10-28;https://www.arxiv.org/abs/2312.07000v2| 1;2023-12-12;https://www.arxiv.org/abs/2312.07000v1	arXiv:2312.07000			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 28 2024	2024	Recent research has made significant strides in aligning large language models (LLMs) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an LLM’s knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining “honesty” inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM’s honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. 																																	2024-12-06	PPRN:86555851		
J	Koh, Jing Yu; Mcaleer, Stephen; Fried, Daniel; Salakhutdinov, Ruslan										Tree Search for Language Model Agents								Arxiv											2	2;2024-10-12;https://www.arxiv.org/abs/2407.01476v2| 1;2024-07-01;https://www.arxiv.org/abs/2407.01476v1	arXiv:2407.01476			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 12 2024	2024	Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. 																																	2024-11-05	PPRN:90658821		
J	Zhang, Ce; Lu, Taixi; Islam, Md Mohaiminul; Wang, Ziyang; Yu, Shoubin; Bansal, Mohit; Bertasius, Gedas				Zhang, Ce/MEK-8968-2025; Bansal, Mohit/Q-9105-2016; Islam, Md Mohaiminul/KDN-0351-2024						A Simple LLM Framework for Long-Range Video Question-Answering								Arxiv											3	3;2024-10-10;https://www.arxiv.org/abs/2312.17235v3| 2;2024-02-26;https://www.arxiv.org/abs/2312.17235v2| 1;2023-12-28;https://www.arxiv.org/abs/2312.17235v1	arXiv:2312.17235			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 10 2024	2024	We present LLoVi, a language-based framework for long-range video question-answering (LVQA). Unlike prior long-range video understanding methods, which are often costly and require specialized long-range video modeling design (e.g., memory queues, state-space layers, etc.), our approach uses a frame/clip-level visual captioner (e.g., BLIP2, LaViLa, LLaVA) coupled with a Large Language Model (GPT-3.5, GPT-4) leading to a simple yet surprisingly effective LVQA framework. Specifically, we decompose short and long-range modeling aspects of LVQA into two stages. First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8s in length) densely sampled from a long input video. Afterward, an LLM aggregates the densely extracted short-term captions to perform long-range temporal reasoning needed to understand the whole video and answer a question. To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our system. Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance. Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost. On EgoSchema, which is best known as a very long-form video question-answering benchmark, our method achieves 50.3% accuracy, outperforming the previous best-performing approach by 18.1% (absolute gain). In addition, our approach outperforms the previous state-of-the-art by 4.1% and 3.1% on NeXT-QA and IntentQA. We also extend LLoVi to grounded LVQA and show that it outperforms all prior methods on the NeXT-GQA dataset. 																																	2024-11-01	PPRN:86851333		
J	Zhu, Jiayuan; Qi, Yunli; Wu, Junde				Zhu, Jiayuan/JAO-3713-2023; 王, 仁杰/IUO-6250-2023						Medical SAM 2: Segment medical images as video via Segment Anything Model 2								Arxiv											1	1;2024-08-01;https://www.arxiv.org/abs/2408.00874v1	arXiv:2408.00874			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 01 2024	2024	In this paper, we introduce Medical SAM 2 (MedSAM-2), an advanced segmentation model that utilizes the SAM 2 framework to address both 2D and 3D medical image segmentation tasks. By adopting the philosophy of taking medical images as videos, MedSAM-2 not only applies to 3D medical images but also unlocks new One-prompt Segmentation capability. That allows users to provide a prompt for just one or a specific image targeting an object, after which the model can autonomously segment the same type of object in all subsequent images, regardless of temporal relationships between the images. We evaluated MedSAM-2 across a variety of medical imaging modalities, including abdominal organs, optic discs, brain tumors, thyroid nodules, and skin lesions, comparing it against state-of-the-art models in both traditional and interactive segmentation settings. Our findings show that MedSAM-2 not only surpasses existing models in performance but also exhibits superior generalization across a range of medical image segmentation tasks. 																																	2024-08-08	PPRN:91225082		
J	Wu, Haoning; Li, Dongxu; Chen, Bei; Li, Junnan				LI, DONGXU/GNM-6998-2022						LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding								Arxiv											1	1;2024-07-22;https://www.arxiv.org/abs/2407.15754v1	arXiv:2407.15754			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 22 2024	2024	Large multimodal models (LMMs) are processing increasingly longer and richer inputs. Albeit the progress, few public benchmark is available to measure such development. To mitigate this gap, we introduce LongVideoBench, a question-answering benchmark that features video-language interleaved inputs up to an hour long. Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding. To achieve this, we interpret the primary challenge as to accurately retrieve and reason over detailed multimodal information from long inputs. As such, we formulate a novel video question-answering task termed referring reasoning. Specifically, as part of the question, it contains a referring query that references related video contexts, called referred context. The model is then required to reason over relevant video details from the referred context. Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding. Evaluations suggest that the LongVideoBench presents significant challenges even for the most advanced proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their open-source counterparts show an even larger performance gap. In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LongVideoBench as a valuable benchmark for evaluating future-generation long-context LMMs.																																	2024-07-27	PPRN:91020565		
J	Zhang, Zhexin; Yang, Junxiao; Ke, Pei; Mi, Fei; Wang, Hongning; Huang, Minlie				Wang, Hongning/GPK-7527-2022						Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization								Arxiv											2	2;2024-06-12;https://www.arxiv.org/abs/2311.09096v2| 1;2023-11-15;https://www.arxiv.org/abs/2311.09096v1	arXiv:2311.09096			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 12 2024	2024	While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs' capability and safety. 																																	2024-07-10	PPRN:86172698		
J	Chen, Guangyao; Dong, Siwei; Shu, Yu; Zhang, Ge; Sesay, Jaward; Karlsson, Borje F.; Fu, Jie; Shi, Yemin				Chen, Guangyao/ABF-2152-2021; Zhang, Ge/N-4150-2013; Dong, Siwei/AFX-5505-2022; Karlsson, Börje/B-4046-2010						AutoAgents: A Framework for Automatic Agent Generation								Arxiv											3	3;2024-04-29;https://www.arxiv.org/abs/2309.17288v3| 2;2023-10-15;https://www.arxiv.org/abs/2309.17288v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17288v1	arXiv:2309.17288			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 29 2024	2024	Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that AutoAgents generates more coherent and accurate solutions than the existing multi-agent methods. This underscores the significance of assigning different roles to different tasks and of team cooperation, offering new perspectives for tackling complex tasks.																																	2024-05-18	PPRN:85339482		
J	Pan, Xichen; Dong, Li; Huang, Shaohan; Peng, Zhiliang; Chen, Wenhu; Wei, Furu				Peng, Zhiliang/JEP-1536-2023; Huang, Shaohan/LDF-3300-2024						Kosmos-G: Generating Images in Context with Multimodal Large Language Models								Arxiv											3	3;2024-04-26;https://www.arxiv.org/abs/2310.02992v3| 2;2024-03-15;https://www.arxiv.org/abs/2310.02992v2| 1;2023-10-04;https://www.arxiv.org/abs/2310.02992v1	arXiv:2310.02992			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 26 2024	2024	Recent advancements in subject -driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test -time tuning and cannot accept interleaved multi -image and text input. These limitations keep them far from the ultimate goal of “image as a foreign language in image generation.” This paper presents KOSMOS-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. KOSMOS-G demonstrates an impressive capability of zero -shot subject -driven generation with interleaved multi -image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U -Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit KOSMOS-G as an initial attempt towards the goal of “image as a foreign language in image generation.” The code can be found at https://aka.ms/Kosmos-G																																	2024-05-09	PPRN:85397929		
J	Zhang, Yadong; Mao, Shaoguang; Ge, Tao; Wang, Xun; de Wynter, Adrian; Xia, Yan; Wu, Wenshan; Song, Ting; Lan, Man; Wei, Furu										LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2404.01230v1	arXiv:2404.01230			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements. [GRAPHICS]																																	2024-04-18	PPRN:88367711		
J	Lynch, Aengus; Guo, Phillip; Ewart, Aidan; Casper, Stephen; Hadfield-Menell, Dylan										Eight Methods to Evaluate Robust Unlearning in LLMs								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2402.16835v1	arXiv:2402.16835			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich (2023). While WHP's unlearning generalizes well when evaluated with the "Familiarity" metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.																																	2024-11-09	PPRN:87890368		
J	Zhu, Yichen; Zhu, Minjie; Liu, Ning; Ou, Zhicai; Mou, Xiaofeng; Tang, Jian				Mou, Xiaofeng/NWH-9186-2025; Zhu, Minjie/JTV-1498-2023; Liu, Ning/JDW-4796-2023						LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model								Arxiv											3	3;2024-02-22;https://www.arxiv.org/abs/2401.02330v4| 2;2024-01-15;https://www.arxiv.org/abs/2401.02330v2| 1;2024-01-04;https://www.arxiv.org/abs/2401.02330v1	arXiv:2401.02330			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	In this paper, we introduce LLaVA-$phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and interaction, while maintaining greater resource efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.																																	2024-03-21	PPRN:86970666		
J	Fu, Yao; Panda, Rameswar; Niu, Xinyao; Yue, Xiang; Hajishirzi, Hannaneh; Kim, Yoon; Peng, Hao				Panda, Rameswar/AAY-9834-2020						Data Engineering for Scaling Language Models to 128K Context								Arxiv											1	1;2024-02-15;https://www.arxiv.org/abs/2402.10171v1	arXiv:2402.10171			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 15 2024	2024	We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the textit{quantity} and textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize textit{domain balance} and textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.																																	2024-03-13	PPRN:87704048		
J	Xu, Haoran; Kim, Young Jin; Sharaf, Amr; Awadalla, Hany Hassan				Xu, Haoran/AEW-7367-2022						A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2309.11674v2	arXiv:2309.11674			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder -decoder translation models. Previous studies have attempted to improve the translation capabilities of these LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two finetuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high -quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model -based trAnslator (ALMA). Based on LLaMA-2 (Touvron et al., 2023b) as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero -shot performance across 10 translation directions from the WMT’21 (2 directions) and WMT’22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model (NLLB TEAM et al., 2022) and GPT3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation. 1																																	2024-05-25	PPRN:86280999		
J	Wang, Chloe; Tsepa, Oleksii; Ma, Jun; Wang, Bo				Wang, Bo/HDO-6738-2022; Ma, Jun/LHA-0128-2024						Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces								Arxiv											1	1;2024-02-01;https://www.arxiv.org/abs/2402.00789v1	arXiv:2402.00789			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 01 2024	2024	Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic -based graph subsampling, which falls short in data -dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non -sequential graph data presents a notable challenge. In this work, we introduce GraphMamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input -dependent node selection mechanism. Specifically, we formulate graph -centric node prioritization and permutation strategies to enhance context -aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that GraphMamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/ bowang-lab/Graph-Mamba.																																	2024-02-18	PPRN:87454936		
J	Zheng, Zibin; Ning, Kaiwen; Wang, Yanlin; Zhang, Jingwen; Zheng, Dewu; Ye, Mingxi; Chen, Jiachi				Chen, Jiachi/HOC-4256-2023; Zhang, Jingwen/AAQ-3561-2020						A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends								Arxiv											2	2;2024-01-08;https://www.arxiv.org/abs/2311.10372v2| 1;2023-11-17;https://www.arxiv.org/abs/2311.10372v1	arXiv:2311.10372			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 08 2024	2024	General large language models (LLMs), represented by ChatGPT, have demonstrated significant potential in software engineering tasks such as code generation. This led to the development of specialized LLMs for software engineering, called Code LLMs. Further, Code LLMs are often derived from general LLMs through fine-tuning and their performance can be influenced by the base LLMs. However, there is a lack of systematic investigation into Code LLMs. In this study, we conduct a comprehensive survey of Code LLMs to address three questions: (1) What LLMs are specifically designed for software engineering tasks, and their relationship? (2) Do Code LLMs outperform general LLMs in software engineering tasks? (3) Which LLMs are more proficient in different software engineering tasks? To answer these questions, we first collect relevant literature and work from four databases and categorize Code LLMs based on their publishers. Next, we investigate the performance differences between general LLMs and Code LLMs in software engineering tasks to demonstrate future trends. Finally, we comprehensively maintained the performance of LLMs to identify the best-performing LLMs for each software engineering task. Our research helps researchers understand the evolution and performance of Code LLMs and provides insights for practitioners to improve Code LLMs.																																	2024-05-25	PPRN:86200285		
J	Li, Zhiyuan; Liu, Hong; Zhou, Denny; Ma, Tengyu				Li, Zhiyuan/GWM-6492-2022; Ma, Tengyu/D-9086-2017						Chain of Thought Empowers Transformers to Solve Inherently Serial Problems								Arxiv											3	3;2024-09-21;https://www.arxiv.org/abs/2402.12875v4| 2;2024-05-23;https://www.arxiv.org/abs/2402.12875v3| 1;2024-02-20;https://www.arxiv.org/abs/2402.12875v1	arXiv:2402.12875			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 21 2024	2024	Instructing the model to generate a sequence of intermediate steps, a.k.a. a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length n , previous works have shown that constant-depth transformers with finite precision poly (n) embedding size can only solve problems in TC0 without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in AC0, a proper subset of TC0. However, with T steps of CoT, constant-depth transformers using constant-bit precision and O (log n) embedding size can solve any problem solvable by boolean circuits of size T. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.																																	2024-10-07	PPRN:87772452		
J	Ye, Vickie; Li, Ruilong; Kerr, Justin; Turkulainen, Matias; Yi, Brent; Pan, Zhuoyang; Seiskari, Otto; Ye, Jianbo; Hu, Jeffrey; Tancik, Matthew; Kanazawa, Angjoo										gsplat: An Open-Source Library for Gaussian Splatting								Arxiv											1	1;2024-09-10;https://www.arxiv.org/abs/2409.06765v1	arXiv:2409.06765			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 10 2024	2024	gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. 																																	2024-09-27	PPRN:91834685		
J	Zhang, Kechi; Li, Jia; Li, Ge; Shi, Xianjie; Jin, Zhi				Jin, Zhi/AAB-2440-2022						CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges								Arxiv											2	2;2024-08-09;https://www.arxiv.org/abs/2401.07339v2| 1;2024-01-14;https://www.arxiv.org/abs/2401.07339v1	arXiv:2401.07339			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 09 2024	2024	Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these real-world repo-level code generation, we present C ODE A GENT , a novel LLM-based agent framework that employs external tools for effective repo-level code generation. C ODE A GENT integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools’ usage. To the best of our knowledge, CODE AGENT is the first agent framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we design a repo-level benchmark CODE AGENT BENCH . The performance on this benchmark shows a significant improvement brought by our method, with improvements in pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CODE AGENT ’s adaptability and efficacy across various code generation tasks. Notably, CODE  GENT outperforms commercial products like GitHub Copilot, showcasing superior accuracy and efficiency. These results demonstrate C ODE A GENT ’s robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.																																	2024-08-21	PPRN:87196426		
J	Wang, Tianlu; Kulikov, Ilia; Golovneva, Olga; Yu, Ping; Yuan, Weizhe; Dwivedi-Yu, Jane; Pang, Richard Yuanzhe; Fazel-Zarandi, Maryam; Weston, Jason; Li, Xian				Wang, Tianlu/GQA-3126-2022						Self-Taught Evaluators								Arxiv											2	2;2024-08-08;https://www.arxiv.org/abs/2408.02666v2| 1;2024-08-05;https://www.arxiv.org/abs/2408.02666v1	arXiv:2408.02666			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Aug 08 2024	2024	Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.																																	2024-08-21	PPRN:91246552		
J	Jin, Haolin; Huang, Linghan; Cai, Haipeng; Yan, Jun; Li, Bo; Chen, Huaming				Yan, J./AGF-3303-2022; Chen, Huaming/AAE-2537-2019; Cai, Haipeng/KYR-4204-2024						From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future								Arxiv											1	1;2024-08-05;https://www.arxiv.org/abs/2408.02479v1	arXiv:2408.02479			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 05 2024	2024	With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.																																	2024-08-09	PPRN:91245543		
J	Alizadeh, Keivan; Mirzadeh, Iman; Belenko, Dmitry; Khatamifard, S. Karen; Cho, Minsik; Del Mundo, Carlo C; Rastegari, Mohammad; Farajtabar, Mehrdad				Cho, Minsik/OGR-1803-2025						LLM <italic>in a flash</italic>: <br>Efficient Large Language Model Inference with Limited Memory								Arxiv											3	3;2024-07-30;https://www.arxiv.org/abs/2312.11514v3| 2;2024-01-04;https://www.arxiv.org/abs/2312.11514v2| 1;2023-12-12;https://www.arxiv.org/abs/2312.11514v1	arXiv:2312.11514			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 30 2024	2024	Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.																																	2024-08-09	PPRN:86721948		
J	Cherti, Mehdi; Beaumont, Romain; Wightman, Ross; Wortsman, Mitchell; Ilharco, Gabriel; Gordon, Cade; Schuhmann, Christoph; Schmidt, Ludwig; Jitsev, Jenia										Reproducible scaling laws for contrastive language-image learning								Arxiv											2	2;2024-07-13;https://www.arxiv.org/abs/2212.07143v2| 1;2022-12-14;https://www.arxiv.org/abs/2212.07143v1	arXiv:2212.07143			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 13 2024	2024	Scaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data & models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to ensure reproducibility and make scaling laws research more accessible. 																																	2024-07-23	PPRN:35866156		
J	Zawalski, Michal; Chen, William; Pertsch, Karl; Mees, Oier; Finn, Chelsea; Levine, Sergey										Robotic Control via Embodied Chain-of-Thought Reasoning								Arxiv											1	1;2024-07-11;https://www.arxiv.org/abs/2407.08693v1	arXiv:2407.08693			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 11 2024	2024	A key limitation of learned robot control policies is their inability to generalize outside their training data. Recent works on vision-language-action models (VLAs) have shown that the use of large, internet pre-trained vision-language models as the backbone of learned robot policies can substantially improve their robustness and generalization ability. Yet, one of the most exciting capabilities of large vision-language models in other domains is their ability to reason iteratively through complex problems. Can that same capability be brought into robotics to allow policies to improve performance by reasoning about a given task before acting? Naive use of “chain-of-thought” (CoT) style prompting is significantly less effective with standard VLAs because of the relatively simple training examples that are available to them. Additionally, the purely-semantic reasoning about subtasks common to regular CoT is insufficient for robot policies that need to ground their reasoning in sensory observations and the robot state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. We design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We demonstrate that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source VLA policy, by 28% across challenging generalization tasks without any additional robot training data. Additionally, ECoT makes it easier for humans to interpret a policy’s failures and correct its behavior interactively using natural language. Finally, we show that our model learns to transfer ECoT reasonings to unseen embodiments and tasks.																																	2024-07-23	PPRN:90770716		
J	Xu, Mingwang; Li, Hui; Su, Qingkun; Shang, Hanlin; Zhang, Liwei; Liu, Ce; Wang, Jingdong; Yao, Yao; Zhu, Siyu				Zhu, Siyu/ISB-3310-2023; Shang, Han/J-6155-2019						Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation								Arxiv											2	2;2024-06-16;https://www.arxiv.org/abs/2406.08801v2| 1;2024-06-13;https://www.arxiv.org/abs/2406.08801v1	arXiv:2406.08801			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 16 2024	2024	The field of portrait image animation, driven by speech audio input, has experienced significant advancements in the generation of realistic and dynamic portraits. This research delves into the complexities of synchronizing facial movements and creating visually appealing, temporally consistent animations within the framework of diffusion-based methodologies. Moving away from traditional paradigms that rely on parametric models for intermediate facial representations, our innovative approach embraces the end-to-end diffusion paradigm and introduces a hierarchical audio-driven visual synthesis module to enhance the precision of alignment between audio inputs and visual outputs, encompassing lip, expression, and pose motion. Our proposed network architecture seamlessly integrates diffusion-based generative models, a UNet-based denoiser, temporal alignment techniques, and a reference network. The proposed hierarchical audio-driven visual synthesis offers adaptive control over expression and pose diversity, enabling more effective personalization tailored to different identities. Through a comprehensive evaluation that incorporates both qualitative and quantitative analyses, our approach demonstrates obvious enhancements in image and video quality, lip synchronization precision, and motion diversity. Further visualization and access to the source code can be found at: https://fudan-generative-vision.github.io/hallo .																																	2024-07-04	PPRN:89301704		
J	Zhang, Jun; Wang, Jue; Li, Huan; Shou, Lidan; Chen, Ke; Chen, Gang; Mehrotra, Sharad				Chen, Ang/HII-3354-2022						Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding								Arxiv											2	2;2024-05-20;https://www.arxiv.org/abs/2309.08168v2| 1;2023-09-15;https://www.arxiv.org/abs/2309.08168v1	arXiv:2309.08168			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 20 2024	2024	We present a novel inference scheme, selfspeculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two -stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting. Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM. Moreover, the proposed method requires no additional neural network training and no extra memory footprint, making it a plug -and -play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its variants demonstrated a speedup up to 1.99×.1 																																	2024-06-01	PPRN:85091453		
J	Lin, Ji; Yin, Hongxu; Ping, Wei; Lu, Yao; Molchanov, Pavlo; Tao, Andrew; Mao, Huizi; Kautz, Jan; Shoeybi, Mohammad; Han, Song				Han, Song/AAR-9464-2020; Yin, Hongxu/AAZ-3328-2020; Mao, Huizi/AAM-1635-2021						VILA: On Pre-training for Visual Language Models								Arxiv											4	4;2024-05-16;https://www.arxiv.org/abs/2312.07533v4| 3;2024-03-05;https://www.arxiv.org/abs/2312.07533v3| 2;2023-12-14;https://www.arxiv.org/abs/2312.07533v2| 1;2023-12-12;https://www.arxiv.org/abs/2312.07533v1	arXiv:2312.07533			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 16 2024	2024	Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge.																																	2024-06-01	PPRN:86556401		
J	Zhai, Jiaqi; Liao, Lucy; Liu, Xing; Wang, Yueming; Li, Rui; Cao, Xuan; Gao, Leon; Gong, Zhaojie; Gu, Fangda; He, Michael; Lu, Yinghai; Shi, Yu				Wang, Yueming/AGN-4764-2022						Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations								Arxiv											3	3;2024-05-06;https://www.arxiv.org/abs/2402.17152v3| 2;2024-04-18;https://www.arxiv.org/abs/2402.17152v2| 1;2024-02-27;https://www.arxiv.org/abs/2402.17152v1	arXiv:2402.17152			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 06 2024	2024	Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework ("Generative Recommenders"), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.																																	2024-05-24	PPRN:87918602		
J	Chen, Pengguang; Liu, Shu; Zhao, Hengshuang; Wang, Xingquan; Jia, Jiaya				Jia, Jiaya/I-3251-2012; Wang, Xing Quan/IUP-5087-2023						GridMask Data Augmentation								Arxiv											2	2;2024-02-01;https://www.arxiv.org/abs/2001.04086v3| 1;2020-01-14;https://www.arxiv.org/abs/2001.04086v2	arXiv:2001.04086			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 01 2024	2024	We propose a novel data augmentation method `GridMask' in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to find the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.																																	2024-05-25	PPRN:22593632		
J	Luo, Weidi; Ma, Siyuan; Liu, Xiaogeng; Guo, Xiaoyu; Xiao, Chaowei				Ma, Siyuan/IVV-8174-2023; Liu, Xiaogeng/KIJ-1671-2024; Xiao, Chaowei/AAT-8772-2021						JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks								Arxiv											4	4;2024-11-24;https://www.arxiv.org/abs/2404.03027v4| 3;2024-07-03;https://www.arxiv.org/abs/2404.03027v3| 2;2024-04-18;https://www.arxiv.org/abs/2404.03027v2| 1;2024-04-03;https://www.arxiv.org/abs/2404.03027v1	arXiv:2404.03027			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 24 2024	2024	With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K 1 , a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2,000 malicious queries that are also proposed in this paper, we generate 20,000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8,000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28,000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open- source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.																																	2025-01-08	PPRN:88414302		
J	Yang, Qian; Xu, Jin; Liu, Wenrui; Chu, Yunfei; Jiang, Ziyue; Zhou, Xiaohuan; Leng, Yichong; Lv, Yuanjun; Zhao, Zhou; Zhou, Chang; Zhou, Jingren				Chu, Yunfei/C-8002-2013; jiang, ziyue/GSI-9122-2022; Yang, Qian/T-1026-2017; Liu, Wenrui/NFS-3756-2025; Zhou, Mingyuan/AAE-8717-2021						AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension								Arxiv											2	2;2024-07-26;https://www.arxiv.org/abs/2402.07729v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.07729v1	arXiv:2402.07729			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 26 2024	2024	Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as automatic speech recognition, and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (Audio InstRuction Benchmark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIRBench encompasses two dimensions: foundation and chat benchmarks. The former consists of 19 tasks with approximately 19k singlechoice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended questionand-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.																																	2024-08-02	PPRN:87638454		
J	Wang, Qianqian; Ye, Vickie; Gao, Hang; Austin, Jake; Li, Zhengqi; Kanazawa, Angjoo				Li, Zhengqi/AEO-7180-2022						Shape of Motion: 4D Reconstruction from a Single Video								Arxiv											1	1;2024-07-18;https://www.arxiv.org/abs/2407.13764v1	arXiv:2407.13764			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 18 2024	2024	Monocular dynamic reconstruction is a challenging and longstanding vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. In this work, we introduce a method capable of reconstructing generic dynamic scenes, featuring explicit, full-sequence-long 3D motion, from casually captured monocular videos. We tackle the under-constrained nature of the problem with two key insights: First, we exploit the lowdimensional structure of 3D motion by representing scene motion with a compact set of SE(3) motion bases. Each point’s motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we utilize a comprehensive set of data-driven priors, including monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes. Project Page: shape-of-motion.github.io																																	2024-07-26	PPRN:90886108		
J	Gou, Yunhao; Liu, Zhili; Chen, Kai; Hong, Lanqing; Xu, Hang; Li, Aoxue; Jiang, Xin; Li, Zhenguo; Yeung, Dit-Yan; Kwok, James T.; Zhang, Yu				LI, ZHUOLING/KHE-1368-2024; zhang, Shifeng/HPH-0217-2023; Li, Jiaqi/HHN-8236-2022; Liu, ZhiLi/MSV-7152-2025						Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning								Arxiv											5	5;2024-07-04;https://www.arxiv.org/abs/2312.12379v5| 4;2024-03-22;https://www.arxiv.org/abs/2312.12379v4| 3;2024-02-08;https://www.arxiv.org/abs/2312.12379v3| 2;2024-01-18;https://www.arxiv.org/abs/2312.12379v2| 1;2023-12-19;https://www.arxiv.org/abs/2312.12379v1	arXiv:2312.12379			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 04 2024	2024	Instruction tuning of Large Vision-language Models (LVLMs) has revolutionized the development of versatile models with zero-shot generalization across a wide range of downstream vision-language tasks. However, the diversity of training tasks of different sources and formats would lead to inevitable task conflicts, where different tasks conflict for the same set of model parameters, resulting in sub-optimal instruction-following abilities. To address that, we propose the Mixture of Cluster-conditional LoRA Experts (MoCLE), a novel Mixture of Experts (MoE) architecture designed to activate the task-customized model parameters based on the instruction clusters. A separate universal expert is further incorporated to improve generalization capabilities of MoCLE for novel instructions. Extensive experiments on InstructBLIP and LLaVA demonstrate the effectiveness of MoCLE.																																	2024-07-20	PPRN:86725592		
J	Liu, Qidong; Wu, Xian; Zhao, Xiangyu; Zhu, Yuanshao; Xu, Derong; Tian, Feng; Zheng, Yefeng				Wu, Xian/JRW-5738-2023; ZHU, Yuanshao/GYE-1908-2022; Zhao, Xiangyu/AAO-2203-2020; Liu, Qidong/GNM-7428-2022; Zheng, Yefeng/ABG-7053-2020						When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications								Arxiv											2	2;2024-05-31;https://www.arxiv.org/abs/2310.18339v2| 1;2023-10-21;https://www.arxiv.org/abs/2310.18339v1	arXiv:2310.18339			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 31 2024	2024	The recent surge in Large Language Models (LLMs) has garnered significant attention across numerous fields. Fine-tuning is often required to fit general LLMs for a specific domain, like the web-based healthcare system. However, two problems arise during fine-tuning LLMs for medical applications. One is the task variety problem, which involves distinct tasks in real-world medical scenarios. The variety often leads to sub-optimal fine-tuning for data imbalance and seesaw problems. Besides, the large amount of parameters in LLMs leads to huge time and computation consumption by fine-tuning. To address these two problems, we propose a novel parameter efficient fine-tuning framework for multi-task medical applications, dubbed as MOELoRA. The designed framework aims to absorb both the benefits of mixture-of-expert (MOE) for multi-task learning and low-rank adaptation (LoRA) for parameter efficient fine-tuning. For unifying MOE and LoRA, we devise multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to retain the small size of trainable parameters. Then, a task-motivated gate function for all MOELoRA layers is proposed, which can control the contributions of each expert and produce distinct parameters for various tasks. We conduct experiments on a multi-task medical dataset, indicating MOELoRA outperforms the existing parameter efficient fine-tuning methods. The code is available online 1.																																	2024-11-20	PPRN:85889418		
J	Ying, Kaining; Meng, Fanqing; Wang, Jin; Li, Zhiqian; Lin, Han; Yang, Yue; Zhang, Hao; Zhang, Wenbo; Lin, Yuqi; Liu, Shuo; Lei, Jiayi; Lu, Quanfeng; Chen, Runjian; Xu, Peng; Zhang, Renrui; Zhang, Haozhe; Gao, Peng; Wang, Yali; Qiao, Yu; Luo, Ping; Zhang, Kaipeng; Shao, Wenqi				zhang, wenbo/GWV-7136-2022; Li, Zhiqian/NPI-7315-2025; Qiao, Yu/ABD-5787-2021; Luo, Ping/HGE-7623-2022; Gao, Peng/B-4675-2012; Zhang, Zhuosheng/AAF-4919-2020; Meng, fanqing/AAE-7775-2022						MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI								Arxiv											1	1;2024-04-24;https://www.arxiv.org/abs/2404.16006v1	arXiv:2404.16006			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 24 2024	2024	Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering 32 core meta-tasks and 162 subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving 30 LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.																																	2024-05-04	PPRN:88634329		
J	Huang, Qian; Vora, Jian; Liang, Percy; Leskovec, Jure										MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation								Arxiv											3	3;2024-04-14;https://www.arxiv.org/abs/2310.03302v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03302v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03302v1	arXiv:2310.03302			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 14 2024	2024	A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination. Our code is released at https://github.com/snap-stanford/MLAgentBench.																																	2024-04-25	PPRN:85435860		
J	Xue, Fuzhao; Zheng, Zian; Fu, Yao; Ni, Jinjie; Zheng, Zangwei; Zhou, Wangchunshu; You, Yang				Xue, Fuzhao/GLT-5176-2022; Wang, Benyou/Y-5146-2019; Zheng, Zangwei/HMD-6292-2023						OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models								Arxiv											2	2;2024-03-27;https://www.arxiv.org/abs/2402.01739v2| 1;2024-01-29;https://www.arxiv.org/abs/2402.01739v1	arXiv:2402.01739			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 27 2024	2024	To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.3																																	2024-04-15	PPRN:87522419		
J	Zhong, Yinmin; Liu, Shengyu; Chen, Junda; Hu, Jianbo; Zhu, Yibo; Liu, Xuanzhe; Jin, Xin; Zhang, Hao				chen, junda/HHD-0290-2022; zhong, yinmin/OHU-3633-2025; Liu, Xuanzhe/MIN-0907-2025						DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving								Arxiv											2	2;2024-03-19;https://www.arxiv.org/abs/2401.09670v2| 1;2024-01-18;https://www.arxiv.org/abs/2401.09670v1	arXiv:2401.09670			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 19 2024	2024	DistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation. Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests. We find that this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase. In the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both. DistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application's TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy tailored for each phase. DistServe also places the two phases according to the serving cluster's bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU. Our evaluations show that on various popular LLMs, applications, and latency requirements, DistServe can serve 4.48x more requests or 10.2x tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for > 90% of requests.																																	2024-04-12	PPRN:87221668		
J	Xie, Chengxing; Chen, Canyu; Jia, Feiran; Ye, Ziyu; Shu, Kai; Bibi, Adel; Hu, Ziniu; Torr, Philip; Ghanem, Bernard; Li, Guohao				Hu, Ziniu/HJI-4899-2023; SHU, Kai/OEN-5324-2025; Chen, Canyu/KFT-0519-2024; Ghanem, Bernard/J-7605-2017						Can Large Language Model Agents Simulate Human Trust Behaviors?								Arxiv											2	2;2024-03-10;https://www.arxiv.org/abs/2402.04559v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.04559v1	arXiv:2402.04559			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 10 2024	2024	Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, particularly for GPT-4, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications of our discoveries for various scenarios where trust is paramount. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans.																																	2024-04-07	PPRN:87561777		
J	Luo, Gen; Zhou, Yiyi; Zhang, Yuxin; Zheng, Xiawu; Sun, Xiaoshuai; Ji, Rongrong										Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models								Arxiv											1	1;2024-03-05;https://www.arxiv.org/abs/2403.03003v1	arXiv:2403.03003			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 05 2024	2024	Despite remarkable progress, existing multimodal large language models (MLLMs) are still inferior in granular visual recognition. Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming. Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also greatly reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR. We conduct extensive experiments on 11 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$times$ inference speed than LLaVA-1.5.																																	2024-11-10	PPRN:88028898		
J	Chen, Ricky T.Q.; Lipman, Yaron				Chen, Ricky/AAS-3168-2021						Flow Matching on General Geometries								Arxiv											2	2;2024-02-26;https://www.arxiv.org/abs/2302.03660v3| 1;2023-05-25;https://www.arxiv.org/abs/2302.03660v2	arXiv:2302.03660			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation -free on simple geometries, does not require divergence computation, and computes its target vector field in closed -form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real -world nonEuclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non -trivial curvature and boundaries.																																	2024-03-24	PPRN:72730986		
J	Gao, Mingqi; Hu, Xinyu; Ruan, Jie; Pu, Xiao; Wan, Xiaojun				Hu, Xinyu/KIK-7663-2024; Gao, Mingqi/NKO-9798-2025						LLM-based NLG Evaluation: Current Status and Challenges								Arxiv											2	2;2024-02-26;https://www.arxiv.org/abs/2402.01383v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01383v1	arXiv:2402.01383			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.																																	2024-03-28	PPRN:87507399		
J	Dong, Bo; Wang, Wenhai; Fan, Deng-Ping; Li, Jinpeng; Fu, Huazhu; Shao, Ling				wang, wenhai/GUX-3226-2022; Fan, Deng-Ping/ABD-4052-2020; Fu, Huazhu/A-1411-2014; Shao, Ling/D-3535-2011						Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers								Arxiv											2	2;2024-02-19;https://www.arxiv.org/abs/2108.06932v8| 1;2021-08-16;https://www.arxiv.org/abs/2108.06932v5	arXiv:2108.06932			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 19 2024	2024	Most polyp segmentation methods use CNNs as their backbone, leading to two key issues when exchanging information between the encoder and decoder: 1) taking into account the differences in contribution between different-level features and 2) designing an effective mechanism for fusing these features. Unlike existing CNN-based methods, we adopt a transformer encoder, which learns more powerful and robust representations. In addition, considering the image acquisition influence and elusive properties of polyps, we introduce three standard modules, including a cascaded fusion module (CFM), a camouflage identification module (CIM), and a similarity aggregation module (SAM). Among these, the CFM is used to collect the semantic and location information of polyps from high-level features; the CIM is applied to capture polyp information disguised in low-level features, and the SAM extends the pixel features of the polyp area with high-level semantic position information to the entire polyp area, thereby effectively fusing cross-level features. The proposed model, named Polyp-PVT, effectively suppresses noises in the features and significantly improves their expressive capabilities. Extensive experiments on five widely adopted datasets show that the proposed model is more robust to various challenging situations (e.g., appearance changes, small objects, rotation) than existing representative methods. The proposed model is available at https://github.com/DengPingFan/Polyp-PVT.																																	2024-03-15	PPRN:23077970		
J	Sarthi, Parth; Abdullah, Salman; Tuli, Aditi; Khanna, Shubh; Goldie, Anna; Manning, Christopher D.				Manning, Christopher/A-1358-2007						RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval								Arxiv											1	1;2024-01-31;https://www.arxiv.org/abs/2401.18059v1	arXiv:2401.18059			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 31 2024	2024	Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.																																	2024-05-25	PPRN:87436459		
J	Talaei, Shayan; Pourreza, Mohammadreza; Chang, Yu-Chen; Mirhoseini, Azalia; Saberi, Amin				Saberi, Amin/ACW-0953-2022						CHESS: Contextual Harnessing for Efficient SQL Synthesis								Arxiv											3	3;2024-11-25;https://www.arxiv.org/abs/2405.16755v3| 2;2024-06-27;https://www.arxiv.org/abs/2405.16755v2| 1;2024-05-27;https://www.arxiv.org/abs/2405.16755v1	arXiv:2405.16755			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 25 2024	2024	Translating natural language questions into SQL queries, known as text-to-SQL, is a long-standing research problem. Effective text-to-SQL synthesis can become very challenging due to (i) the extensive size of database catalogs (descriptions of tables and their columns) and database values, (ii) reasoning over large database schemas, (iii) ensuring the functional validity of the generated queries, and (iv) navigating the ambiguities of natural language questions. We introduce CHESS, a Large Language Model (LLM) based multi-agent framework for efficient and scalable SQL synthesis, comprising four specialized agents, each targeting one of the aforementioned challenges: the Information Retriever (IR) extracts relevant data, the Schema Selector (SS) prunes large schemas, the Candidate Generator (CG) generates high-quality candidates and refines queries iteratively, and the Unit Tester (UT) validates queries through LLM-based natural language unit tests. Our framework offers configurable features that adapt to various deployment constraints, including 1) Supporting industrial-scale databases: leveraging the Schema Selector agent, CHESS efficiently narrows down very large database schemas into manageable sub-schemas, boosting system accuracy by approximately 2% and reducing the number of LLM tokens by × 5. 2) State-of-the-Art privacy-preserving performance: Among the methods using open-source models, CHESS achieves state-of-the-art performance, resulting in a high-performing, privacy-preserving system suitable for industrial deployment. 3) Scalablity with additional compute budget: In settings with high computational budgets, CHESS achieves 71.10% accuracy on the BIRD test set, within 2% of the leading proprietary method, while requiring approximately 83% fewer LLM calls.																																	2025-01-08	PPRN:89071425		
J	Karaev, Nikita; Makarov, Iurii; Wang, Jianyuan; Neverova, Natalia; Vedaldi, Andrea; Rupprecht, Christian				Неверова, Наталья/A-8316-2014; Rupprecht, Christian/ABF-7744-2021						CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos								Arxiv											1	1;2024-10-15;https://www.arxiv.org/abs/2410.11831v1	arXiv:2410.11831			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 15 2024	2024	Most state-of-the-art point trackers are trained on synthetic data due to the difficulty of annotating real videos for this task. However, this can result in suboptimal performance due to the statistical gap between synthetic and real videos. In order to understand these issues better, we introduce CoTracker3, comprising a new tracking model and a new semi-supervised training recipe. This allows real videos without annotations to be used during training by generating pseudo-labels using off-the-shelf teachers. The new model eliminates or simplifies components from previous trackers, resulting in a simpler and often smaller architecture. This training scheme is much simpler than prior work and achieves better results using 1,000 times less data. We further study the scaling behaviour to understand the impact of using more real unsupervised data in point tracking. The model is available in online and offline variants and reliably tracks visible and occluded points.																																	2024-11-10	PPRN:112920327		
J	Kang, Hao; Zhang, Qingru; Kundu, Souvik; Jeong, Geonhwa; Liu, Zaoxing; Krishna, Tushar; Zhao, Tuo										GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM								Arxiv											4	4;2024-09-30;https://www.arxiv.org/abs/2403.05527v4| 3;2024-08-29;https://www.arxiv.org/abs/2403.05527v3| 2;2024-03-11;https://www.arxiv.org/abs/2403.05527v2| 1;2024-03-08;https://www.arxiv.org/abs/2403.05527v1	arXiv:2403.05527			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 30 2024	2024	Key-value (KV) caching has become the de-facto technique to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing entries group-wise. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient ffi cient error reduction framework that augments a quantization scheme with two error reduction components and achieves nearlossless performance at high compression ratios. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low-rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments show that GEAR can maintain similar accuracy to that of FP16 cache with improvement up to 24.42% .42% over the SOTA baselines at 2-bit compression. Additionally, compared to LLM inference with FP16 KV cache, GEAR can reduce peak-memory of up to 2.39×, bringing 2.1×∼ 5 .07× throughput improvement. Our code is publicly available at. https://github.com/HaoKang-Timmy/GEAR. .																																	2024-10-11	PPRN:88082072		
J	Xu, Mengwei; Yin, Wangsong; Cai, Dongqi; Yi, Rongjie; Xu, Daliang; Wang, Qipeng; Wu, Bingyang; Zhao, Yihao; Yang, Chen; Wang, Shihe; Zhang, Qiyang; Lu, Zhenyan; Zhang, Li; Wang, Shangguang; Li, Yuanchun; Liu, Yunxin; Jin, Xin; Liu, Xuanzhe				Xu, Mengwei/AAE-5567-2020; Wu, Bingyang/MNO-3288-2025; Cai, Dongqi/ADO-6199-2022; zhao, yihao/HCI-2361-2022; Liu, Xuanzhe/MIN-0907-2025; Li, Yuanchun/LTD-1972-2024; Zhang, Qiyang/A-7619-2017; Xu, Daliang/AHB-0418-2022; ZY/AAE-8847-2019						A Survey of Resource-efficient LLM and Multimodal Foundation Models								Arxiv											2	2;2024-09-23;https://www.arxiv.org/abs/2401.08092v2| 1;2024-01-16;https://www.arxiv.org/abs/2401.08092v1	arXiv:2401.08092			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 23 2024	2024	Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.																																	2024-10-03	PPRN:87186497		
J	Yang, Chenhongyi; Chen, Zehui; Espinosa, Miguel; Ericsson, Linus; Wang, Zhenyu; Liu, Jiaming; Crowley, Elliot J.				Chen, Zehui/HDN-3605-2022; liu, jiaming/KVA-6603-2024						PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition								Arxiv											2	2;2024-08-15;https://www.arxiv.org/abs/2403.17695v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17695v1	arXiv:2403.17695			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 15 2024	2024	We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the need for special tokens. We evaluate PlainMamba on a variety of visual recognition tasks, achieving performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives. For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance. 																																	2024-08-23	PPRN:88296416		
J	Wang, Peng; Zhang, Ningyu; Tian, Bozhong; Xi, Zekun; Yao, Yunzhi; Xu, Ziwen; Wang, Mengru; Mao, Shengyu; Wang, Xiaohan; Cheng, Siyuan; Liu, Kangwei; Ni, Yuansheng; Zheng, Guozhou; Chen, Huajun				Huajun, Chen/B-6340-2013; Wang, Xiaohan/G-9777-2011; Wang, Peng/AAV-7760-2021; Zhang, Ningyu/AAQ-7391-2021; Cheng, Siyuan/HIR-2767-2022						EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models								Arxiv											2	2;2024-06-24;https://www.arxiv.org/abs/2308.07269v3| 1;2023-08-14;https://www.arxiv.org/abs/2308.07269v1	arXiv:2308.07269			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 24 2024	2024	Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub1, along with Google Colab tutorials and comprehensive documentation2 for beginners to get started. Besides, we present an online system3 for real-time knowledge editing, and a demo video4.																																	2024-07-12	PPRN:77316577		
J	Xu, Jundong; Fei, Hao; Pan, Liangming; Liu, Qian; Lee, Mong-Li; Hsu, Wynne				Liu, Qian/AGD-5748-2022; Pan, Liangming/LIF-2753-2024; Fei, Hao/IZD-5292-2023						Faithful Logical Reasoning via Symbolic Chain-of-Thought								Arxiv											2	2;2024-06-11;https://www.arxiv.org/abs/2405.18357v2| 1;2024-05-28;https://www.arxiv.org/abs/2405.18357v1	arXiv:2405.18357			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 11 2024	2024	While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. 																																	2024-07-10	PPRN:89090769		
J	Ma, Ruochen; Zhang, Jian-Hao; Bi, Zhen; Cheng, Meng; Wang, Chong				Bi, Zhen/IAQ-8089-2023; Zhang, Jianhao/ABB-8327-2020						Topological Phases with Average Symmetries: the Decohered, the Disordered, and the Intrinsic								Arxiv											4	4;2025-01-23;https://www.arxiv.org/abs/2305.16399v4| 3;2024-05-19;https://www.arxiv.org/abs/2305.16399v3| 2;2023-10-16;https://www.arxiv.org/abs/2305.16399v2| 1;2023-05-25;https://www.arxiv.org/abs/2305.16399v1	arXiv:2305.16399			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	May 19 2024	2024	Global symmetries greatly enrich the landscape of topological quantum phases, playing an essential role from topological insulators to fractional quantum Hall effect. Topological phases in mixed quantum states, originating from textit{decoherence} in open quantum systems or textit{disorders} in imperfect crystalline solids, have recently garnered significant interest. Unlike pure states, mixed quantum states can exhibit textit{average symmetries} -- symmetries that keep the total ensemble invariant but not on each individual state. In this work, we present a systematic classification and characterization of average symmetry-protected topological (ASPT) phases applicable to generic symmetry groups, encompassing both average and exact symmetries, for bosonic and fermionic systems. Moreover, we formulate the theory of average symmetry-enriched topological (ASET) orders in disordered bosonic systems. Our systematic approach helps clarify nuanced issues in previous literature and uncovers compelling new physics. Notably, we discover that (1) the definition and classification of ASPT phases in decohered and disordered systems exhibit subtle differences; (2) despite these differences, ASPT phases in both settings can be classified and characterized under a unified framework of defect decoration and spectral sequence; (3) this systematic classification uncovers a plethora of ASPT phases that are textit{intrinsically mixed}, implying they can exclusively manifest in decohered or disordered systems where part of the symmetry is average; (4) similarly for ASET, we find intrinsically disordered phases exhibiting exotic anyon behaviors -- the ground states of such phases necessarily contain localized anyons, with gapless (yet still localized) excitation spectral.																																	2025-08-07	PPRN:72727824		
J	Gao, Ruiyuan; Chen, Kai; Xie, Enze; Hong, Lanqing; Li, Zhenguo; Yeung, Dit-Yan; Xu, Qiang				Gao, Ruiyuan/LIC-0347-2024; Lv, Zhengtong/AAW-9611-2020; Li, Jiaqi/HHN-8236-2022; zhang, Shifeng/HPH-0217-2023						MagicDrive: Street View Generation with Diverse 3D Geometry Control								Arxiv											6	6;2024-05-03;https://www.arxiv.org/abs/2310.02601v7| 5;2024-03-01;https://www.arxiv.org/abs/2310.02601v6| 4;2024-01-26;https://www.arxiv.org/abs/2310.02601v5| 3;2023-12-18;https://www.arxiv.org/abs/2310.02601v4| 2;2023-10-13;https://www.arxiv.org/abs/2310.02601v3| 1;2023-10-05;https://www.arxiv.org/abs/2310.02601v2	arXiv:2310.02601			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 03 2024	2024	Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework, offering diverse 3D geometry controls including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view image & video synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection. [GRAPHICS]																																	2024-05-26	PPRN:85436143		
J	Fan, Chongyu; Liu, Jiancheng; Zhang, Yihua; Wong, Eric; Wei, Dennis; Liu, Sijia				Liu, JC/LPI-0149-2024; Liu, Sijia/HOC-2459-2023						SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation								Arxiv											4	4;2024-04-04;https://www.arxiv.org/abs/2310.12508v5| 3;2024-02-19;https://www.arxiv.org/abs/2310.12508v3| 2;2023-11-06;https://www.arxiv.org/abs/2310.12508v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.12508v1	arXiv:2310.12508			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often suffer limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' for MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting data points). To the best of our knowledge, SalUn is the first principled MU approach that can effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation tasks. As highlighted below, For example, SalUn yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from generating harmful images, SalUn achieves nearly 100% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not.																																	2024-04-19	PPRN:85724450		
J	Zhan, Qiusi; Liang, Zhixiang; Ying, Zifan; Kang, Daniel										InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents								Arxiv											1	1;2024-03-25;https://www.arxiv.org/abs/2403.02691v2	arXiv:2403.02691			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative. In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.																																	2025-08-07	PPRN:123159978		
J	Xia, Heming; Yang, Zhe; Dong, Qingxiu; Wang, Peiyi; Li, Yongqi; Ge, Tao; Liu, Tianyu; Li, Wenjie; Sui, Zhifang				Liu, Tianyu/JXN-8107-2024; sun, xu/JCN-6491-2023; Li, Wenjie/AAC-2500-2021						Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding								Arxiv											2	2;2024-02-20;https://www.arxiv.org/abs/2401.07851v2| 1;2024-01-15;https://www.arxiv.org/abs/2401.07851v1	arXiv:2401.07851			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 20 2024	2024	To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.																																	2024-11-09	PPRN:87187013		
J	Chen, Zhihong; Varma, Maya; Xu, Justin; Paschali, Magdalini; Van Veen, Dave; Johnston, Andrew; Youssef, Alaa; Blankemeier, Louis; Bluethgen, Christian; Altmayer, Stephan; Valanarasu, Jeya Maria Jose; Muneer, Mohamed Siddig Eltayeb; Reis, Eduardo Pontes; Cohen, Joseph Paul; Olsen, Cameron; Abraham, Tanishq Mathew; Tsai, Emily B.; Beaulieu, Christopher F.; Jitsev, Jenia; Gatidis, Sergios; Delbrouck, Jean-Benoit; Chaudhari, Akshay S.; Langlotz, Curtis P.				Valanarasu, Jeya Maria Jose/ABB-3334-2021; Paschali, Magdalini/JNE-9460-2023; Bluethgen, Christian/LFJ-7963-2024; Johnston, Andrew/IWE-5368-2023; Chaudhari, Akshay/ABE-1734-2021; Tsai, Emily/AAF-8583-2019; Gatidis, Sergios/AAF-4858-2020						A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation								Arxiv											2	2;2024-12-18;https://www.arxiv.org/abs/2401.12208v2| 1;2024-01-22;https://www.arxiv.org/abs/2401.12208v1	arXiv:2401.12208			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 18 2024	2024	Over 1.4 billion chest X-rays (CXRs) are performed annually due to their cost-effectiveness as an initial diagnostic test. This scale of radiological studies provides a significant opportunity to streamline CXR interpretation and documentation. While foundation models are a promising solution, the lack of publicly available large-scale datasets and benchmarks inhibits their iterative development and real-world evaluation. To overcome these challenges, we constructed a large-scale dataset (CheXinstruct), which we utilized to train a vision-language foundation model (CheXagent). We systematically demonstrated competitive performance across eight distinct task types on our novel evaluation benchmark (CheXbench). Beyond technical validation, we assessed the real-world utility of CheXagent in directly drafting radiology reports. Our clinical assessment with eight radiologists revealed a 36% time saving for residents using CheXagent-drafted reports, while attending radiologists showed no significant time difference editing resident-drafted or CheXagent-drafted reports. The CheXagent-drafted reports improved the writing efficiency of both radiology residents and attending radiologists in 81% and 61% of cases, respectively, without loss of quality. Overall, we demonstrate that CheXagent can effectively perform a variety of CXR interpretation tasks and holds potential to assist radiologists in routine clinical workflows.																																	2025-01-27	PPRN:87277257		
J	Liang, Jian; He, Ran; Tan, Tieniu				Liang, Jian/HJI-1125-2023; Huang, Yan/HCH-6526-2022						A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts								Arxiv											2	2;2024-12-12;https://www.arxiv.org/abs/2303.15361v2| 1;2023-03-27;https://www.arxiv.org/abs/2303.15361v1	arXiv:2303.15361			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 12 2024	2024	Machine learning methods strive to acquire a robust model during the training process that can effectively generalize to test samples, even in the presence of distribution shifts. However, these methods often suffer from performance degradation due to unknown test distributions. Test-time adaptation (TTA), an emerging paradigm, has the potential to adapt a pre-trained model to unlabeled data during testing, before making predictions. Recent progress in this paradigm has highlighted the significant benefits of using unlabeled data to train self-adapted models prior to inference. In this survey, we categorize TTA into several distinct groups based on the form of test data, namely, test- time domain adaptation, test-time batch adaptation, and online test-time adaptation. For each category, we provide a comprehensive taxonomy of advanced algorithms and discuss various learning scenarios. Furthermore, we analyze relevant applications of TTA and discuss open challenges and promising areas for future research. 																																	2025-01-20	PPRN:49639205		
J	Liao, Zeyi; Sun, Huan										AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs								Arxiv											3	3;2024-11-24;https://www.arxiv.org/abs/2404.07921v3| 2;2024-05-02;https://www.arxiv.org/abs/2404.07921v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.07921v1	arXiv:2404.07921			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 24 2024	2024	As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.																																	2025-01-08	PPRN:88500328		
J	Chen, Chao; Liu, Kai; Chen, Ze; Gu, Yi; Wu, Yue; Tao, Mingyuan; Fu, Zhihang; Ye, Jieping										INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection								Arxiv											2	2;2024-10-21;https://www.arxiv.org/abs/2402.03744v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.03744v1	arXiv:2402.03744			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 21 2024	2024	Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' textbf{IN}ternal textbf{S}tates for halluctextbf{I}nation textbf{DE}tection (textbf{INSIDE}). In particular, a simple yet effective textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.																																	2024-11-20	PPRN:87533854		
J	Liu, Junwei; Wang, Kaixin; Chen, Yixuan; Peng, Xin; Chen, Zhenpeng; Zhang, Lingming; Lou, Yiling				liu, yiling/HNS-6339-2023; Chen, Yixuan/KZU-3668-2024; Liu, Junwei/KPA-7637-2024						Large Language Model-Based Agents for Software Engineering: A Survey								Arxiv											1	1;2024-09-04;https://www.arxiv.org/abs/2409.02977v1	arXiv:2409.02977			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 04 2024	2024	The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. 																																	2024-09-18	PPRN:91750309		
J	Du, Zhihao; Wang, Jiaming; Chen, Qian; Chu, Yunfei; Gao, Zhifu; Li, Zerui; Hu, Kai; Zhou, Xiaohuan; Xu, Jin; Ma, Ziyang; Wang, Wen; Zheng, Siqi; Zhou, Chang; Yan, Zhijie; Zhang, Shiliang				Hu, Kai/AAC-1429-2022; Zhang, ShiLiang/AAA-4638-2020; Chu, Yunfei/C-8002-2013						LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2310.04673v4| 1;2023-10-11;https://www.arxiv.org/abs/2310.04673v3	arXiv:2310.04673			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.																																	2024-12-06	PPRN:85540731		
J	Ying, Huaiyuan; Zhang, Shuo; Li, Linyang; Zhou, Zhejian; Shao, Yunfan; Fei, Zhaoye; Ma, Yichuan; Hong, Jiawei; Liu, Kuikun; Wang, Ziyi; Wang, Yudong; Wu, Zijian; Li, Shuaibin; Zhou, Fengzhe; Liu, Hongwei; Zhang, Songyang; Zhang, Wenwei; Yan, Hang; Qiu, Xipeng; Wang, Jiayu; Chen, Kai; Lin, Dahua				Wang, Yudong/GQY-6481-2022; wang, ziyi/IWL-9285-2023; Zhang, Wenwei/HKO-4277-2023; Zhang, Songyang/GPX-5621-2022; Wang, jiayu/ABB-6772-2020; Wu, Zijian/GQO-9782-2022; liu, hongwei/ACJ-6082-2022; Ma, Yichuan/JJF-2892-2023; Lin, Dahua/W-6576-2019						InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning								Arxiv											2	2;2024-05-24;https://www.arxiv.org/abs/2402.06332v2| 1;2024-02-09;https://www.arxiv.org/abs/2402.06332v1	arXiv:2402.06332			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	May 24 2024	2024	The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced stateof -the -art performance under the setting of in -context learning, supervised fine -tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBenchZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine -tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learning which shows the possibility of using LEAN as a unified platform for solving and proving in math. Our models, codes, and data are released at https://github.com/InternLM/InternLM-Math .																																	2024-06-12	PPRN:87608295		
J	Pooladian, Aram-Alexandre; Niles-Weed, Jonathan										Entropic estimation of optimal transport maps								Arxiv											2	2;2024-05-12;https://www.arxiv.org/abs/2109.12004v3| 1;2021-09-24;https://www.arxiv.org/abs/2109.12004v2	arXiv:2109.12004			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 12 2024	2024	We develop a computationally tractable method for estimating the optimal transport map between two distributions over Rd with rigorous finite -sample guarantees. Leveraging an entropic version of Brenier’s theorem, we show that our estimator—the barycentric projection of the optimal entropic plan—is easy to compute using Sinkhorn’s algorithm. As a result, unlike current approaches for map estimation, which are slow to evaluate when the dimension or number of samples is large, our approach is parallelizable and extremely efficient even for massive data sets. Under smoothness assumptions on the optimal map, we show that our estimator enjoys comparable statistical performance to other estimators in the literature, but with much lower computational cost. We showcase the efficacy of our proposed estimator through numerical examples, even ones not explicitly covered by our assumptions. By virtue of Lepski’s method, we propose a modified version of our estimator that is adaptive to the smoothness of the underlying optimal transport map. Our proofs are based on a modified duality principle for entropic optimal transport and on a method for approximating optimal entropic plans due to Pal (2019).																																	2024-05-29	PPRN:12835386		
J	Zhang, Yifan; Yang, Jingqin; Yuan, Yang; Yao, Andrew Chi-Chih				Zhang, Yifan/KMY-8838-2024						Cumulative Reasoning with Large Language Models								Arxiv											4	4;2024-04-02;https://www.arxiv.org/abs/2308.04371v6| 3;2023-12-02;https://www.arxiv.org/abs/2308.04371v5| 2;2023-08-25;https://www.arxiv.org/abs/2308.04371v4| 1;2023-08-10;https://www.arxiv.org/abs/2308.04371v3	arXiv:2308.04371			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	Despite the recent advancements in language models (LMs), their ability to solve complex problems remains limited. This paper introduces Cumulative Reasoning (CR), a novel approach that utilizes LMs cumulatively and iteratively, mirroring human thought processes for problem-solving. CR decomposes tasks into smaller, manageable components and leverages previous propositions for effective composition, significantly enhancing problem-solving capabilities. We demonstrate CR's superiority through several complex reasoning tasks: it outperforms existing methods in logical inference tasks with up to a 9.3% improvement, achieving 98.04% accuracy on the curated FOLIO wiki dataset. In the Game of 24, it achieves 98% accuracy, marking a 24% improvement over the prior state-of-the-art. Additionally, CR sets new state-of-the-art on the MATH dataset, achieving a 4.2% increase from previous methods and a 43% relative improvement in the most challenging problems. By extending CR to incorporate a code environment without external aids like retrieval or web browsing, we further harness the computational and logical reasoning capabilities of LMs, achieving a remarkable 72.2% accuracy on the MATH dataset and outperforming the PAL/PoT method by 38.8%. Our work not only sets new state-of-the-art but also paves the way toward more sophisticated AI reasoning methods. The code is available at https://github.com/iiis-ai/cumulative-reasoning.																																	2024-04-18	PPRN:75369674		
J	Li, Chen; Wang, Weiqi; Hu, Jingcheng; Wei, Yixuan; Zheng, Nanning; Hu, Han; Zhang, Zheng; Peng, Houwen										Common 7B Language Models Already Possess Strong Math Capabilities								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.04706v1	arXiv:2403.04706			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.																																	2024-04-05	PPRN:88062511		
J	Liu, Xiaogeng; Yu, Zhiyuan; Zhang, Yizhe; Zhang, Ning; Xiao, Chaowei				Yu, Zhiyuan/JPX-8768-2023; Xiao, Chaowei/AAT-8772-2021						Automatic and Universal Prompt Injection Attacks against Large Language Models								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.04957v1	arXiv:2403.04957			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	Large Language Models (LLMs) excel in processing and generating human language, powered by their ability to interpret and follow instructions. However, their capabilities can be exploited through prompt injection attacks. These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker’s injected content, deviating from the user’s actual requests. The substantial risks posed by these attacks underscore the need for a thorough understanding of the threats. Yet, research in this area faces challenges due to the lack of a unified goal for such attacks and their reliance on manually crafted prompts, complicating comprehensive assessments of prompt injection robustness. We introduce a unified framework for understanding the objectives of prompt injection attacks and present an automated gradient -based method for generating highly effective and universal prompt injection data, even in the face of defensive measures. With only five training samples (0.3% relative to the test data), our attack can achieve superior performance compared with baselines. Our findings emphasize the importance of gradient -based testing, which can avoid overestimation of robustness, especially for defense mechanisms. Code is available at https://github.com/SheltonLiu-N/ Universal -Prompt -Injection																																	2024-04-07	PPRN:88082049		
J	Huang, Yue; Shi, Jiawen; Li, Yuan; Fan, Chenrui; Wu, Siyuan; Zhang, Qihui; Liu, Yixin; Zhou, Pan; Wan, Yao; Gong, Neil Zhenqiang; Sun, Lichao				Liu, Yixin/ABC-7725-2021; ZHANG, QIHUI/OAJ-1060-2025						MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use								Arxiv											6	6;2024-02-23;https://www.arxiv.org/abs/2310.03128v5| 5;2024-01-18;https://www.arxiv.org/abs/2310.03128v4| 4;2023-10-23;https://www.arxiv.org/abs/2310.03128v3| 3;2023-10-12;https://www.arxiv.org/abs/2310.03128v2| 2;2023-10-04;https://www.arxiv.org/abs/2310.03128v1| 1;2023-10-04;https://www.arxiv.org/abs/2310.03128v1	arXiv:2310.03128			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 23 2024	2024	Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision -making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce METATOOL, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called TOOLE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single -tool and multi -tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios , tool selection with possible reliability issues, and multi -tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers – we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to. Our TOOLE dataset is available at URL and code is in Github.																																	2024-11-20	PPRN:85435016		
J	Zhang, Jingyi; Huang, Jiaxing; Jin, Sheng; Lu, Shijian				Lu, Shijian/AAU-4831-2021						Vision-Language Models for Vision Tasks: A Survey								Arxiv											2	2;2024-02-16;https://www.arxiv.org/abs/2304.00685v2| 1;2023-04-03;https://www.arxiv.org/abs/2304.00685v1	arXiv:2304.00685			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 16 2024	2024	Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition.																																	2024-03-14	PPRN:53729264		
J	Huang, Xu; Lian, Jianxun; Lei, Yuxuan; Yao, Jing; Lian, Defu; Xie, Xing				Yao, Jing/T-5499-2019; Lian, Defu/AFN-4573-2022						Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations								Arxiv											3	3;2024-01-30;https://www.arxiv.org/abs/2308.16505v3| 2;2023-09-01;https://www.arxiv.org/abs/2308.16505v2| 1;2023-08-31;https://www.arxiv.org/abs/2308.16505v1	arXiv:2308.16505			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 30 2024	2024	Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called textbf{InteRecAgent}, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs. 																																	2024-02-16	PPRN:84623886		
J	Chen, Zhikai; Mao, Haitao; Li, Hang; Jin, Wei; Wen, Hongzhi; Wei, Xiaochi; Wang, Shuaiqiang; Yin, Dawei; Fan, Wenqi; Liu, Hui; Tang, Jiliang				Yin, Dawei/JOR-9201-2023; Li, Hang/LOS-5863-2024; WEN, Hongzhi/JFJ-3882-2023						Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs								Arxiv											2	2;2024-01-16;https://www.arxiv.org/abs/2307.03393v4| 1;2023-07-10;https://www.arxiv.org/abs/2307.03393v2	arXiv:2307.03393			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 16 2024	2024	Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs.																																	2024-01-31	PPRN:73866373		
J	Cheng, Yuheng; Zhang, Ceyao; Zhang, Zhengwen; Meng, Xiangrui; Hong, Sirui; Li, Wenhao; Wang, Zihao; Wang, Zekai; Yin, Feng; Zhao, Junhua; He, Xiuqiang				Meng, Xiangrui/D-5955-2012; Zhao, Junhua/HTP-2601-2023						Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects								Arxiv											1	1;2024-01-07;https://www.arxiv.org/abs/2401.03428v1	arXiv:2401.03428			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 07 2024	2024	Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them.   Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications – from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities.   This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.																																	2024-01-23	PPRN:87047573		
J	Zhao, Huaqin; Liu, Zhengliang; Wu, Zihao; Li, Yiwei; Yang, Tianze; Shu, Peng; Xu, Shaochen; Dai, Haixing; Zhao, Lin; Jiang, Hanqi; Pan, Yi; Chen, Junhao; Zhou, Yifan; Mai, Gengchen; Liu, Ninghao; Liu, Tianming				Mai, Gengchen/ABF-8620-2020; yuan, yixuan/KLZ-6092-2024; Liu, Tianming/GLS-1211-2022; SHU, PENG/LDF-4318-2024; Zhao, Lin/ABM-7665-2022; Chen, Junhao/LCD-2351-2024; Li, Yiwei/JAX-1635-2023; wu, zihao/R-8745-2019						Revolutionizing Finance with LLMs: An Overview of Applications and Insights								Arxiv											2	2;2024-12-12;https://www.arxiv.org/abs/2401.11641v2| 1;2024-01-22;https://www.arxiv.org/abs/2401.11641v1	arXiv:2401.11641			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 12 2024	2024	In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry.																																	2025-01-20	PPRN:87272844		
J	Gao, Shenyuan; Yang, Jiazhi; Chen, Li; Chitta, Kashyap; Qiu, Yihang; Geiger, Andreas; Zhang, Jun; Li, Hongyang				Li, Hongyang/HHZ-5735-2022; Qiu, yihang/KBQ-0841-2024; CHEN, Li/HIZ-5543-2022; Zhang, Jun/M-8009-2013; Gao, Shenyuan/IAM-3062-2023						<italic>Vista</italic>: A Generalizable Driving World Model with High Fidelity and Versatile Controllability								Arxiv											4	4;2024-10-28;https://www.arxiv.org/abs/2405.17398v5| 3;2024-07-22;https://www.arxiv.org/abs/2405.17398v4| 2;2024-06-06;https://www.arxiv.org/abs/2405.17398v2| 1;2024-05-27;https://www.arxiv.org/abs/2405.17398v1	arXiv:2405.17398			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 28 2024	2024	World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.																																	2024-12-06	PPRN:89074285		
J	Mcintosh, Timothy R.; Susnjak, Teo; Arachchilage, Nalin; Liu, Tong; Watters, Paul; Halgamuge, Malka N.				Halgamuge, Malka/JZE-1722-2024; McIntosh, Timothy/AAD-8439-2022						Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence								Arxiv											3	3;2024-10-14;https://www.arxiv.org/abs/2402.09880v2| 2;2024-02-15;https://www.arxiv.org/abs/2402.09880v1| 1;2024-02-15;https://www.arxiv.org/abs/2402.09880v1	arXiv:2402.09880			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 14 2024	2024	The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their own LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of benchmark functionality and integrity. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs’ complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems’ integration into society.																																	2024-11-05	PPRN:87699369		
J	Vaca-Rubio, Cristian J.; Blanco, Luis; Pereira, Roberto; Caus, Marius				Pereira, Roberto/LKN-7701-2024; Blanco, Luis Antonio/JVY-6866-2024						Kolmogorov-Arnold Networks (KANs) for Time Series Analysis								Arxiv											2	2;2024-09-25;https://www.arxiv.org/abs/2405.08790v2| 1;2024-05-14;https://www.arxiv.org/abs/2405.08790v1	arXiv:2405.08790			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 25 2024	2024	This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.																																	2024-10-08	PPRN:89046813		
J	Hu, Shujie; Zhou, Long; Liu, Shujie; Chen, Sanyuan; Meng, Lingwei; Hao, Hongkun; Pan, Jing; Liu, Xunying; Li, Jinyu; Sivasankaran, Sunit; Liu, Linquan; Wei, Furu				Sivasankaran, Sunit/GQR-0582-2022; Liu, Linquan/HTR-1187-2023; Chen, Sanyuan/GLR-3754-2022						WavLLM: Towards Robust and Adaptive Speech Large Language Model								Arxiv											3	3;2024-09-21;https://www.arxiv.org/abs/2404.00656v3| 2;2024-08-14;https://www.arxiv.org/abs/2404.00656v2| 1;2024-03-31;https://www.arxiv.org/abs/2404.00656v1	arXiv:2404.00656			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 21 2024	2024	Recent advancements in large language models (LLMs) have expanded their scope in natural language processing (NLP) to encompass multimodal functions. However, integrating listening capabilities effectively remains a significant challenge for generalization and complex auditory task execution. In this work, we introduce WavLLM, a robust and adaptive speech large language model featuring dual encoders—a Whisper encoder for semantics and a WavLM encoder for speaker characteristics. Within the two-stage curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks combining elementary ones. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks and also apply it to specialized speech-question-answer (SQA) dataset, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks in the setting of the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. 																																	2024-10-07	PPRN:88362006		
J	Sturua, Saba; Mohr, Isabelle; Akram, Mohammad Kalim; Guenther, Michael; Wang, Bo; Krimmel, Markus; Wang, Feng; Mastrapas, Georgios; Koukounas, Andreas; Wang, Nan; Xiao, Han										jina-embeddings-v3: Multilingual Embeddings With Task LoRA								Arxiv											2	2;2024-09-19;https://www.arxiv.org/abs/2409.10173v3| 1;2024-09-17;https://www.arxiv.org/abs/2409.10173v2	arXiv:2409.10173			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Sep 19 2024	2024	We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks. With a default output dimension of 1024, users can flexibly reduce the embedding dimensions to as low as 32 without compromising performance, enabled by Matryoshka Representation Learning.																																	2024-10-02	PPRN:91938978		
J	Khan, Akbir; Hughes, John; Valentine, Dan; Ruis, Laura; Sachan, Kshitij; Radhakrishnan, Ansh; Grefenstette, Edward; Bowman, Samuel R.; Rocktaeschel, Tim; Perez, Ethan										Debating with More Persuasive LLMs Leads to More Truthful Answers								Arxiv											4	4;2024-07-25;https://www.arxiv.org/abs/2402.06782v4| 3;2024-05-30;https://www.arxiv.org/abs/2402.06782v3| 2;2024-02-15;https://www.arxiv.org/abs/2402.06782v2| 1;2024-02-09;https://www.arxiv.org/abs/2402.06782v1	arXiv:2402.06782			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 25 2024	2024	Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.																																	2024-08-02	PPRN:87649852		
J	Zou, Andy; Phan, Long; Wang, Justin; Duenas, Derek; Lin, Maxwell; Andriushchenko, Maksym; Wang, Rowan; Kolter, Zico; Fredrikson, Matt; Hendrycks, Dan				Fredrikson, Matt/GQR-0633-2022; Zou, Andy/MGU-4410-2025						Improving Alignment and Robustness with Circuit Breakers								Arxiv											4	4;2024-07-12;https://www.arxiv.org/abs/2406.04313v4| 3;2024-07-08;https://www.arxiv.org/abs/2406.04313v3| 2;2024-06-10;https://www.arxiv.org/abs/2406.04313v2| 1;1800-01-01;https://www.arxiv.org/abs/2406.04313v1	arXiv:2406.04313			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 12 2024	2024	AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with "circuit breakers." Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image "hijacks" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.																																	2024-07-23	PPRN:89249511		
J	Solaiman, Irene; Talat, Zeerak; Agnew, William; Ahmad, Lama; Baker, Dylan; Blodgett, Su Lin; Chen, Canyu; Daume III, Hal; Dodge, Jesse; Duan, Isabella; Evans, Ellie; Friedrich, Felix; Ghosh, Avijit; Gohar, Usman; Hooker, Sara; Jernite, Yacine; Kalluri, Ria; Lusoli, Alberto; Leidinger, Alina; Lin, Michelle; Lin, Xiuzhu; Luccioni, Sasha; Mickel, Jennifer; Mitchell, Margaret; Newman, Jessica; Ovalle, Anaelia; Png, Marie-Therese; Singh, Shubham; Strait, Andrew; Struppek, Lukas; Subramonian, Arjun				Friedrich, Felix/LKM-6617-2024; Chen, Canyu/KFT-0519-2024; Gohar, Usman/JND-9216-2023; Lin, Michelle/KFB-7964-2024; Agnew, William/ONI-6729-2025; Lusoli, Alberto/ACX-5125-2022						Evaluating the Social Impact of Generative AI Systems in Systems and Society								Arxiv											2	2;2024-06-28;https://www.arxiv.org/abs/2306.05949v4| 1;2023-06-12;https://www.arxiv.org/abs/2306.05949v2	arXiv:2306.05949			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 28 2024	2024	Generative AI systems across modalities, ranging from text (including code), image, audio, and video, have broad social impacts, but there is no official standard for means of evaluating those impacts or for which impacts should be evaluated. In this paper, we present a guide that moves toward a standard approach in evaluating a base generative AI system for any modality in two overarching categories: what can be evaluated in a base system independent of context and what can be evaluated in a societal context. Importantly, this refers to base systems that have no predetermined application or deployment context, including a model itself, as well as system components, such as training data. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to listed generative modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what can be evaluated in a broader societal context, each with its own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm.																																	2024-07-30	PPRN:73298886		
J	Lin, Zhiqiu; Pathak, Deepak; Li, Baiqi; Li, Jiayao; Xia, Xide; Neubig, Graham; Zhang, Pengchuan; Ramanan, Deva				Li, Baiqi/HKN-0957-2023; Li, Jiayao/KJL-0230-2024						Evaluating Text-to-Visual Generation with Image-to-Text Generation								Arxiv											2	2;2024-06-18;https://www.arxiv.org/abs/2404.01291v2| 1;2024-04-01;https://www.arxiv.org/abs/2404.01291v1	arXiv:2404.01291			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.																																	2024-07-06	PPRN:88365890		
J	Zong, Yongshuo; Bohdal, Ondrej; Yu, Tingyang; Yang, Yongxin; Hospedales, Timothy										Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2402.02207v2| 1;2024-02-03;https://www.arxiv.org/abs/2402.02207v1	arXiv:2402.02207			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases.																																	2024-07-06	PPRN:87523339		
J	He, Tairan; Luo, Zhengyi; He, Xialin; Xiao, Wenli; Zhang, Chong; Zhang, Weinan; Kitani, Kris; Liu, Changliu; Shi, Guanya				Liu, Changliu/T-2360-2019						OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning								Arxiv											1	1;2024-06-13;https://www.arxiv.org/abs/2406.08858v1	arXiv:2406.08858			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 13 2024	2024	We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets.																																	2024-07-02	PPRN:89294587		
J	Karamcheti, Siddharth; Nair, Suraj; Balakrishna, Ashwin; Liang, Percy; Kollar, Thomas; Sadigh, Dorsa										Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2402.07865v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.07865v1	arXiv:2402.07865			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 30 2024	2024	Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are underexplored, making it challenging to understand what factors account for model performance – a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs.																																	2024-07-04	PPRN:87638356		
J	Hui, Mude; Yang, Siwei; Zhao, Bingchen; Shi, Yichun; Wang, Heng; Wang, Peng; Zhou, Yuyin; Xie, Cihang				Zhao, Bing/HNJ-6617-2023; Shi, Yichun/AGV-8080-2022; Wang, Peng/AAV-7760-2021						HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing								Arxiv											1	1;2024-04-15;https://www.arxiv.org/abs/2404.09990v1	arXiv:2404.09990			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 15 2024	2024	This study introduces HQ-Edit, a high-quality instruction-based image editing dataset with around 200,000 edits. Unlike prior approaches relying on attribute guidance or human feedback on building datasets, we devise a scalable data collection pipeline leveraging advanced foundation models, namely GPT-4V and DALL-E 3. To ensure its high quality, diverse examples are first collected online, expanded, and then used to create high-quality diptychs featuring input and output images with detailed text prompts, followed by precise alignment ensured through post-processing. In addition, we propose two evaluation metrics, Alignment and Coherence, to quantitatively assess the quality of image edit pairs using GPT-4V. HQ-Edits high-resolution images, rich in detail and accompanied by comprehensive editing prompts, substantially enhance the capabilities of existing image editing models. For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image editing performance, even surpassing those models fine-tuned with human-annotated data. 																																	2024-04-25	PPRN:88529412		
J	Wei, Jerry; Yang, Chengrun; Song, Xinying; Lu, Yifeng; Hu, Nathan; Huang, Jie; Tran, Dustin; Peng, Daiyi; Liu, Ruibo; Huang, Da; Du, Cosmo; Le, Quoc V.				Yang, Chengrun/HII-3621-2022						LONG-FORM FACTUALITY IN LARGE LANGUAGE MODELS								Arxiv											3	3;2024-04-03;https://www.arxiv.org/abs/2403.18802v3| 2;2024-04-01;https://www.arxiv.org/abs/2403.18802v2| 1;2024-03-27;https://www.arxiv.org/abs/2403.18802v1	arXiv:2403.18802			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 03 2024	2024	Large language models (LLMs) often generate content that contains factual errors when responding to fact -seeking prompts on open-ended topics. To benchmark a model’s long -form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for longform factuality through a method which we call Search -Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long -form response into a set of individual facts and to evaluate the accuracy of each fact using a multi -step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long -form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user’s preferred response length (recall). Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators—on a set of ∼16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long -form factuality. LongFact, SAFE, and all experimental code are available at https://github. com/google-deepmind/long-form-factuality.   [GRAPHICS]																																	2024-04-19	PPRN:88334921		
J	Xu, Jiashu; Ma, Mingyu Derek; Wang, Fei; Xiao, Chaowei; Chen, Muhao				Chen, Muhao/AAA-3634-2021; Xiao, Chaowei/AAT-8772-2021						Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models								Arxiv											2	2;2024-04-03;https://www.arxiv.org/abs/2305.14710v2| 1;2023-05-24;https://www.arxiv.org/abs/2305.14710v1	arXiv:2305.14710			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 03 2024	2024	We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.																																	2024-04-18	PPRN:72717194		
J	Zhang, Ruohong; Gui, Liangke; Sun, Zhiqing; Feng, Yihao; Xu, Keyang; Zhang, Yuanhan; Fu, Di; Li, Chunyuan; Hauptmann, Alexander; Bisk, Yonatan; Yang, Yiming				Li, Chunyuan/KHY-0771-2024; Feng, Yihao/LVR-7524-2024						Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward								Arxiv											1	1;2024-04-02;https://www.arxiv.org/abs/2404.01258v2	arXiv:2404.01258			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input. Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks.																																	2024-04-18	PPRN:88378391		
J	Hansen, Nicklas; Su, Hao; Wang, Xiaolong				Su, Hao/HHZ-1048-2022						TD-MPC2: Scalable, Robust World Models for Continuous Control								Arxiv											3	3;2024-03-21;https://www.arxiv.org/abs/2310.16828v2| 2;2023-10-25;https://www.arxiv.org/abs/2310.16828v1| 1;2023-10-25;https://www.arxiv.org/abs/2310.16828v1	arXiv:2310.16828			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 21 2024	2024	TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://tdmpc2.com																																	2024-04-13	PPRN:85809213		
J	Fu, Rao; Liu, Jingyu; Chen, Xilun; Nie, Yixin; Xiong, Wenhan				Fu, Rao/AAC-4352-2019						Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.11401v1	arXiv:2403.11401			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and ego-centric 3D information. This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization. Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the scene. Our experiments with Scene-LLM demonstrate its strong capabilities in dense captioning, question answering, and interactive planning. We believe Scene-LLM advances the field of 3D visual understanding and reasoning, offering new possibilities for sophisticated agent interactions in indoor settings.																																	2024-04-11	PPRN:88190177		
J	Yu, Tianyu; Yao, Yuan; Zhang, Haoye; He, Taiwen; Han, Yifeng; Cui, Ganqu; Hu, Jinyi; Liu, Zhiyuan; Zheng, Hai-Tao; Sun, Maosong; Chua, Tat-Seng				Hu, Jinyi/GXF-7296-2022; Liu, Zhiyuan/I-2233-2014; Wang, Meng/AEZ-9059-2022; Yu, Tianyu/OFO-0197-2025; SHEN, YING/HJH-6938-2023						RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback								Arxiv											2	2;2024-03-08;https://www.arxiv.org/abs/2312.00849v2| 1;2023-12-01;https://www.arxiv.org/abs/2312.00849v1	arXiv:2312.00849			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 08 2024	2024	Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V.																																	2024-04-01	PPRN:86379308		
J	Yi, Zihao; Ouyang, Jiarui; Liu, Yuwen; Liao, Tianhao; Xu, Zhe; Shen, Ying				yi, zihao/LMN-8276-2024; Liao, Tianhao/LJK-4167-2024						A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems								Arxiv											2	2;2025-08-15;https://www.arxiv.org/abs/2402.18013v2| 1;2024-02-28;https://www.arxiv.org/abs/2402.18013v1	arXiv:2402.18013			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Feb 28 2024	2024	This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.																																	2024-03-28	PPRN:87987559		
J	Jing, Bowen; Berger, Bonnie; Jaakkola, Tommi				Jing, Bowen/AAX-7860-2021						AlphaFold Meets Flow Matching for Generating Protein Ensembles								Arxiv											1	1;2024-02-07;https://www.arxiv.org/abs/2402.04845v1	arXiv:2402.04845			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 07 2024	2024	The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow -based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single -state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFLOW and ESMFLOW. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all -atom MD, our method accurately captures conformational flexibility, positional distributions, and higherorder ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall -clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics -based simulations. Code is available at https://github.com/ bjing2016/alphaflow.																																	2024-02-23	PPRN:87561963		
J	Wang, Jiaqi; Wu, Zihao; Li, Yiwei; Jiang, Hanqi; Shu, Peng; Shi, Enze; Hu, Huawen; Ma, Chong; Liu, Yiheng; Wang, Xuhui; Yao, Yincheng; Liu, Xuan; Zhao, Huaqin; Liu, Zhengliang; Dai, Haixing; Zhao, Lin; Ge, Bao; Li, Xiang; Liu, Tianming; Zhang, Shu				Zhao, Lin/ABM-7665-2022; wang, jiaqi/HHS-0123-2022; Li, Yiwei/JAX-1635-2023; SHU, PENG/LDF-4318-2024; Ma, Chong/MIT-9373-2025; Shi, Enze/IWD-9102-2023; Wu, Zihao/KDP-2552-2024; Li, Xiang/J-6924-2019; Liu, Tianming/GLS-1211-2022; yuan, yixuan/KLZ-6092-2024						Large Language Models for Robotics: Opportunities, Challenges, and Perspectives								Arxiv											1	1;2024-01-09;https://www.arxiv.org/abs/2401.04334v1	arXiv:2401.04334			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 09 2024	2024	Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.																																	2024-01-26	PPRN:87082942		
J	Lee, Mina; Srivastava, Megha; Hardy, Amelia; Thickstun, John; Durmus, Esin; Paranjape, Ashwin; Gerard-Ursin, Ines; Li, Xiang Lisa; Ladhak, Faisal; Rong, Frieda; Wang, Rose E.; Kwon, Minae; Park, Joon Sung; Cao, Hancheng; Lee, Tony; Bommasani, Rishi; Bernstein, Michael; Liang, Percy										Evaluating Human-Language Model Interaction								Arxiv											3	3;2024-01-05;https://www.arxiv.org/abs/2212.09746v5| 2;2023-09-10;https://www.arxiv.org/abs/2212.09746v4| 1;2023-07-12;https://www.arxiv.org/abs/2212.09746v3	arXiv:2212.09746			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 05 2024	2024	Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurassic-1), we find that better non-interactive performance does not always translate to better human-LM interaction. In particular, we highlight three cases where the results from non-interactive and interactive metrics diverge and underscore the importance of human-LM interaction for LM evaluation.																																	2024-05-25	PPRN:73887810		
J	Shi, Haizhou; Xu, Zihao; Wang, Hengyi; Qin, Weiyi; Wang, Wenyuan; Wang, Yibin; Wang, Zifeng; Ebrahimi, Sayna; Wang, Hao				xu, zihao/JDM-7099-2023						Continual Learning of Large Language Models: A Comprehensive Survey								Arxiv											3	3;2024-11-25;https://www.arxiv.org/abs/2404.16789v3| 2;2024-06-30;https://www.arxiv.org/abs/2404.16789v2| 1;2024-04-25;https://www.arxiv.org/abs/2404.16789v1	arXiv:2404.16789			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 25 2024	2024	The challenge of effectively and efficiently adapting statically pre-trained Large Language Models (LLMs) to ever-evolving data distributions remains predominant. When tailored for specific needs, pre-trained LLMs often suffer from significant performance degradation in previous knowledge domains – a phenomenon known as “catastrophic forgetting”.While extensively studied in the Continual Learning (CL) community, this problem presents new challenges in the context of LLMs. In this survey, we provide a comprehensive overview and detailed discussion of the current research progress on LLMs within the context of CL. Besides the introduction of the preliminary knowledge, this survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). Following vertical continuity, we summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). We then provide an overview of evaluation protocols for continual learning with LLMs, along with currently available data sources (Section 5). Finally, we discuss intriguing questions related to continual learning for LLMs (Section 6). This survey sheds light on the relatively understudied domain of continually pre-training, adapting, and fine-tuning large language models, suggesting the necessity for greater attention from the community. Key areas requiring immediate focus include the development of practical and accessible evaluation benchmarks, along with methodologies specifically designed to counter forgetting and enable knowledge transfer within the evolving landscape of LLM learning paradigms. 																																	2025-01-08	PPRN:88651007		
J	Liao, Bencheng; Chen, Shaoyu; Zhang, Yunchi; Jiang, Bo; Zhang, Qian; Liu, Wenyu; Huang, Chang; Wang, Xinggang				Wang, Xinggang/LSL-0946-2024; Wenyu, Liu/GRS-3009-2022; Chen, Shaoyu/MGA-4947-2025						MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction								Arxiv											2	2;2024-10-25;https://www.arxiv.org/abs/2308.05736v2| 1;2023-08-10;https://www.arxiv.org/abs/2308.05736v1	arXiv:2308.05736			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 25 2024	2024	High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present Map TR ansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i . e ., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-tomany matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at https://github.com/hustvl/MapTR for facilitating further studies and applications.																																	2024-11-29	PPRN:75366317		
J	Zhuge, Mingchen; Zhao, Changsheng; Ashley, Dylan; Wang, Wenyi; Khizbullin, Dmitrii; Xiong, Yunyang; Liu, Zechun; Chang, Ernie; Krishnamoorthi, Raghuraman; Tian, Yuandong; Shi, Yangyang; Chandra, Vikas; Schmidhuber, Jurgen				Shi, yangyang/AAN-4910-2020; Khizbullin, Dmitrii/KSM-5227-2024; liu, zechun/LBH-4471-2024; Zhao, Changhong/KRQ-4153-2024; Xiong, Yunyang/HHZ-6012-2022						Agent-as-a-Judge: Evaluate Agents with Agents								Arxiv											1	1;2024-10-16;https://www.arxiv.org/abs/2410.10934v2	arXiv:2410.10934			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 16 2024	2024	Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.																																	2024-11-07	PPRN:115379506		
J	Bailey, Luke; Ong, Euan; Russell, Stuart; Emmons, Scott										Image Hijacks: Adversarial Images can Control Generative Models at Runtime								Arxiv											4	4;2024-09-17;https://www.arxiv.org/abs/2309.00236v4| 3;2024-04-22;https://www.arxiv.org/abs/2309.00236v3| 2;2023-09-18;https://www.arxiv.org/abs/2309.00236v2| 1;2023-09-01;https://www.arxiv.org/abs/2309.00236v1	arXiv:2309.00236			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 17 2024	2024	Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.																																	2024-09-30	PPRN:84668314		
J	Lu, Xing Han; Kasner, Zdenek; Reddy, Siva				Kasner, Zdenek/HPD-5993-2023						WebLINX: Real-World Website Navigation with Multi-Turn Dialogue								Arxiv											2	2;2024-09-10;https://www.arxiv.org/abs/2402.05930v2| 1;2024-02-08;https://www.arxiv.org/abs/2402.05930v1	arXiv:2402.05930			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Sep 10 2024	2024	We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. 																																	2024-09-26	PPRN:87572400		
J	Xu, Zhangchen; Jiang, Fengqing; Niu, Luyao; Jia, Jinyuan; Lin, Bill Yuchen; Poovendran, Radha				Niu, Luyao/AEN-7350-2022; Jia, Jinyuan/AAQ-5278-2020; Xu, Zhangchen/IYJ-6907-2023						SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding								Arxiv											5	5;2024-07-25;https://www.arxiv.org/abs/2402.08983v4| 4;2024-06-07;https://www.arxiv.org/abs/2402.08983v3| 3;2024-02-24;https://www.arxiv.org/abs/2402.08983v2| 2;2024-02-14;https://www.arxiv.org/abs/2402.08983v1| 1;2024-02-14;https://www.arxiv.org/abs/2402.08983v1	arXiv:2402.08983			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 25 2024	2024	As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks. We perform extensive experiments on five LLMs using six state-of-the-art jailbreak attacks and four benchmark datasets. Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries. SafeDecoding outperforms six defense methods.																																	2024-08-02	PPRN:87688335		
J	Yu, Ping; Xu, Jing; Weston, Jason; Kulikov, Ilia										Distilling System 2 into System 1								Arxiv											1	1;2024-07-09;https://www.arxiv.org/abs/2407.06023v2	arXiv:2407.06023			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 09 2024	2024	Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to produce better final responses. Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2 Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al., 2023). In this work we investigate self-supervised methods to ``compile'' (distill) higher quality outputs from System 2 techniques back into LLM generations without intermediate reasoning token sequences, as this reasoning has been distilled into System 1. We show that several such techniques can be successfully distilled, resulting in improved results compared to the original System 1 performance, and with less inference cost than System 2. We posit that such System 2 distillation will be an important feature of future continually learning AI systems, enabling them to focus System 2 capabilities on the reasoning tasks that they cannot yet do well.																																	2024-07-21	PPRN:90749698		
J	Nie, Yuqi; Kong, Yaxuan; Dong, Xiaowen; Mulvey, John M.; Poor, H. Vincent; Wen, Qingsong; Zohren, Stefan				Nie, Yuqi/GQP-6290-2022; Dong, Xiaowen/AAJ-5058-2021; Wen, Qingsong/LTF-7625-2024; Poor, H./S-5027-2016						A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges								Arxiv											1	1;2024-06-15;https://www.arxiv.org/abs/2406.11903v1	arXiv:2406.11903			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 15 2024	2024	Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.																																	2024-07-04	PPRN:89358943		
J	Mavromatis, Costas; Karypis, George										GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning								Arxiv											1	1;2024-05-30;https://www.arxiv.org/abs/2405.20139v1	arXiv:2405.20139			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 30 2024	2024	Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG. Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.																																	2024-06-16	PPRN:89113338		
J	McKenzie, Ian R.; Lyzhov, Alexander; Pieler, Michael; Parrish, Alicia; Mueller, Aaron; Prabhu, Ameya; McLean, Euan; Kirtland, Aaron; Ross, Alexis; Liu, Alisa; Gritsevskiy, Andrew; Wurgaft, Daniel; Kauffman, Derik; Recchia, Gabriel; Liu, Jiacheng; Cavanagh, Joe; Weiss, Max; Huang, Sicong; Tseng, Tom; Korbak, Tomasz; Shen, Xudong; Zhang, Yuhui; Zhou, Zhengping; Kim, Najoung; Bowman, Samuel R.; Perez, Ethan		Floating Droid		Liu, Jiacheng/ISS-1763-2023; Zhang, Yuhui/IST-0722-2023; Zhou, Zhengping/LCD-3744-2024; Shen, Xudong/KTI-7535-2024						Inverse Scaling: When Bigger Isn't Better								Arxiv											2	2;2024-05-13;https://www.arxiv.org/abs/2306.09479v2| 1;2023-06-15;https://www.arxiv.org/abs/2306.09479v1	arXiv:2306.09479			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 13 2024	2024	Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.																																	2024-08-02	PPRN:73398185		
J	Ge, Tao; Hu, Jing; Wang, Lei; Wang, Xun; Chen, Si-Qing; Wei, Furu										In-context Autoencoder for Context Compression in a Large Language Model								Arxiv											4	4;2024-05-08;https://www.arxiv.org/abs/2307.06945v4| 3;2024-03-18;https://www.arxiv.org/abs/2307.06945v3| 2;2023-10-02;https://www.arxiv.org/abs/2307.06945v2| 1;2023-07-13;https://www.arxiv.org/abs/2307.06945v1	arXiv:2307.06945			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 08 2024	2024	We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1% additional parameters, effectively achieves $4times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. [GRAPHICS]																																	2024-05-28	PPRN:73907289		
J	Tang, Jiabin; Yang, Yuhao; Wei, Wei; Shi, Lei; Su, Lixin; Cheng, Suqi; Yin, Dawei; Huang, Chao				Shi, Lei/T-7301-2019						GraphGPT: Graph Instruction Tuning for Large Language Models								Arxiv											3	3;2024-05-07;https://www.arxiv.org/abs/2310.13023v3| 2;2023-12-19;https://www.arxiv.org/abs/2310.13023v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.13023v1	arXiv:2310.13023			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 07 2024	2024	Graph Neural Networks (GNNs) have evolved to understand graph structures through recursive exchanges and aggregations among nodes. To enhance robustness, self-supervised learning (SSL) has become a vital tool for data augmentation. Traditional methods often depend on fine-tuning with task-specific labels, limiting their effectiveness when labeled data is scarce. Our research tackles this by advancing graph model generalization in zero-shot learning environments. Inspired by the success of large language models (LLMs), we aim to create a graph-oriented LLM capable of exceptional generalization across various datasets and tasks without relying on downstream graph data. We introduce the GraphGPT framework, which integrates LLMs with graph structural knowledge through graph instruction tuning. This framework includes a text-graph grounding component to link textual and graph structures and a dual-stage instruction tuning approach with a lightweight graph-text alignment projector. These innovations allow LLMs to comprehend complex graph structures and enhance adaptability across diverse datasets and tasks. Our framework demonstrates superior generalization in both supervised and zero-shot graph learning tasks, surpassing existing benchmarks. The open-sourced model implementation of our GraphGPT is available at https://github.com/HKUDS/GraphGPT.																																	2024-05-28	PPRN:85741784		
J	Stureborg, Rickard; Alikaniotis, Dimitris; Suhara, Yoshi										Large Language Models are Inconsistent and Biased Evaluators								Arxiv											1	1;2024-05-02;https://www.arxiv.org/abs/2405.01724v1	arXiv:2405.01724			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 02 2024	2024	The zero -shot capability of Large Language Models (LLMs) has enabled highly flexible, reference -free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias—a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi -attribute judgments. We also found that LLMs are inconsistent evaluators, showing low “inter -sample” agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.																																	2024-05-29	PPRN:88867848		
J	Wang, Shihao; Yu, Zhiding; Jiang, Xiaohui; Lan, Shiyi; Shi, Min; Chang, Nadine; Kautz, Jan; Li, Ying; Alvarez, Jose M.				jiang, xiaohui/GWZ-4110-2022						OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning								Arxiv											1	1;2024-05-02;https://www.arxiv.org/abs/2405.01533v1	arXiv:2405.01533			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	The advances in multimodal large language models (MLLMs) have led to growing interests in LLM-based autonomous driving agents to leverage their strong reasoning capabilities. However, capitalizing on MLLMs' strong reasoning capabilities for improved planning behavior is challenging since planning requires full 3D situational awareness beyond 2D reasoning. To address this challenge, our work proposes a holistic framework for strong alignment between agent models and 3D driving tasks. Our framework starts with a novel 3D MLLM architecture that uses sparse queries to lift and compress visual representations into 3D before feeding them into an LLM. This query-based representation allows us to jointly encode dynamic objects and static map elements (e.g., traffic lanes), providing a condensed world model for perception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new visual question-answering dataset challenging the true 3D situational awareness of a model with comprehensive visual question-answering (VQA) tasks, including scene description, traffic regulation, 3D grounding, counterfactual reasoning, decision making and planning. Extensive studies show the effectiveness of the proposed architecture as well as the importance of the VQA tasks for reasoning and planning in complex 3D scenes.																																	2024-05-19	PPRN:88722545		
J	Kim, Dahyun; Park, Chanjun; Kim, Sanghoon; Lee, Wonsung; Song, Wonho; Kim, Yunsu; Kim, Hyeonwoo; Kim, Yungi; Lee, Hyeonju; Kim, Jihoo; Ahn, Changbae; Yang, Seonghoon; Lee, Sukyung; Park, Hyunbyung; Gim, Gyoungjin; Cha, Mikyoung; Lee, Hwalsuk; Kim, Sunghun										SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling								Arxiv											3	3;2024-04-04;https://www.arxiv.org/abs/2312.15166v3| 2;2023-12-29;https://www.arxiv.org/abs/2312.15166v2| 1;2023-12-23;https://www.arxiv.org/abs/2312.15166v1	arXiv:2312.15166			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 04 2024	2024	We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field1.																																	2024-04-19	PPRN:86827246		
J	Pei, Xiaohuan; Huang, Tao; Xu, Chang				Xu, Chang/AAG-9337-2019						EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba								Arxiv											1	1;2024-03-15;https://www.arxiv.org/abs/2403.09977v1	arXiv:2403.09977			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 15 2024	2024	Prior efforts in light -weight model development mainly centered on CNN and Transformer -based designs yet faced persistent challenges. CNNs adept at local feature extraction compromise resolution while Transformers offer global reach but escalate computational demands O(N2). This ongoing trade-off between accuracy and efficiency remains a significant hurdle. Recently, state space models (SSMs), such as Mamba, have shown outstanding performance and competitiveness in various tasks such as language modeling and computer vision, while reducing the time complexity of global information extraction to O(N). Inspired by this, this work proposes to explore the potential of visual state space models in light -weight model design and introduce a novel efficient model variant dubbed EfficientVMamba. Concretely, our EfficientVMamba integrates a atrous-based selective scan approach by efficient skip sampling, constituting building blocks designed to harness both global and local representational features. Additionally, we investigate the integration between SSM blocks and convolutions, and introduce an efficient visual state space block combined with an additional convolution branch, which further elevate the model performance. Experimental results show that, EfficientVMamba scales down the computational complexity while yields competitive results across a variety of vision tasks. For example, our EfficientVMamba-S with 1.3G FLOPs improves VimTi with 1.5G FLOPs by a large margin of 5.6% accuracy on ImageNet. 																																	2024-04-11	PPRN:88170531		
J	Coste, Thomas; Anwar, Usman; Kirk, Robert; Krueger, David										Reward Model Ensembles Help Mitigate Overoptimization								Arxiv											2	2;2024-03-10;https://www.arxiv.org/abs/2310.02743v2| 1;2023-10-04;https://www.arxiv.org/abs/2310.02743v1	arXiv:2310.02743			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 10 2024	2024	Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the “true” reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger “gold” reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble -based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best -of -n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real -world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble -based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble -based conservative optimization can effectively counter overoptimization.																																	2024-04-08	PPRN:85399078		
J	Colombo, Pierre; Pires, Telmo Pessoa; Boudiaf, Malik; Culver, Dominic; Melo, Rui; Corro, Caio; Martins, Andre F.T.; Esposito, Fabrizio; Raposo, Vera Lucia; Morgado, Sofia; Desa, Michael				Torres Martins, Andre Filipe/JXL-9782-2024; Corro, Caio/GSO-3608-2022; Raposo, Vera Lúcia/S-8708-2019						SaulLM-7B: A pioneering Large Language Model for Law								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.03883v2	arXiv:2403.03883			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 07 2024	2024	In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional finetuning method that leverages legal datasets to further enhance SaulLM-7B’s performance in legal tasks. SaulLM-7B is released under the MIT License.																																	2024-04-05	PPRN:88055610		
J	Scheurer, Jeremy; Campos, Jon Ander; Korbak, Tomasz; Chan, Jun Shern; Chen, Angelica; Cho, Kyunghyun; Perez, Ethan				Campos, Juan/AFU-0131-2022						Training Language Models with Language Feedback at Scale								Arxiv											2	2;2024-02-22;https://www.arxiv.org/abs/2303.16755v3| 1;2023-03-28;https://www.arxiv.org/abs/2303.16755v1	arXiv:2303.16755			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 22 2024	2024	Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.																																	2024-03-23	PPRN:56492575		
J	Golchin, Shahriar; Surdeanu, Mihai										Time Travel in LLMs: Tracing Data Contamination in Large Language Models								Arxiv											3	3;2024-02-21;https://www.arxiv.org/abs/2308.08493v3| 2;2023-10-01;https://www.arxiv.org/abs/2308.08493v2| 1;2023-08-16;https://www.arxiv.org/abs/2308.08493v1	arXiv:2308.08493			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 21 2024	2024	Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs’ real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ “guided instruction:” a prompt consisting of the dataset name, partition type, and the random -length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM’s output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE -L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a “general instruction” that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few -shot in -context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.1																																	2024-03-21	PPRN:78566705		
J	Gunjal, Anisha; Yin, Jihan; Bas, Erhan										Detecting and Preventing Hallucinations in Large Vision Language Models								Arxiv											2	2;2024-02-11;https://www.arxiv.org/abs/2308.06394v3| 1;2023-08-11;https://www.arxiv.org/abs/2308.06394v1	arXiv:2308.06394			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 11 2024	2024	Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi -modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi -modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multimodal reward models from InstructBLIP and evaluate their effectiveness with best -of -n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi -modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.																																	2024-05-25	PPRN:77272306		
J	Chen, Sizhe; Piet, Julien; Sitawarin, Chawin; Wagner, David				Sitawarin, Chawin/KIB-4488-2024; Chen, Sizhe/ACB-4737-2022						StruQ: Defending Against Prompt Injection with Structured Queries								Arxiv											1	1;2024-02-09;https://www.arxiv.org/abs/2402.06363v1	arXiv:2402.06363			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 09 2024	2024	Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model to deviate from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate the prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained LLM that can produce high-quality outputs from these inputs. The LLM is trained using a novel fine-tuning strategy: we convert a base (non-instruction-tuned) LLM to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. 																																	2024-02-26	PPRN:87616184		
J	Maiolino, Roberto; Scholtz, Jan; Witstok, Joris; Carniani, Stefano; D'Eugenio, Francesco; Graaff, Anna de; Uebler, Hannah; Tacchella, Sandro; Curtis-Lake, Emma; Arribas, Santiago; Bunker, Andrew; Charlot, Stephane; Chevallard, Jacopo; Curti, Mirko; Looser, Tobias J.; Maseda, Michael V.; Rawle, Tim; Pino, Bruno Rodriguez Del; Willott, Chris J.; Egami, Eiichi; Eisenstein, Daniel; Hainline, Kevin; Robertson, Brant; Williams, Christina C.; Willmer, Christopher N.A.; Baker, William M.; Boyett, Kristan; Decoursey, Christa; Fabian, Andrew C.; Helton, Jakob M.; Ji, Zhiyuan; Jones, Gareth C.; Kumari, Nimisha; Laporte, Nicolas; Nelson, Erica; Perna, Michele; Sandles, Lester; Shivaei, Irene; Sun, Fengwu				Jones, Gareth/AAD-7663-2022; ji, zhiyuan/HGC-6180-2022; Nelson, Erica/OUI-1817-2025; Witstok, Joris/GQA-8643-2022; D'Eugenio, Francesco/H-2606-2019; Arribas, Santiago/F-9277-2015; Tacchella, Sandro/AAT-1602-2021; Helton, Jakob/KXS-1907-2024; Robertson, Brant/AAA-6124-2022; Kumari, Nimisha/AFS-1631-2022; Del Pino, Bruno/C-3326-2017; Baker, William/KUD-6412-2024						A small and vigorous black hole in the early Universe								Arxiv											1	1;2024-01-17;https://www.arxiv.org/abs/2305.12492v2	arXiv:2305.12492			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 17 2024	2024	Multiple theories have been proposed to describe the formation of black hole seeds in the early Universe and to explain the emergence of very massive black holes observed in the first billion years after Big Bang [1–3]. Models consider different seeding and accretion scenarios [4–7], which require the detection and characterisation of black holes in the first few hundred million years after Big Bang to be validated. Here we present an extensive analysis of the JWST-NIRSpec spectrum of GN-z11, an exceptionally luminous galaxy at z=10.6, revealing the detection of the [NeIV]λ2423 and CII*λ1335 transitions (typical of Active Galactic Nuclei, AGN), as well as semi-forbidden nebular lines tracing gas densities higher than 109 cm−3, typical of the Broad Line Region of AGN. These spectral features indicate that GN-z11 hosts an accreting black hole. The spectrum also reveals a deep and blueshifted CIVλ1549 absorption trough, tracing an outflow with velocity 800 − 1000 km s−1, likely driven by the AGN. Assuming local virial relations, we derive a black hole mass of log (MBH/M⊙) = 6.2 ± 0.3, accreting at about 5 times the Eddington rate. These properties are consistent with both heavy seeds scenarios, or scenarios envisaging inter-mediate/light seeds experiencing episodic super-Eddington phases. Our finding naturally explains the high luminosity of GN-z11 and can also provide an explanation for its exceptionally high nitrogen abundance.																																	2024-02-29	PPRN:70941959		
J	Beznosikov, Aleksandr; Horvath, Samuel; Richtarik, Peter; Safaryan, Mher				Richtarik, Peter/O-5797-2018; Safaryan, Mher/ACX-4038-2022						On Biased Compression for Distributed Learning								Arxiv											2	2;2024-01-14;https://www.arxiv.org/abs/2002.12410v4| 1;2020-02-27;https://www.arxiv.org/abs/2002.12410v2	arXiv:2002.12410			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 14 2024	2024	In the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. We prove that distributed compressed SGD method, employed with error feedback mechanism, enjoys the ergodic rate $Oleft( delta L exp left[-frac{mu K}{delta L}right] + frac{(C + delta D)}{Kmu}right)$, where $deltage 1$ is a compression parameter which grows when more compression is applied, $L$ and $mu$ are the smoothness and strong convexity constants, $C$ captures stochastic gradient noise ($C=0$ if full gradients are computed on each node) and $D$ captures the variance of the gradients at the optimum ($D=0$ for over-parameterized models). Further, via a theoretical study of several synthetic and empirical distributions of communicated gradients, we shed light on why and by how much biased compressors outperform their unbiased variants. Finally, we propose several new biased compressors with promising theoretical guarantees and practical performance.																																	2024-05-25	PPRN:12003327		
J	Tong, Shengbang; Fan, David; Zhu, Jiachen; Xiong, Yunyang; Chen, Xinlei; Sinha, Koustuv; Rabbat, Michael; Lecun, Yann; Xie, Saining; Liu, Zhuang				Rabbat, Michael/G-4582-2012; Xiong, Yunyang/HHZ-6012-2022						MetaMorph: Multimodal Understanding and Generation via Instruction Tuning								Arxiv											1	1;2024-12-18;https://www.arxiv.org/abs/2412.14164v1	arXiv:2412.14164			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 18 2024	2024	In this work, we propose Visual-Predictive Instruction Tuning (VPiT)—a simple and effective extension to visual instruction tuning that enables a pretrained LLM to quickly morph into an unified autoregressive model capable of generating both text and visual tokens. VPiT teaches an LLM to predict discrete text tokens and continuous visual tokens from any input sequence of image and text data curated in an instruction-following format. Our empirical investigation reveals several intriguing properties of VPiT: (1) visual generation ability emerges as a natural byproduct of improved visual understanding, and can be unlocked efficiently with a small amount of generation data; (2) while we find understanding and generation to be mutually beneficial, understanding data contributes to both capabilities more effectively than generation data. Building upon these findings, we train our MetaMorph model and achieve competitive performance on both visual understanding and generation. In visual generation, MetaMorph can leverage the world knowledge and reasoning abilities gained from LLM pretraining, and overcome common failure modes exhibited by other generation models. Our results suggest that LLMs may have strong “prior” vision capabilities that can be efficiently adapted to both visual understanding and generation with a relatively simple instruction tuning process.																																	2025-01-24	PPRN:120038851		
J	Aalbers, J.; Akerib, D.S.; Al Musalhi, A.K.; Alder, F.; Amarasinghe, C.S.; Ames, A.; Anderson, T.J.; Angelides, N.; Araujo, H.M.; Armstrong, J.E.; Arthurs, M.; Baker, A.; Balashov, S.; Bang, J.; Bargemann, J.W.; Barillier, E.E.; Bauer, D.; Beattie, K.; Benson, T.; Bhatti, A.; Biekert, A.; Biesiadzinski, T.P.; Birch, H.J.; Bishop, E.; Blockinger, G.M.; Boxer, B.; Brew, C.A.J.; Bras, P.; Burdin, S.; Buuck, M.; Carmona-Benitez, M.C.; Carter, M.; Chawla, A.; Chen, H.; Cherwinka, J.J.; Chin, Y.T.; Chott, N.I.; Converse, M.V.; Coronel, R.; Cottle, A.; Cox, G.; Curran, D.; Dahl, C.E.; Darlington, I.; Dave, S.; David, A.; Delgaudio, J.; Dey, S.; de Viveiros, L.; Di Felice, L.; Ding, C.; Dobson, J.E.Y.; Druszkiewicz, E.; Dubey, S.; Eriksen, S.R.; Fan, A.; Fayer, S.; Fearon, N.M.; Fieldhouse, N.; Fiorucci, S.; Flaecher, H.; Fraser, E.D.; Fruth, T.M.A.; Gaitskell, R.J.; Geffre, A.; Genovesi, J.; Ghag, C.; Ghosh, A.; Gibbons, R.; Gokhale, S.; Green, J.; van der Grinten, M.G.D.; Haiston, J.J.; Hall, C.R.; Hall, T.J.; Han, S.; Hartigan-O'Connor, E.; Haselschwardt, S.J.; Hernandez, M.A.; Hertel, S.A.; Heuermann, G.; Homenides, G.J.; Horn, M.; Huang, D.Q.; Hunt, D.; Jacquet, E.; James, R.S.; Johnson, J.; Kaboth, A.C.; Kamaha, A.C.; Khaitan, D.; Khazov, A.; Khurana, I.; Kim, J.; Kim, Y.D.; Kingston, J.; Kirk, R.; Kodroff, D.; Korley, L.; Korolkova, E.V.; Kraus, H.; Kravitz, S.; Kreczko, L.; Kudryavtsev, V.A.; Lawes, C.; Leonard, D.S.; Lesko, K.T.; Levy, C.; Lin, J.; Lindote, A.; Lippincott, W.H.; Lopes, M.I.; Lorenzon, W.; Lu, C.; Luitz, S.; Majewski, P.A.; Manalaysay, A.; Mannino, R.L.; Maupin, C.; Mccarthy, M.E.; Mcdowell, G.; Mckinsey, D.N.; Mclaughlin, J.; Mclaughlin, J.B.; Mcmonigle, R.; Mizrachi, E.; Monte, A.; Monzani, M.E.; Morales Mendoza, J.D.; Morrison, E.; Mount, B.J.; Murdy, M.; Murphy, A.St.J.; Naylor, A.; Nelson, H.N.; Neves, F.; Nguyen, A.; O'Brien, C.L.; Olcina, I.; Oliver-Mallory, K.C.; Orpwood, J.; Oyulmaz, K.; Palladino, K.J.; Palmer, J.; Pannifer, N.J.; Parveen, N.; Patton, S.J.; Penning, B.; Pereira, G.; Perry, E.; Pershing, T.; Piepke, A.; Qie, Y.; Reichenbacher, J.; Rhyne, C.A.; Richards, A.; Riffard, Q.; Rischbieter, G.R.C.; Ritchey, E.; Riyat, H.S.; Rosero, R.; Rushton, T.; Rynders, D.; Santone, D.; Sazzad, A.B.M.R.; Schnee, R.W.; Sehr, G.; Shafer, B.; Shaw, S.; Shutt, T.; Silk, J.J.; Silva, C.; Sinev, G.; Siniscalco, J.; Smith, R.; Solovov, V.N.; Sorensen, P.; Soria, J.; Stancu, I.; Stevens, A.; Stifter, K.; Suerfu, B.; Sumner, T.J.; Szydagis, M.; Tiedt, D.R.; Timalsina, M.; Tong, Z.; Tovey, D.R.; Tranter, J.; Trask, M.; Tripathi, M.; Uson, A.; Vacheret, A.; Vaitkus, A.C.; Valentino, O.; Velan, V.; Wang, A.; Wang, J.J.; Wang, Y.; Watson, J.R.; Weeldreyer, L.; Whitis, T.J.; Wild, K.; Williams, M.; Wisniewski, W.J.; Wolf, L.; Wolfs, F.L.H.; Woodford, S.; Woodward, D.; Wright, C.J.; Xia, Q.; Xu, J.; Xu, Y.; Yeh, M.; Yeum, D.; Zha, W.; Zweig, E.A.				de Viveiros, Luiz/M-9205-2013; Ghag, Chamkaur/H-2869-2012; Wild, Kristoffer/IUW-2166-2023; Birch, Heather/ABB-5432-2021; Pascoal da Silva, Cláudio Frederico/JRX-3674-2023; Lopes, Max/I-6835-2016; Coronel, Richard/NAX-5305-2025; Lippincott, Hugh/AAW-8550-2020; Monzani, Maria Elena/JBS-2456-2023; Fraser, Evan/F-7967-2011; del Carmen Carmona Benitez, Maria/AAY-4411-2020; Burdin, Sergey/AAZ-9062-2021						Dark Matter Search Results from 4.2 Tonne-Years of Exposure of the LUX-ZEPLIN (LZ) Experiment								Arxiv											1	1;2024-10-22;https://www.arxiv.org/abs/2410.17036v1	arXiv:2410.17036			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 22 2024	2024	We report results of a search for nuclear recoils induced by weakly interacting massive particle (WIMP) dark matter using the LUX-ZEPLIN (LZ) two-phase xenon time projection chamber. This analysis uses a total exposure of 4.2 ± 0.1 tonne-years from 280 live days of LZ operation, of which 3.3 ± 0.1 tonne-years and 220 live days are new. A technique to actively tag background electronic recoils from 214 Pb β decays is featured for the first time. Enhanced electron-ion recombination is observed in two-neutrino double electron capture decays of 124 Xe, representing a noteworthy new background. After removal of artificial signal-like events injected into the data set to mitigate analyzer bias, we find no evidence for an excess over expected backgrounds. World-leading constraints are placed on spin-independent (SI) and spin-dependent WIMP-nucleon cross sections for masses ≥9 GeV/c2. The strongest SI exclusion set is 2.1 × 10−48 cm2 at the 90% confidence level at a mass of 36 GeV/c2, and the best SI median sensitivity achieved is 5.0 × 10−48 cm2 for a mass of 40 GeV/c2.																																	2025-01-24	PPRN:118770927		
J	Cheng, An-Chieh; Yin, Hongxu; Fu, Yang; Guo, Qiushan; Yang, Ruihan; Kautz, Jan; Wang, Xiaolong; Liu, Sifei				Guo, Qiushan/KYP-2884-2024; Liu, Sifei/AGE-1968-2022; Yang, Ruihan/AAC-7500-2020; Fu, Yang/AAC-6064-2019; Yin, Hongxu/AAZ-3328-2020						SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models								Arxiv											3	3;2024-10-15;https://www.arxiv.org/abs/2406.01584v3| 2;2024-06-18;https://www.arxiv.org/abs/2406.01584v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01584v1	arXiv:2406.01584			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 15 2024	2024	Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs' spatial perception and reasoning capabilities. SpatialRGPT advances VLMs' spatial understanding through two key innovations: (1) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (2) a flexible plugin module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in VLMs. Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. 																																	2024-11-10	PPRN:89161061		
J	Xie, Enze; Chen, Junsong; Chen, Junyu; Cai, Han; Tang, Haotian; Lin, Yujun; Zhang, Zhekai; Li, Muyang; Zhu, Ligeng; Lu, Yao; Han, Song				Lin, Yujun/AAR-9588-2020; Chen, Junsong/HII-4662-2022; Cai, Han/NKP-2131-2025						SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers								Arxiv											1	1;2024-10-15;https://www.arxiv.org/abs/2410.10629v2	arXiv:2410.10629			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 15 2024	2024	We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096×4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8×, we trained an AE that can compress images 32×, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024×1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released.																																	2024-11-10	PPRN:112886430		
J	Mao, Jiageng; Ye, Junjie; Qian, Yuxi; Pavone, Marco; Wang, Yue				Mao, Jiageng/HLG-4855-2023						A Language Agent for Autonomous Driving								Arxiv											3	3;2024-07-28;https://www.arxiv.org/abs/2311.10813v4| 2;2023-11-21;https://www.arxiv.org/abs/2311.10813v2| 1;2023-11-17;https://www.arxiv.org/abs/2311.10813v1	arXiv:2311.10813			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 28 2024	2024	Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perceptionprediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our system, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our system on both open-loop and close-loop driving challenges, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin (more than 30% on the nuScenes dataset). Our approach also demonstrates superior interpretability and few-shot learning ability to these methods. Please visit our webpage for more details.																																	2024-08-06	PPRN:86210493		
J	Liu, Zechun; Zhao, Changsheng; Iandola, Forrest; Lai, Chen; Tian, Yuandong; Fedorov, Igor; Xiong, Yunyang; Chang, Ernie; Shi, Yangyang; Krishnamoorthi, Raghuraman; Lai, Liangzhen; Chandra, Vikas				Shi, yangyang/AAN-4910-2020; Xiong, Yunyang/HHZ-6012-2022; Zhao, Changhong/KRQ-4153-2024; liu, zechun/LBH-4471-2024; Fedorov, Igor/ABG-5486-2021						MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases								Arxiv											2	2;2024-06-27;https://www.arxiv.org/abs/2402.14905v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.14905v1	arXiv:2402.14905			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 27 2024	2024	This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.																																	2024-07-17	PPRN:87868564		
J	Chen, Justin Chih-Yao; Saha, Swarnadeep; Bansal, Mohit				Bansal, Mohit/Q-9105-2016						ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs								Arxiv											2	2;2024-06-21;https://www.arxiv.org/abs/2309.13007v3| 1;2023-09-22;https://www.arxiv.org/abs/2309.13007v1	arXiv:2309.13007			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 21 2024	2024	Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose R E C ONCILE , a multi-model multiagent framework designed as a round table conference among diverse LLM agents. RECONCILE enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, RECONCILE initiates discussion between agents via a ‘discussion prompt’ that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that RECONCILE significantly improves LLMs’ reasoning – both individually and as a team – surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. RECONCILE also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of RECONCILE , demonstrating that the diversity originating from different models is critical to its superior performance.																																	2024-07-12	PPRN:85175661		
J	Wang, Yufei; Xian, Zhou; Chen, Feng; Wang, Tsun-Hsuan; Wang, Yian; Fragkiadaki, Katerina; Erickson, Zackory; Held, David; Gan, Chuang				Wang, Tsun-Hsuan/JYP-5264-2024						RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation								Arxiv											3	3;2024-06-14;https://www.arxiv.org/abs/2311.01455v3| 2;2023-11-13;https://www.arxiv.org/abs/2311.01455v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01455v1	arXiv:2311.01455			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	We present RoboGen, , a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. . RoboGen leverages the latest advancements in foundation and generative models. Instead of directly adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self -guided propose -generate - learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates simulation environments by populating pertinent assets with proper spatial configurations. Afterwards, the agent decomposes the proposed task into sub -tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.																																	2024-07-04	PPRN:85984864		
J	Deng, Yuntian; Choi, Yejin; Shieber, Stuart				Shieber, Stuart/ABE-8961-2020; Deng, yuntian/KIB-9835-2024						From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14838v1	arXiv:2405.14838			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.																																	2024-06-05	PPRN:88989510		
J	Yue, Xiang; Zheng, Tuney; Zhang, Ge; Chen, Wenhu										MAmmoTH2: Scaling Instructions from the Web								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2405.03548v4| 1;2024-05-06;https://www.arxiv.org/abs/2405.03548v1	arXiv:2405.03548			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data. [GRAPHICS]																																	2024-06-05	PPRN:88839343		
J	Ma, Jian; Liang, Junhao; Chen, Chen; Lu, Haonan				JIAN, MA/KQU-7977-2024						Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning								Arxiv											1	1;2024-05-19;https://www.arxiv.org/abs/2307.11410v2	arXiv:2307.11410			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 19 2024	2024	Recent progress in personalized image generation using diffusion models has been significant. However, development in the area of open-domain and non-fine-tuning personalized image generation is proceeding rather slowly. In this paper, we propose Subject-Diffusion, a novel open-domain personalized image generation model that, in addition to not requiring test-time fine-tuning, also only requires a single reference image to support personalized generation of single- or multi-subject in any domain. Firstly, we construct an automatic data labeling tool and use the LAION-Aesthetics dataset to construct a large-scale dataset consisting of 76M images and their corresponding subject detection bounding boxes, segmentation masks and text descriptions. Secondly, we design a new unified framework that combines text and image semantics by incorporating coarse location and fine-grained reference image control to maximize subject fidelity and generalization. Furthermore, we also adopt an attention control mechanism to support multi-subject generation. Extensive qualitative and quantitative results demonstrate that our method outperforms other SOTA frameworks in single, multiple, and human customized image generation.																																	2024-06-01	PPRN:88942880		
J	Zhao, Guosheng; Wang, Xiaofeng; Zhu, Zheng; Chen, Xinze; Huang, Guan; Bao, Xiaoyi; Wang, Xingang				Wang, Xiaofeng/GLR-2215-2022; Zhu, Zheng/AGE-0902-2022						DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation								Arxiv											2	2;2024-04-11;https://www.arxiv.org/abs/2403.06845v2| 1;2024-03-11;https://www.arxiv.org/abs/2403.06845v1	arXiv:2403.06845			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 11 2024	2024	World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos. However, significant challenges still exist in generating customized driving videos. In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos. Specifically, an LLM interface is initially incorporated to convert a user's query into agent trajectories. Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories. Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos. DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking). Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of  ∼30% and ∼50%.																																	2024-04-25	PPRN:88107254		
J	Dong, Xiaoyi; Zhang, Pan; Zang, Yuhang; Cao, Yuhang; Wang, Bin; Ouyang, Linke; Zhang, Songyang; Duan, Haodong; Zhang, Wenwei; Li, Yining; Yan, Hang; Gao, Yang; Chen, Zhe; Zhang, Xinyue; Li, Wei; Li, Jingwen; Wang, Wenhai; Chen, Kai; He, Conghui; Zhang, Xingcheng; Dai, Jifeng; Qiao, Yu; Lin, Dahua; Wang, Jiaqi				Dong, Xiaoyi/AAC-8666-2019; GAO, Yang/HMO-8142-2023; Zang, Yuhang/AES-3018-2022; Duan, Haodong/ITV-1505-2023; Wang, Wen-Jing/HOH-7164-2023; He, Conghui/AAZ-3323-2021; Lin, Dahua/W-6576-2019; Zhang, Wenwei/HKO-4277-2023; WANG, JIAQI/KBB-8837-2024; Zhang, Songyang/GPX-5621-2022; Zhang, Xingcheng/AAC-6392-2019; Dai, Jifeng/HGU-8741-2022						InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD								Arxiv											1	1;2024-04-09;https://www.arxiv.org/abs/2404.06512v1	arXiv:2404.06512			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 09 2024	2024	The Large Vision-Language Model (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution. Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 x 1500 pixels and constrained to a relatively narrow resolution range. This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 x 1600) and beyond. Concurrently, considering the ultra-high resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability. Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration. It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained Vision Transformer (ViT) (336 x 336), leading to dynamic training resolution from 336 pixels to 4K standard. Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements. InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks. 																																	2024-04-22	PPRN:88468388		
J	Ataallah, Kirolos; Shen, Xiaoqian; Abdelrahman, Eslam; Sleiman, Essam; Zhu, Deyao; Ding, Jian; Elhoseiny, Mohamed				Elhoseiny, Mohamed/X-6406-2019						MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens								Arxiv											1	1;2024-04-04;https://www.arxiv.org/abs/2404.03413v1	arXiv:2404.03413			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 04 2024	2024	This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. 																																	2024-04-19	PPRN:88405686		
J	Lyu, Hanjia; Jiang, Song; Zeng, Hanqing; Xia, Yinglong; Wang, Qifan; Zhang, Si; Chen, Ren; Leung, Christopher; Tang, Jiajie; Luo, Jiebo				Luo, Jiebo/AAI-7549-2020; Zeng, Hanqing/AAG-4038-2019; JiaJie, Tang/MSW-4346-2025; LYU, HANJIA/JCD-5591-2023						LLM-Rec: Personalized Recommendation via Prompting Large Language Models								Arxiv											2	2;2024-04-02;https://www.arxiv.org/abs/2307.15780v3| 1;2023-07-24;https://www.arxiv.org/abs/2307.15780v1	arXiv:2307.15780			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model's comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.																																	2024-04-18	PPRN:74187717		
J	Bai, Fan; Du, Yuxin; Huang, Tiejun; Meng, Max Q.-H.; Zhao, Bo				Zhao, Bo/AGZ-0290-2022; du, yuxin/IAP-7044-2023; 孟, 祥/KHX-7464-2024						M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models								Arxiv											1	1;2024-03-31;https://www.arxiv.org/abs/2404.00578v1	arXiv:2404.00578			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 31 2024	2024	Medical image analysis is essential to clinical diagnosis and treatment, which is increasingly supported by multi-modal large language models (MLLMs). However, previous research has primarily focused on 2D medical images, leaving 3D images under-explored, despite their richer spatial information. This paper aims to advance 3D medical image analysis with MLLMs. To this end, we present a large-scale 3D multi-modal medical dataset, M3D-Data, comprising 120K image-text pairs and 662K instruction-response pairs specifically tailored for various 3D medical tasks, such as image-text retrieval, report generation, visual question answering, positioning, and segmentation. Additionally, we propose M3D-LaMed, a versatile multi-modal large language model for 3D medical image analysis. Furthermore, we introduce a new 3D multi-modal medical benchmark, M3D-Bench, which facilitates automatic evaluation across eight tasks. Through comprehensive evaluation, our method proves to be a robust model for 3D medical image analysis, outperforming existing solutions. 																																	2024-04-18	PPRN:88366955		
J	Shao, Hao; Qian, Shengju; Xiao, Han; Song, Guanglu; Zong, Zhuofan; Wang, Letian; Liu, Yu; Li, Hongsheng				Li, Hongsheng/AES-5328-2022; wang, letian/NDS-0599-2025						Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models								Arxiv											3	3;2024-11-04;https://www.arxiv.org/abs/2403.16999v3| 2;2024-07-08;https://www.arxiv.org/abs/2403.16999v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16999v1	arXiv:2403.16999			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 25 2024	2024	This paper presents Visual CoT, a novel pipeline that leverages the reasoning capabilities of multi-modal large language models (MLLMs) by incorporating visual Chain-of-Thought (CoT) reasoning. While MLLMs have shown promise in various visual tasks, they often lack interpretability and struggle with complex visual inputs. To address these challenges, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We collect and introduce the Visual CoT dataset comprising 373k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Importantly, the introduced benchmark is capable of evaluating MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available to foster further research in this direction.																																	2025-08-07	PPRN:88278196		
J	Duval, Alexandre; Mathis, Simon V.; Joshi, Chaitanya K.; Schmidt, Victor; Miret, Santiago; Malliaros, Fragkiskos D.; Cohen, Taco; Lio, Pietro; Bengio, Yoshua; Bronstein, Michael				P. Lió, Pietro/AAV-3358-2021						A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems								Arxiv											2	2;2024-03-13;https://www.arxiv.org/abs/2312.07511v2| 1;2023-12-12;https://www.arxiv.org/abs/2312.07511v1	arXiv:2312.07511			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 13 2024	2024	Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations. In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation. Their specificity lies in the inductive biases they leverage - such as physical symmetries and chemical properties - to learn informative representations of these geometric graphs. In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems. We cover fundamental background material and introduce a pedagogical taxonomy of Geometric GNN architectures: (1) invariant networks, (2) equivariant networks in Cartesian basis, (3) equivariant networks in spherical basis, and (4) unconstrained networks. Additionally, we outline key datasets and application areas and suggest future research directions. The objective of this work is to present a structured perspective on the field, making it accessible to newcomers and aiding practitioners in gaining an intuition for its mathematical abstractions.																																	2024-04-11	PPRN:86573219		
J	Zhu, Banghua; Jordan, Michael I.; Jiao, Jiantao				Jordan, Michael/C-5253-2013						Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons								Arxiv											2	2;2024-02-08;https://www.arxiv.org/abs/2301.11270v5| 1;2023-01-26;https://www.arxiv.org/abs/2301.11270v1	arXiv:2301.11270			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 08 2024	2024	We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley -Terry -Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the K -wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. We also unify the problem of RLHF and max -entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound for max -entropy IRL.																																	2024-05-25	PPRN:35976909		
J	Li, Yizhi; Yuan, Ruibin; Zhang, Ge; Ma, Yinghao; Chen, Xingran; Yin, Hanzhi; Xiao, Chenghao; Lin, Chenghua; Ragni, Anton; Benetos, Emmanouil; Gyenge, Norbert; Dannenberg, Roger; Liu, Ruibo; Chen, Wenhu; Xia, Gus; Shi, Yemin; Huang, Wenhao; Wang, Zili; Guo, Yike; Fu, Jie				Huang, Wenhao/GWU-9337-2022; li, yizhi/HZM-4299-2023; Benetos, Emmanouil/S-1932-2018; Chen, Xingran/KUD-3118-2024; Chen, Guanyi/HJY-0461-2023						MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training								Arxiv											4	4;2024-12-27;https://www.arxiv.org/abs/2306.00107v5| 3;2024-04-22;https://www.arxiv.org/abs/2306.00107v4| 2;2024-02-07;https://www.arxiv.org/abs/2306.00107v3| 1;2023-05-31;https://www.arxiv.org/abs/2306.00107v1	arXiv:2306.00107			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Dec 27 2024	2024	Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.																																	2025-02-05	PPRN:72815166		
J	Liu, Yifeng; Zheng, Weizhe				Zheng, Weizhe/H-9830-2012						Enhanced six operations and base change theorem for higher Artin stacks								Arxiv											2	2;2024-12-17;https://www.arxiv.org/abs/1211.5948v4| 1;2017-09-26;https://www.arxiv.org/abs/1211.5948v3	arXiv:1211.5948			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 17 2024	2024	In this article, we develop a theory of Grothendieck’s six operations for derived categories in étale cohomology of Artin stacks, for both torsion and adic coefficients. We prove several desired properties of the operations, including the base change theorem in derived categories. This extends many previous theories on this subject, including the one developed by Laszlo and Olsson, in which the operations are subject to more assumptions and the base change isomorphism is only constructed on the level of sheaves. Moreover, our theory works for higher Artin stacks as well. In addition, we define perverse t-structures on higher Artin stacks for general perversity, extending Gabber’s work on schemes. Our method differs from previous approaches, as we exploit the theory of stable oocategories developed by Lurie. We enhance derived categories, functors, and natural isomorphisms to the level of oo-categories and introduce oo-categorical (co)homological descent. To handle the issue of “homotopy coherence”, we develop a general technique for gluing subcategories of ∞-categories and several other oo-categorical techniques. We obtain categorical equivalences between simplicial sets associated to certain multisimplicial sets. Such equivalences can be used to construct functors in different contexts. One of our category-theoretical results generalizes Deligne’s gluing theory developed in the construction of the extraordinary pushforward operation in étale cohomology of schemes.																																	2025-01-24	PPRN:12731575		
J	Samvelyan, Mikayel; Raparthy, Sharath Chandra; Lupu, Andrei; Hambro, Eric; Markosyan, Aram H.; Bhatt, Manish; Mao, Yuning; Jiang, Minqi; Parker-Holder, Jack; Foerster, Jakob; Rocktaschel, Tim; Raileanu, Roberta				Markosyan, Aram/N-7981-2013; Samvelyan, Mikayel/AAF-2149-2019						Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts								Arxiv											2	2;2024-12-11;https://www.arxiv.org/abs/2402.16822v3| 1;2024-07-22;https://www.arxiv.org/abs/2402.16822v2	arXiv:2402.16822			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 11 2024	2024	As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.																																	2025-01-19	PPRN:91020361		
J	Zhang, Xin; Zhang, Yanzhao; Long, Dingkun; Xie, Wen; Dai, Ziqi; Tang, Jialong; Lin, Huan; Yang, Baosong; Xie, Pengjun; Huang, Fei; Zhang, Meishan; Li, Wenjie; Zhang, Min				Yanzhao, Zhang/AAI-4175-2021; Tang, Jialong/JMC-2179-2023; Li, Wenjie/AAC-2500-2021						mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2407.19669v2| 1;2024-07-29;https://www.arxiv.org/abs/2407.19669v1	arXiv:2407.19669			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 14 2024	2024	We present systematic efforts in building long-context multilingual text representation model (TRM) and reranker from scratch for text retrieval. We first introduce a text encoder (base size) enhanced with RoPE and unpadding, pre-trained in a native 8192-token context (longer than 512 of previous multilingual encoders). Then we construct a hybrid TRM and a cross-encoder reranker by contrastive learning. Evaluations show that our text encoder outperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM and reranker match the performance of large-sized state-of-the-art BGE-M3 models and achieve better results on long-context retrieval benchmarks. Further analysis demonstrate that our proposed models exhibit higher efficiency during both training and inference. We believe their efficiency and effectiveness could benefit various researches and industrial applications.1																																	2024-11-04	PPRN:91216123		
J	Shi, Wenhao; Hu, Zhiqiang; Bin, Yi; Liu, Junhua; Yang, Yang; Ng, See-Kiong; Bing, Lidong; Lee, Roy Ka-Wei				Lee, Roy Ka-Wei/LFG-0170-2024; Hu, Zhiqiang/HIR-5043-2022						Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models								Arxiv											2	2;2024-10-08;https://www.arxiv.org/abs/2406.17294v3| 1;2024-06-26;https://www.arxiv.org/abs/2406.17294v2	arXiv:2406.17294			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 08 2024	2024	Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problemsolving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce MathLLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista’s minitest split, and yielding leading performance on Math-V and MathVerse. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs’ mathematical reasoning abilities. The code and data are available at: https: //github.com/HZQ950419/Math-LLaVA .																																	2024-10-24	PPRN:89908773		
J	Zeng, Wenjun; Liu, Yuchi; Mullins, Ryan; Peran, Ludovic; Fernandez, Joe; Harkous, Hamza; Narasimhan, Karthik; Proud, Drew; Kumar, Piyush; Radharapu, Bhaktipriya; Sturman, Olivia; Wahltinez, Oscar		ShieldGemma Team Google LLC		Zeng, wenjun/ABE-9737-2021						ShieldGemma: Generative AI Content Moderation Based on Gemma								Arxiv											1	1;2024-07-31;https://www.arxiv.org/abs/2407.21772v1	arXiv:2407.21772			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 31 2024	2024	We present ShieldGemma, a comprehensive suite of LLM-based safety content moderation models built upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and LLM-generated output. By evaluating on both public and internal benchmarks, we demonstrate superior performance compared to existing models, such as Llama Guard (+10.8% AU-PRC on public benchmarks) and WildCard (+4.3%). Additionally, we present a novel LLM-based data curation pipeline, adaptable to a variety of safety-related tasks and beyond. We have shown strong generalization performance for model trained mainly on synthetic data. By releasing ShieldGemma, we provide a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers.																																	2024-08-08	PPRN:91172222		
J	Zhu, Rui-Jie; Zhao, Qihang; Li, Guoqi; Eshraghian, Jason K.				Eshraghian, Jason/W-8893-2019; LI, Guoqi/AAG-7110-2020; Zhu, Ruijie/AGC-4742-2022						SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks								Arxiv											2	2;2024-07-11;https://www.arxiv.org/abs/2302.13939v5| 1;2023-02-27;https://www.arxiv.org/abs/2302.13939v4	arXiv:2302.13939			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 11 2024	2024	As the size of large language models continue to scale, so does the computational resources required to run them. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and until now, SNNs have yet to succeed at language generation on large-scale datasets. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement ‘SpikeGPT’, a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 46M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model when released, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self-attention to reduce quadratic computational complexity O ( T 2 ) to linear complexity O ( T ) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 32.2 × fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. 																																	2024-07-23	PPRN:73567246		
J	Denison, Carson; MacDiarmid, Monte; Barez, Fazl; Duvenaud, David; Kravec, Shauna; Marks, Samuel; Schiefer, Nicholas; Soklaski, Ryan; Tamkin, Alex; Kaplan, Jared; Shlegeris, Buck; Bowman, Samuel R.; Perez, Ethan; Hubinger, Evan										S YCOPHANCY TO S UBTERFUGE : I NVESTIGATING R EWARD T AMPERING IN L ANGUAGE M ODELS								Arxiv											2	2;2024-06-29;https://www.arxiv.org/abs/2406.10162v3| 1;2024-06-17;https://www.arxiv.org/abs/2406.10162v2	arXiv:2406.10162			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 29 2024	2024	In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering , where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game earlycurriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.																																	2024-07-18	PPRN:89350368		
J	Fan, Wenqi; Ding, Yujuan; Ning, Liangbo; Wang, Shijie; Li, Hengyun; Yin, Dawei; Chua, Tat-Seng; Li, Qing				Li, Qing/H-4100-2011; Wang, Shijie/HDM-3891-2022; Li, Hengyun/ABE-6467-2020; Wang, Meng/AEZ-9059-2022; Yin, Dawei/JOR-9201-2023						A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2405.06211v3| 1;2024-05-10;https://www.arxiv.org/abs/2405.06211v1	arXiv:2405.06211			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. 																																	2024-07-04	PPRN:88998645		
J	Xu, Ruijie; Wang, Zengzhi; Fan, Run-Ze; Liu, Pengfei				Liu, Pengfei/JUV-0307-2023						Benchmarking Benchmark Leakage in Large Language Models								Arxiv											1	1;2024-04-29;https://www.arxiv.org/abs/2404.18824v1	arXiv:2404.18824			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 29 2024	2024	Amid the expanding use of pre -training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field’s healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N -gram accuracy—two simple and scalable metrics that gauge a model’s prediction precision on benchmark—to identify potential data leakages. By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the “ Benchmark Transparency Card” ” (Tab. 19) to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.																																	2024-06-04	PPRN:88967231		
J	Wu, Junchao; Yang, Shu; Zhan, Runzhe; Yuan, Yulin; Wong, Derek Fai; Chao, Lidia Sam				Wong, Derek F/CAI-7740-2022; Wu, Junchao/MVV-3796-2025						A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions								Arxiv											2	2;2024-04-19;https://www.arxiv.org/abs/2310.14724v3| 1;2023-10-24;https://www.arxiv.org/abs/2310.14724v2	arXiv:2310.14724			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 19 2024	2024	The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, statistics-based detectors, neural-base detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues and the lack of effective evaluation framework. Conclusively, we highlight interesting directions for future research in LLM-generated text detection to advance the implementation of responsible artificial intelligence (AI). Our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of LLM-generated text detection.																																	2024-04-29	PPRN:85764564		
J	Wen, Hao; Li, Yuanchun; Liu, Guohong; Zhao, Shanhui; Yu, Tao; Li, Toby Jia-Jun; Jiang, Shiqi; Liu, Yunhao; Zhang, Yaqin; Liu, Yunxin				Liu, Yunhao/NIS-8274-2025; Li, Yuanchun/LTD-1972-2024; Liu, Yunxin/LCD-3829-2024; Liu, Guohong/GZA-9206-2022; Wen, Hao/MEO-8739-2025						AutoDroid: LLM-powered Task Automation in Android								Arxiv											4	4;2024-03-09;https://www.arxiv.org/abs/2308.15272v4| 3;2023-09-09;https://www.arxiv.org/abs/2308.15272v3| 2;2023-09-06;https://www.arxiv.org/abs/2308.15272v2| 1;2023-08-29;https://www.arxiv.org/abs/2308.15272v1	arXiv:2308.15272			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 09 2024	2024	Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system capable of handling arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%.																																	2024-04-08	PPRN:84507453		
J	Liu, Jiarun; Yang, Hao; Zhou, Hong-Yu; Xi, Yan; Yu, Lequan; Yu, Yizhou; Liang, Yong; Shi, Guangming; Zhang, Shaoting; Zheng, Hairong; Wang, Shanshan				Zhou, Hong-Yu/ADF-2386-2022; Shi, Guangming/I-8614-2014; Yu, Lequan/U-5377-2019						Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining								Arxiv											2	2;2024-03-06;https://www.arxiv.org/abs/2402.03302v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.03302v1	arXiv:2402.03302			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 06 2024	2024	Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining. Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models. Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba_Enc by an average score of 2.72%.																																	2024-04-03	PPRN:87522058		
J	Zhang, Xiaotian; Li, Chunyang; Yi, Zong; Ying, Zhengyu; He, Liang; Qiu, Xipeng				chunyang, Li/AAM-3833-2020						Evaluating the Performance of Large Language Models on GAOKAO Benchmark								Arxiv											2	2;2024-02-24;https://www.arxiv.org/abs/2305.12474v3| 1;2023-05-21;https://www.arxiv.org/abs/2305.12474v2	arXiv:2305.12474			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 24 2024	2024	Large Language Models(LLMs) have demonstrated remarkable performance across various natural language processing tasks; however, how to comprehensively and accurately assess their performance becomes an urgent issue to be addressed. This paper introduces GAOKAOBench, an intuitive benchmark that employs questions from the Chinese GAOKAO examination as test samples, including both subjective and objective questions. To align with human examination methods, we design a method based on zero -shot settings to evaluate the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and ERNIE-Bot. Our findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across various subjects. We also use LLMs to grade the subjective questions, and find that model scores achieve a moderate level of consistency with human scores. In conclusion, this research contributes a robust evaluation benchmark for future large language models and offers valuable insights into the advantages and limitations of such models. 																																	2024-03-24	PPRN:71651981		
J	Zhang, Xinrong; Chen, Yingfa; Hu, Shengding; Xu, Zihang; Chen, Junhao; Hao, Moo Khai; Han, Xu; Thai, Zhen Leng; Wang, Shuo; Liu, Zhiyuan; Sun, Maosong				Hu, Shengding/JRY-6064-2023; xu, zihang/KGL-7699-2024; Liu, Zhiyuan/I-2233-2014; zhang, xian/JAC-5480-2023; Chen, Junhao/LCD-2351-2024						∞Bench: Extending Long Context Evaluation Beyond 100K Tokens								Arxiv											2	2;2024-02-24;https://www.arxiv.org/abs/2402.13718v3| 1;2024-02-22;https://www.arxiv.org/abs/2402.13718v2	arXiv:2402.13718			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 24 2024	2024	Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose ∞BENCH, the first LLM benchmark featuring an average data length surpassing 100K tokens. ∞BENCH comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in ∞BENCH are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on ∞BENCH, we evaluate the state-ofthe-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released12.																																	2024-03-24	PPRN:87798783		
J	Jin, Bowen; Liu, Gang; Han, Chi; Jiang, Meng; Ji, Heng; Han, Jiawei				Jin, Bowen/HTR-0099-2023; han, jiawei/GVT-3012-2022; han, chi/LPR-1781-2024; Jiang, Meng/AAE-4976-2020; Liu, Gang/NFS-8012-2025						Large Language Models on Graphs: A Comprehensive Survey								Arxiv											3	3;2024-10-03;https://www.arxiv.org/abs/2312.02783v3| 2;2024-02-01;https://www.arxiv.org/abs/2312.02783v2| 1;2023-12-05;https://www.arxiv.org/abs/2312.02783v1	arXiv:2312.02783			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.  																																	2024-10-21	PPRN:86405226		
J	Koo, Ryan; Lee, Minhwa; Raheja, Vipul; Park, Jonginn; Kim, Zae Myung; Kang, Dongyeop				Raheja, Vipul/KFB-2673-2024						Benchmarking Cognitive Biases in Large Language Models as Evaluators								Arxiv											3	3;2024-09-25;https://www.arxiv.org/abs/2309.17012v3| 2;2024-08-12;https://www.arxiv.org/abs/2309.17012v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17012v1	arXiv:2309.17012			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Sep 25 2024	2024	Large Language Models are cognitively biased judges. Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.																																	2024-12-06	PPRN:85331444		
J	Swamy, Gokul; Dann, Christoph; Kidambi, Rahul; Wu, Zhiwei Steven; Agarwal, Alekh										A Minimaximalist Approach to Reinforcement Learning from Human Feedback								Arxiv											2	2;2024-06-13;https://www.arxiv.org/abs/2401.04056v2| 1;2024-01-08;https://www.arxiv.org/abs/2401.04056v1	arXiv:2401.04056			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 13 2024	2024	We present Self-Play Preference Optimization (SPO), an algorithm for reinforcement learning from human feedback. Our approach is minimalist in that it does not require training a reward model nor unstable adversarial training and is therefore rather simple to implement. Our approach is maximalist in that it provably handles non-Markovian, intransitive, and stochastic preferences while being robust to the compounding errors that plague offline approaches to sequential prediction. To achieve the preceding qualities, we build upon the concept of a Minimax Winner (MW), a notion of preference aggregation from the social choice theory literature that frames learning from preferences as a zero-sum game between two policies. By leveraging the symmetry of this game, we prove that rather than using the traditional technique of dueling two policies to compute the MW, we can simply have a single agent play against itself while maintaining strong convergence guarantees. Practically, this corresponds to sampling multiple trajectories from a policy, asking a preference or teacher model to compare them, and then using the proportion of wins as the reward for a particular trajectory. We demonstrate that on a suite of continuous control tasks, we are able to learn significantly more efficiently than reward-model based approaches while maintaining robustness to the intransitive and stochastic preferences that frequently occur in practice when aggregating human judgments.																																	2024-07-02	PPRN:87070649		
J	Lv, Kai; Yang, Yuqing; Liu, Tengxiao; Gao, Qinghui; Guo, Qipeng; Qiu, Xipeng				yang, yuqing/GRJ-8747-2022						Full Parameter Fine-tuning for Large Language Models with Limited Resources								Arxiv											2	2;2024-06-06;https://www.arxiv.org/abs/2306.09782v2| 1;2023-06-16;https://www.arxiv.org/abs/2306.09782v1	arXiv:2306.09782			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.Code and data are available at https://github.com/OpenLMLab/LOMO.																																	2024-06-22	PPRN:73401455		
J	Nandy, Pratik; Matsoukas-Roubeas, Apollonas S.; Martinez-Azcona, Pablo; Dymarsky, Anatoly; Del Campo, Adolfo				del Campo, Adolfo/B-8439-2009						Quantum Dynamics in Krylov Space: Methods and Applications								Arxiv											2	2;2024-06-05;https://www.arxiv.org/abs/2405.09628v2| 1;2024-05-15;https://www.arxiv.org/abs/2405.09628v1	arXiv:2405.09628			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 05 2024	2024	The dynamics of quantum systems unfolds within a subspace of the state space or operator space, known as the Krylov space. This review presents the use of Krylov subspace methods to provide a compact and computationally efficient description of quantum evolution, with emphasis on nonequilibrium phenomena of many-body systems with a large Hilbert space. It provides a comprehensive update of recent developments, focused on the quantum evolution of operators in the Heisenberg picture as well as pure and mixed states. It further explores the notion of Krylov complexity and associated metrics as tools for quantifying operator growth, their bounds by generalized quantum speed limits, the universal operator growth hypothesis, and its relation to quantum chaos, scrambling, and generalized coherent states. A comparison of several generalizations of the Krylov construction for open quantum systems is presented. A closing discussion addresses the application of Krylov subspace methods in quantum field theory, holography, integrability, quantum control, and quantum computing, as well as current open problems.																																	2024-07-04	PPRN:88918220		
J	Tajwar, Fahim; Singh, Anikait; Sharma, Archit; Rafailov, Rafael; Schneider, Jeff; Xie, Tengyang; Ermon, Stefano; Finn, Chelsea; Kumar, Aviral				Xie, Tengyang/ABM-6089-2022						Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data								Arxiv											2	2;2024-06-02;https://www.arxiv.org/abs/2404.14367v3| 1;2024-04-23;https://www.arxiv.org/abs/2404.14367v2	arXiv:2404.14367			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 02 2024	2024	Learning from preference labels plays a crucial role in fine-tuning large language models. There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning. Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient. This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why? In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a "negative gradient") outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.																																	2024-06-22	PPRN:88626684		
J	Yin, Tianwei; Gharbi, Michael; Park, Taesung; Zhang, Richard; Shechtman, Eli; Durand, Fredo; Freeman, William T.				Shechtman, Eli/B-2736-2012						Improved Distribution Matching Distillation for Fast Image Synthesis								Arxiv											2	2;2024-05-24;https://www.arxiv.org/abs/2405.14867v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14867v1	arXiv:2405.14867			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	Recent approaches have shown promises distilling expensive diffusion models into efficient one-step generators. Amongst them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution , i.e., the distillation process does not enforce a one-to-one correspondence with the sampling trajectories of their teachers. However, to ensure stable training in practice, DMD requires an additional regression loss computed using a large set of noise–image pairs, generated by the teacher with many steps of a deterministic sampler. This is not only computationally expensive for large-scale text-to-image synthesis, but it also limits the student’s quality, tying it too closely to the teacher’s original sampling paths. We introduce DMD2, a set of techniques that lift this limitation and improve DMD training. First, we eliminate the regression loss and the need for expensive dataset construction. We show that the resulting instability is due to the “fake” critic not estimating the distribution of generated samples with sufficient accuracy and propose a two time-scale update rule as a remedy. Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images. This lets us train the student model on real data, thus mitigating the imperfect “real” score estimation from the teacher model, and thereby enhancing quality. Third, we introduce a new training procedure that enables multi-step sampling in the student, and addresses the training–inference input mismatch of previous work, by simulating inference-time generator samples during training. Taken together, our improvements set new benchmarks in onestep image generation, with FID scores of 1.28 on ImageNet-64×64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500 × reduction in inference cost. Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods, and surpassing the teacher. We release our code and pretrained models.																																	2024-06-08	PPRN:88990452		
J	Reis, Dillon; Hong, Jacqueline; Kupec, Jordan; Daoudi, Ahmad										Real-Time Flying Object Detection with YOLOv8								Arxiv											2	2;2024-05-22;https://www.arxiv.org/abs/2305.09972v2| 1;2023-05-17;https://www.arxiv.org/abs/2305.09972v1	arXiv:2305.09972			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 22 2024	2024	This paper presents a generalized model for real-time detection of flying objects that can be used for transfer learning and further research, as well as a refined model that achieves state-of-the-art results for flying object detection. We achieve this by training our first (generalized) model on a data set containing 40 different classes of flying objects, forcing the model to extract abstract feature representations. We then perform transfer learning with these learned parameters on a data set more representative of “real world” environments (i.e. higher frequency of occlusion, very small spatial sizes, rotations, etc.) to generate our refined model. Object detection of flying objects remains challenging due to large variances of object spatial sizes/aspect ratios, rate of speed, occlusion, and clustered backgrounds. To address some of the presented challenges while simultaneously maximizing performance, we utilize the current state-of-the-art single -shot detector, YOLOv8, in an attempt to find the best trade-off between inference speed and mean average precision (mAP). While YOLOv8 is being regarded as the new state-of-the-art [19], an official paper has not been released as of yet. Thus, we provide an in-depth explanation of the new architecture and functionality that YOLOv8 has adapted. Our final generalized model achieves a mAP50 of 79.2%, mAP50-95 of 68.5%, and an average inference speed of 50 frames per second (fps) on 1080p videos. Our final refined model maintains this inference speed and achieves an improved mAP50 of 99.1% and mAP50-95 of 83.5%.																																	2024-06-06	PPRN:70033780		
J	Lin, Xi Victoria; Chen, Xilun; Chen, Mingda; Shi, Weijia; Lomeli, Maria; James, Rich; Rodriguez, Pedro; Kahn, Jacob; Szilvasy, Gergely; Lewis, Mike; Zettlemoyer, Luke; Yih, Scott										RA-DIT: Retrieval-Augmented Dual Instruction Tuning								Arxiv											4	4;2024-05-06;https://www.arxiv.org/abs/2310.01352v4| 3;2023-11-05;https://www.arxiv.org/abs/2310.01352v3| 2;2023-10-08;https://www.arxiv.org/abs/2310.01352v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01352v1	arXiv:2310.01352			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.																																	2024-05-28	PPRN:85355298		
J	Ding, Dujian; Mallick, Ankur; Wang, Chi; Sim, Robert; Mukherjee, Subhabrata; Ruhle, Victor; Lakshmanan, Laks V.S.; Awadallah, Ahmed Hassan				Mallick, Ankur/HZK-3219-2023						Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing								Arxiv											1	1;2024-04-22;https://www.arxiv.org/abs/2404.14618v1	arXiv:2404.14618			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 22 2024	2024	Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.																																	2024-05-02	PPRN:88626834		
J	Zheng, Kaizhi; He, Xuehai; Wang, Xin Eric				He, Xuehai/AAF-5270-2020; Wang, Xin/ABD-3905-2020						MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens								Arxiv											3	3;2024-03-15;https://www.arxiv.org/abs/2310.02239v3| 2;2023-10-05;https://www.arxiv.org/abs/2310.02239v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02239v1	arXiv:2310.02239			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 15 2024	2024	The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of "generative vokens". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.																																	2024-04-11	PPRN:85378612		
J	Schiff, Yair; Kao, Chia-Hsiang; Gokaslan, Aaron; Dao, Tri; Gu, Albert; Kuleshov, Volodymyr										Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling								Arxiv											1	1;2024-03-05;https://www.arxiv.org/abs/2403.03234v1	arXiv:2403.03234			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 05 2024	2024	Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance.																																	2024-04-03	PPRN:88043611		
J	Li, Xingxuan; Zhao, Ruochen; Chia, Yew Ken; Ding, Bosheng; Joty, Shafiq; Poria, Soujanya; Bing, Lidong				PORIA, SOUJANYA/KIJ-4789-2024						Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources								Arxiv											4	4;2024-02-21;https://www.arxiv.org/abs/2305.13269v4| 3;2023-12-04;https://www.arxiv.org/abs/2305.13269v3| 2;2023-10-03;https://www.arxiv.org/abs/2305.13269v2| 1;2023-05-22;https://www.arxiv.org/abs/2305.13269v1	arXiv:2305.13269			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	We present chain -of -knowledge (CoK) , a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge -intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge -intensive tasks across different domains. Our code is available at https://github.com/DAMO-NLP-SG/chain-of-knowledge.																																	2024-03-20	PPRN:70800123		
J	Lajszczak, Mateusz; Cambara, Guillermo; Li, Yang; Beyhan, Fatih; Van Korlaar, Arent; Yang, Fan; Joly, Arnaud; Martin-Cortinas, Alvaro; Abbas, Ammar; Michalski, Adam; Moinet, Alexis; Karlapati, Sri; Muszynska, Ewa; Guo, Haohan; Putrycz, Bartosz; Gambino, Soledad Lopez; Yoo, Kayeon; Sokolova, Elena; Drugman, Thomas				Michalski, Adam/AAC-3282-2020; Guo, Haohan/MGU-0040-2025						BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data								Arxiv											2	2;2024-02-15;https://www.arxiv.org/abs/2402.08093v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.08093v1	arXiv:2402.08093			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 15 2024	2024	We introduce a text-to-speech (TTS) model called BASE TTS, which stands for Big Adaptive Streamable TTS with Emergent abilities. BASE TTS is the largest TTS model to -date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billionparameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution -based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte -pair encoding. Echoing the widely -reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text -tospeech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.																																	2024-11-09	PPRN:87669768		
J	Bai, Yushi; Tu, Shangqing; Zhang, Jiajie; Peng, Hao; Wang, Xiaozhi; Lv, Xin; Cao, Shulin; Xu, Jiazheng; Hou, Lei; Dong, Yuxiao; Tang, Jie; Li, Juanzi				Wang, Xiaozhi/IQT-4844-2023; Tu, Shangqing/HSF-1919-2023; Li, Zhiyuan/ESQ-7168-2022						LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks								Arxiv											1	1;2024-12-19;https://www.arxiv.org/abs/2412.15204v1	arXiv:2412.15204			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 19 2024	2024	This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1- preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2.																																	2025-01-24	PPRN:120065144		
J	Xu, Derong; Chen, Wei; Peng, Wenjun; Zhang, Chao; Xu, Tong; Zhao, Xiangyu; Wu, Xian; Zheng, Yefeng; Wang, Yang; Chen, Enhong				Zheng, Yefeng/ABG-7053-2020; Wu, Xian/JRW-5738-2023; Zhao, Xiangyu/AAO-2203-2020						Large Language Models for Generative Information Extraction: A Survey								Arxiv											2	2;2024-10-31;https://www.arxiv.org/abs/2312.17617v3| 1;2024-06-04;https://www.arxiv.org/abs/2312.17617v2	arXiv:2312.17617			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	Information extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub.																																	2024-12-06	PPRN:89261385		
J	Liu, Bingchen; Akhgari, Ehsan; Visheratin, Alexander; Kamko, Aleks; Xu, Linmiao; Shrirao, Shivam; Lambert, Chase; Souza, Joao; Doshi, Suhail; Li, Daiqing				Doshi, Smit/IQS-3468-2023; Corrêa de Souza, João Henrique/AFK-5987-2022; LIU, BING-CHEN/AAG-9141-2020						Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models								Arxiv											2	2;2024-10-21;https://www.arxiv.org/abs/2409.10695v2| 1;2024-09-16;https://www.arxiv.org/abs/2409.10695v1	arXiv:2409.10695			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 21 2024	2024	We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities. Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM. Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures. We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance. Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering. User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs. Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.																																	2024-11-23	PPRN:91941704		
J	Elhoushi, Mostafa; Shrivastava, Akshat; Liskovich, Diana; Hosmer, Basil; Wasti, Bram; Lai, Liangzhen; Mahmoud, Anas; Acun, Bilge; Agarwal, Saurabh; Roman, Ahmed; Aly, Ahmed A; Chen, Beidi; Wu, Carole-Jean				Alaa, Mahmoud/AGZ-3537-2022; Elhoushi, Mostafa/C-2497-2015						LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding								Arxiv											4	4;2024-10-18;https://www.arxiv.org/abs/2404.16710v4| 3;2024-10-17;https://www.arxiv.org/abs/2404.16710v3| 2;2024-04-29;https://www.arxiv.org/abs/2404.16710v2| 1;2024-04-25;https://www.arxiv.org/abs/2404.16710v1	arXiv:2404.16710			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 18 2024	2024	We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task. 																																	2024-11-16	PPRN:88650989		
J	Mahan, Dakota; Phung, Duy Van; Rafailov, Rafael; Blagden, Chase; Lile, Nathan; Castricato, Louis; Franken, Jan-Philipp; Finn, Chelsea; Albalak, Alon										Generative Reward Models								Arxiv											1	1;2024-10-02;https://www.arxiv.org/abs/2410.12832v1	arXiv:2410.12832			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 02 2024	2024	Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments (Zeng et al., 2023). To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM , an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 26%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.																																	2024-11-13	PPRN:115453376		
J	Hu, Yafei; Xie, Quanting; Jain, Vidhi; Francis, Jonathan; Patrikar, Jay; Keetha, Nikhil; Kim, Seungchan; Xie, Yaqi; Zhang, Tianyi; Fang, Hao-Shu; Zhao, Shibo; Omidshafiei, Shayegan; Kim, Dong-Ki; Agha-mohammadi, Ali-akbar; Sycara, Katia; Johnson-Roberson, Matthew; Batra, Dhruv; Wang, Xiaolong; Scherer, Sebastian; Wang, Chen; Kira, Zsolt; Xia, Fei; Bisk, Yonatan				Xia, Fei/AAW-8782-2021; Xie, Yaqi/ISB-3523-2023; ZHANG, TIANYI/HLW-7929-2023; 赵, 世博/JFS-5024-2023						Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis								Arxiv											3	3;2024-10-01;https://www.arxiv.org/abs/2312.08782v3| 2;2023-12-15;https://www.arxiv.org/abs/2312.08782v2| 1;2023-12-14;https://www.arxiv.org/abs/2312.08782v1	arXiv:2312.08782			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 01 2024	2024	Building general-purpose robots that operate seamlessly in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. However, as a community, we have been constraining most robotic systems by designing them for specific tasks, training them on specific datasets, and deploying them within specific environments. These systems require extensively-labeled data and task-specific models. When deployed in real-world scenarios, such systems face several generalization issues and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of general-purpose robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing a generalized formulation of how foundation models are used in robotics, and the fundamental barriers to making generalist robots universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository2 of resources, including papers reviewed in this survey, as well as related projects and repositories for developing foundation models for robotics.																																	2024-10-13	PPRN:86587684		
J	Egiazarian, Vage; Panferov, Andrei; Kuznedelev, Denis; Frantar, Elias; Babenko, Artem; Alistarh, Dan				Babenko, Artem/M-3540-2016						Extreme Compression of Large Language Models via Additive Quantization								Arxiv											4	4;2024-09-11;https://www.arxiv.org/abs/2401.06118v4| 3;2024-06-08;https://www.arxiv.org/abs/2401.06118v3| 2;2024-02-06;https://www.arxiv.org/abs/2401.06118v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.06118v1	arXiv:2401.06118			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 11 2024	2024	The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression-defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter-from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.																																	2024-09-27	PPRN:87124544		
J	Liu, Xiao; Lei, Xuanyu; Wang, Shengyuan; Huang, Yue; Feng, Zhuoer; Wen, Bosi; Cheng, Jiale; Ke, Pei; Xu, Yifan; Tam, Weng Lam; Zhang, Xiaohan; Sun, Lichao; Gu, Xiaotao; Wang, Hongning; Zhang, Jing; Huang, Minlie; Dong, Yuxiao; Tang, Jie				航航, 张/KBC-0720-2024; Wang, Hongning/GPK-7527-2022; Xu, Yifan/I-9273-2014						AlignBench: Benchmarking Chinese Alignment of Large Language Models								Arxiv											3	3;2024-08-25;https://www.arxiv.org/abs/2311.18743v4| 2;2023-12-05;https://www.arxiv.org/abs/2311.18743v3| 1;2023-11-30;https://www.arxiv.org/abs/2311.18743v1	arXiv:2311.18743			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 25 2024	2024	Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, the effective evaluation of alignment for emerging Chinese LLMs is still largely unexplored. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. We design a human-in-the-loop data curation pipeline, containing eight main categories, 683 real-scenario rooted queries and corresponding human verified references. To ensure the correctness of references, each knowledge-intensive query is accompanied with evidences collected from reliable web sources (including URLs and quotations) by our annotators. For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge~cite{zheng2023judging} approach with Chain-of-Thought to generate explanations and final ratings, ensuring high reliability and interpretability. 																																	2024-09-06	PPRN:86341197		
J	Yang, Tao; Wu, Rongyuan; Ren, Peiran; Xie, Xuansong; Zhang, Lei				Yang, Tao/E-5466-2015; Zhang, Lei/HSC-7058-2023; Wu, Rongyuan/OGA-8118-2025						Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and Personalized Stylization								Arxiv											4	4;2024-07-09;https://www.arxiv.org/abs/2308.14469v4| 3;2024-03-14;https://www.arxiv.org/abs/2308.14469v3| 2;2023-09-07;https://www.arxiv.org/abs/2308.14469v2| 1;2023-08-28;https://www.arxiv.org/abs/2308.14469v1	arXiv:2308.14469			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 09 2024	2024	Diffusion models have demonstrated impressive performance in various image generation, editing, enhancement and translation tasks. In particular, the pre-trained text-to-image stable diffusion models provide a potential solution to the challenging realistic image super-resolution (Real-ISR) and image stylization problems with their strong generative priors. However, the existing methods along this line often fail to keep faithful pixel-wise image structures. If extra skip connections between the encoder and the decoder of a VAE are used to reproduce details, additional training in image space will be required, limiting the application to tasks in latent space such as image stylization. In this work, we propose a pixel-aware stable diffusion (PASD) network to achieve robust Real-ISR and personalized image stylization. Specifically, a pixel-aware cross attention module is introduced to enable diffusion models perceiving image local structures in pixel-wise level, while a degradation removal module is used to extract degradation insensitive features to guide the diffusion process together with image high level information. An adjustable noise schedule is introduced to further improve the image restoration results. By simply replacing the base diffusion model with a stylized one, PASD can generate diverse stylized images without collecting pairwise training data, and by shifting the base model with an aesthetic one, PASD can bring old photos back to life. Extensive experiments in a variety of image enhancement and stylization tasks demonstrate the effectiveness of our proposed PASD approach.																																	2024-07-21	PPRN:84475146		
J	Leroy, Vincent; Cabon, Yohann; Revaud, Jerome										Grounding Image Matching in 3D with MASt3R								Arxiv											1	1;2024-06-14;https://www.arxiv.org/abs/2406.09756v1	arXiv:2406.09756			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision. Yet despite matching being fundamentally a 3D problem, intrinsically linked to camera pose and scene geometry, it is typically treated as a 2D problem. This makes sense as the goal of matching is to establish correspondences between 2D pixel fields, but also seems like a potentially hazardous choice. In this work, we take a different stance and propose to cast matching as a 3D task with DUSt3R, a recent and powerful 3D reconstruction framework based on Transformers. Based on pointmaps regression, this method displayed impressive robustness in matching views with extreme viewpoint changes, yet with limited accuracy. We aim here to improve the matching capabilities of such an approach while preserving its robustness. We thus propose to augment the DUSt3R network with a new head that outputs dense local features, trained with an additional matching loss. We further address the issue of quadratic complexity of dense matching, which becomes prohibitively slow for downstream applications if not carefully treated. We introduce a fast reciprocal matching scheme that not only accelerates matching by orders of magnitude, but also comes with theoretical guarantees and, lastly, yields improved results. Extensive experiments show that our approach, coined MASt3R, significantly outperforms the state of the art on multiple matching tasks. In particular, it beats the best published methods by 30% (absolute improvement) in VCRE AUC on the extremely challenging Map-free localization dataset.																																	2024-07-02	PPRN:89332567		
J	Tang, Yunhao; Guo, Zhaohan Daniel; Zheng, Zeyu; Calandriello, Daniele; Munos, Remi; Rowland, Mark; Richemond, Pierre Harvey; Valko, Michal; Pires, Bernardo Avila; Piot, Bilal				zheng, zeyu/A-3867-2013						Generalized Preference Optimization: A Unified Approach to Offline Alignment								Arxiv											2	2;2024-05-28;https://www.arxiv.org/abs/2402.05749v2| 1;2024-02-08;https://www.arxiv.org/abs/2402.05749v1	arXiv:2402.05749			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In a controlled setting akin to Gao et al 2023, we also show that different GPO variants achieve similar trade-offs between regularization and performance, though the optimal values of hyper-parameter might differ as predicted by theory. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.																																	2024-08-25	PPRN:87572336		
J	Lin, Toru; Zhang, Yu; Li, Qiyang; Qi, Haozhi; Yi, Brent; Levine, Sergey; Malik, Jitendra				Qi, Haozhi/ABD-9753-2021						Learning Visuotactile Skills with Two Multifingered Hands								Arxiv											2	2;2024-05-22;https://www.arxiv.org/abs/2404.16823v2| 1;2024-04-25;https://www.arxiv.org/abs/2404.16823v1	arXiv:2404.16823			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 22 2024	2024	Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data. Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing. To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment. To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research. Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback. Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning. Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data. Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .																																	2024-06-04	PPRN:88650982		
J	Xie, Tianyi; Zong, Zeshun; Qiu, Yuxing; Li, Xuan; Feng, Yutao; Yang, Yin; Jiang, Chenfanfu				Li, Xuan/OEP-3441-2025; Xie, Tianyi/MGU-9357-2025						PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2311.12198v3| 1;2023-11-22;https://www.arxiv.org/abs/2311.12198v2	arXiv:2311.12198			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 15 2024	2024	We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high -quality novel motion synthesis. Employing a custom Material Point Method (MPM), our approach enriches 3D Gaussian kernels with physically meaningful kinematic deformation and mechanical stress attributes, all evolved in line with continuum mechanics principles. A defining characteristic of our method is the seamless integration between physical simulation and visual rendering: both components utilize the same 3D Gaussian kernels as their discrete representations. This negates the necessity for triangle/tetrahedron meshing, marching cubes, “cage meshes,” or any other geometry embedding, highlighting the principle of “what you see is what you simulate (WS2).” Our method demonstrates exceptional versatility across a wide variety of materials–including elastic entities, plastic metals, non -Newtonian fluids, and granular materials–showcasing its strong capabilities in creating diverse visual content with novel viewpoints and movements. 																																	2024-04-25	PPRN:86243815		
J	Saad-Falcon, Jon; Khattab, Omar; Potts, Christopher; Zaharia, Matei										ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems								Arxiv											2	2;2024-03-31;https://www.arxiv.org/abs/2311.09476v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09476v1	arXiv:2311.09476			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 31 2024	2024	Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.																																	2024-04-17	PPRN:86176811		
J	Zhou, Yongchao; Lyu, Kaifeng; Rawat, Ankit Singh; Menon, Aditya Krishna; Rostamizadeh, Afshin; Kumar, Sanjiv; Kagy, Jean-Francois; Agarwal, Rishabh				Rawat, Ankit/V-3483-2019						DistillSpec: Improving Speculative Decoding via Knowledge Distillation								Arxiv											2	2;2024-03-31;https://www.arxiv.org/abs/2310.08461v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08461v1	arXiv:2310.08461			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 31 2024	2024	Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.																																	2024-04-17	PPRN:85603041		
J	Bowles, Joseph; Ahmed, Shahnawaz; Schuld, Maria				Bowles, Joseph/ABC-8023-2020						Better than classical? The subtle art of benchmarking quantum machine learning models								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2403.07059v2| 1;2024-03-11;https://www.arxiv.org/abs/2403.07059v1	arXiv:2403.07059			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that "quantumness" may not be the crucial ingredient for the small learning tasks considered here. Our benchmarks also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results.																																	2024-04-11	PPRN:88120221		
J	Xie, Junlin; Chen, Zhihong; Zhang, Ruifei; Wan, Xiang; Li, Guanbin				Qiao, Yu/ABD-5787-2021						Large Multimodal Agents: A Survey								Arxiv											1	1;2024-02-23;https://www.arxiv.org/abs/2402.15116v1	arXiv:2402.15116			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 23 2024	2024	Large language models (LLMs) have achieved superior performance in powering text -based AI agents, endowing them with decision -making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents (LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs, enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs. Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/ jun0wanan/awesome-large-multimodal-agents.																																	2024-03-23	PPRN:87864452		
J	Gao, Shanghua; Zhou, Pan; Cheng, Ming-Ming; Yan, Shuicheng				Yan, Shuicheng/HCI-1431-2022; Gao, Shanghua/ACY-3354-2022; Cheng, Ming-Ming/A-2527-2009						MDTv2: Masked Diffusion Transformer is a Strong Image Synthesizer								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2303.14389v2| 1;2023-03-25;https://www.arxiv.org/abs/2303.14389v1	arXiv:2303.14389			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	Despite its success in image synthesis, we observe that diffusion probabilistic models (DPMs) often lack contextual reasoning ability to learn the relations among object parts in an image, leading to a slow learning process. To solve this issue, we propose a Masked Diffusion Transformer (MDT) that introduces a mask latent modeling scheme to explicitly enhance the DPMs’ ability to contextual relation learning among object semantic parts in an image. During training, MDT operates in the latent space to mask certain tokens. Then, an asymmetric diffusion transformer is designed to predict masked tokens from unmasked ones while maintaining the diffusion generation process. Our MDT can reconstruct the full information of an image from its incomplete contextual input, thus enabling it to learn the associated relations among image tokens. We further improve MDT with a more efficient macro network structure and training strategy, named MDTv2. Experimental results show that MDTv2 achieves superior image synthesis performance, e.g., a new SOTA FID score of 1.58 on the ImageNet dataset, and has more than 10× faster learning speed than the previous SOTA DiT. The source code is released at https://github.com/sail-sg/MDT.																																	2024-03-20	PPRN:49702914		
J	Ma, Ziyang; Yang, Guanrou; Yang, Yifan; Gao, Zhifu; Wang, Jiaming; Du, Zhihao; Yu, Fan; Chen, Qian; Zheng, Siqi; Zhang, Shiliang; Chen, Xie				YANG, YIFAN/HPF-1451-2023; 于, 凡/IGW-8338-2023						An Embarrassingly Simple Approach for LLM with Strong ASR Capacity								Arxiv											1	1;2024-02-13;https://www.arxiv.org/abs/2402.08846v1	arXiv:2402.08846			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 13 2024	2024	In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community.																																	2024-05-25	PPRN:87688859		
J	Wang, Xinyi; Zhu, Wanrong; Saxon, Michael; Steyvers, Mark; Wang, William Yang				Steyvers, Mark/B-8085-2013; Wang, Xinyi/IAM-0594-2023						Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning								Arxiv											3	3;2024-02-12;https://www.arxiv.org/abs/2301.11916v4| 2;2023-10-17;https://www.arxiv.org/abs/2301.11916v3| 1;2023-01-27;https://www.arxiv.org/abs/2301.11916v2	arXiv:2301.11916			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 12 2024	2024	In recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the real-world LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information.																																	2024-05-25	PPRN:67346301		
J	Olausson, Theo X.; Inala, Jeevana Priya; Wang, Chenglong; Gao, Jianfeng; Solar-Lezama, Armando										Is Self-Repair a Silver Bullet for Code Generation?								Arxiv											3	3;2024-02-02;https://www.arxiv.org/abs/2306.09896v5| 2;2023-10-17;https://www.arxiv.org/abs/2306.09896v4| 1;2023-06-16;https://www.arxiv.org/abs/2306.09896v2	arXiv:2306.09896			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 02 2024	2024	Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.																																	2024-05-25	PPRN:73400026		
J	Suzgun, Mirac; Kalai, Adam Tauman										Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding								Arxiv											1	1;2024-01-23;https://www.arxiv.org/abs/2401.12954v1	arXiv:2401.12954			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 23 2024	2024	We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.1   [GRAPHIC]																																	2024-05-25	PPRN:87296995		
J	Hu, Cunchen; Huang, Heyang; Xu, Liangliang; Chen, Xusheng; Xu, Jiang; Chen, Shuang; Feng, Hao; Wang, Chenxi; Wang, Sa; Bao, Yungang; Sun, Ninghui; Shan, Yizhou				XU, LIANGLIANG/HHR-9617-2022; Shan, Yizhou/AAF-3021-2020						Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads								Arxiv											1	1;2024-01-20;https://www.arxiv.org/abs/2401.11181v1	arXiv:2401.11181			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 20 2024	2024	Transformer-based large language model (LLM) inference serving is now the backbone of many cloud services. LLM inference consists of a prefill phase and a decode phase. However, existing LLM deployment practices often overlook the distinct characteristics of these phases, leading to significant interference. To mitigate interference, our insight is to carefully schedule and group inference requests based on their characteristics. We realize this idea in TetriInfer through three pillars. First, it partitions prompts into fixed-size chunks so that the accelerator always runs close to its computation-saturated limit. Second, it disaggregates prefill and decode instances so each can run independently. Finally, it uses a smart two-level scheduling algorithm augmented with predicted resource usage to avoid decode scheduling hotspots. Results show that TetriInfer improves time-to-first-token (TTFT), job completion time (JCT), and inference efficiency in turns of performance per dollar by a large margin, e.g., it uses 38% less resources all the while lowering average TTFT and average JCT by 97% and 47%, respectively.																																	2024-02-15	PPRN:87278948		
J	Zhu, Lipeng; Wong, Kai-Kit				Wong, Kai-Kit/OUH-7591-2025; Zhu, Lipeng/AAL-6718-2020						Historical Review of Fluid Antenna and Movable Antenna								Arxiv											2	2;2024-01-13;https://www.arxiv.org/abs/2401.02362v2| 1;2024-01-04;https://www.arxiv.org/abs/2401.02362v1	arXiv:2401.02362			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 13 2024	2024	Recently, significant attention has been drawn to the development of two emerging antenna technologies known as "Fluid Antenna" and "Movable Antenna" in wireless communication research community, which is greatly motivated by their unprecedented flexibility and reconfigurability for improving system performance in wireless applications. However, some confusions have also ensued on their nomenclature. In fact, both "Fluid Antenna" and "Movable Antenna" are not newly-made terms, and they have a longstanding presence in the field of antenna technology. This article wishes to provide some clarity on this closely related terminology and help dispel any confusion, concern or even dispute on the appropriate use of their names in the literature. Our hope is to unite researchers and encourage more research endeavours to focus on resolving the technical issues on this topic. This article begins by reviewing the historical evolution of these technologies for fostering a clear understanding of their origins and recent development in the realm of wireless communication. We will conclude this article by commenting on other nomenclatures that have emerged in the recent research of Fluid/Movable Antenna.																																	2024-05-25	PPRN:86964777		
J	Yin, Yuyang; Xu, Dejia; Wang, Zhangyang; Zhao, Yao; Wei, Yunchao				Yang, Wenhan/KWU-3208-2024; Zhihua, Wang/AFO-5263-2022; YIN, Yuyang/HTL-3947-2023						4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency								Arxiv											3	3;2024-12-04;https://www.arxiv.org/abs/2312.17225v3| 2;2024-03-17;https://www.arxiv.org/abs/2312.17225v2| 1;2023-12-28;https://www.arxiv.org/abs/2312.17225v1	arXiv:2312.17225			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 04 2024	2024	Aided by text-to-image and text-to-video diffusion models, existing 4D content creation pipelines utilize score distillation sampling to optimize the entire dynamic 3D scene. However, as these pipelines generate 4D content from text or image inputs directly, they are constrained by limited motion capabilities and depend on unreliable prompt engineering for desired results. To address these problems, this work introduces 4DGen, a novel framework for grounded 4D content creation. We identify monocular video sequences as a key component in constructing the 4D content. Our pipeline facilitates controllable 4D generation, enabling users to specify the motion via monocular video or adopt image-to-video generations, thus offering superior control over content creation. Furthermore, we construct our 4D representation using dynamic 3D Gaussians, which permits efficient, high-resolution supervision through rendering during training, thereby facilitating high-quality 4D generation. Additionally, we employ spatial-temporal pseudo labels on anchor frames, along with seamless consistency priors implemented through 3D-aware score distillation sampling and smoothness regularizations. Compared to existing video-to-4D baselines, our approach yields superior results in faithfully reconstructing input signals and realistically inferring renderings from novel viewpoints and timesteps. More importantly, compared to previous image-to-4D and text-to-4D works, 4DGen supports grounded generation, offering users enhanced control and improved motion generation capabilities, a feature difficult to achieve with previous methods. 																																	2025-01-15	PPRN:86851985		
J	Li, Qixiu; Liang, Yaobo; Wang, Zeyu; Luo, Lin; Chen, Xi; Liao, Mozheng; Wei, Fangyun; Deng, Yu; Xu, Sicheng; Zhang, Yizhong; Wang, Xiaofan; Liu, Bei; Fu, Jianlong; Bao, Jianmin; Chen, Dong; Shi, Yuanchun; Yang, Jiaolong; Guo, Baining										CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation								Arxiv											1	1;2024-11-29;https://www.arxiv.org/abs/2411.19650v1	arXiv:2411.19650			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 29 2024	2024	The advancement of large Vision-Language-Action (VLA) models has significantly improved robotic manipulation in terms of language-guided task execution and generalization to unseen scenarios. While existing VLAs adapted from pretrained large Vision-Language-Models (VLM) have demonstrated promising generalizability, their task performance is still unsatisfactory as indicated by the low tasks success rates in different environments. In this paper, we present a new advanced VLA architecture derived from VLM. Unlike previous works that directly repurpose VLM for action prediction by simple action quantization, we propose a omponentized VLA architecture that has a specialized action module conditioned on VLM output. We systematically study the design of the action module and demonstrates the strong performance enhancement with diffusion action transformers for action sequence modeling, as well as their favorable scaling behaviors. We also conduct comprehensive experiments and ablation studies to evaluate the efficacy of our models with varied designs. The evaluation on 5 robot embodiments in simulation and real work shows that our model not only significantly surpasses existing VLAs in task performance and but also exhibits remarkable adaptation to new robots and generalization to unseen objects and backgrounds. It exceeds the average success rates of OpenVLA which has similar model size (7B) with ours by over 35% in simulated evaluation and 55% in real robot experiments. It also outperforms the large RT-2-X model (55B) by 18% absolute success rates in simulation. 																																	2025-01-10	PPRN:119581233		
J	Daras, Giannis; Chung, Hyungjin; Lai, Chieh-Hsin; Mitsufuji, Yuki; Ye, Jong Chul; Milanfar, Peyman; Dimakis, Alexandros G.; Delbracio, Mauricio				Chung, Hyungjin/AAL-1161-2021; Dimakis, Alexandros/A-5496-2011; Ye, Jong/C-1623-2011; Mitsufuji, Yuki/H-4052-2019; Lai, Chieh-Hsin/JXN-7654-2024						A Survey on Diffusion Models for Inverse Problems								Arxiv											1	1;2024-09-30;https://www.arxiv.org/abs/2410.00083v1	arXiv:2410.00083			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 30 2024	2024	Diffusion models have become increasingly popular for generative modeling due to their ability to generate high-quality samples. This has unlocked exciting new possibilities for solving inverse problems, especially in image restoration and reconstruction, by treating diffusion models as unsupervised priors. This survey provides a comprehensive overview of methods that utilize pre-trained diffusion models to solve inverse problems without requiring further training. We introduce taxonomies to categorize these methods based on both the problems they address and the techniques they employ. We analyze the connections between different approaches, offering insights into their practical implementation and highlighting important considerations. We further discuss specific challenges and potential solutions associated with using latent diffusion models for inverse problems. This work aims to be a valuable resource for those interested in learning about the intersection of diffusion models and inverse problems.																																	2024-10-12	PPRN:100750668		
J	Gibbs, Isaac; Cherian, John J.; Candes, Emmanuel J.				Gibbs, Isaac/NLO-5741-2025						Conformal Prediction With Conditional Guarantees								Arxiv											3	3;2024-09-16;https://www.arxiv.org/abs/2305.12616v4| 2;2023-12-20;https://www.arxiv.org/abs/2305.12616v3| 1;2023-05-22;https://www.arxiv.org/abs/2305.12616v1	arXiv:2305.12616			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 16 2024	2024	We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees. Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples. Thus, most popular methods only guarantee marginal coverage over the covariates or are restricted to a limited set of conditional targets, e.g. coverage over a finite set of pre-specified subgroups. This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity. We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts. When the target class of shifts is finite-dimensional, we show how to simultaneously obtain exact finite-sample coverage over all possible shifts. For example, given a collection of subgroups, our prediction sets guarantee coverage over each group. For more flexible, infinite-dimensional classes where exact coverage is impossible, we provide a procedure for quantifying the coverage errors of our algorithm. Moreover, by tuning interpretable hyperparameters, we allow the practitioner to control the size of these errors across shifts of interest. Our methods can be incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.																																	2024-10-03	PPRN:70908404		
J	Liu, Jiajun; Wang, Yibing; Ma, Hanghang; Wu, Xiaoping; Ma, Xiaoqi; Wei, Xiaoming; Jiao, Jianbin; Wu, Enhua; Hu, Jie										Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input								Arxiv											1	1;2024-08-28;https://www.arxiv.org/abs/2408.15542v1	arXiv:2408.15542			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 28 2024	2024	Rapid advancements have been made in extending Large Language Models (LLMs) to Large Multi-modal Models (LMMs). However, extending input modality of LLMs to video data remains a challenging endeavor, especially for long videos. Due to insufficient access to large-scale high-quality video data and the excessive compression of visual features, current methods exhibit limitations in effectively processing long videos. In this paper, we introduce Kangaroo, a powerful Video LMM aimed at addressing these challenges. Confronted with issue of inadequate training data, we develop a data curation system to build a large-scale dataset with high-quality annotations for vision-language pre-training and instruction tuning. In addition, we design a curriculum training pipeline with gradually increasing resolution and number of input frames to accommodate long videos. Evaluation results demonstrate that, with 8B parameters, Kangaroo achieves state-of-the-art performance across a variety of video understanding benchmarks while exhibiting competitive results on others. Particularly, on benchmarks specialized for long videos, Kangaroo excels some larger models with over 10B parameters and proprietary models.																																	2024-09-19	PPRN:91781380		
J	Zhou, Zhanhui; Liu, Jie; Shao, Jing; Yue, Xiangyu; Yang, Chao; Ouyang, Wanli; Qiao, Yu										Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization								Arxiv											5	5;2024-08-17;https://www.arxiv.org/abs/2310.03708v4| 4;2023-12-15;https://www.arxiv.org/abs/2310.03708v3| 3;2023-10-17;https://www.arxiv.org/abs/2310.03708v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03708v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03708v1	arXiv:2310.03708			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 17 2024	2024	A single language model, even when aligned with labelers through reinforcement learning from human feedback (RLHF), may not suit all human preferences. Recent approaches therefore prefer customization, gathering multi-dimensional feedback, and creating distinct reward models for each dimension. Different language models are then optimized for various preferences using multi-objective RLHF (MORLHF) with varying reward weights. However, RL fine-tuning is unstable and resource-heavy, especially with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free extension of Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO folds language modeling directly into reward modeling, training language models as implicit collective reward models that combine all objectives with specific weights. MODPO theoretically yields the same optimal solutions as MORLHF but is practically more stable and efficient. Empirical results in safety alignment and long-form question answering show that MODPO matches or outperforms existing methods, producing a Pareto front of language models catering to diverse preferences with three times less computational resources compared to MORLHF. Code is available at https://github.com/ZHZisZZ/modpo.																																	2024-08-28	PPRN:85430688		
J	Geshkovski, Borjan; Letrouit, Cyril; Polyanskiy, Yury; Rigollet, Philippe										A mathematical perspective on Transformers								Arxiv											4	4;2024-08-12;https://www.arxiv.org/abs/2312.10794v4| 3;2024-02-06;https://www.arxiv.org/abs/2312.10794v3| 2;2023-12-22;https://www.arxiv.org/abs/2312.10794v2| 1;2023-12-17;https://www.arxiv.org/abs/2312.10794v1	arXiv:2312.10794			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 12 2024	2024	Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.																																	2024-08-21	PPRN:86680864		
J	Lu, Yadong; Yang, Jianwei; Shen, Yelong; Awadallah, Ahmed										OmniParser for Pure Vision Based GUI Agent								Arxiv											1	1;2024-08-01;https://www.arxiv.org/abs/2408.00203v1	arXiv:2408.00203			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 01 2024	2024	The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, we argue that the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, we introduce textsc{OmniParser}, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. We first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. textsc{OmniParser} significantly improves GPT-4V's performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, textsc{OmniParser} with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot.																																	2024-08-08	PPRN:91197394		
J	Bandarkar, Lucas; Liang, Davis; Muller, Benjamin; Artetxe, Mikel; Shukla, Satya Narayan; Husa, Donald; Goyal, Naman; Krishnan, Abhinandan; Zettlemoyer, Luke; Khabsa, Madian										The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants								Arxiv											2	2;2024-07-25;https://www.arxiv.org/abs/2308.16884v2| 1;2023-08-31;https://www.arxiv.org/abs/2308.16884v1	arXiv:2308.16884			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 25 2024	2024	We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.																																	2024-08-02	PPRN:84625207		
J	Feng, Shangbin; Shi, Weijia; Wang, Yike; Ding, Wenxuan; Balachandran, Vidhisha; Tsvetkov, Yulia				Wang, Yike/JEP-6766-2023						Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2402.00367v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.00367v1	arXiv:2402.00367			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps—missing or outdated information in LLMs—might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on heldout sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our abstention methods pinpoint failure cases in retrieval augmentation and knowledge gaps in multi-hop reasoning.1 																																	2024-07-18	PPRN:87450928		
J	Yu, Jifan; Wang, Xiaozhi; Tu, Shangqing; Cao, Shulin; Zhang-Li, Daniel; Lv, Xin; Peng, Hao; Yao, Zijun; Zhang, Xiaohan; Li, Hanming; Li, Chunyang; Zhang, Zheyuan; Bai, Yushi; Liu, Yantao; Xin, Amy; Lin, Nianyi; Yun, Kaifeng; Gong, Linlu; Chen, Jianhui; Wu, Zhili; Qi, Yunjia; Li, Weikai; Guan, Yong; Zeng, Kaisheng; Qi, Ji; Jin, Hailong; Liu, Jinxin; Gu, Yu; Yao, Yuan; Ding, Ning; Hou, Lei; Liu, Zhiyuan; Xu, Bin; Tang, Jie; Li, Juanzi				peng, hao/IVV-8271-2023; Chen, Jianhui/I-7286-2016; Jin, Hailong/JEF-1495-2023; Xu, Bin/GRE-7630-2022; Tu, Shangqing/HSF-1919-2023; Xin, Amy/NGS-3843-2025; tang, jie/KIE-8633-2024; Zhang-Li, Daniel/OJT-2553-2025; Li, Weikai/JOJ-6825-2023; Wang, Xiaozhi/IQT-4844-2023; chunyang, Li/AAM-3833-2020; Liu, Zhiyuan/I-2233-2014; 航航, 张/KBC-0720-2024; 谷, 钰/JMP-3607-2023; Li, Zhiyuan/ESQ-7168-2022						KoLA: Carefully Benchmarking World Knowledge of Large Language Models								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2306.09296v3| 1;2023-06-15;https://www.arxiv.org/abs/2306.09296v1	arXiv:2306.09296			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling , we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data , to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria , we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 28 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge systems.																																	2024-07-18	PPRN:73385354		
J	Wang, Junyang; Xu, Haiyang; Jia, Haitao; Zhang, Xi; Yan, Ming; Shen, Weizhou; Zhang, Ji; Huang, Fei; Sang, Jitao				wang, junyang/HMP-6590-2023; Xu, Haiyang/AAC-2095-2021						Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration								Arxiv											1	1;2024-06-03;https://www.arxiv.org/abs/2406.01014v1	arXiv:2406.01014			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks, task progress navigation and focus content navigation, are significantly complicated under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent generates task progress, making the navigation of history operations more efficient. To retain focus content, we design a memory unit that updates with task progress. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistakes accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.																																	2024-11-20	PPRN:89154677		
J	Puigcerver, Joan; Riquelme, Carlos; Mustafa, Basil; Houlsby, Neil				Puigcerver, Joan/L-9908-2014; Riquelme, Carlos/GQH-4442-2022						From Sparse to Soft Mixtures of Experts								Arxiv											2	2;2024-05-27;https://www.arxiv.org/abs/2308.00951v2| 1;2023-08-02;https://www.arxiv.org/abs/2308.00951v1	arXiv:2308.00951			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 27 2024	2024	Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoEs, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity (and performance) at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens Choice and Experts Choice). Furthermore, Soft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40× × more parameters than ViT Huge/14, with only 2% increased inference time, and substantially better quality.																																	2024-06-08	PPRN:74222869		
J	Li, Ziyao				Li, Ziyao/KCL-1869-2024						Kolmogorov-Arnold Networks are Radial Basis Function Networks								Arxiv											1	1;2024-05-10;https://www.arxiv.org/abs/2405.06721v1	arXiv:2405.06721			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 10 2024	2024	This short paper is a fast proof-of-concept that the 3-order B-splines used in Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian radial basis functions. Doing so leads to FastKAN, a much faster implementation of KAN which is also a radial basis function (RBF) network.																																	2024-06-08	PPRN:89033152		
J	Peeperkorn, Max; Kouwenhoven, Tom; Brown, Dan; Jordanous, Anna				Jordanous, Anna/AAF-7976-2020						Is Temperature the Creativity Parameter of Large Language Models?								Arxiv											1	1;2024-05-01;https://www.arxiv.org/abs/2405.00492v1	arXiv:2405.00492			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 01 2024	2024	Large language models (LLMs) are applied to all sorts of creative tasks, and their outputs vary from beautiful, to peculiar, to pastiche, into plain plagiarism. The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter. Here, we investigate this claim using a narrative generation task with a predetermined fixed context, model and prompt. Specifically, we present an empirical analysis of the LLM output for different temperature values using four necessary conditions for creativity in narrative generation: novelty, typicality, cohesion, and coherence. We find that temperature is weakly correlated with novelty, and unsurprisingly, moderately correlated with incoherence, but there is no relationship with either cohesion or typicality. However, the influence of temperature on creativity is far more nuanced and weak than suggested by the "creativity parameter" claim; overall results suggest that the LLM generates slightly more novel outputs as temperatures get higher. Finally, we discuss ideas to allow more controlled LLM creativity, rather than relying on chance via changing the temperature parameter.																																	2024-05-18	PPRN:88711339		
J	Chan, Chi-Min; Xu, Chunpu; Yuan, Ruibin; Luo, Hongyin; Xue, Wei; Guo, Yike; Fu, Jie										RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation								Arxiv											1	1;2024-03-31;https://www.arxiv.org/abs/2404.00610v1	arXiv:2404.00610			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 31 2024	2024	Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets.																																	2024-04-17	PPRN:88360696		
J	An, Shengnan; Ma, Zexiong; Lin, Zeqi; Zheng, Nanning; Lou, Jian-Guang; Chen, Weizhu										Learning From Mistakes Makes LLM Better Reasoner								Arxiv											3	3;2024-03-29;https://www.arxiv.org/abs/2310.20689v4| 2;2024-02-03;https://www.arxiv.org/abs/2310.20689v3| 1;2023-10-31;https://www.arxiv.org/abs/2310.20689v1	arXiv:2310.20689			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 29 2024	2024	Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA.																																	2024-04-17	PPRN:85896527		
J	Bellagente, Marco; Tow, Jonathan; Mahan, Dakota; Phung, Duy; Zhuravinskyi, Maksym; Adithyan, Reshinth; Baicoianu, James; Brooks, Ben; Cooper, Nathan; Datta, Ashish; Lee, Meng; Mostaque, Emad; Pieler, Michael; Pinnaparju, Nikhil; Rocha, Paulo; Saini, Harry; Teufel, Hannah; Zanichelli, Niccolo; Riquelme, Carlos				Riquelme, Carlos/GQH-4442-2022						Stable LM 2 1.6B Technical Report								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17834v1	arXiv:2402.17834			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.																																	2024-03-22	PPRN:87985530		
J	Qin, Yiwei; Song, Kaiqiang; Hu, Yebowen; Yao, Wenlin; Cho, Sangwoo; Wang, Xiaoyang; Wu, Xuansheng; Liu, Fei; Liu, Pengfei; Yu, Dong				Wu, Xuansheng/JCE-2579-2023; Hu, Yebowen/KHW-5602-2024; Liu, Pengfei/JUV-0307-2023						InFoBench: Evaluating Instruction Following Ability in Large Language Models								Arxiv											1	1;2024-01-07;https://www.arxiv.org/abs/2401.03601v1	arXiv:2401.03601			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 07 2024	2024	This paper introduces the Decomposed Requirements Following Ratio (DRFR), a new metric for evaluating Large Language Models' (LLMs) ability to follow instructions. Addressing a gap in current methodologies, DRFR breaks down complex instructions into simpler criteria, facilitating a detailed analysis of LLMs' compliance with various aspects of tasks. Alongside this metric, we present InFoBench, a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories. Our experiments compare DRFR with traditional scoring methods and explore annotation sources, including human experts, crowd-sourced workers, and GPT-4. The findings demonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a cost-efficient annotator. The evaluation of several advanced LLMs using this framework reveals their strengths and areas needing improvement, particularly in complex instruction-following. This study contributes a novel metric and benchmark, offering insights for future LLM development and evaluation.																																	2024-05-25	PPRN:87033020		
J	Keevash, Peter										The existence of designs								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/1401.3665v4	arXiv:1401.3665			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	We prove the existence conjecture for combinatorial designs, answering a question of Steiner from 1853. More generally, we show that the natural divisibility conditions are sufficient for clique decompositions of simplicial complexes that satisfy a certain pseudorandomness condition. As a further generalisation, we obtain the same conclusion only assuming an extendability property and the existence of a robust fractional clique decomposition.																																	2025-01-08	PPRN:119466354		
J	Zeng, Yifan; Wu, Yiran; Zhang, Xiao; Wang, Huazheng; Wu, Qingyun				Wang, Huazheng/GWZ-4217-2022; Wu, Qing-Yun/G-9976-2017; Zeng, Yifan/GXF-5642-2022						AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks								Arxiv											2	2;2024-11-14;https://www.arxiv.org/abs/2403.04783v2| 1;2024-03-02;https://www.arxiv.org/abs/2403.04783v1	arXiv:2403.04783			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 14 2024	2024	Despite extensive pre-training in moral alignment to prevent generating harmful information, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense framework that filters harmful responses from LLMs. With the response-filtering mechanism, our framework is robust against different jailbreak attack prompts, and can be used to defend different victim models. AutoDefense assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. With AutoDefense, small open-source LMs can serve as agents and defend larger models against jailbreak attacks. Our experiments show that AutoDefense can effectively defense against different jailbreak attacks, while maintaining the performance at normal user request. For example, we reduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using LLaMA-2-13b with a 3-agent system. 																																	2024-12-23	PPRN:88087130		
J	Gat, Itai; Remez, Tal; Shaul, Neta; Kreuk, Felix; Chen, Ricky T.Q.; Synnaeve, Gabriel; Adi, Yossi; Lipman, Yaron				Adi, Yossi/HLG-5748-2023; Chen, Ricky/AAS-3168-2021						Discrete Flow Matching								Arxiv											2	2;2024-11-05;https://www.arxiv.org/abs/2407.15595v2| 1;2024-07-22;https://www.arxiv.org/abs/2407.15595v1	arXiv:2407.15595			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 05 2024	2024	Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: (i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser (x-prediction) and noise-prediction (E-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.																																	2024-12-10	PPRN:91028960		
J	Hwang, Jyh-Jing; Xu, Runsheng; Lin, Hubert; Hung, Wei-Chih; Ji, Jingwei; Choi, Kristy; Huang, Di; He, Tong; Covington, Paul; Sapp, Benjamin; Zhou, Yin; Guo, James; Anguelov, Dragomir; Tan, Mingxing				Ji, Jingwei/HSF-1480-2023						EMMA: End-to-End Multimodal Model for Autonomous Driving								Arxiv											1	1;2024-11-04;https://www.arxiv.org/abs/2410.23262v2	arXiv:2410.23262			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving. Built on a multi-modal large language model foundation, EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements. EMMA maximizes the utility of world knowledge from the pre-trained large language models, by representing all non-sensor inputs (e.g. navigation instructions and ego vehicle status) and outputs (e.g. trajectories and 3D locations) as natural language text. This approach allows EMMA to jointly process various driving tasks in a unified language space, and generate the outputs for each task using task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by achieving state-of-the-art performance in motion planning on nuScenes as well as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also yields competitive results for camera-primary 3D object detection on the Waymo Open Dataset (WOD). We show that co-training EMMA with planner trajectories, object detection, and road graph tasks yields improvements across all three domains, highlighting EMMA's potential as a generalist model for autonomous driving applications. However, EMMA also exhibits certain limitations: it can process only a small amount of image frames, does not incorporate accurate 3D sensing modalities like LiDAR or radar and is computationally expensive. We hope that our results will inspire further research to mitigate these issues and to further evolve the state of the art in autonomous driving model architectures.																																	2024-12-16	PPRN:119092278		
J	Ji, Jiaming; Chen, Boyuan; Lou, Hantao; Hong, Donghai; Zhang, Borong; Pan, Xuehai; Dai, Juntao; Qiu, Tianyi; Yang, Yaodong				Chen, Bo/B-7817-2018						Aligner: Efficient Alignment by Learning to Correct								Arxiv											4	4;2024-11-02;https://www.arxiv.org/abs/2402.02416v5| 3;2024-06-24;https://www.arxiv.org/abs/2402.02416v4| 2;2024-06-03;https://www.arxiv.org/abs/2402.02416v3| 1;2024-02-06;https://www.arxiv.org/abs/2402.02416v2	arXiv:2402.02416			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 02 2024	2024	With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9% in helpfulness and 23.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).																																	2024-12-16	PPRN:87529344		
J	Zhang, Kai; Zhou, Rong; Adhikarla, Eashan; Yan, Zhiling; Liu, Yixin; Yu, Jun; Liu, Zhengliang; Chen, Xun; Davison, Brian D.; Ren, Hui; Huang, Jing; Chen, Chen; Zhou, Yuyin; Fu, Sunyang; Liu, Wei; Liu, Tianming; Li, Xiang; Chen, Yong; He, Lifang; Zou, James; Li, Quanzheng; Liu, Hongfang; Sun, Lichao				Davison, Brian/I-2774-2019; Liu, Wei/JYQ-6082-2024; Quanzheng, Li/OHU-0205-2025; He, Lifang/D-8175-2016; Liu, Hongfang/ISU-9369-2023; Fu, Sunyang/JAO-5130-2023; Zhou, Yuyin/AAS-7674-2021; Li, Xiang/J-6924-2019; Zhang, Kai/AAX-4721-2020						BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse Biomedical Tasks								Arxiv											4	4;2024-08-11;https://www.arxiv.org/abs/2305.17100v4| 3;2024-07-19;https://www.arxiv.org/abs/2305.17100v3| 2;2024-01-09;https://www.arxiv.org/abs/2305.17100v2| 1;2023-05-26;https://www.arxiv.org/abs/2305.17100v1	arXiv:2305.17100			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 11 2024	2024	Traditional biomedical artificial intelligence (AI) models, designed for specific tasks or modalities, often exhibit limited flexibility in real-world deployment and struggle to utilize holistic information. Generalist AI holds the potential to address these limitations due to its versatility in interpreting different data types and generating tailored outputs for diverse needs. However, existing biomedical generalist AI solutions are typically heavyweight and closed source to researchers, practitioners, and patients. Here, we propose BiomedGPT, the first open-source and lightweight vision-language foundation model, designed as a generalist capable of performing various biomedical tasks. BiomedGPT achieved state-of-the-art results in 16 out of 25 experiments while maintaining a computing-friendly model scale. We also conducted human evaluations to assess the capabilities of BiomedGPT in radiology visual question answering, report generation, and summarization. BiomedGPT exhibits robust prediction ability with a low error rate of 3.8% in question answering, satisfactory performance with an error rate of 8.3% in writing complex radiology reports, and competitive summarization ability with a nearly equivalent preference score to human experts. Our method demonstrates that effective training with diverse data can lead to more practical biomedical AI for improving diagnosis and workflow efficiency.																																	2024-08-25	PPRN:72730916		
J	Zhang, Jiwen; Wu, Jihao; Teng, Yihua; Liao, Minghui; Xu, Nuo; Xiao, Xiao; Wei, Zhongyu; Tang, Duyu				Liao, Minghui/J-9662-2019; Wei, Zhongyu/KSL-9373-2024						Android in the Zoo: Chain-of-Action-Thought for GUI Agents								Arxiv											2	2;2024-07-13;https://www.arxiv.org/abs/2403.02713v2| 1;2024-03-05;https://www.arxiv.org/abs/2403.02713v1	arXiv:2403.02713			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 13 2024	2024	Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typically consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon three off-the-shelf LMMs, CoAT significantly improves the action prediction compared to previous proposed context modeling. To further facilitate the research in this line, we construct a dataset Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 1B model (i.e. AUTO-UI-base) on our AitZ dataset achieves on-par performance with CogAgent-Chat-18B.																																	2024-07-23	PPRN:88033017		
J	Sidharth, S S; Gokul, R; Anas, K P; Keerthana, A R										Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation								Arxiv											2	2;2024-06-14;https://www.arxiv.org/abs/2405.07200v3| 1;2024-05-14;https://www.arxiv.org/abs/2405.07200v2	arXiv:2405.07200			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 14 2024	2024	Accurate approximation of complex nonlinear functions is a fundamental challenge across many scientific and engineering domains. Traditional neural network architectures, such as Multi-Layer Perceptrons (MLPs), often struggle to efficiently capture intricate patterns and irregularities present in high-dimensional functions. This paper presents the Chebyshev Kolmogorov-Arnold Network (Chebyshev KAN), a new neural network architecture inspired by the Kolmogorov-Arnold representation theorem, incorporating the powerful approximation capabilities of Chebyshev polynomials. By utilizing learnable functions parametrized by Chebyshev polynomials on the network's edges, Chebyshev KANs enhance flexibility, efficiency, and interpretability in function approximation tasks. We demonstrate the efficacy of Chebyshev KANs through experiments on digit classification, synthetic function approximation, and fractal function generation, highlighting their superiority over traditional MLPs in terms of parameter efficiency and interpretability. Our comprehensive evaluation, including ablation studies, confirms the potential of Chebyshev KANs to address longstanding challenges in nonlinear function approximation, paving the way for further advancements in various scientific and engineering applications.																																	2024-08-24	PPRN:89046313		
J	Deng, Gelei; Liu, Yi; Mayoral-Vilches, Victor; Liu, Peng; Li, Yuekang; Xu, Yuan; Zhang, Tianwei; Liu, Yang; Pinzger, Martin; Rass, Stefan				Liu, Yang/D-2306-2013; Liu, Peng/NIU-2310-2025; Zhang, Tianwei/AAV-8818-2020						PentestGPT: An LLM-empowered Automatic Penetration Testing Tool								Arxiv											2	2;2024-06-02;https://www.arxiv.org/abs/2308.06782v2| 1;2023-08-13;https://www.arxiv.org/abs/2308.06782v1	arXiv:2308.06782			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 02 2024	2024	Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this work, we establish a comprehensive benchmark using real-world penetration testing targets and further use it to explore the capabilities of LLMs in this domain. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining a whole context of the overall testing scenario. Based on these insights, we introduce P ENTEST GPT, an LLM-empowered automated penetration testing framework that leverages the abundant domain knowledge inherent in LLMs. P ENTEST GPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that P ENTEST GPT not only outperforms LLMs with a task-completion increase of 228.6% compared to the GPT-3 .5 model among the benchmark targets, but also proves effective in tackling real-world penetration testing targets and CTF challenges. Having been open-sourced on GitHub, P ENTEST GPT has garnered over 6,200 stars in 9 months and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.																																	2024-06-19	PPRN:77423384		
J	Zhao, Yiran; Zhang, Wenxuan; Chen, Guizhen; Kawaguchi, Kenji; Bing, Lidong				Kawaguchi, Kenji/AAR-8297-2020; zhao, yiran/JNE-4693-2023; Zhang, Wenxuan/LKN-9746-2024						How do Large Language Models Handle Multilingualism?								Arxiv											2	2;2024-05-24;https://www.arxiv.org/abs/2402.18815v2| 1;2024-02-29;https://www.arxiv.org/abs/2402.18815v1	arXiv:2402.18815			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	May 24 2024	2024	Large language models (LLMs) have demonstrated impressive capabilities across diverse languages. This study explores how LLMs handle multilingualism. Based on observed language ratio shifts among layers and the relationships between network structures and certain capabilities, we hypothesize the LLM's multilingual workflow (MWork): LLMs initially understand the query, converting multilingual inputs into English for task-solving. In the intermediate layers, they employ English for thinking and incorporate multilingual knowledge with self-attention and feed-forward structures, respectively. In the final layers, LLMs generate responses aligned with the original language of the query. To verify MWork, we introduce Parallel Language-specific Neuron Detection (PLND) to identify activated neurons for inputs in different languages without any labeled data. Using PLND, we validate MWork through extensive experiments involving the deactivation of language-specific neurons across various layers and structures. Moreover, MWork allows fine-tuning of language-specific neurons with a small dataset, enhancing multilingual abilities in a specific language without compromising others. This approach results in an average improvement of 3.6% for high-resource languages and 2.3% for low-resource languages across all tasks with just 400 documents.																																	2024-06-08	PPRN:87989724		
J	Chung, Hyungjin; Sim, Byeongsu; Ryu, Dohoon; Ye, Jong Chul				Ye, Jong/C-1623-2011; Chung, Hyungjin/AAL-1161-2021						Improving Diffusion Models for Inverse Problems using Manifold Constraints								Arxiv											2	2;2022-06-02;https://www.arxiv.org/abs/2206.00941v1| 1;2024-05-01;	arXiv:2206.00941			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	May 01 2024	2024	Recently, diffusion models have been used to solve various inverse problems in an unsupervised manner with appropriate modifications to the sampling process. However, the current solvers, which recursively apply a reverse diffusion step followed by a projection-based measurement consistency step, often produce suboptimal results. By studying the generative sampling path, here we show that current solvers throw the sample path off the data manifold, and hence the error accumulates. To address this, we propose an additional correction term inspired by the manifold constraint, which can be used synergistically with the previous solvers to make the iterations close to the manifold. The proposed manifold constraint is straightforward to implement within a few lines of code, yet boosts the performance by a surprisingly large margin. With extensive experiments, we show that our method is superior to the previous methods both theoretically and empirically, producing promising results in many applications such as image inpainting, colorization, and sparse-view computed tomography. Code available https://github.com/HJ-harry/MCG_diffusion																																	2025-11-07	PPRN:12145920		
J	Jin, Chao; Zhang, Zili; Jiang, Xuanlin; Liu, Fangyue; Liu, Xin; Liu, Xuanzhe; Jin, Xin				Liu, Xuanzhe/MIN-0907-2025						RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation								Arxiv											2	2;2024-04-25;https://www.arxiv.org/abs/2404.12457v2| 1;2024-04-18;https://www.arxiv.org/abs/2404.12457v1	arXiv:2404.12457			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 25 2024	2024	Retrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose RAGCache, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge’s intermediate states). Based on these insights, we design RAGCache, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement RAGCache and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that RAGCache reduces the time to first token (TTFT) by up to 4× and improves the throughput by up to 2.1× compared to vLLM integrated with Faiss.																																	2024-05-04	PPRN:88590528		
J	Wu, Wenhao; Wang, Yizhong; Xiao, Guangxuan; Peng, Hao; Fu, Yao				Wu, Wenhao/KDM-6203-2024						Retrieval Head Mechanistically Explains Long-Context Factuality								Arxiv											1	1;2024-04-24;https://www.arxiv.org/abs/2404.15574v1	arXiv:2404.15574			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 24 2024	2024	Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.																																	2024-05-04	PPRN:88634896		
J	Goyal, Sachin; Ji, Ziwei; Rawat, Ankit Singh; Menon, Aditya Krishna; Kumar, Sanjiv; Nagarajan, Vaishnavh				Rawat, Ankit/V-3483-2019						Think before you speak: Training Language Models With Pause Tokens								Arxiv											3	3;2024-04-21;https://www.arxiv.org/abs/2310.02226v3| 2;2024-03-13;https://www.arxiv.org/abs/2310.02226v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02226v1	arXiv:2310.02226			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 21 2024	2024	Transformer-based language models generate responses by producing a series of tokens in immediate succession: the (K + 1)th token is an outcome of manipulating K hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, K + 10 hidden vectors, before it outputs the (K + 1)th token? We operationalize this idea by performing training and inference on language models with a (learnable) pause token, a sequence of which is appended to the input prefix. We then delay extracting the model’s outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate pause-training on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains on our tasks when the model is both pretrained and finetuned with delays. For the 1B model, we witness gains on eight tasks, most prominently, a gain of 18% EM score on the QA task of SQuAD, 8% on CommonSenseQA and 1% accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.																																	2024-04-30	PPRN:85379482		
J	Deng, Yihe; Zhang, Weitong; Chen, Zixiang; Gu, Quanquan				Zhang, Weitong/AHA-3224-2022; CHEN, ZIXIANG/OKT-0372-2025						Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves								Arxiv											2	2;2024-04-18;https://www.arxiv.org/abs/2311.04205v2| 1;2023-11-07;https://www.arxiv.org/abs/2311.04205v1	arXiv:2311.04205			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 18 2024	2024	Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named ‘Rephrase and Respond’ (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive discussion with the popular Chain -of -Thought (CoT) methods. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities.																																	2024-04-30	PPRN:86076138		
J	Huang, Qidong; Dong, Xiaoyi; Zhang, Pan; Wang, Bin; He, Conghui; Wang, Jiaqi; Lin, Dahua; Zhang, Weiming; Yu, Nenghai				Dong, Xiaoyi/AAC-8666-2019; Wang, Bin/MVU-8917-2025; Zhang, Zhiyong/KPY-6346-2024; WANG, JIAQI/KBB-8837-2024; Lin, Dahua/W-6576-2019; He, Conghui/AAZ-3323-2021						OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation								Arxiv											2	2;2024-03-12;https://www.arxiv.org/abs/2311.17911v3| 1;2024-02-29;https://www.arxiv.org/abs/2311.17911v2	arXiv:2311.17911			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 12 2024	2024	Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs. In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training. Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens. Such partial over-trust inclination results in the neglecting of image tokens and describes the image content with hallucination. Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary. With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality. 																																	2024-04-04	PPRN:88006553		
J	Wang, Samson; Fontana, Enrico; Cerezo, M.; Sharma, Kunal; Sone, Akira; Cincio, Lukasz; Coles, Patrick J.				Cerezo, Marco/ABD-9254-2020; Sone, Akira/ABG-1292-2021; Sharma, Kunal/HZM-4702-2023						Noise-Induced Barren Plateaus in Variational Quantum Algorithms								Arxiv											2	2;2024-03-02;https://www.arxiv.org/abs/2007.14384v6| 1;2020-07-28;https://www.arxiv.org/abs/2007.14384v5	arXiv:2007.14384			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 02 2024	2024	Variational Quantum Algorithms (VQAs) may be a path to quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) computers. A natural question is whether noise on NISQ devices places fundamental limitations on VQA performance. We rigorously prove a serious limitation for noisy VQAs, in that the noise causes the training landscape to have a barren plateau (i.e., vanishing gradient). Specifically, for the local Pauli noise considered, we prove that the gradient vanishes exponentially in the number of qubits n if the depth of the ansatz grows linearly with n. These noise-induced barren plateaus (NIBPs) are conceptually different from noise-free barren plateaus, which are linked to random parameter initialization. Our result is formulated for a generic ansatz that includes as special cases the Quantum Alternating Operator Ansatz and the Unitary Coupled Cluster Ansatz, among others. For the former, our numerical heuristics demonstrate the NIBP phenomenon for a realistic hardware noise model.																																	2024-03-31	PPRN:12226784		
J	Lv, Huijie; Wang, Xiao; Zhang, Yuansen; Huang, Caishuang; Dou, Shihan; Ye, Junjie; Gui, Tao; Zhang, Qi; Huang, Xuanjing				Ye, Junjie/AAW-8101-2021; yuansen, zhang/LKM-7514-2024; Gui, Tao/LWI-6783-2024						CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2402.16717v1	arXiv:2402.16717			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our method achieves an 86.6% ASR on GPT-4-1106.																																	2024-03-27	PPRN:87934430		
J	Perozzi, Bryan; Fatemi, Bahare; Zelle, Dustin; Tsitsulin, Anton; Kazemi, Mehran; Al-Rfou, Rami; Halcrow, Jonathan										Let Your Graph Do the Talking: Encoding Structured Data for LLMs								Arxiv											1	1;2024-02-08;https://www.arxiv.org/abs/2402.05862v1	arXiv:2402.05862			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 08 2024	2024	How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.																																	2024-02-24	PPRN:87572263		
J	Riebesell, Janosh; Goodall, Rhys E.A.; Benner, Philipp; Chiang, Yuan; Deng, Bowen; Ceder, Gerbrand; Asta, Mark; Lee, Alpha A.; Jain, Anubhav; Persson, Kristin A.				Chiang, Yuan/GLR-2596-2022; Persson, Kristin/ABE-6343-2020; Goodall, Rhys/HKE-8338-2023; Jain, Anubhav/JYO-3714-2024						Matbench Discovery -- A framework to evaluate machine learning crystal stability predictions								Arxiv											3	3;2024-12-10;https://www.arxiv.org/abs/2308.14920v3| 2;2024-02-04;https://www.arxiv.org/abs/2308.14920v2| 1;2023-08-28;https://www.arxiv.org/abs/2308.14920v1	arXiv:2308.14920			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 10 2024	2024	The rapid adoption of machine learning (ML) in domain sciences necessitates best practices and standardized benchmarking for performance evaluation. We present Matbench Discovery, an evaluation framework for ML energy models, applied as pre-filters for high-throughput searches of stable inorganic crystals. This framework addresses the disconnect between thermodynamic stability and formation energy, as well as retrospective vs. prospective benchmarking in materials discovery. We release a Python package to support model submissions and maintain an online leaderboard, offering insights into performance trade-offs. To identify the best-performing ML methodologies for materials discovery, we benchmarked various approaches, including random forests, graph neural networks (GNNs), one-shot predictors, iterative Bayesian optimizers, and universal interatomic potentials (UIP). Our initial results rank models by test set F1 scores for thermodynamic stability prediction: EquiformerV2 + DeNS > Orb > SevenNet > MACE > CHGNet > M3GNet > ALIGNN > MEGNet > CGCNN > CGCNN+P > Wrenformer > BOWSR > Voronoi fingerprint random forest. UIPs emerge as the top performers, achieving F1 scores of 0.57–0.82 and discovery acceleration factors (DAF) of up to 6x on the first 10k stable predictions compared to random selection. We also identify a misalignment between regression metrics and task-relevant classification metrics. Accurate regressors can yield high false-positive rates near the decision boundary at 0 eV/atom above the convex hull. Our results demonstrate UIPs’ ability to optimize computational budget allocation for expanding materials databases. However, their limitations remain underexplored in traditional benchmarks. We advocate for task-based evaluation frameworks, as implemented here, to address these limitations and advance ML-guided materials discovery.																																	2025-01-19	PPRN:84492643		
J	Qu, Changle; Dai, Sunhao; Wei, Xiaochi; Cai, Hengyi; Wang, Shuaiqiang; Yin, Dawei; Xu, Jun; Wen, Ji-Rong				Yin, Dawei/JOR-9201-2023; Dai, Sunhao/JJD-4340-2023						Tool Learning with Large Language Models: A Survey								Arxiv											3	3;2024-11-04;https://www.arxiv.org/abs/2405.17935v3| 2;2024-05-30;https://www.arxiv.org/abs/2405.17935v2| 1;2024-05-28;https://www.arxiv.org/abs/2405.17935v1	arXiv:2405.17935			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 04 2024	2024	Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the "why" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of "how", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area.																																	2024-12-16	PPRN:89087598		
J	Chen, Junying; Gui, Chi; Ouyang, Ruyi; Gao, Anningzhe; Chen, Shunian; Chen, Guiming Hardy; Wang, Xidong; Zhang, Ruifei; Cai, Zhenyang; Ji, Ke; Yu, Guangjun; Wan, Xiang; Wang, Benyou				Wang, Benyou/Y-5146-2019						HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale								Arxiv											4	4;2024-09-30;https://www.arxiv.org/abs/2406.19280v4| 3;2024-09-25;https://www.arxiv.org/abs/2406.19280v3| 2;2024-09-15;https://www.arxiv.org/abs/2406.19280v2| 1;2024-06-27;https://www.arxiv.org/abs/2406.19280v1	arXiv:2406.19280			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 15 2024	2024	The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed's large-scale, de-identified medical image-text pairs to address these limitations, they still fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an 'unblinded' capacity to denoise and reformat the data, resulting in the creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of current MLLMs, showing significant improvement in benchmarks including the MMMU Health & Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows superior performance in medical multimodal scenarios among open-source MLLMs.																																	2024-12-24	PPRN:90133577		
J	Zhong, Li; Wang, Zilong; Shang, Jingbo				Wang, Zilong/C-3784-2012; Shang, Jingbo/T-4207-2019						Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step								Arxiv											3	3;2024-06-06;https://www.arxiv.org/abs/2402.16906v6| 2;2024-06-04;https://www.arxiv.org/abs/2402.16906v5| 1;2024-04-29;https://www.arxiv.org/abs/2402.16906v4	arXiv:2402.16906			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated program as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments programs into basic blocks and tracks the values of intermediate variables after each block throughout runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and effectively pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state -of -the -art performance in code debugging for various LLM selections.																																	2024-07-04	PPRN:88696800		
J	Yang, Enneng; Wang, Zhenyi; Shen, Li; Liu, Shiwei; Guo, Guibing; Wang, Xingwei; Tao, Dacheng				Shen, Li/AEZ-9528-2022; wang, zhenyi/HDZ-8319-2022; Yang, Enneng/GRO-5053-2022						AdaMerging: Adaptive Model Merging for Multi-Task Learning								Arxiv											3	3;2024-05-28;https://www.arxiv.org/abs/2310.02575v2| 2;2023-10-04;https://www.arxiv.org/abs/2310.02575v1| 1;2023-10-04;https://www.arxiv.org/abs/2310.02575v1	arXiv:2310.02575			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	Multi-task learning (MTL) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute MTL without necessitating a retraining process using the initial training data. Nevertheless, this direct addition of models often leads to a significant deterioration in the overall performance of the merged model. This decline occurs due to potential conflicts and intricate correlations among the multiple tasks. Consequently, the challenge emerges of how to merge pre-trained models more effectively without using their original training data. This paper introduces an innovative technique called Adaptive Model Merging ( AdaMerging ). This approach aims to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data. Specifically, our AdaMerging method operates as an automatic, unsupervised task arithmetic scheme. It leverages entropy minimization on unlabeled test samples from the multi-task setup as a surrogate objective function to iteratively refine the merging coefficients of the multiple models. Our experimental findings across eight tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared to the current state-of-the-art task arithmetic merging scheme, AdaMerging showcases a remarkable 11% improvement in performance. Notably, AdaMerging also exhibits superior generalization capabilities when applied to unseen downstream tasks. Furthermore, it displays a significantly enhanced robustness to data distribution shifts that may occur during the testing phase. The code is available at AdaMerging.																																	2024-06-14	PPRN:86282416		
J	Xu, Zihao; Liu, Yi; Deng, Gelei; Li, Yuekang; Picek, Stjepan				Li, Yuekang/U-9646-2019; xu, zihao/JDM-7099-2023; LIU, YI/LUY-0513-2024						A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models								Arxiv											2	2;2024-05-17;https://www.arxiv.org/abs/2402.13457v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.13457v1	arXiv:2402.13457			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 17 2024	2024	Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of "jailbreaking", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain.																																	2024-06-01	PPRN:87791914		
J	Zhang, Yanan; Su, Dajun; Huang, Yanen; Shan, Zhaoyang; Sun, Hualei; Huo, Mengwu; Ye, Kaixin; Zhang, Jiawen; Yang, Zihan; Xu, Yongkang; Su, Yi; Li, Rui; Smidman, Michael; Wang, Meng; Jiao, Lin; Yuan, Huiqiu				yang, zihan/LMO-3264-2024; WANG, MENG/E-6595-2012						High-temperature superconductivity with zero-resistance and strange metal behaviour in La3Ni2O7−δ								Arxiv											2	2;2024-04-18;https://www.arxiv.org/abs/2307.14819v2| 1;2023-07-27;https://www.arxiv.org/abs/2307.14819v1	arXiv:2307.14819			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 18 2024	2024	Recently signatures of superconductivity were observed close to 80 K in La3Ni2O7 under pressure [1]. This discovery positions La3Ni2O7 as the first bulk nickelate with high -temperature superconductivity, but the lack of zero resistance presents a significant drawback for validating the findings. Here we report pressure measurements up to over 30 GPa using a liquid pressure medium and show that single crystals of La3Ni2O7−δ do exhibit zero resistance. We find that La3Ni2O7−δ remains metallic under applied pressures, suggesting the absence of a metal -insulator transition proximate to the superconductivity. Analysis of the normal state T -linear resistance suggests an intricate link between this strange metal behaviour and superconductivity, whereby at high pressures both the linear resistance coefficient and superconducting transition are slowly suppressed by pressure, while at intermediate pressures both the superconductivity and strange metal behaviour appear disrupted, possibly due to a nearby structural instability. The association between strange metal behaviour and high -temperature superconductivity is very much in line with diverse classes of unconventional superconductors, including the cuprates and Fe -based superconductors [2–6]. Understanding the superconductivity of La3Ni2O7−δ evidently requires further revealing the interplay of strange metal behaviour, superconductivity, as well as possible competing electronic or structural phases.																																	2024-04-28	PPRN:74129298		
J	Peng, Bo; Goldstein, Daniel; Anthony, Quentin; Albalak, Alon; Alcaide, Eric; Biderman, Stella; Cheah, Eugene; Du, Xingjian; Ferdinan, Teddy; Hou, Haowen; Kazienko, Przemyslaw; Kiran, G V Kranthi; Kocon, Jan; Koptyra, Bartlomiej; Krishna, Satyapriya; McClelland Jr, Ronald; Muennighoff, Niklas; Obeid, Fares; Saito, Atsushi; Song, Guangyu; Tu, Haoqin; Wozniak, Stanislaw; Zhang, Ruichong; Zhao, Bingchen; Zhao, Qihang; Zhou, Peng; Zhu, Jian; Zhu, Rui-Jie				Ferdinan, Teddy/JZT-9061-2024; Zhao, Bingchen/NRB-6996-2025; Koptyra, Bartlomiej/LDF-8467-2024; Zhou, Peng/C-8845-2018; Woźniak, Stanisław/LIH-4606-2024; Zhu, Ruijie/AGC-4742-2022; Saito, Atsushi/M-7382-2018; Kazienko, Przemysław/F-1849-2014; Kocon, Jan/IXN-3388-2023; Du, Xingjian/JMB-3340-2023; Qihang, Zhao/AAX-7897-2021						Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence								Arxiv											2	2;2024-04-10;https://www.arxiv.org/abs/2404.05892v2| 1;2024-04-08;https://www.arxiv.org/abs/2404.05892v1	arXiv:2404.05892			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 10 2024	2024	We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer																																	2024-05-02	PPRN:88470201		
J	Allen-Zhu, Zeyuan; Li, Yuanzhi										Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws								Arxiv											1	1;2024-04-08;https://www.arxiv.org/abs/2404.05405v1	arXiv:2404.05405			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 08 2024	2024	Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation. More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include: * The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train. * Prepending training data with domain names (e.g., wikipedia.org) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.																																	2024-04-21	PPRN:88441594		
J	Slivkins, Aleksandrs										Introduction to Multi-Armed Bandits								Arxiv											2	2;2024-04-03;https://www.arxiv.org/abs/1904.07272v8| 1;2019-04-15;https://www.arxiv.org/abs/1904.07272v7	arXiv:1904.07272			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises. The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence. The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.																																	2024-04-19	PPRN:12029537		
J	Liang, Weixin; Zhang, Yaohui; Wu, Zhengxuan; Lepp, Haley; Ji, Wenlong; Zhao, Xuandong; Cao, Hancheng; Liu, Sheng; He, Siyu; Huang, Zhi; Yang, Diyi; Potts, Christopher; Manning, Christopher D; Zou, James Y.				Manning, Christopher/A-1358-2007; Lepp, Haley/MTB-9361-2025; Zhao, Xuandong/LIG-4204-2024						Mapping the Increasing Use of LLMs in Scientific Papers								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2404.01268v1	arXiv:2404.01268			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 01 2024	2024	Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.																																	2024-04-17	PPRN:88365473		
J	Quek, Yihui; Franca, Daniel Stilck; Khatri, Sumeet; Meyer, Johannes Jakob; Eisert, Jens				Khatri, Sumeet/AAE-8376-2020						Exponentially tighter bounds on limitations of quantum error mitigation								Arxiv											2	2;2024-03-29;https://www.arxiv.org/abs/2210.11505v3| 1;2022-10-20;https://www.arxiv.org/abs/2210.11505v1	arXiv:2210.11505			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	Quantum error mitigation has been proposed as a means to combat unwanted and unavoidable errors in near-term quantum computing without the heavy resource overheads required by fault-tolerant schemes. Recently, error mitigation has been successfully applied to reduce noise in near-term applications. In this work, however, we identify strong limitations to the degree to which quantum noise can be effectively ‘undone’ for larger system sizes. Our framework rigorously captures large classes of error mitigation schemes in use today. By relating error mitigation to a statistical inference problem, we show that even at shallow circuit depths comparable to the current experiments, a superpolynomial number of samples is needed in the worst case to estimate the expectation values of noiseless observables, the principal task of error mitigation. Notably, our construction implies that scrambling due to noise can kick in at exponentially smaller depths than previously thought. They also impact other near-term applications, constraining kernel estimation in quantum machine learning, causing an earlier emergence of noise-induced barren plateaus in variational quantum algorithms and ruling out exponential quantum speed -ups in estimating expectation values in the presence of noise or preparing the ground state of a Hamiltonian.																																	2024-04-17	PPRN:22137791		
J	Ye, Jiasheng; Liu, Peiju; Sun, Tianxiang; Zhou, Yunhua; Zhan, Jun; Qiu, Xipeng				ZHOU, YUN/ISA-9160-2023; Sun, Tianxiang/AAA-7123-2022						Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance								Arxiv											2	2;2025-03-20;https://www.arxiv.org/abs/2403.16952v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16952v1	arXiv:2403.16952			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 25 2024	2024	Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 1B model trained for 100B tokens in RedPajama, reaching a performance comparable to the one trained for 48% more steps on the default mixture. Extending the application of data mixing laws to continual training accurately predicts the critical mixture proportion that avoids catastrophic forgetting and outlooks the potential for dynamic data schedules																																	2025-08-07	PPRN:88282476		
J	Agrawal, Ayush; Suzgun, Mirac; Mackey, Lester; Kalai, Adam Tauman										Do Language Models Know When They're Hallucinating References?								Arxiv											3	3;2024-03-20;https://www.arxiv.org/abs/2305.18248v3| 2;2023-09-13;https://www.arxiv.org/abs/2305.18248v2| 1;2023-05-29;https://www.arxiv.org/abs/2305.18248v1	arXiv:2305.18248			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 20 2024	2024	State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda. In this work, we focus on hallucinated book and article references and present them as the "model organism" of language model hallucination research, due to their frequent and easy-to-discern nature. We posit that if a language model cites a particular reference in its output, then it should ideally possess sufficient information about its authors and content, among other relevant details. Using this basic insight, we illustrate that one can identify hallucinated references without ever consulting any external resources, by asking a set of direct or indirect queries to the language model about the references. These queries can be considered as "consistency checks." Our findings highlight that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references. In this sense, the LM can be said to "know" when it is hallucinating references. Furthermore, these findings show how hallucinated references can be dissected to shed light on their nature. 																																	2024-04-12	PPRN:72753830		
J	Qiao, Yanyuan; Yu, Zheng; Guo, Longteng; Chen, Sihan; Zhao, Zijia; Sun, Mingzhen; Wu, Qi; Liu, Jing				Yanyuan, Qiao/KBY-0525-2024; Wu, Qi/ABD-6304-2021						VL-Mamba: Exploring State Space Models for Multimodal Learning								Arxiv											1	1;2024-03-20;https://www.arxiv.org/abs/2403.13600v1	arXiv:2403.13600			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 20 2024	2024	Multimodal large language models (MLLMs) have attracted widespread interest and have rich applications. However, the inherent attention mechanism in its Transformer structure requires quadratic complexity and results in expensive computational overhead. Therefore, in this work, we propose VL-Mamba, a multimodal large language model based on state space models, which have been shown to have great potential for long-sequence modeling with fast inference and linear scaling in sequence length. Specifically, we first replace the transformer-based backbone language model such as LLama or Vicuna with the pre-trained Mamba language model. Then, we empirically explore how to effectively apply the 2D vision selective scan mechanism for multimodal learning and the combinations of different vision encoders and variants of pretrained Mamba language models. The extensive experiments on diverse multimodal benchmarks with competitive performance show the effectiveness of our proposed VL-Mamba and demonstrate the great potential of applying state space models for multimodal learning tasks.																																	2025-08-07	PPRN:123160734		
J	Hamamci, Ibrahim Ethem; Er, Sezgin; Almas, Furkan; Simsek, Ayse Gulnihan; Esirgun, Sevval Nil; Dogan, Irem; Dasdelen, Muhammed Furkan; Wittmann, Bastian; Simsar, Enis; Simsar, Mehmet; Erdemir, Emine Bensu; Alanbay, Abdullah; Sekuboyina, Anjany; Lafci, Berkan; Ozdemir, Mehmet K.; Menze, Bjoern				er, sezgin/HKN-9436-2023; Menze, Bjoern/JOK-2720-2023; Almas, Furkan/LFT-9855-2024; Hatt, Mathieu/M-8917-2017; TEZCAN, Alperen/GWV-5068-2022; Ozdemir, Kemal/ISA-7737-2023; Bluethgen, Christian/LFJ-7963-2024						A foundation model utilizing chest CT volumes and radiology reports for supervised-level zero-shot detection of abnormalities								Arxiv											2	2;2024-10-16;https://www.arxiv.org/abs/2403.17834v2| 1;2024-03-01;	arXiv:2403.17834			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 01 2024	2024	A major challenge in computational research in 3D medical imaging is the lack of comprehensive datasets. Addressing this issue, our study introduces CT-RATE, the first 3D medical imaging dataset that pairs images with textual reports. CT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188 through various reconstructions, from 21,304 unique patients, along with corresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP, a CT-focused contrastive language-image pre-training framework. As a versatile, self-supervised model, CT-CLIP is designed for broad application and does not require task-specific training. Remarkably, CT-CLIP outperforms state-of-the-art, fully supervised methods in multi-abnormality detection across all key metrics, thus eliminating the need for manual annotation. We also demonstrate its utility in case retrieval, whether using imagery or textual queries, thereby advancing knowledge dissemination. The open-source release of CT-RATE and CT-CLIP marks a significant advancement in medical AI, enhancing 3D imaging analysis and fostering innovation in healthcare.																																	2025-11-07	PPRN:88295117		
J	Zhao, Yuyang; Yan, Zhiwen; Xie, Enze; Hong, Lanqing; Li, Zhenguo; Lee, Gim Hee				Lee, Gim/K-5241-2015; Lv, Zhengtong/AAW-9611-2020						Animate124: Animating One Image to 4D Dynamic Scene								Arxiv											2	2;2024-02-19;https://www.arxiv.org/abs/2311.14603v2| 1;2023-11-24;https://www.arxiv.org/abs/2311.14603v1	arXiv:2311.14603			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 19 2024	2024	We introduce Animate124 (Animate-one-image-to-4D), the first work to animate a single in-the-wild image into 3D video through textual motion descriptions, an underexplored problem with significant applications. Our 4D generation leverages an advanced 4D grid dynamic Neural Radiance Field (NeRF) model, optimized in three distinct stages using multiple diffusion priors. Initially, a static model is optimized using the reference image, guided by 2D and 3D diffusion priors, which serves as the initialization for the dynamic NeRF. Subsequently, a video diffusion model is employed to learn the motion specific to the subject. However, the object in the 3D videos tends to drift away from the reference image over time. This drift is mainly due to the misalignment between the text prompt and the reference image in the video diffusion model. In the final stage, a personalized diffusion prior is therefore utilized to address the semantic drift. As the pioneering image-text-to-4D generation framework, our method demonstrates significant advancements over existing baselines, evidenced by comprehensive quantitative and qualitative assessments.																																	2024-11-09	PPRN:86277691		
J	Mudgal, Sidharth; Lee, Jong; Ganapathy, Harish; Li, YaGuang; Wang, Tao; Huang, Yanping; Chen, Zhifeng; Cheng, Heng-Tze; Collins, Michael; Strohman, Trevor; Chen, Jilin; Beutel, Alex; Beirami, Ahmad				Teng, Chieh-Lin/AAC-7441-2020; Chen, Zhifeng/ABG-9762-2021; HUANG, Yanping/A-2726-2010						CONTROLLED DECODING FROM LANGUAGE MODELS								Arxiv											2	2;2024-02-13;https://www.arxiv.org/abs/2310.17022v2| 1;2023-10-25;https://www.arxiv.org/abs/2310.17022v1	arXiv:2310.17022			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 13 2024	2024	KL-regularized reinforcement learning (RL) is a popular alignment framework to control the language model responses towards high reward outcomes. We propose a modular solver for this RL objective, called controlled decoding (CD), which exerts control through a separate prefix scorer module. At training time, the prefix scorer learns a value function for the reward, and it is used at inference time to control the generation from a frozen base model, provably sampling from a solution to the RL objective. We empirically demonstrate that CD is effective as a control mechanism on popular benchmarks. We also show that a single prefix scorer can learn multiple rewards and different reward combinations can be configurable at inference time, effectively solving a multi-objective RL problem with no additional training. We show that the benefits of applying CD transfer to an unseen base model with no further tuning. Finally, we show that CD can be applied in a blockwise decoding fashion at inference-time, essentially bridging the gap between the popular best-of-n strategy and token-level control through reinforcement learning. This makes CD a promising approach for alignment of language models.																																	2024-05-25	PPRN:85822145		
J	Mu, Jesse; Li, Xiang Lisa; Goodman, Noah										Learning to Compress Prompts with Gist Tokens								Arxiv											2	2;2024-02-12;https://www.arxiv.org/abs/2304.08467v3| 1;2023-04-17;https://www.arxiv.org/abs/2304.08467v1	arXiv:2304.08467			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 12 2024	2024	Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of "gist" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.																																	2024-05-25	PPRN:63431629		
J	Lin, Zihan; Chen, Dong; Lu, Wenlong; Liang, Xin; Feng, Shiyu; Yamagami, Kohei; Osiecki, Jacek; Leandersson, Mats; Thiagarajan, Balasubramanian; Liu, Junwei; Felser, Claudia; Ma, Junzhang				Feng, Shi-yu/JSK-8076-2023; Felser, Claudia/A-5779-2009; Liu, Junwei/B-1468-2010						Observation of Giant Spin Splitting and d-wave Spin Texture in Room Temperature Altermagnet RuO2								Arxiv											1	1;2024-02-07;https://www.arxiv.org/abs/2402.04995v1	arXiv:2402.04995			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 07 2024	2024	Recently, a new magnetic phase called altermagnetism has been proposed, ushering in a third distinct magnetic phase beyond ferromagnetism and antiferromagnetism. It is expected that this groundbreaking phase exhibits unique physical properties such as C-paired spin-valley locking, anomalous Hall effect, nontrivial Berry phase, and giant magnetoresistance. Among all the predicted candidates, several room temperature altermagnets are suggested to host significant potential applications. Nevertheless, direct evidence about the spin pattern of the room temperature altermagnet is still unrevealed. RuO2 is identified as the most promising candidate for room temperature d-wave altermagnetism exhibiting a substantial spin splitting of up to 1.4 eV in previous research. In this study, utilizing angle-resolved photoemission spectroscopy (ARPES), we report experimental observation of the giant spin splitting in RuO2. Furthermore, employing spin-ARPES, we directly observed the d-wave spin pattern. Our results unequivocally show that RuO2 is a perfect d-wave altermagnet with great potential for upcoming spintronic applications.																																	2024-02-23	PPRN:87561815		
J	Chen, Shaoxiang; Jie, Zequn; Ma, Lin										LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2401.16160v2	arXiv:2401.16160			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 30 2024	2024	Instruction finetuning on a variety of image-text instruction data is the key to obtaining a versatile Multimodal Large Language Model (MLLM), and different configurations of the instruction data can lead to finetuned models with different capabilities. However, we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain. To address this issue, we propose to apply an efficient Mixture of Experts (MoE) design, which is a sparse Mixture of LoRA Experts (MoLE) for instruction finetuning MLLMs. Within the Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method by creating a set of LoRA experts specifically for the MLP layer, and route each token to the top-1 expert based on a routing function, allowing adaptive choices for tokens from different domains. Since the LoRA experts are sparsely activated, the training and inference cost are kept roughly constant compared to the original LoRA method. By replacing the plain-LoRA of LLaVA-1.5 with our MoE design, our final model is named LLaVA-MoLE. Extensive experiments proved that LLaVA-MoLE effectively mitigates the data conflict issue when mixing multiple distinct instruction datasets with various configurations, and achieves consistent performance gains over the strong plain-LoRA baselines. Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform the plain-LoRA baseline trained with twice the samples.																																	2024-05-25	PPRN:87417751		
J	Qiu, Haonan; Xia, Menghan; Zhang, Yong; He, Yingqing; Wang, Xintao; Shan, Ying; Liu, Ziwei				Liu, Ziwei/AAG-6939-2021; Qiu, Haonan/KIC-7530-2024						FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling								Arxiv											3	3;2024-01-30;https://www.arxiv.org/abs/2310.15169v3| 2;2023-10-27;https://www.arxiv.org/abs/2310.15169v2| 1;2023-10-23;https://www.arxiv.org/abs/2310.15169v1	arXiv:2310.15169			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 30 2024	2024	With the availability of large-scale video datasets and the advances of diffusion models, text-driven video generation has achieved substantial progress. However, existing video generation models are typically trained on a limited number of frames, resulting in the inability to generate high-fidelity long videos during inference. Furthermore, these models only support single-text conditions, whereas real-life scenarios often require multi-text conditions as the video content changes over time. To tackle these challenges, this study explores the potential of extending the text-driven capability to generate longer videos conditioned on multiple texts. 1) We first analyze the impact of initial noise in video diffusion models. Then building upon the observation of noise, we propose FreeNoise, a tuning-free and time-efficient paradigm to enhance the generative capabilities of pretrained video diffusion models while preserving content consistency. Specifically, instead of initializing noises for all frames, we reschedule a sequence of noises for long-range correlation and perform temporal attention over them by window-based function. 2) Additionally, we design a novel motion injection method to support the generation of videos conditioned on multiple text prompts. Extensive experiments validate the superiority of our paradigm in extending the generative capabilities of video diffusion models. It is noteworthy that compared with the previous best-performing method which brought about 255% extra time cost, our method incurs only negligible time cost of approximately 17%. 																																	2024-05-25	PPRN:85758384		
J	Chen, Boyuan; Xu, Zhuo; Kirmani, Sean; Driess, Danny; Florence, Pete; Ichter, Brian; Sadigh, Dorsa; Guibas, Leonidas; Xia, Fei				Xia, Fei/AAW-8782-2021						SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities								Arxiv											1	1;2024-01-22;https://www.arxiv.org/abs/2401.12168v1	arXiv:2401.12168			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 22 2024	2024	Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/																																	2024-05-25	PPRN:87278671		
J	Rame, Alexandre; Vieillard, Nino; Hussenot, Leonard; Dadashi, Robert; Cideron, Geoffrey; Bachem, Olivier; Ferret, Johan										WARM: On the Benefits of Weight Averaged Reward Models								Arxiv											1	1;2024-01-22;https://www.arxiv.org/abs/2401.12187v1	arXiv:2401.12187			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 22 2024	2024	Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.																																	2024-02-06	PPRN:87277542		
J	Behrouz, Ali; Zhong, Peilin; Mirrokni, Vahab										Titans: Learning to Memorize at Test Time								Arxiv											1	1;2024-12-31;https://www.arxiv.org/abs/2501.00663v1	arXiv:2501.00663			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 31 2024	2024	Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps an attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of a fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.																																	2025-03-06	PPRN:120486672		
J	Lee, Jong Yeon; You, Yi-Zhuang; Xu, Cenke				Lee, jongyeon/HJP-3141-2023; You, Yi-Zhuang/L-1850-2019						Symmetry protected topological phases under decoherence								Arxiv											4	4;2024-12-29;https://www.arxiv.org/abs/2210.16323v6| 3;2024-12-04;https://www.arxiv.org/abs/2210.16323v5| 2;2024-02-22;https://www.arxiv.org/abs/2210.16323v4| 1;2022-10-28;https://www.arxiv.org/abs/2210.16323v2	arXiv:2210.16323			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 29 2024	2024	We study ensembles described by density matrices with potentially nontrivial topological features. In particular, we study a class of symmetry protected topological (SPT) phases under various types of decoherence, which can drive a pure SPT state into a mixed state. We demonstrate that the system can still retain the nontrivial topological information from the SPT ground state even under decoherence. In the "doubled Hilbert space", we provide a general definition for symmetry protected topological ensemble (SPT ensemble), and the main quantity that we investigate is various types of (boundary) anomalies in the doubled Hilbert space. We show that the notion of the strange correlator, previously proposed to as a diagnosis for the SPT ground states, can be generalized to capture these anomalies in mixed-state density matrices. Using both exact calculations of the stabilizer Hamiltonians and field theory evaluations, we demonstrate that under decoherence the nontrivial features of the SPT state can persist in the two types of strange correlators: type-I and type-II. We show that the nontrivial type-I strange correlator corresponds to the presence of the SPT information that can be efficiently identified and utilized from experiments, such as for the purpose of preparing for long-range entangled states. The nontrivial type-II strange correlator encodes the full topological response of the decohered mixed state density matrix, i.e., the information about the presence of the SPT state before decoherence. Therefore, our work provides a unified framework to understand decohered SPT phases from the information-theoretic viewpoint.																																	2025-03-22	PPRN:22930586		
J	Bao, Guangsheng; Zhao, Yanbin; Teng, Zhiyang; Yang, Linyi; Zhang, Yue										Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature								Arxiv											3	3;2024-12-16;https://www.arxiv.org/abs/2310.05130v3| 2;2024-02-22;https://www.arxiv.org/abs/2310.05130v2| 1;2023-10-08;https://www.arxiv.org/abs/2310.05130v1	arXiv:2310.05130			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 16 2024	2024	Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine- generated and human-authored content. The leading zero-shot detector, DetectGPT (Mitchell et al., 2023), showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present Fast-DetectGPT 1 , an optimized zero-shot detector, which substitutes DetectGPT’s perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1.																																	2025-01-24	PPRN:85573475		
J	Chen, Yiming; Yue, Xianghu; Zhang, Chen; Gao, Xiaoxue; Tan, Robby T.; Li, Haizhou				Yue, Xianghu/NXX-7562-2025; Haizhou, LI/ITT-8410-2023						VoiceBench: Benchmarking LLM-Based Voice Assistants								Arxiv											3	3;2024-12-11;https://www.arxiv.org/abs/2410.17196v3| 2;2024-11-24;https://www.arxiv.org/abs/2410.17196v2| 1;2024-10-22;https://www.arxiv.org/abs/2410.17196v1	arXiv:2410.17196			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 11 2024	2024	Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.1																																	2025-01-19	PPRN:118771193		
J	Xu, Sicheng; Chen, Guojun; Guo, Yu-Xiao; Yang, Jiaolong; Li, Chong; Zang, Zhenyu; Zhang, Yizhong; Tong, Xin; Guo, Baining				Chen, Guojun/HRA-4222-2023						VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time								Arxiv											2	2;2024-10-31;https://www.arxiv.org/abs/2404.10667v2| 1;2024-04-16;https://www.arxiv.org/abs/2404.10667v1	arXiv:2404.10667			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 31 2024	2024	We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only generating lip movements that are exquisitely synchronized with the audio, but also producing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a diffusion-based holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method delivers high video quality with realistic facial and head dynamics and also supports the online generation of 512×512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors. Project webpage: https://www.microsoft.com/en-us/research/project/vasa-1/																																	2024-12-06	PPRN:88544506		
J	Toshniwal, Shubham; Du, Wei; Moshkov, Ivan; Kisacanin, Branislav; Ayrapetyan, Alexan; Gitman, Igor										OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data								Arxiv											1	1;2024-10-05;https://www.arxiv.org/abs/2410.01560v2	arXiv:2410.01560			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 05 2024	2024	Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become closed-source due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released Llama3.1 family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms equally-sized data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (≈ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base using OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by an absolute 15.9% (51.9% → 67.8%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license.																																	2024-10-27	PPRN:104403259		
J	Wang, Haoyu; Guo, Sizheng; Ye, Jin; Deng, Zhongying; Cheng, Junlong; Li, Tianbin; Chen, Jianpin; Su, Yanzhou; Huang, Ziyan; Shen, Yiqing; Fu, Bin; Zhang, Shaoting; He, Junjun; Qiao, Yu				Wang, Haoyu/AAI-7412-2021; Deng, Zhongying/JDW-1393-2023; huang, ziyan/KJM-6164-2024; Cheng, Junlong/ABP-8353-2022						SAM-Med3D: Towards General-purpose Segmentation Models for Volumetric Medical Images								Arxiv											3	3;2024-09-14;https://www.arxiv.org/abs/2310.15161v3| 2;2023-10-29;https://www.arxiv.org/abs/2310.15161v2| 1;2023-10-23;https://www.arxiv.org/abs/2310.15161v1	arXiv:2310.15161			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Sep 14 2024	2024	Existing volumetric medical image segmentation models are typically task-specific, excelling at specific target but struggling to generalize across anatomical structures or modalities. This limitation restricts their broader clinical use. In this paper, we introduce SAM-Med3D for general-purpose segmentation on volumetric medical images. Given only a few 3D prompt points, SAM-Med3D can accurately segment diverse anatomical structures and lesions across various modalities. To achieve this, we gather and process a large-scale 3D medical image dataset, SA-Med3D-140K, from a blend of public sources and licensed private datasets. This dataset includes 22K 3D images and 143K corresponding 3D masks. Then SAM-Med3D, a promptable segmentation model characterized by the fully learnable 3D structure, is trained on this dataset using a two-stage procedure and exhibits impressive performance on both seen and unseen segmentation targets. We comprehensively evaluate SAM-Med3D on 16 datasets covering diverse medical scenarios, including different anatomical structures, modalities, targets, and zero-shot transferability to new/unseen tasks. The evaluation shows the efficiency and efficacy of SAM-Med3D, as well as its promising application to diverse downstream tasks as a pre-trained model. Our approach demonstrates that substantial medical resources can be utilized to develop a general-purpose medical AI for various potential applications. 																																	2024-12-23	PPRN:85757414		
J	Li, Jian; Lu, Weiheng; Fei, Hao; Luo, Meng; Dai, Ming; Xia, Min; Jin, Yizhang; Gan, Zhenye; Qi, Ding; Fu, Chaoyou; Tai, Ying; Yang, Wankou; Wang, Yabiao; Wang, Chengjie				Tai, Ying/AID-2000-2022; Wang, Yuanhao/KBA-1055-2024; Fei, Hao/IZD-5292-2023; Xing, Junliang/HGE-9630-2022						A Survey on Benchmarks of Multimodal Large Language Models								Arxiv											1	1;2024-09-06;https://www.arxiv.org/abs/2408.08632v2	arXiv:2408.08632			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 06 2024	2024	Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to support the development of MLLMs better. 																																	2024-09-19	PPRN:91786783		
J	Chen, Liang; Zhao, Haozhe; Liu, Tianyu; Bai, Shuang; Lin, Junyang; Zhou, Chang; Chang, Baobao				Liu, Tianyu/HJZ-0979-2023						An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models								Arxiv											2	2;2024-09-02;https://www.arxiv.org/abs/2403.06764v3| 1;2024-03-11;https://www.arxiv.org/abs/2403.06764v1	arXiv:2403.06764			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 02 2024	2024	In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models. 																																	2024-09-11	PPRN:88107009		
J	Zhang, Bowen; Ding, Daijun; Jing, Liwen; Dai, Genan; Yin, Nan				Zhang, Bowen/HHS-0516-2022; jing, liwen/HGB-5785-2022						How would Stance Detection Techniques Evolve after the Launch of ChatGPT?								Arxiv											2	2;2024-08-12;https://www.arxiv.org/abs/2212.14548v4| 1;2022-12-30;https://www.arxiv.org/abs/2212.14548v2	arXiv:2212.14548			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 12 2024	2024	Stance detection refers to the task of extracting the standpoint (Favor, Against or Neither) towards a target in given texts. Such research gains increasing attention with the proliferation of social media contents. The conventional framework of handling stance detection is converting it into text classification tasks. Deep learning models have already replaced rule-based models and traditional machine learning models in solving such problems. Current deep neural networks are facing two main challenges which are insufficient labeled data and information in social media posts and the unexplainable nature of deep learning models. A new pre-trained language model chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance. At the same time, ChatGPT can provide explanation for its own prediction, which is beyond the capability of any existing model. The explanations for the cases it cannot provide classification results are especially useful. ChatGPT has the potential to be the best AI model for stance detection tasks in NLP, or at least change the research paradigm of this field. ChatGPT also opens up the possibility of building explanatory AI for stance detection.																																	2024-08-21	PPRN:43712304		
J	Ma, Yifeng; Zhang, Shiwei; Wang, Jiayu; Wang, Xiang; Zhang, Yingya; Deng, Zhidong				Ma, Yifeng/ITU-4239-2023; Wang, jiayu/ABB-6772-2020						DreamTalk: When Emotional Talking Head Generation Meets Diffusion Probabilistic Models								Arxiv											3	3;2024-08-10;https://www.arxiv.org/abs/2312.09767v3| 2;1800-01-01;https://www.arxiv.org/abs/2312.09767v2| 1;2023-12-15;https://www.arxiv.org/abs/2312.09767v1	arXiv:2312.09767			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 10 2024	2024	Emotional talking head generation has attracted growing attention. Previous methods, which are mainly GAN-based, still struggle to consistently produce satisfactory results across diverse emotions and cannot conveniently specify personalized emotions. In this work, we leverage powerful diffusion models to address the issue and propose DreamTalk, a framework that employs meticulous design to unlock the potential of diffusion models in generating emotional talking heads. Specifically, DreamTalk consists of three crucial components: a denoising network, a style-aware lip expert, and a style predictor. The diffusion-based denoising network can consistently synthesize high-quality audio-driven face motions across diverse emotions. To enhance lip-motion accuracy and emotional fullness, we introduce a style-aware lip expert that can guide lip-sync while preserving emotion intensity. To more conveniently specify personalized emotions, a diffusion-based style predictor is utilized to predict the personalized emotion directly from the audio, eliminating the need for extra emotion reference. By this means, DreamTalk can consistently generate vivid talking faces across diverse emotions and conveniently specify personalized emotions. Extensive experiments validate DreamTalk's effectiveness and superiority.																																	2024-08-21	PPRN:86646860		
J	Geiger, Atticus; Ibeling, Duligur; Zur, Amir; Chaudhary, Maheep; Chauhan, Sonakshi; Huang, Jing; Arora, Aryaman; Wu, Zhengxuan; Goodman, Noah; Potts, Christopher; Icard, Thomas										Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability								Arxiv											2	2;2024-08-07;https://www.arxiv.org/abs/2301.04709v3| 1;2024-07-07;https://www.arxiv.org/abs/2301.04709v2	arXiv:2301.04709			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 07 2024	2024	Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of modular features, polysemantic neurons, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methodologies in the common language of causal abstraction, namely activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and activation steering.																																	2024-08-21	PPRN:90741176		
J	Evans, Zach; Parker, Julian D.; Carr, C J; Zukowski, Zack; Taylor, Josiah; Pons, Jordi										Stable Audio Open								Arxiv											2	2;2024-07-31;https://www.arxiv.org/abs/2407.14358v2| 1;2024-07-19;https://www.arxiv.org/abs/2407.14358v1	arXiv:2407.14358			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 31 2024	2024	Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.																																	2024-08-08	PPRN:91010445		
J	Li, Dengchun; Ma, Yingzi; Wang, Naizheng; Ye, Zhengmao; Cheng, Zhiyuan; Tang, Yinghao; Zhang, Yan; Duan, Lei; Zuo, Jie; Yang, Cal; Tang, Mingjie				Tang, Mingjie/NTQ-4211-2025; Ye, Zhengmao/AAT-9774-2020						MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts								Arxiv											3	3;2024-07-20;https://www.arxiv.org/abs/2404.15159v3| 2;2024-05-23;https://www.arxiv.org/abs/2404.15159v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.15159v1	arXiv:2404.15159			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 20 2024	2024	Fine-tuning Large Language Models (LLMs) is a common practice to adapt pre-trained models for specific applications. While methods like LoRA have effectively addressed GPU memory constraints during fine-tuning, their performance often falls short, especially in multi-task scenarios. In contrast, Mixture-of-Expert (MoE) models, such as Mixtral 8x7B, demonstrate remarkable performance in multi-task learning scenarios while maintaining a reduced parameter count. However, the resource requirements of these MoEs remain challenging, particularly for consumer-grade GPUs with less than 24GB memory. To tackle these challenges, we propose MixLoRA, an approach to construct a resource-efficient sparse MoE model based on LoRA. MixLoRA inserts multiple LoRA-based experts within the feed-forward network block of a frozen pre-trained dense model and employs a commonly used top-k router. Unlike other LoRA-based MoE methods, MixLoRA enhances model performance by utilizing independent attention-layer LoRA adapters. Additionally, an auxiliary load balance loss is employed to address the imbalance problem of the router. Our evaluations show that MixLoRA improves about 9% accuracy compared to state-of-the-art PEFT methods in multi-task learning scenarios. We also propose a new high-throughput framework to alleviate the computation and memory bottlenecks during the training and inference of MOE models. This framework reduces GPU memory consumption by 40% and token computation latency by 30% during both training and inference.																																	2024-07-27	PPRN:88622386		
