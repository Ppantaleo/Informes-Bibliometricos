PT	AU	BA	CA	GP	RI	OI	BE	Z2	AU	AA	TI	X1	Y1	Z1	FT	PN	AE	Z3	SO	S1	SE	BS	VL	IS	SI	MA	BP	EP	AR	VN	VH	DI	D2	L1	L2	L3	EA	SU	DT	PD	PY	AB	X4	Y4	Z4	AK	CT	CY	SP	CL	TC	Z8	ZR	ZA	ZB	ZS	Z9	U1	U2	SN	EI	BN	G1	NR	CR	LA	AS	AC	CG	DG	C1	C3	EC	DE	DA	UT	PM	
J	Xu, Cheng; Guan, Shuhao; Greene, Derek; Kechadi, M-Tahar				Kechadi, M-Tahar/AAH-3906-2020; Guan, Shuhao/OPM-8309-2025						Benchmark Data Contamination of Large Language Models: A Survey								Arxiv											1	1;2024-06-06;https://www.arxiv.org/abs/2406.04244v1	arXiv:2406.04244			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.																																	2024-06-22	PPRN:89175792		
J	Milionis, Jason; Moallemi, Ciamac C.; Roughgarden, Tim; Zhang, Anthony Lee				Zhang, Anthony Lee/KHE-4031-2024; Milionis, Jason/JJF-2844-2023						Automated Market Making and Loss-Versus-Rebalancing								Arxiv											3	3;2024-05-27;https://www.arxiv.org/abs/2208.06046v5| 2;2023-11-27;https://www.arxiv.org/abs/2208.06046v4| 1;2022-08-11;https://www.arxiv.org/abs/2208.06046v1	arXiv:2208.06046			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 27 2024	2024	We consider the market microstructure of automated market makers (AMMs) from the perspective of liquidity providers (LPs). Our central contribution is a “Black-Scholes formula for AMMs”. We identify the main adverse selection cost incurred by LPs, which we call “loss-versusrebalancing” (LVR, pronounced “lever”). LVR captures costs incurred by AMM LPs due to stale prices that are picked off by better informed arbitrageurs. We derive closed-form expressions for LVR applicable to all automated market makers. Our model is quantitatively realistic, matching actual LP returns empirically, and shows how CFMM protocols can be redesigned to reduce or eliminate LVR.																																	2024-06-13	PPRN:12127760		
J	Tang, Yunhao; Guo, Daniel Zhaohan; Zheng, Zeyu; Calandriello, Daniele; Cao, Yuan; Tarassov, Eugene; Munos, Remi; Pires, Bernardo avila; Valko, Michal; Cheng, Yong; Dabney, Will				zheng, zeyu/A-3867-2013						Understanding the performance gap between online and offline alignment algorithms								Arxiv											1	1;2024-05-14;https://www.arxiv.org/abs/2405.08448v1	arXiv:2405.08448			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 14 2024	2024	Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.																																	2024-06-08	PPRN:89046772		
J	Shypula, Alexander; Madaan, Aman; Zeng, Yimeng; Alon, Uri; Gardner, Jacob; Hashemi, Milad; Neubig, Graham; Ranganathan, Parthasarathy; Bastani, Osbert; Yazdanbakhsh, Amir				Yazdanbakhsh, Amir/AFK-5828-2022						Learning Performance-Improving Code Edits								Arxiv											3	3;2024-04-26;https://www.arxiv.org/abs/2302.07867v5| 2;2023-11-08;https://www.arxiv.org/abs/2302.07867v4| 1;2023-02-15;https://www.arxiv.org/abs/2302.07867v3	arXiv:2302.07867			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	With the decline of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77,000 competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious "improvements." To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves a mean speedup of 6.86x with eight generations, higher than average optimizations from individual programmers (3.66x). Using our model's fastest generations, we set a new upper limit on the fastest speedup possible for our dataset at 9.64 compared to using the fastest human submissions available (9.56x).																																	2024-05-05	PPRN:38489992		
J	Li, Yuhan; Li, Zhixun; Wang, Peisong; Li, Jia; Sun, Xiangguo; Cheng, Hong; Yu, Jeffrey Xu				Sun, Xiangguo/JSD-9441-2023; Li, YuHan/GZM-2095-2022; Yu, Jeffrey/F-6005-2011						A Survey of Graph Meets Large Language Model: Progress and Future Directions								Arxiv											4	4;2024-04-24;https://www.arxiv.org/abs/2311.12399v4| 3;2024-01-19;https://www.arxiv.org/abs/2311.12399v3| 2;2023-11-28;https://www.arxiv.org/abs/2311.12399v2| 1;2023-11-21;https://www.arxiv.org/abs/2311.12399v1	arXiv:2311.12399			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 24 2024	2024	Graph plays a significant role in representing and analyzing complex relationships in real -world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph -related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph -related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated 																																	2024-05-03	PPRN:86308821		
J	Qin, Libo; Chen, Qiguang; Zhou, Yuhang; Chen, Zhi; Li, Yinghui; Liao, Lizi; Li, Min; Che, Wanxiang; Yu, Philip S.				Li, Yinghui/AAY-1776-2020; Qin, Libo/IUQ-5109-2023; Zhou, Yuhang/HHC-4107-2022; Chen, Qiguang/IVH-6127-2023; Yu, Philip/A-2815-2012; XIN, WANG/KGK-5385-2024						Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers								Arxiv											1	1;2024-04-07;https://www.arxiv.org/abs/2404.04925v1	arXiv:2404.04925			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 07 2024	2024	Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs.																																	2024-04-21	PPRN:88443895		
J	He, Zhenyu; Zhong, Zexuan; Cai, Tianle; Lee, Jason D.; He, Di										REST: Retrieval-Based Speculative Decoding								Arxiv											1	1;2024-04-04;https://www.arxiv.org/abs/2311.08252v2	arXiv:2311.08252			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation. The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns. Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens. This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context. Its plug-and-play nature allows for seamless integration and acceleration of any language models, all without necessitating additional training. When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of 1.62× to 2.36× on code or text generation.																																	2024-04-19	PPRN:86742544		
J	Chen, Guo; Huang, Yifei; Xu, Jilan; Pei, Baoqi; Chen, Zhe; Li, Zhiqi; Wang, Jiahao; Li, Kunchang; Lu, Tong; Wang, Limin				jiahao, wang/ADP-2362-2022; Wang, Limin/AAE-3419-2019; Li, Kunchang/KFA-4043-2024; Huang, Yifei/ABE-4692-2020; Li, Zhiqi/JOK-4033-2023						Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding								Arxiv											1	1;2024-03-14;https://www.arxiv.org/abs/2403.09626v1	arXiv:2403.09626			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Understanding videos is one of the fundamental directions in computer vision research, with extensive efforts dedicated to exploring various architectures such as RNN, 3D CNN, and Transformers. The newly proposed architecture of state space model, e.g., Mamba, shows promising traits to extend its success in long sequence modeling to video modeling. To assess whether Mamba can be a viable alternative to Transformers in the video understanding domain, in this work, we conduct a comprehensive set of studies, probing different roles Mamba can play in modeling videos, while investigating diverse tasks where Mamba could exhibit superiority. We categorize Mamba into four roles for modeling videos, deriving a Video Mamba Suite composed of 14 models/modules, and evaluating them on 12 video understanding tasks. Our extensive experiments reveal the strong potential of Mamba on both video-only and video-language tasks while showing promising efficiency-performance trade-offs. We hope this work could provide valuable data points and insights for future research on video understanding.																																	2024-04-11	PPRN:88144922		
J	Stoica, George; Bolya, Daniel; Bjorner, Jakob; Ramesh, Pratik; Hearn, Taylor; Hoffman, Judy				Stoica, George/IAM-6477-2023						ZIPIT! MERGING MODELS FROM DIFFERENT TASKS <italic>without Training</italic>								Arxiv											3	3;2024-03-13;https://www.arxiv.org/abs/2305.03053v3| 2;2024-01-22;https://www.arxiv.org/abs/2305.03053v2| 1;2023-05-04;https://www.arxiv.org/abs/2305.03053v1	arXiv:2305.03053			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 13 2024	2024	Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then averages them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for 20-60% improvement over prior work, making it more feasible to merge models trained on disjoint tasks without retraining.																																	2024-04-08	PPRN:67342774		
J	Shum, Kashun; Diao, Shizhe; Zhang, Tong				Zhang, tong/IAP-2587-2023						Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/2302.12822v3| 1;2023-02-24;https://www.arxiv.org/abs/2302.12822v1	arXiv:2302.12822			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%).																																	2024-03-28	PPRN:44143160		
J	Sachdeva, Noveen; Coleman, Benjamin; Kang, Wang-Cheng; Ni, Jianmo; Hong, Lichan; Chi, Ed H.; Caverlee, James; Mcauley, Julian; Cheng, Derek Zhiyuan				Sachdeva, Noveen/CAH-0103-2022; Ni, Jianmo/AAV-3555-2020						How to Train Data-Efficient LLMs								Arxiv											1	1;2024-02-15;https://www.arxiv.org/abs/2402.09668v1	arXiv:2402.09668			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 15 2024	2024	The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster.																																	2024-03-10	PPRN:87702868		
J	Bulatov, Aydar; Kuratov, Yuri; Kapushev, Yermek; Burtsev, Mikhail S.				Burtsev, Mikhail/G-6293-2010; Kuratov, Yuri/LTY-5852-2024						Scaling Transformer to 1M tokens and beyond with RMT								Arxiv											2	2;2024-02-06;https://www.arxiv.org/abs/2304.11062v2| 1;2023-04-19;https://www.arxiv.org/abs/2304.11062v1	arXiv:2304.11062			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	A major limitation for the broader scope of problems solvable by transformers is the quadratic scaling of computational complexity with input size. In this study, we investigate the recurrent memory augmentation of pre-trained transformer models to extend input context length while linearly scaling compute. Our approach demonstrates the capability to store information in memory for sequences of up to an unprecedented two million tokens while maintaining high retrieval accuracy. Experiments with language modeling tasks show perplexity improvement as the number of processed input segments increases. These results underscore the effectiveness of our method, which has significant potential to enhance long-term dependency handling in natural language understanding and generation tasks, as well as enable large-scale context processing for memory-intensive applications.																																	2024-02-21	PPRN:65038909		
J	Zhao, Jun; Zhang, Zhihao; Gao, Luhui; Zhang, Qi; Gui, Tao; Huang, Xuanjing				Gui, Tao/LWI-6783-2024; Gao, Luhui/OIS-5217-2025						LLaMA Beyond English: An Empirical Study on Language Capability Transfer								Arxiv											2	2;2024-01-12;https://www.arxiv.org/abs/2401.01055v2| 1;2024-01-02;https://www.arxiv.org/abs/2401.01055v1	arXiv:2401.01055			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 12 2024	2024	In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as accuracy, fluency, informativeness, logical coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting instruction tasks from 17 diverse categories. Our evaluation results demonstrate that comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data, both in terms of knowledge alignment and response quality. Furthermore, the experimental outcomes across the thirteen low-resource languages also exhibit similar trends. We anticipate that the conclusions revealed by the experiments will aid the community in developing non-English LLMs.																																	2024-05-25	PPRN:86914427		
J	Genet, Remi; Inzirillo, Hugo										TKAN: Temporal Kolmogorov-Arnold Networks								Arxiv											2	2;2024-12-17;https://www.arxiv.org/abs/2405.07344v3| 1;2024-05-12;https://www.arxiv.org/abs/2405.07344v1	arXiv:2405.07344			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 17 2024	2024	Recurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural networks architecture inspired by KAN and the LSTM, the Temporal Kolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both networks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers embedding memory management. This innovation enables us to perform multi-step time series forecasting with enhanced accuracy and efficiency. By addressing the limitations of traditional models in handling complex sequential patterns, the TKAN architecture offers significant potential for advancements in fields requiring more than one step ahead forecasting. [GRAPHICS]																																	2025-01-25	PPRN:89025027		
J	Ren, Allen Z.; Lidard, Justin; Ankile, Lars L.; Simeonov, Anthony; Agrawal, Pulkit; Majumdar, Anirudha; Burchfiel, Benjamin; Dai, Hongkai; Simchowitz, Max										Diffusion Policy Policy Optimization								Arxiv											3	3;2024-12-09;https://www.arxiv.org/abs/2409.00588v3| 2;2024-11-07;https://www.arxiv.org/abs/2409.00588v2| 1;2024-09-01;https://www.arxiv.org/abs/2409.00588v1	arXiv:2409.00588			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 09 2024	2024	We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task.																																	2025-01-18	PPRN:91713061		
J	Jiang, Xue; Dong, Yihong; Wang, Lecheng; Fang, Zheng; Shang, Qiwei; Li, Ge; Jin, Zhi; Jiao, Wenpin				Dong, Yihong/LCE-6194-2024; Jin, Zhi/AAB-2440-2022						Self-planning Code Generation with Large Language Models								Arxiv											3	3;2024-10-25;https://www.arxiv.org/abs/2303.06689v4| 2;2024-05-31;https://www.arxiv.org/abs/2303.06689v3| 1;2023-03-12;https://www.arxiv.org/abs/2303.06689v1	arXiv:2303.06689			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 25 2024	2024	Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4% in Pass@1 compared to direct code generation, and up to 11.9% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.																																	2024-11-30	PPRN:46476814		
J	Fan, Lijie; Li, Tianhong; Qin, Siyang; Li, Yuanzhen; Sun, Chen; Rubinstein, Michael; Sun, Deqing; He, Kaiming; Tian, Yonglong				Sun, Deqing/KLD-7402-2024; QIN, SIYANG/KNB-3229-2024						Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens								Arxiv											1	1;2024-10-17;https://www.arxiv.org/abs/2410.13863v1	arXiv:2410.13863			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens are generated in a random or fixed raster order using BERT- or GPT-like transformer architectures. Our empirical results show that, while all models scale effectively in terms of validation loss, their evaluation performance - measured by FID, GenEval score, and visual quality - follows different trends. Models based on continuous tokens achieve significantly better visual quality than those using discrete tokens. Furthermore, the generation order and attention mechanisms significantly affect the GenEval score: random-order models achieve notably better GenEval scores compared to raster-order models. Inspired by these findings, we train Fluid, a random-order autoregressive model on continuous tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16 on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our findings and results will encourage future efforts to further bridge the scaling gap between vision and language models.																																	2024-11-13	PPRN:115453832		
J	Wang, Jun; Fang, Meng; Wan, Ziyu; Wen, Muning; Zhu, Jiachen; Liu, Anjie; Gong, Ziqin; Song, Yan; Chen, Lei; Ni, Lionel M.; Yang, Linyi; Wen, Ying; Zhang, Weinan				Wen, Muning/OOK-5749-2025						OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models								Arxiv											1	1;2024-10-12;https://www.arxiv.org/abs/2410.09671v1	arXiv:2410.09671			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 12 2024	2024	In this technical report, we introduce OpenR, an open-source framework designed to integrate key components for enhancing the reasoning capabilities of large language models (LLMs). OpenR unifies data acquisition, reinforcement learning training (both online and offline), and non-autoregressive decoding into a cohesive software platform. Our goal is to establish an open-source platform and community to accelerate the development of LLM reasoning. Inspired by the success of OpenAI's o1 model, which demonstrated improved reasoning abilities through step-by-step reasoning and reinforcement learning, OpenR integrates test-time compute, reinforcement learning, and process supervision to improve reasoning in LLMs. Our work is the first to provide an open-source framework that explores the core techniques of OpenAI's o1 model with reinforcement learning, achieving advanced reasoning capabilities beyond traditional autoregressive methods. We demonstrate the efficacy of OpenR by evaluating it on the MATH dataset, utilising publicly available data and search methods. Our initial experiments confirm substantial gains, with relative improvements in reasoning and performance driven by test-time computation and reinforcement learning through process reward models. 																																	2024-11-05	PPRN:112577494		
J	Lin, Bill Yuchen; Deng, Yuntian; Chandu, Khyathi; Brahman, Faeze; Ravichander, Abhilasha; Pyatkin, Valentina; Dziri, Nouha; Le Bras, Ronan; Choi, Yejin				Deng, yuntian/KIB-9835-2024						WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild								Arxiv											2	2;2024-10-05;https://www.arxiv.org/abs/2406.04770v2| 1;2024-06-07;https://www.arxiv.org/abs/2406.04770v1	arXiv:2406.04770			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 05 2024	2024	We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ''slightly better/worse'' to ''tie'' if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.																																	2024-10-27	PPRN:89232212		
J	Jing, Liqiang; Li, Ruosen; Chen, Yunmo; Du, Xinya				Chen, Yunmo/KRP-9297-2024; Jing, Liqiang/IAM-4938-2023						FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models								Arxiv											2	2;2024-09-26;https://www.arxiv.org/abs/2311.01477v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01477v1	arXiv:2311.01477			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 26 2024	2024	We introduce FaithScore (Faithfulness to Atomic Image Facts Score), a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The FaithScore evaluation first identifies sub-sentences containing descriptive statements that need to be verified, then extracts a comprehensive list of atomic facts from these sub-sentences, and finally conducts consistency verification between fine-grained atomic facts and the input image. Meta-evaluation demonstrates that our metric highly correlates with human judgments of faithfulness. We collect two benchmark datasets (i.e. LLaVA-1k and MSCOCO-Cap) for evaluating LVLMs instruction-following hallucinations. We measure hallucinations in state-of-the-art LVLMs with FaithScore on the datasets. Results reveal that current systems are prone to generate hallucinated content unfaithful to the image, which leaves room for future improvements. We hope our metric FaithScore can help evaluate future LVLMs in terms of faithfulness and provide insightful advice for enhancing LVLMs' faithfulness.																																	2024-10-09	PPRN:86034941		
J	Han, Ridong; Yang, Chaohao; Peng, Tao; Tiwari, Prayag; Wan, Xiang; Liu, Lu; Wang, Benyou				Tiwari, Prayag/N-6261-2017; Wang, Benyou/Y-5146-2019						An Empirical Study on Information Extraction using Large Language Models								Arxiv											2	2;2024-09-10;https://www.arxiv.org/abs/2305.14450v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.14450v1	arXiv:2305.14450			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 10 2024	2024	Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks. Therefore, various attempts have been made to apply LLMs to information extraction (IE), which is a fundamental NLP task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in LLMs' information extraction ability, we assess the information extraction ability of GPT-4 (the latest version of GPT at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between GPT-4 and state-of-the-art (SOTA) IE methods. To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.																																	2024-09-29	PPRN:72715165		
J	Shi, Weijia; Lee, Jaechan; Huang, Yangsibo; Malladi, Sadhika; Zhao, Jieyu; Holtzman, Ari; Liu, Daogao; Zettlemoyer, Luke; Smith, Noah A.; Zhang, Chiyuan				Zhao, jieyu/LXB-1295-2024						MUSE: Machine Unlearning Six-Way Evaluation for Language Models								Arxiv											2	2;2024-07-14;https://www.arxiv.org/abs/2407.06460v2| 1;2024-07-08;https://www.arxiv.org/abs/2407.06460v1	arXiv:2407.06460			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 14 2024	2024	Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content, and data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models, leading to the development of many approximate unlearning algorithms. Evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, , a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer’s expectations, because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations.1 																																	2024-07-23	PPRN:90751550		
J	Wang, Chaojie; Deng, Yanchen; Lv, Zhiyi; Zeng, Liang; He, Jujie; Yan, Shuicheng; An, Bo				yan, shuicheng/HCH-9860-2022						Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning								Arxiv											3	3;2024-06-27;https://www.arxiv.org/abs/2406.14283v3| 2;2024-06-24;https://www.arxiv.org/abs/2406.14283v2| 1;2024-06-20;https://www.arxiv.org/abs/2406.14283v1	arXiv:2406.14283			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 27 2024	2024	Large Language Models (LLMs) have demonstrated impressive capability in many natural language tasks. However, the auto -regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi -step reasoning. In this paper, by casting multi -step reasoning of LLMs as a heuristic search problem, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug -and -play Q -value model as heuristic function for estimating expected future rewards, our Q* can effectively guide LLMs to select the most promising next reasoning step without fine-tuning LLMs for the current task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP demonstrate the superiority of our method, contributing to improving the reasoning performance of existing open -source LLMs.																																	2024-07-17	PPRN:89377093		
J	Fang, Xi; Xu, Weijie; Tan, Fiona Anting; Zhang, Jiani; Hu, Ziqing; Qi, Yanjun; Nickleach, Scott; Socolinsky, Diego; Sengamedu, Srinivasan; Faloutsos, Christos				Xu, Weijie/B-3763-2010						Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey								Arxiv											4	4;2024-06-21;https://www.arxiv.org/abs/2402.17944v4| 3;2024-06-10;https://www.arxiv.org/abs/2402.17944v3| 2;2024-03-01;https://www.arxiv.org/abs/2402.17944v2| 1;2024-02-27;https://www.arxiv.org/abs/2402.17944v1	arXiv:2402.17944			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 21 2024	2024	Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.																																	2024-07-15	PPRN:88755965		
J	Bodner, Alexander Dylan; Spolski, Jack Natan; Tepsich, Antonio Santiago; Pourteau, Santiago										Convolutional Kolmogorov-Arnold Networks								Arxiv											3	3;2025-03-31;https://www.arxiv.org/abs/2406.13155v3| 2;2024-11-04;https://www.arxiv.org/abs/2406.13155v2| 1;2024-06-19;https://www.arxiv.org/abs/2406.13155v1	arXiv:2406.13155			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 19 2024	2024	In this paper, we introduce the Convolutional Kolmogorov-Arnold Networks (Convolutional KANs), an innovative alternative to the standard Convolutional Neural Networks (CNNs) that have revolutionized the field of computer vision. We integrate the non-linear activation functions presented in Kolmogorov-Arnold Networks (KANs) into convolutions to build a new layer. Throughout the paper, we empirically validate the performance of Convolutional KANs against traditional architectures across MNIST and Fashion-MNIST benchmarks, illustrating that this new approach maintains a similar level of accuracy while using half the amount of parameters. This significant reduction of parameters opens up a new approach to advance the optimization of neural network architectures.																																	2025-08-07	PPRN:89379123		
J	Chen, Dongping; Chen, Ruoxi; Zhang, Shilin; Wang, Yaochen; Liu, Yinuo; Zhou, Huichi; Zhang, Qihui; Wan, Yao; Zhou, Pan; Sun, Lichao				王, 耀琛/KBC-5843-2024; ZHANG, QIHUI/OAJ-1060-2025; Chen, Dongping/LSL-8606-2024						MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark								Arxiv											2	2;2024-06-11;https://www.arxiv.org/abs/2402.04788v3| 1;2024-02-07;https://www.arxiv.org/abs/2402.04788v1	arXiv:2402.04788			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. 																																	2024-07-10	PPRN:87561849		
J	Bai, Xuechunzi; Wang, Angelina; Sucholutsky, Ilia; Griffiths, Thomas L.				Sucholutsky, Ilia/AAE-6498-2021						Measuring Implicit Bias in Explicitly Unbiased Large Language Models								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2402.04105v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.04105v1	arXiv:2402.04105			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two new measures of bias: LLM Implicit Bias, a prompt-based method for revealing implicit bias; and LLM Decision Bias, a strategy to detect subtle discrimination in decision-making tasks. Both measures are based on psychological research: LLM Implicit Bias adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Decision Bias operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). Our prompt-based LLM Implicit Bias measure correlates with existing language model embedding-based bias methods, but better predicts downstream behaviors measured by LLM Decision Bias. These new prompt-based measures draw from psychology's long history of research into measuring stereotype biases based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks.																																	2024-06-04	PPRN:87533838		
J	Li, Bohao; Ge, Yuying; Chen, Yi; Ge, Yixiao; Zhang, Ruimao; Shan, Ying				Li, Bohao/KGL-4634-2024						SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension								Arxiv											1	1;2024-04-25;https://www.arxiv.org/abs/2404.16790v1	arXiv:2404.16790			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 25 2024	2024	Comprehending text -rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text -rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images. Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs. However, their proficiency in text -rich scenarios has yet to be comprehensively and objectively assessed, since current MLLM benchmarks primarily focus on evaluating general visual comprehension. In this work, we introduce SEED -Bench -2 -Plus, a benchmark specifically designed for evaluating text -rich visual comprehension of MLLMs. Our benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of textrich scenarios in the real world. These categories, due to their inherent complexity and diversity, effectively simulate real -world text -rich environments. We further conduct a thorough evaluation involving 34 prominent MLLMs (including GPT-4V, Gemini -Pro -Vision and Claude -3 -Opus) and emphasize the current limitations of MLLMs in textrich visual comprehension. We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further research in the area of text -rich visual comprehension with MLLMs. 																																	2024-05-04	PPRN:88650933		
J	Liu, Fuxiao; Wang, Xiaoyang; Yao, Wenlin; Chen, Jianshu; Song, Kaiqiang; Cho, Sangwoo; Yacoob, Yaser; Yu, Dong										MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2311.10774v2| 1;2023-11-15;https://www.arxiv.org/abs/2311.10774v1	arXiv:2311.10774			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	With the rapid development of large language models (LLMs) and their integration into large multimodal models (LMMs), there has been impressive progress in zero -shot completion of user -oriented vision -language tasks. However, a gap remains in the domain of chart image understanding due to the distinct abstract components in charts. To address this, we introduce a large-scale Multi-Modal Chart Instruction (MMC-Instruction) dataset comprising 600k instances supporting diverse tasks and chart types. Leveraging this data, we develop Multi-Modal Chart Assistant (MMCA), an LMM that achieves state-of-the-art performance on existing chart QA benchmarks. Recognizing the need for a comprehensive evaluation of LMM chart understanding, we also propose a Multi-Modal Chart Benchmark (MMC-Benchmark), a comprehensive human annotated benchmark with nine distinct tasks evaluating reasoning capabilities over charts. Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the most recent GPT-4V model. Our work provides an instruction -tuning methodology and benchmark to advance multimodal understanding of charts.																																	2024-04-25	PPRN:86209403		
J	Zheng, Xu; Liu, Yexin; Lu, Yunfan; Hua, Tongyan; Pan, Tianbo; Zhang, Weiming; Tao, Dacheng; Wang, Lin				Zhang, Wei-Ming/L-5761-2017; Tao, Dacheng/A-5449-2012; wang, Lin/GQO-7901-2022; Lu, Yunfan/LWZ-9161-2024						Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks								Arxiv											2	2;2024-04-11;https://www.arxiv.org/abs/2302.08890v3| 1;2023-02-17;https://www.arxiv.org/abs/2302.08890v1	arXiv:2302.08890			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 11 2024	2024	Event cameras are bio-inspired sensors that capture the per-pixel intensity changes asynchronously and produce event streams encoding the time, pixel position, and polarity (sign) of the intensity changes. Event cameras possess a myriad of advantages over canonical frame-based cameras, such as high temporal resolution, high dynamic range, low latency, etc. Being capable of capturing information in challenging visual conditions, event cameras have the potential to overcome the limitations of frame-based cameras in the computer vision and robotics community. In very recent years, deep learning (DL) has been brought to this emerging field and inspired active research endeavors in mining its potential. However, there is still a lack of taxonomies in DL techniques for event-based vision. We first scrutinize the typical event representations with quality enhancement methods as they play a pivotal role as inputs to the DL models. We then provide a comprehensive survey of existing DL-based methods by structurally grouping them into two major categories: 1) image/video reconstruction and restoration; 2) event-based scene understanding and 3D vision. We conduct benchmark experiments for the existing methods in some representative research directions, i.e., image reconstruction, deblurring, and object recognition, to identify some critical insights and problems. Finally, we have discussions regarding the challenges and provide new perspectives for inspiring more research studies.																																	2024-04-25	PPRN:43943371		
J	Ding, Yangruibo; Fu, Yanjun; Ibrahim, Omniyyah; Sitawarin, Chawin; Chen, Xinyun; Alomair, Basel; Wagner, David; Ray, Baishakhi; Chen, Yizheng				Sitawarin, Chawin/KIB-4488-2024; Chen, Xinyun/ABZ-9877-2022; Ding, Yangruibo/JEP-6503-2023						Vulnerability Detection with Code Language Models: How Far Are We?								Arxiv											1	1;2024-03-27;https://www.arxiv.org/abs/2403.18624v1	arXiv:2403.18624			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 27 2024	2024	In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.   To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.   Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.																																	2024-04-15	PPRN:88335320		
J	Kannan, Shyam Sundar; Venkatesh, Vishnunandan L.N.; Min, Byung-Cheol										SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models								Arxiv											1	1;2024-03-23;https://www.arxiv.org/abs/2309.10062v2	arXiv:2309.10062			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 23 2024	2024	In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be 																																	2024-04-13	PPRN:86280499		
J	Fujimoto, S.; Ouchi, M.; Kohno, K.; Valentino, F.; Gimenez-Arteaga, C.; Brammer, G.B.; Furtak, L.J.; Kohandel, M.; Oguri, M.; Pallottini, A.; Richard, J.; Zitrin, A.; Bauer, F.E.; Boylan-Kolchin, M.; Dessauges-Zavadsky, M.; Egami, E.; Finkelstein, S.L.; Ma, Z.; Smail, I.; Watson, D.; Hutchison, T.A.; Rigby, J.R.; Welch, B.D.; Ao, Y.; Bradley, L.D.; Caminha, G.B.; Caputi, K.I.; Espada, D.; Endsley, R.; Fudamoto, Y.; Gonzalez-Lopez, J.; Hatsukade, B.; Koekemoer, A.M.; Kokorev, V.; Laporte, N.; Lee, M.; Magdis, G.E.; Ono, Y.; Rizzo, F.; Shibuya, T.; Shimasaku, K.; Sun, F.; Toft, S.; Umehata, H.; Wang, T.; Yajima, H.				Bauer, Franz Erik/AEU-0656-2022; Labbe, Ivo/B-1408-2016; Kokorev, Vasily/GPK-2541-2022; Espada, Daniel/X-2434-2018; Ono, Yoshiaki/AAY-4463-2020; Wang, Tao/N-7399-2019; Caminha, Gabriel/C-8952-2013; Yoshinobu, Fudamoto/JBR-7809-2023; KOHNO, KOTARO/G-4910-2014; Toft, Sune/JEZ-2766-2023; Endsley, Ryan/AAJ-5103-2021; Rigby, Jane/D-4588-2012; Zitrin, Adi/M-3402-2018						Primordial Rotating Disk Composed of ≥15 Dense StarForming Clumps at Cosmic Dawn								Arxiv											2	2;2024-03-04;https://www.arxiv.org/abs/2402.18543v2| 1;2024-02-28;https://www.arxiv.org/abs/2402.18543v1	arXiv:2402.18543			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 04 2024	2024	Early galaxy formation, initiated by the dark matter and gas assembly, evolves through frequent mergers and feedback processes into dynamically hot, chaotic structures1. In contrast, dynamically cold, smooth rotating disks have been observed in massive evolved galaxies merely 1.4 billion years after the Big Bang2, suggesting rapid morphological and dynamical evolution in the early Universe. Probing this evolution mechanism necessitates studies of young galaxies, yet efforts have been hindered by observational limitations in both sensitivity and spatial resolution. Here we report high -resolution observations of a strongly lensed and quintuply imaged, low -luminosity, young galaxy at z = 6.072 (dubbed the Cosmic Grapes), 930 million years after the Big Bang. Magnified by gravitational lensing, the galaxy is resolved into at least 15 individual star -forming clumps with effective radii of re ≃ 10–60 parsec (pc), which dominate 70% of the galaxy’s total flux. The cool gas emission unveils a smooth, underlying rotating disk characterized by a high rotational -to -random motion ratio and a gravitationally unstable state (Toomre Q≃ 0.2–0.3), with high surface gas densities comparable to local dusty starbursts with 103−5 solar mass (M⊙) per pc2. These gas properties suggest that the numerous star -forming clumps are formed through disk instabilities with weak feedback effects. The clumpiness of the Cosmic Grapes significantly exceeds that of galaxies at later epochs and the predictions from current simulations for early galaxies. Our findings shed new light on internal galaxy substructures and their relation to the underlying dynamics and feedback mechanisms at play during their early formation phases, potentially explaining the high abundance of bright galaxies observed in the early Universe3 and the dark matter core -cusp problem4.																																	2024-04-02	PPRN:87987107		
J	Liu, Peiqi; Orru, Yaswanth; Vakil, Jay; Paxton, Chris; Shafiullah, Nur Muhammad Mahi; Pinto, Lerrel				Vakil, Jay/KMX-1143-2024; Shafiullah, Nur Muhammad Mahi/IZD-7285-2023						OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics								Arxiv											2	2;2024-02-29;https://www.arxiv.org/abs/2401.12202v2| 1;2024-01-22;https://www.arxiv.org/abs/2401.12202v1	arXiv:2401.12202			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8× the performance of prior work. On cleaner, uncluttered environments, OK-Robot’s performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. We published our code and robot videos on https://ok-robot.github.io to encourage further investigation.																																	2024-07-31	PPRN:87278220		
J	Chen, Hong; Zhang, Yipeng; Wu, Simin; Wang, Xin; Duan, Xuguang; Zhou, Yuwei; Zhu, Wenwu				Zhu, Wenwu/C-5025-2018; Zhang, Yipeng/JVP-2915-2024						DISENBOOTH: IDENTITY-PRESERVING DISENTAN-GLED TUNING FOR SUBJECT-DRIVEN TEXT-TO-IMAGE GENERATION								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/2305.03374v4| 1;2023-05-05;https://www.arxiv.org/abs/2305.03374v1	arXiv:2305.03374			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	Subject-driven text-to-image generation aims to generate customized images of the given subject based on the text descriptions, which has drawn increasing attention. Existing methods mainly resort to finetuning a pretrained generative model, where the identity-relevant information (e.g., the boy) and the identity-irrelevant information (e.g., the background or the pose of the boy) are entangled in the latent embedding space. However, the highly entangled latent embedding may lead to the failure of subject-driven text-to-image generation as follows: (i) the identity-irrelevant information hidden in the entangled embedding may dominate the generation process, resulting in the generated images heavily dependent on the irrelevant information while ignoring the given text descriptions; (ii) the identity-relevant information carried in the entangled embedding can not be appropriately preserved, resulting in identity change of the subject in the generated images. To tackle the problems, we propose DisenBooth, an identity-preserving disentangled tuning framework for subject-driven text-to-image generation. Specifically, DisenBooth finetunes the pretrained diffusion model in the denoising process. Different from previous works that utilize an entangled embedding to denoise each image, DisenBooth instead utilizes disentangled embeddings to respectively preserve the subject identity and capture the identity-irrelevant information. We further design the novel weak denoising and contrastive embedding auxiliary tuning objectives to achieve the disentanglement. Extensive experiments show that our proposed DisenBooth framework outperforms baseline models for subject-driven text-to-image generation with the identity-preserved embedding. Additionally, by combining the identity-preserved embedding and identity-irrelevant embedding, DisenBooth demonstrates more generation flexibility and controllability																																	2024-03-27	PPRN:68147924		
J	Fang, Richard; Bindu, Rohan; Gupta, Akul; Zhan, Qiusi; Kang, Daniel										LLM Agents can Autonomously Hack Websites								Arxiv											3	3;2024-02-16;https://www.arxiv.org/abs/2402.06664v3| 2;2024-02-15;https://www.arxiv.org/abs/2402.06664v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.06664v1	arXiv:2402.06664			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 16 2024	2024	In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.   In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.																																	2024-03-14	PPRN:87638255		
J	Jelassi, Samy; Brandfonbrener, David; Kakade, Sham M.; Malach, Eran										Repeat After Me: Transformers are Better than State Space Models at Copying								Arxiv											1	1;2024-02-01;https://www.arxiv.org/abs/2402.01032v1	arXiv:2402.01032			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 01 2024	2024	Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.																																	2024-02-19	PPRN:87508346		
J	Li, Ruosen; Patel, Teerth; Du, Xinya										PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations								Arxiv											3	3;2024-12-31;https://www.arxiv.org/abs/2307.02762v3| 2;2024-07-03;https://www.arxiv.org/abs/2307.02762v2| 1;2023-07-06;https://www.arxiv.org/abs/2307.02762v1	arXiv:2307.02762			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 31 2024	2024	Nowadays, the quality of responses generated by different modern large language models (LLMs) is hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs for reference-free evaluation of open-ended question answering. More specifically, they use the recognized “strongest” LLM as the evaluator, which conducts pairwise comparisons of candidate models’ answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose (1) the peer rank (PR) algorithm that takes into account each peer LLM’s pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on the preferences of two answers. We conduct experiments on two benchmark datasets. We find that our approaches achieve higher accuracy and align better with human judgments. Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model’s name is unrevealed. Our work provides space to explore evaluating models that are hard to compare for humans.1																																	2025-02-15	PPRN:73805709		
J	Yuan, Lifan; Li, Wendi; Chen, Huayu; Cui, Ganqu; Ding, Ning; Zhang, Kaiyan; Zhou, Bowen; Liu, Zhiyuan; Peng, Hao				Liu, Zhiyuan/I-2233-2014; Zhou, Bowen/AAH-1042-2020						Free Process Rewards without Process Labels								Arxiv											1	1;2024-12-02;https://www.arxiv.org/abs/2412.01981v1	arXiv:2412.01981			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Dec 02 2024	2024	Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models rθ(y) = β log πθ(y)/ πref(y) , which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline á la Math-Shepherd (Wang et al., 2023) using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible1 .																																	2025-01-15	PPRN:119686648		
J	Chen, Chang; Wu, Yi-Fu; Yoon, Jaesik; Ahn, Sungjin				Ahn, Sungjin/LCD-8968-2024						TransDreamer: Reinforcement Learning with Transformer World Models								Arxiv											2	2;2024-11-19;https://www.arxiv.org/abs/2202.09481v2| 1;2022-02-19;https://www.arxiv.org/abs/2202.09481v1	arXiv:2202.09481			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 19 2024	2024	The Dreamer agent provides various benefits of Model-Based Reinforcement Learning (MBRL) such as sample efficiency, reusable knowledge, and safe planning. However, its world model and policy networks inherit the limitations of recurrent neural networks and thus an important question is how an MBRL framework can benefit from the recent advances of transformers and what the challenges are in doing so. In this paper, we propose a transformer-based MBRL agent, called TransDreamer. We first introduce the Transformer State-Space Model, a world model that leverages a transformer for dynamics predictions. We then share this world model with a transformer-based policy network and obtain stability in training a transformer-based RL agent. In experiments, we apply the proposed model to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning. We show that the proposed model outperforms Dreamer in these complex tasks.																																	2024-12-28	PPRN:12069648		
J	Andriushchenko, Maksym; Souly, Alexandra; Dziemian, Mateusz; Duenas, Derek; Lin, Maxwell; Wang, Justin; Hendrycks, Dan; Zou, Andy; Kolter, Zico; Fredrikson, Matt; Winsor, Eric; Wynne, Jerome; Gal, Yarin; Davies, Xander				Zou, Andy/MGU-4410-2025						AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents								Arxiv											1	1;2024-10-14;https://www.arxiv.org/abs/2410.09024v2	arXiv:2410.09024			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 14 2024	2024	The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents - which use external tools and can execute multi-stage tasks - may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. 																																	2024-11-05	PPRN:112578630		
J	Ruan, Yangjun; Maddison, Chris J.; Hashimoto, Tatsunori										Observational Scaling Laws and the Predictability of Language Model Performance								Arxiv											2	2;2024-10-01;https://www.arxiv.org/abs/2405.10938v3| 1;2024-07-02;https://www.arxiv.org/abs/2405.10938v2	arXiv:2405.10938			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 01 2024	2024	Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.																																	2024-10-17	PPRN:89091907		
J	Wang, Song; Zhu, Yaochen; Liu, Haochen; Zheng, Zaiyi; Chen, Chen; Li, Jundong				Zhu, Yaochen/AAO-2023-2020						Knowledge Editing for Large Language Models: A Survey								Arxiv											3	3;2024-09-19;https://www.arxiv.org/abs/2310.16218v4| 2;2023-12-14;https://www.arxiv.org/abs/2310.16218v3| 1;2023-10-26;https://www.arxiv.org/abs/2310.16218v2	arXiv:2310.16218			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 19 2024	2024	Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.																																	2024-10-03	PPRN:85822530		
J	Chen, Zhiyuan; Cao, Jiajiong; Chen, Zhiquan; Li, Yuming; Ma, Chenguang										EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions								Arxiv											2	2;2024-07-12;https://www.arxiv.org/abs/2407.08136v2| 1;2024-07-11;https://www.arxiv.org/abs/2407.08136v1	arXiv:2407.08136			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	The area of portrait image animation, propelled by audio input, has witnessed notable progress in the generation of lifelike and dynamic portraits. Conventional methods are limited to utilizing either audios or facial key points to drive images into videos, while they can yield satisfactory results, certain issues exist. For instance, methods driven solely by audios can be unstable at times due to the relatively weaker audio signal, while methods driven exclusively by facial key points, although more stable in driving, can result in unnatural outcomes due to the excessive control of key point information. In addressing the previously mentioned challenges, in this paper, we introduce a novel approach which we named EchoMimic. EchoMimic is concurrently trained using both audios and facial landmarks. Through the implementation of a novel training strategy, EchoMimic is capable of generating portrait videos not only by audios and facial landmarks individually, but also by a combination of both audios and selected facial landmarks. EchoMimic has been comprehensively compared with alternative algorithms across various public datasets and our collected dataset, showcasing superior performance in both quantitative and qualitative evaluations. Additional visualization and access to the source code can be located on the EchoMimic project page.																																	2024-07-23	PPRN:90770472		
J	Li, Lei; Wang, Yuqi; Xu, Runxin; Wang, Peiyi; Feng, Xiachong; Kong, Lingpeng; Liu, Qi				LIU, Qi/I-4900-2017; Li, Lei/LMN-0940-2024; wang, peiyi/LNR-6224-2024; kong, lingpeng/NHQ-3170-2025; wang, yuqi/LGZ-5915-2024						Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models								Arxiv											3	3;2024-06-02;https://www.arxiv.org/abs/2403.00231v3| 2;2024-03-04;https://www.arxiv.org/abs/2403.00231v2| 1;2024-03-01;https://www.arxiv.org/abs/2403.00231v1	arXiv:2403.00231			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 02 2024	2024	Large vision-language models (LVLMs) excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances open-sourced LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs. Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, while domain-specific training yields substantial performance gains. Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.																																	2024-06-22	PPRN:87999013		
J	Thomas, Paul; Spielman, Seth; Craswell, Nick; Mitra, Bhaskar				Thomas, Paul/B-7511-2011; Spielman, Seth/AAZ-2453-2021						Large language models can accurately predict searcher preferences								Arxiv											3	3;2024-05-16;https://www.arxiv.org/abs/2309.10621v3| 2;2024-05-02;https://www.arxiv.org/abs/2309.10621v2| 1;2023-09-19;https://www.arxiv.org/abs/2309.10621v1	arXiv:2309.10621			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 16 2024	2024	Much of the evaluation and tuning of a search system relies on relevance labels—annotations that say whether a document is useful for a given search and searcher. Ideally these come from real searchers, but it is hard to collect this data at scale, so typical experiments rely on third-party labellers who may or may not produce accurate annotations. Label quality is managed with ongoing auditing, training, and monitoring. We discuss an alternative approach. We take careful feedback from real searchers and use this to select a large language model (LLM), and prompt, that agrees with this feedback; the LLM can then produce labels at scale. Our experiments show LLMs are as accurate as human labellers and as useful for finding the best systems and hardest queries. LLM performance varies with prompt features, but also varies unpredictably with simple paraphrases. This unpredictability reinforces the need for high-quality “gold” labels.																																	2024-06-01	PPRN:85052914		
J	Patro, Badri N.; Agneeswaran, Vijay S.				Patro, Badri/AAK-2539-2021; Agneeswaran, Vijay Srinivas/LWH-7627-2024						SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series								Arxiv											2	2;2024-04-24;https://www.arxiv.org/abs/2403.15360v2| 1;2024-03-22;https://www.arxiv.org/abs/2403.15360v1	arXiv:2403.15360			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 24 2024	2024	Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art transformers. Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and transfer learning benchmarks such as Stanford Car and Flower as well as task learning benchmarks as well as seven time series benchmark datasets.																																	2024-05-04	PPRN:88261562		
J	Bhatt, Manish; Chennabasappa, Sahana; Li, Yue; Nikolaidis, Cyrus; Song, Daniel; Wan, Shengye; Ahmad, Faizan; Aschermann, Cornelius; Chen, Yaohui; Kapil, Dhaval; Molnar, David; Whitman, Spencer; Saxe, Joshua				Wan, Shengye/JHS-4767-2023; Chen, Yaohui/OMK-5416-2025; Molnár, Dávid/A-9572-2013; Bhatt, Manish/O-5347-2019						CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models								Arxiv											1	1;2024-04-19;https://www.arxiv.org/abs/2404.13161v1	arXiv:2404.13161			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 19 2024	2024	Large language models (LLMs) introduce new security risks, but there are few comprehensive evaluation suites to measure and reduce these risks. We present BenchmarkName, a novel benchmark to quantify LLM security risks and capabilities. We introduce two new areas for testing: prompt injection and code interpreter abuse. We evaluated multiple state-of-the-art (SOTA) LLMs, including GPT-4, Mistral, Meta Llama 3 70B-Instruct, and Code Llama. Our results show that conditioning away risk of attack remains an unsolved problem; for example, all tested models showed between 26% and 41% successful prompt injection tests. We further introduce the safety-utility tradeoff: conditioning an LLM to reject unsafe prompts can cause the LLM to falsely reject answering benign prompts, which lowers utility. We propose quantifying this tradeoff using False Refusal Rate (FRR). As an illustration, we introduce a novel test set to quantify FRR for cyberattack helpfulness risk. We find many LLMs able to successfully comply with "borderline" benign requests while still rejecting most unsafe requests. Finally, we quantify the utility of LLMs for automating a core cybersecurity task, that of exploiting software vulnerabilities. This is important because the offensive capabilities of LLMs are of intense interest; we quantify this by creating novel test sets for four representative problems. We find that models with coding capabilities perform better than those without, but that further work is needed for LLMs to become proficient at exploit generation. Our code is open source and can be used to evaluate other LLMs.																																	2024-04-30	PPRN:88600676		
J	Adlakha, Vaibhav; Behnamghader, Parishad; Lu, Xing Han; Meade, Nicholas; Reddy, Siva										Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering								Arxiv											2	2;2024-04-17;https://www.arxiv.org/abs/2307.16877v2| 1;2023-07-31;https://www.arxiv.org/abs/2307.16877v1	arXiv:2307.16877			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 17 2024	2024	Instruction-following models are attractive alternatives to fine-tuned approaches for question answering (QA). By simply prepending relevant documents and an instruction to their input, these models can be adapted to various information domains and tasks without additional training. However, these models tend to produce verbose responses with supplementary information, which makes traditional QA metrics like exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user’s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness and propose simple token-overlap metrics that correlate highly with human judgments. Our analysis reveals that for correctness, instruction-following models perform comparably to models specifically fine-tuned for that task. However, they struggle to accurately judge the relevance of the provided knowledge and often hallucinate in their responses. We hope our work encourages more holistic evaluation of instruction-following models for QA. 																																	2024-04-27	PPRN:74188101		
J	Liu, Alisa; Han, Xiaochuang; Wang, Yizhong; Tsvetkov, Yulia; Choi, Yejin; Smith, Noah A.										Tuning Language Models by Proxy								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2401.08565v3| 1;2024-01-16;https://www.arxiv.org/abs/2401.08565v1	arXiv:2401.08565			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it to domain adaptation on code, and task-specific finetuning on question-answering and math problems. Finally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal adaptation, increasing its knowledge about recent events. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.																																	2024-04-25	PPRN:87188855		
J	Hu, Qitian Jason; Bieker, Jacob; Li, Xiuyu; Jiang, Nan; Keigwin, Benjamin; Ranganath, Gaurav; Keutzer, Kurt; Upadhyay, Shriyash Kaustubh										RouterBench: A Benchmark for Multi-LLM Routing System								Arxiv											2	2;2024-03-28;https://www.arxiv.org/abs/2403.12031v2| 1;2024-03-18;https://www.arxiv.org/abs/2403.12031v1	arXiv:2403.12031			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 28 2024	2024	As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and deliver a comparative analysis of various routing approaches through ROUTERBENCH, highlighting their potentials and limitations within our evaluation framework. This work not only formalizes and advances the development of LLM routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable LLM deployments. The code and data are available at https://github.com/withmartian/routerbench.																																	2024-04-15	PPRN:88190207		
J	Sukhbaatar, Sainbayar; Golovneva, Olga; Sharma, Vasu; Xu, Hu; Lin, Xi Victoria; Roziere, Baptiste; Kahn, Jacob; Li, Daniel; Yih, Wen-tau; Weston, Jason; Li, Xian										Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM								Arxiv											1	1;2024-03-12;https://www.arxiv.org/abs/2403.07816v1	arXiv:2403.07816			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 12 2024	2024	We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.																																	2024-04-04	PPRN:88112938		
J	Wang, Zengzhi; Xie, Qiming; Feng, Yi; Ding, Zixiang; Yang, Zinong; Xia, Rui				ding, zixiang/JMD-0121-2023						Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study								Arxiv											2	2;2024-02-17;https://www.arxiv.org/abs/2304.04339v2| 1;2023-04-10;https://www.arxiv.org/abs/2304.04339v1	arXiv:2304.04339			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 17 2024	2024	Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly interested in whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of emph{opinions}, emph{sentiments}, and emph{emotions} contained in the text. Specifically, we evaluate it in three settings, including emph{standard} evaluation, emph{polarity shift} evaluation and emph{open-domain} evaluation. We conduct an evaluation on 7 representative sentiment analysis tasks covering 17 benchmark datasets and compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on them. We also attempt several popular prompting techniques to elicit the ability further. Moreover, we conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.																																	2024-03-18	PPRN:57396316		
J	Ricker, Jonas; Damm, Simon; Holz, Thorsten; Fischer, Asja				Holz, Thorsten/H-5546-2013						Towards the Detection of Diffusion Model Deepfakes								Arxiv											2	2;2024-01-22;https://www.arxiv.org/abs/2210.14571v4| 1;2022-10-26;https://www.arxiv.org/abs/2210.14571v1	arXiv:2210.14571			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 22 2024	2024	In the course of the past few years, diffusion models (DMs) have reached an unprecedented level of visual quality. However, relatively little attention has been paid to the detection of DM-generated images, which is critical to prevent adverse impacts on our society. In contrast, generative adversarial networks (GANs), have been extensively studied from a forensic perspective. In this work, we therefore take the natural next step to evaluate whether previous methods can be used to detect images generated by DMs. Our experiments yield two key findings: (1) state-of-the-art GAN detectors are unable to reliably distinguish real from DM-generated images, but (2) re-training them on DM-generated images allows for almost perfect detection, which remarkably even generalizes to GANs. Together with a feature space analysis, our results lead to the hypothesis that DMs produce fewer detectable artifacts and are thus more difficult to detect compared to GANs. One possible reason for this is the absence of grid-like frequency artifacts in DM-generated images, which are a known weakness of GANs. However, we make the interesting observation that diffusion models tend to underestimate high frequencies, which we attribute to the learning objective.																																	2024-02-07	PPRN:22173609		
J	Wang, Xinglei; Fang, Meng; Zeng, Zichao; Cheng, Tao				Zeng, Zichao/JXX-4588-2024; Wang, Xinglei/LZE-6418-2025						Where Would I Go Next? Large Language Models as Human Mobility Predictors								Arxiv											2	2;2024-01-09;https://www.arxiv.org/abs/2308.15197v2| 1;2023-08-29;https://www.arxiv.org/abs/2308.15197v1	arXiv:2308.15197			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 09 2024	2024	Accurate human mobility prediction underpins many important applications across a variety of domains, including epidemic modelling, transport planning, and emergency responses. Due to the sparsity of mobility data and the stochastic nature of people's daily activities, achieving precise predictions of people's locations remains a challenge. While recently developed large language models (LLMs) have demonstrated superior performance across numerous language-related tasks, their applicability to human mobility studies remains unexplored. Addressing this gap, this article delves into the potential of LLMs for human mobility prediction tasks. We introduce a novel method, LLM-Mob, which leverages the language understanding and reasoning capabilities of LLMs for analysing human mobility data. We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement and enable time-aware prediction by using time information of the prediction target. Additionally, we design context-inclusive prompts that enable LLMs to generate more accurate predictions. Comprehensive evaluations of our method reveal that LLM-Mob excels in providing accurate and interpretable predictions, highlighting the untapped potential of LLMs in advancing human mobility prediction techniques. We posit that our research marks a significant paradigm shift in human mobility modelling, transitioning from building complex domain-specific models to harnessing general-purpose LLMs that yield accurate predictions through language instructions.																																	2024-01-25	PPRN:84521102		
J	Ma, Chang; Zhang, Junlei; Zhu, Zhihao; Yang, Cheng; Yang, Yujiu; Jin, Yaohui; Lan, Zhenzhong; Kong, Lingpeng; He, Junxian				kong, lingpeng/NHQ-3170-2025; HE, Junxian/OHV-2278-2025; Yang, Yujiu/JGM-0303-2023						AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents								Arxiv											2	2;2024-12-23;https://www.arxiv.org/abs/2401.13178v2| 1;2024-01-24;https://www.arxiv.org/abs/2401.13178v1	arXiv:2401.13178			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 23 2024	2024	Evaluating Large Language Models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce A GENT B OARD , a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. A GENT B OARD offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, A GENT B OARD serves as a step towards demystifying agent behaviors and accelerating the development of stronger LLM agents. 																																	2025-02-05	PPRN:87316027		
J	Shi, Lin; Ma, Chiyu; Liang, Wenhua; Ma, Weicheng; Vosoughi, Soroush				Shi, Lin/LEM-4882-2024						Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge								Arxiv											5	5;2024-12-15;https://www.arxiv.org/abs/2406.07791v7| 4;2024-10-31;https://www.arxiv.org/abs/2406.07791v6| 3;2024-10-02;https://www.arxiv.org/abs/2406.07791v5| 2;2024-09-27;https://www.arxiv.org/abs/2406.07791v4| 1;2024-06-12;https://www.arxiv.org/abs/2406.07791v1	arXiv:2406.07791			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 15 2024	2024	LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases—particularly position bias, the tendency to favor solutions based on their position within the prompt—compromise its reliability. This study investigates position bias in LLM judges across pairwise and list- wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 12 LLM judges across MT- Bench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 100,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task- Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.																																	2025-01-23	PPRN:89290616		
J	Hu, Wenbo; Gao, Xiangjun; Li, Xiaoyu; Zhao, Sijie; Cun, Xiaodong; Zhang, Yong; Quan, Long; Shan, Ying				HU, WENBO/HCI-8815-2022; Cun, Xiaodong/AAA-4674-2022						DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/2409.02095v2	arXiv:2409.02095			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 27 2024	2024	Estimating video depth in open-world scenarios is challenging due to the diversity of videos in appearance, content motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. The generalization ability to open-world videos is achieved by training the video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that can process extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero- shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.																																	2025-01-11	PPRN:119584870		
J	Zhang, Di; Wu, Jianbo; Lei, Jingdi; Che, Tong; Li, Jiatong; Xie, Tong; Huang, Xiaoshui; Zhang, Shufei; Pavone, Marco; Li, Yuqiang; Ouyang, Wanli; Zhou, Dongzhan				li, jiatong/LRB-6560-2024; Huang, Xiaoshui/HPG-0735-2023; Wu, Jianbo/LJL-2907-2024; Zhou, Dongzhan/IXN-4421-2023						LLaMA-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like Monte Carlo Tree Search								Arxiv											1	1;2024-11-21;https://www.arxiv.org/abs/2410.02884v2	arXiv:2410.02884			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 21 2024	2024	This paper presents an advanced mathematical reasoning framework, LLaMA-Berry, for enhancing the problem-solving ability of large language models (LLMs). The framework combines Monte Carlo Tree Search with Self- Refine (SR-MCTS) to optimize the reasoning paths and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critique and rewriting capabilities of LLMs, our SR-MCTS overcomes the inefficiencies and limitations of conventional stepwise and greedy search algorithms by fostering a more efficient exploration of solution spaces. To guide the search process, we propose Pair- wise Preference Reward Model (PPRM) to predict pairwise preferences between solutions through instruction-following capabilities trained by Reinforcement Learning from Human Feedback (RLHF). Finally, the Enhanced Borda Count (EBC) method is adopted to synthesize pairwise preferences into global quantile scores for evaluations. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior search efficiency and performance compared to existing open-source and closed-source methods, particularly in complex Olympiad-level benchmarks, including AIME24 and AMC23.																																	2024-12-31	PPRN:119317349		
J	Robledo-Moreno, Javier; Motta, Mario; Haas, Holger; Javadi-Abhari, Ali; Jurcevic, Petar; Kirby, William; Martiel, Simon; Sharma, Kunal; Sharma, Sandeep; Shirakawa, Tomonori; Sitdikov, Iskandar; Sun, Rong-Yang; Sung, Kevin J.; Takita, Maika; Tran, Minh C.; Yunoki, Seiji; Mezzacapo, Antonio				Kirby, William/R-7003-2019; Jurcevic, Petar/LTC-8798-2024						Chemistry Beyond Exact Solutions on a Quantum-Centric Supercomputer								Arxiv											2	2;2024-11-14;https://www.arxiv.org/abs/2405.05068v2| 1;2024-05-08;https://www.arxiv.org/abs/2405.05068v1	arXiv:2405.05068			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 14 2024	2024	A universal quantum computer can be used as a simulator capable of predicting properties of diverse quantum systems. Electronic structure problems in chemistry offer practical use cases around the hundred-qubit mark. This appears promising since current quantum processors have reached these sizes. However, mapping these use cases onto quantum computers yields deep circuits, and for pre-fault-tolerant quantum processors, the large number of measurements to estimate molecular energies leads to prohibitive runtimes. As a result, realistic chemistry is out of reach of current quantum computers in isolation. A natural question is whether classical distributed computation can relieve quantum processors from parsing all but a core, intrinsically quantum component of a chemistry workflow. Here, we incorporate quantum computations of chemistry in a quantum-centric supercomputing architecture, using up to 6400 nodes of the supercomputer Fugaku to assist a quantum computer with a Heron superconducting processor. We simulate the N$_2$ triple bond breaking in a correlation-consistent cc-pVDZ basis set, and the active-space electronic structure of [2Fe-2S] and [4Fe-4S] clusters, using 58, 45 and 77 qubits respectively, with quantum circuits of up to 10570 (3590 2-qubit) quantum gates. We obtain our results using a class of quantum circuits that approximates molecular eigenstates, and a hybrid estimator. The estimator processes quantum samples, produces upper bounds to the ground-state energy and wavefunctions supported on a polynomial number of states. This guarantees an unconditional quality metric for quantum advantage, certifiable by classical computers at polynomial cost. For current error rates, our results show that classical distributed computing coupled to quantum computers can produce good approximate solutions for practical problems beyond sizes amenable to exact diagonalization.																																	2024-12-21	PPRN:88978444		
J	Hans, Abhimanyu; Schwarzschild, Avi; Cherepanova, Valeriia; Kazemi, Hamid; Saha, Aniruddha; Goldblum, Micah; Geiping, Jonas; Goldstein, Tom				kazemi, hamid/AAO-8835-2021; Saha, Aniruddha/V-1837-2019						Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text								Arxiv											3	3;2024-10-13;https://www.arxiv.org/abs/2401.12070v3| 2;2024-07-01;https://www.arxiv.org/abs/2401.12070v2| 1;2024-01-22;https://www.arxiv.org/abs/2401.12070v1	arXiv:2401.12070			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 13 2024	2024	Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.																																	2024-11-05	PPRN:87278357		
J	Yu, Runpeng; Yu, Weihao; Wang, Xinchao				Yu, Weihao/HJH-1824-2023						KAN or MLP: A Fairer Comparison								Arxiv											2	2;2024-08-17;https://www.arxiv.org/abs/2407.16674v2| 1;2024-07-23;https://www.arxiv.org/abs/2407.16674v1	arXiv:2407.16674			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 17 2024	2024	This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives. 																																	2024-08-30	PPRN:91043892		
J	Zhang, Yuntong; Ruan, Haifeng; Fan, Zhiyu; Roychoudhury, Abhik				Fan, Zhiyu/LUZ-0320-2024; Zhang, Yuntong/MYS-4463-2025						AutoCodeRover: Autonomous Program Improvement								Arxiv											3	3;2024-07-25;https://www.arxiv.org/abs/2404.05427v3| 2;2024-04-15;https://www.arxiv.org/abs/2404.05427v2| 1;2024-04-08;https://www.arxiv.org/abs/2404.05427v1	arXiv:2404.05427			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 25 2024	2024	Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AUTOCODEROVER, , LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM’s understanding of the issue’s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported SWE-AGENT. . Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AUTOCODEROVER achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.																																	2024-08-02	PPRN:88441515		
J	Xu, Mingze; Gao, Mingfei; Gan, Zhe; Chen, Hong-You; Lai, Zhengfeng; Gang, Haiming; Kang, Kai; Dehghan, Afshin										SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models								Arxiv											1	1;2024-07-22;https://www.arxiv.org/abs/2407.15841v1	arXiv:2407.15841			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 22 2024	2024	We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video large language model (LLM) that can jointly capture the detailed spatial semantics and long-range temporal context without exceeding the token budget of commonly used LLMs. This is realized by using a two-stream SlowFast design of inputs for Video LLMs to aggregate features from sampled video frames in an effective way. Specifically, the Slow pathway extracts features at a low frame rate while keeping as many spatial details as possible (e.g., with 24x24 tokens), and the Fast pathway operates on a high frame rate but uses a larger spatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As a result, this design allows us to adequately capture both spatial and temporal features that are beneficial for understanding details along the video. Experimental results show that SF-LLaVA outperforms existing training-free methods on a wide range of video tasks. On some benchmarks, it achieves comparable or even better performance compared to state-of-the-art Video LLMs that are fine-tuned on video datasets. [GRAPHICS]																																	2024-07-28	PPRN:91027561		
J	Padmakumar, Vishakh; He, He										Does Writing with Language Models Reduce Content Diversity?								Arxiv											3	3;2024-07-01;https://www.arxiv.org/abs/2309.05196v3| 2;2024-03-06;https://www.arxiv.org/abs/2309.05196v2| 1;2023-09-11;https://www.arxiv.org/abs/2309.05196v1	arXiv:2309.05196			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.																																	2024-07-18	PPRN:84948189		
J	Qiao, Runqi; Tan, Qiuna; Dong, Guanting; Wu, Minhui; Sun, Chong; Song, Xiaoshuai; GongQue, Zhuoma; Lei, Shanglin; Wei, Zhe; Zhang, Miaoxuan; Qiao, Runfeng; Zhang, Yifan; Zong, Xiao; Xu, Yida; Diao, Muxi; Bao, Zhimin; Li, Chen; Zhang, Honggang				song, xiaoshuai/NIS-5627-2025; dong, guanting/JGL-9364-2023; ZHANG, HONGGANG/KGK-6243-2024						We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.01284v1	arXiv:2407.01284			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks, such as MathVista and MathVerse, focus more on the result-oriented performance but neglect the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond end-to-end performance. We meticulously collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and five layers of knowledge granularity. We decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM), to hierarchically assess inherent issues in LMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and problem-specific performance. We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategies. More notably, the primary challenge of GPT-4o has significantly transitioned from IK to IG, establishing it as the first LMM advancing towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorization - they correctly solve composite problems involving multiple knowledge concepts yet fail to answer sub-problems. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs.																																	2024-07-18	PPRN:90653794		
J	Tseng, Yu-Min; Huang, Yu-Chao; Hsiao, Teng-Yun; Chen, Wei-Lin; Huang, Chao-Wei; Meng, Yu; Chen, Yun-Nung				Meng, Yu/GLU-0811-2022; Chen, Weilin/AAB-7953-2022; Chen, Jung-Chien/J-5386-2015						Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization								Arxiv											2	2;2024-06-26;https://www.arxiv.org/abs/2406.01171v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01171v1	arXiv:2406.01171			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 26 2024	2024	The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors.																																	2024-07-15	PPRN:89156301		
J	Zhu, Shenhao; Chen, Junming Leo; Dai, Zuozhuo; Su, Qingkun; Xu, Yinghui; Cao, Xun; Yao, Yao; Zhu, Hao; Zhu, Siyu				Cao, Xun/AAM-2895-2021						Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance								Arxiv											2	2;2024-06-01;https://www.arxiv.org/abs/2403.14781v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14781v1	arXiv:2403.14781			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	In this study, we introduce a methodology for human image animation by leveraging a 3D human parametric model within a latent diffusion framework to enhance shape alignment and motion guidance in curernt human generative techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear) model as the 3D human parametric model to establish a unified representation of body shape and pose. This facilitates the accurate capture of intricate human geometry and motion characteristics from source videos. Specifically, we incorporate rendered depth images, normal maps, and semantic maps obtained from SMPL sequences, alongside skeleton-based motion guidance, to enrich the conditions to the latent diffusion model with comprehensive 3D shape and detailed pose attributes. A multi-layer motion fusion module, integrating self-attention mechanisms, is employed to fuse the shape and motion latent representations in the spatial domain. By representing the 3D human parametric model as the motion guidance, we can perform parametric shape alignment of the human body between the reference image and the source video motion. Experimental evaluations conducted on benchmark datasets demonstrate the methodology's superior ability to generate high-quality human animations that accurately capture both pose and shape variations. Furthermore, our approach also exhibits superior generalization capabilities on the proposed in-the-wild dataset. 																																	2024-06-22	PPRN:88268424		
J	Xie, Tianbao; Zhao, Siheng; Wu, Chen Henry; Liu, Yitao; Luo, Qian; Zhong, Victor; Yang, Yanchao; Yu, Tao				Liu, Yitao/KSM-2343-2024; Yang, Yanchao/GVT-7785-2022						Text2Reward: Reward Shaping with Language Models for Reinforcement Learning								Arxiv											1	1;2024-05-25;https://www.arxiv.org/abs/2309.11489v3	arXiv:2309.11489			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 25 2024	2024	Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce T EXT 2R EWARD , a data -free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, T EXT 2R EWARD generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, T EXT 2R EWARD produces interpretable, free -form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate T EXT 2R EWARD on two robotic manipulation benchmarks (M ANI S KILL 2, M ETA - W ORLD ) and two locomotion environments of MUJOCO. U J O C O . On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert -written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, T EXT 2R EWARD further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/																																	2024-06-11	PPRN:86281136		
J	Kristiano, Jason; Yokoyama, Junichi										Constraining Primordial Black Hole Formation from Single-Field Inflation								Arxiv											2	2;2024-05-20;https://www.arxiv.org/abs/2211.03395v3| 1;2023-08-14;https://www.arxiv.org/abs/2211.03395v2	arXiv:2211.03395			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 20 2024	2024	The most widely studied formation mechanism of a primordial black hole is collapse of large-amplitude perturbation on small scales generated in single-field inflation. In this Letter, we calculate one-loop correction to the large-scale power spectrum in a model with sharp transition of the second slow-roll parameter. We find that models producing an appreciable amount of primordial black holes induce nonperturbative coupling on a large scale probed by cosmic microwave background radiation. Our result implies that a small-scale power spectrum can be constrained by large-scale cosmological observations.																																	2024-06-01	PPRN:77521162		
J	Qin, Yuzhe; Yang, Wei; Huang, Binghao; Van Wyk, Karl; Su, Hao; Wang, Xiaolong; Chao, Yu-Wei; Fox, Dieter				Huang, Binghao/MGU-9640-2025; Qin, Yuzhe/IXN-5900-2023; Su, Hao/HHZ-1048-2022						AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System								Arxiv											2	2;2023-07-10;https://www.arxiv.org/abs/2307.04577v1| 1;2024-05-01;	arXiv:2307.04577			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 01 2024	2024	Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator. Project page: https://yzqin.github.io/anyteleop/.																																	2025-11-07	PPRN:73864128		
J	Sargent, Kyle; Li, Zizhang; Shah, Tanmay; Herrmann, Charles; Yu, Hong-Xing; Zhang, Yunzhi; Chan, Eric Ryan; Lagun, Dmitry; Li, Fei-Fei; Sun, Deqing; Wu, Jiajun				Wu, Jiajun/C-4123-2013; Sun, Deqing/KLD-7402-2024; Li, Feifei/C-3476-2017						ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image								Arxiv											2	2;2024-04-24;https://www.arxiv.org/abs/2310.17994v2| 1;2023-10-27;https://www.arxiv.org/abs/2310.17994v1	arXiv:2310.17994			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 24 2024	2024	We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view synthesis for in-the-wild scenes. While existing methods are designed for single objects with masked backgrounds, we propose new techniques to address challenges introduced by in-the-wild multi-object scenes with complex backgrounds. Specifically, we train a generative prior on a mixture of data sources that capture object-centric, indoor, and outdoor scenes. To address issues from data mixture such as depth-scale ambiguity, we propose a novel camera conditioning parameterization and normalization scheme. Further, we observe that Score Distillation Sampling (SDS) tends to truncate the distribution of complex backgrounds during distillation of 360-degree scenes, and propose "SDS anchoring" to improve the diversity of synthesized novel views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset in the zero-shot setting, even outperforming methods specifically trained on DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark for single-image novel view synthesis, and demonstrate strong performance in this setting.																																	2024-05-04	PPRN:85860132		
J	Wang, Xidong; Chen, Guiming Hardy; Song, Dingjie; Zhang, Zhiyi; Chen, Zhihong; Xiao, Qingying; Jiang, Feng; Li, Jianquan; Wan, Xiang; Wang, Benyou; Li, Haizhou				Wang, Xidong/IZD-5718-2023; Jiang, Feng/KRP-8568-2024; Li, Haizhou/Q-6438-2019; Wang, Benyou/Y-5146-2019						CMB: A Comprehensive Medical Benchmark in Chinese								Arxiv											2	2;2024-04-04;https://www.arxiv.org/abs/2308.08833v2| 1;2023-08-17;https://www.arxiv.org/abs/2308.08833v1	arXiv:2308.08833			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 04 2024	2024	Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English -based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. 																																	2024-04-19	PPRN:86001033		
J	Huang, Yi-Hua; Sun, Yang-Tian; Yang, Ziyi; Lyu, Xiaoyang; Cao, Yan-Pei; Qi, Xiaojuan				yang, ziyi/JGD-5349-2023; Qi, Xiaojuan/MVV-7776-2025						SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes								Arxiv											2	2;2024-03-31;https://www.arxiv.org/abs/2312.14937v3| 1;2023-12-04;https://www.arxiv.org/abs/2312.14937v1	arXiv:2312.14937			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 31 2024	2024	Novel view synthesis for dynamic scenes is still a challenging problem in computer vision and graphics. Recently, Gaussian splatting has emerged as a robust technique to represent static scenes and enable high-quality and real-time novel view synthesis. Building upon this technique, we propose a new representation that explicitly decomposes the motion and appearance of dynamic scenes into sparse control points and dense Gaussians, respectively. Our key idea is to use sparse control points, significantly fewer in number than the Gaussians, to learn compact 6 DoF transformation bases, which can be locally interpolated through learned interpolation weights to yield the motion field of 3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF transformations for each control point, which reduces learning complexities, enhances learning abilities, and facilitates obtaining temporal and spatial coherent motion patterns. Then, we jointly learn the 3D Gaussians, the canonical space locations of control points, and the deformation MLP to reconstruct the appearance, geometry, and dynamics of 3D scenes. During learning, the location and number of control points are adaptively adjusted to accommodate varying motion complexities in different regions, and an ARAP loss following the principle of as rigid as possible is developed to enforce spatial continuity and local rigidity of learned motions. Finally, thanks to the explicit sparse motion representation and its decomposition from appearance, our method can enable user-controlled motion editing while retaining high-fidelity appearances. Extensive experiments demonstrate that our approach outperforms existing approaches on novel view synthesis with a high rendering speed and enables novel appearance-preserved motion editing applications. Project page: https://yihua7.github.io/SC-GS-web/																																	2024-04-27	PPRN:86825976		
J	Chen, Zilong; Wang, Yikai; Wang, Feng; Wang, Zhengyi; Liu, Huaping				wang, yikai/HLW-7052-2023						V3D: Video Diffusion Models are Effective 3D Generators								Arxiv											1	1;2024-03-11;https://www.arxiv.org/abs/2403.06738v1	arXiv:2403.06738			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 11 2024	2024	Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at https://github.com/heheyas/V3D																																	2024-04-07	PPRN:88100240		
J	Cong, Yuren; Xu, Mengmeng; Simon, Christian; Chen, Shoufa; Ren, Jiawei; Xie, Yanping; Perez-Rua, Juan-Manuel; Rosenhahn, Bodo; Xiang, Tao; He, Sen				CHEN, SHOUFA/HSH-2485-2023; Xu, Mengmeng/OKS-2100-2025; Xie, Yanping/C-7950-2016						FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing								Arxiv											2	2;2024-02-29;https://www.arxiv.org/abs/2310.05922v3| 1;2023-10-09;https://www.arxiv.org/abs/2310.05922v1	arXiv:2310.05922			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 29 2024	2024	Text-to-video editing aims to edit the visual appearance of a source video conditional on textual prompts. A major challenge in this task is to ensure that all frames in the edited video are visually consistent. Most recent works apply advanced text-to-image diffusion models to this task by inflating 2D spatial attention in the U-Net into spatio-temporal attention. Although temporal context can be added through spatio-temporal attention, it may introduce some irrelevant information for each patch and therefore cause inconsistency in the edited video. In this paper, for the first time, we introduce optical flow into the attention module in the diffusion model's U-Net to address the inconsistency issue for text-to-video editing. Our method, FLATTEN, enforces the patches on the same flow path across different frames to attend to each other in the attention module, thus improving the visual consistency in the edited videos. Additionally, our method is training-free and can be seamlessly integrated into any diffusion-based text-to-video editing methods and improve their visual consistency. Experiment results on existing text-to-video editing benchmarks show that our proposed method achieves the new state-of-the-art performance. In particular, our method excels in maintaining the visual consistency in the edited videos.																																	2024-03-28	PPRN:85583491		
J	Furuta, Hiroki; Lee, Kuang-Huei; Nachum, Ofir; Matsuo, Yutaka; Faust, Aleksandra; Gu, Shixiang Shane; Gur, Izzeddin				Matsuo, Yutaka/GPK-5851-2022						MULTIMODAL WEB NAVIGATION WITH INSTRUCTION- FINETUNED FOUNDATION MODELS								Arxiv											4	4;2024-02-25;https://www.arxiv.org/abs/2305.11854v4| 3;2024-02-19;https://www.arxiv.org/abs/2305.11854v3| 2;2023-10-01;https://www.arxiv.org/abs/2305.11854v2| 1;2023-05-19;https://www.arxiv.org/abs/2305.11854v1	arXiv:2305.11854			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 25 2024	2024	The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain -specific model designs that make it difficult to leverage generalization from rich out -of -domain data. In this work, we study data -driven offline training for web agents with vision -language foundation models. We propose an instruction -following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent’s ability of grounded multimodal perception, HTML comprehension, and multi -step reasoning, outperforming prior works by a significant margin. On the MiniWoB, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the WebShop benchmark, our 3 -billion -parameter model achieves superior performance to the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positive transfer to the real -world planning tasks on the Mind2Web. We also collect 347K high -quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.																																	2024-03-25	PPRN:70568364		
J	Wang, Wenjie; Lin, Xinyu; Feng, Fuli; He, Xiangnan; Chua, Tat-Seng				Wang, Meng/AEZ-9059-2022; He, Xiangnan/G-3986-2011						Generative Recommendation: Towards Next-generation Recommender Paradigm								Arxiv											2	2;2024-02-25;https://www.arxiv.org/abs/2304.03516v2| 1;2023-04-07;https://www.arxiv.org/abs/2304.03516v1	arXiv:2304.03516			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 25 2024	2024	Recommender systems typically retrieve items from an item corpus for personalized recommendations. However, such a retrieval-based recommender paradigm faces two limitations: 1) the human-generated items in the corpus might fail to satisfy the users' diverse information needs, and 2) users usually adjust the recommendations via inefficient passive feedback, e.g., clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success, offering the potential to overcome these limitations: 1) generative AI can produce personalized items to satisfy users' information needs, and 2) the newly emerged large language models significantly reduce the efforts of users to precisely express information needs via natural language instructions. In this light, the boom of AIGC points the way towards the next-generation recommender paradigm with two new objectives: 1) generating personalized content through generative AI, and 2) integrating user instructions to guide content generation.   To this end, we propose a novel Generative Recommender paradigm named GeneRec, which adopts an AI generator to personalize content generation and leverages user instructions. Specifically, we pre-process users' instructions and traditional feedback via an instructor to output the generation guidance. Given the guidance, we instantiate the AI generator through an AI editor and an AI creator to repurpose existing items and create new items. Eventually, GeneRec can perform content retrieval, repurposing, and creation to satisfy users' information needs. Besides, to ensure the trustworthiness of the generated items, we emphasize various fidelity checks. Moreover, we provide a roadmap to envision future developments of GeneRec and several domain-specific applications of GeneRec with potential research tasks. Lastly, we study the feasibility of implementing AI editor and AI creator on micro-video generation, showing promising results.																																	2024-03-24	PPRN:56803382		
J	Wang, Zilong; Zhang, Hao; Li, Chun-Liang; Eisenschlos, Julian Martin; Perot, Vincent; Wang, Zifeng; Miculicich, Lesly; Fujii, Yasuhisa; Shang, Jingbo; Lee, Chen-Yu; Pfister, Tomas				Shang, Jingbo/T-4207-2019						Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding								Arxiv											2	2;2024-01-19;https://www.arxiv.org/abs/2401.04398v2| 1;2024-01-09;https://www.arxiv.org/abs/2401.04398v1	arXiv:2401.04398			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 19 2024	2024	Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.																																	2024-05-25	PPRN:87082362		
J	Agarwal, Rishabh; Vieillard, Nino; Zhou, Yongchao; Stanczyk, Piotr; Ramos, Sabela; Geist, Matthieu; Bachem, Olivier										On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes								Arxiv											3	3;2024-01-17;https://www.arxiv.org/abs/2306.13649v3| 2;2023-10-03;https://www.arxiv.org/abs/2306.13649v2| 1;2023-06-23;https://www.arxiv.org/abs/2306.13649v1	arXiv:2306.13649			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 17 2024	2024	Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks, and task-agnostic distillation for instruction-tuning.																																	2024-02-02	PPRN:73494370		
J	Chen, Junsong; Wu, Yue; Luo, Simian; Xie, Enze; Paul, Sayak; Luo, Ping; Zhao, Hang; Li, Zhenguo				Chen, Junsong/HII-4662-2022; pluo/GPG-2707-2022						PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models								Arxiv											1	1;2024-01-10;https://www.arxiv.org/abs/2401.05252v1	arXiv:2401.05252			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 10 2024	2024	This technical report introduces PIXART-δ, a text-to-image synthesis framework that integrates the Latent Consistency Model (LCM) and ControlNet into the advanced PIXART-a model. PIXART-a is recognized for its ability to generate high quality images of 1024px resolution through a remarkably efficient training process. The integration of LCM in PIXART-δ significantly accelerates the inference speed, enabling the production of high-quality images in just 2-4 steps. Notably, PIXART-δ achieves a breakthrough 0.5 seconds for generating 1024 x 1024 pixel images, marking a 7x improvement over the PIXART-a. Additionally, PIXART-δ is designed to be efficiently trainable on 32GB V100 GPUs within a single day. With its 8-bit inference capability (von Platen et al., 2023), PIXART-δ can synthesize 1024px images within 8GB GPU memory constraints, greatly enhancing its usability and accessibility. Furthermore, incorporating a ControlNet-like module enables fine-grained control over text-to-image diffusion models. We introduce a novel ControlNet-Transformer architecture, specifically tailored for Transformers, achieving explicit controllability alongside high-quality image generation. As a state-of-the-art, open-source image generation model, PIXART-δ offers a promising alternative to the Stable Diffusion family of models, contributing significantly to text-to-image synthesis.																																	2024-01-31	PPRN:87173685		
J	Fomin, Sergey; Williams, Lauren; Zelevinsky, Andrei				Fomin, Sergey/A-1158-2007						Introduction to Cluster Algebras. Chapters 1-3								Arxiv											2	2;2024-12-10;https://www.arxiv.org/abs/1608.05735v5| 1;2016-08-19;https://www.arxiv.org/abs/1608.05735v4	arXiv:1608.05735			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 10 2024	2024	This is a preliminary draft of Chapters 1-3 of our forthcoming textbook "Introduction to Cluster Algebras." This installment contains: Chapter 1. Total positivity Chapter 2. Mutations of quivers and matrices Chapter 3. Clusters and seeds																																	2025-08-07	PPRN:11914653		
J	Li, Cheng; Chen, Mengzhou; Wang, Jindong; Sitaram, Sunayana; Xie, Xing				wang, jindong/ACD-8485-2022						CultureLLM: Incorporating Cultural Differences into Large Language Models								Arxiv											3	3;2024-12-03;https://www.arxiv.org/abs/2402.10946v3| 2;2024-10-29;https://www.arxiv.org/abs/2402.10946v2| 1;2024-02-09;https://www.arxiv.org/abs/2402.10946v1	arXiv:2402.10946			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 03 2024	2024	Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM significantly outperforms various counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with comparable performance to GPT-4 or even better. Our human study shows that the generated samples are semantically equivalent to the original samples, providing an effective solution for LLMs augmentation. 																																	2025-01-15	PPRN:87761395		
J	Liu, Yupei; Jia, Yuqi; Geng, Runpeng; Jia, Jinyuan; Gong, Neil Zhenqiang				Liu, Yupei/JRW-9828-2023; Jia, Jinyuan/AAQ-5278-2020						Formalizing and Benchmarking Prompt Injection Attacks and Defenses								Arxiv											4	4;2024-11-24;https://www.arxiv.org/abs/2310.12815v4| 3;2024-06-01;https://www.arxiv.org/abs/2310.12815v3| 2;2024-05-30;https://www.arxiv.org/abs/2310.12815v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.12815v1	arXiv:2310.12815			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 24 2024	2024	A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses.																																	2025-01-08	PPRN:85719546		
J	Wen, Junjie; Zhu, Yichen; Li, Jinming; Zhu, Minjie; Wu, Kun; Xu, Zhiyuan; Liu, Ning; Cheng, Ran; Shen, Chaomin; Peng, Yaxin; Feng, Feifei; Tang, Jian				Zhu, Minjie/JTV-1498-2023						TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation								Arxiv											4	4;2024-11-14;https://www.arxiv.org/abs/2409.12514v4| 3;2024-09-27;https://www.arxiv.org/abs/2409.12514v3| 2;2024-09-24;https://www.arxiv.org/abs/2409.12514v2| 1;2024-09-19;https://www.arxiv.org/abs/2409.12514v1	arXiv:2409.12514			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 14 2024	2024	Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. 																																	2024-12-21	PPRN:92374109		
J	Hu, Yushi; Shi, Weijia; Fu, Xingyu; Roth, Dan; Ostendorf, Mari; Zettlemoyer, Luke; Smith, Noah A; Krishna, Ranjay				Fu, Xingyu/GZM-3129-2022; Hu, Yushi/MGV-6188-2025						Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models								Arxiv											3	3;2024-11-11;https://www.arxiv.org/abs/2406.09403v3| 2;2024-07-10;https://www.arxiv.org/abs/2406.09403v2| 1;2024-06-13;https://www.arxiv.org/abs/2406.09403v1	arXiv:2406.09403			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 11 2024	2024	Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce S KETCHPAD , a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, S KETCHPAD enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. S KETCHPAD can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment on a wide range of math tasks (including geometry, functions, graph, chess) and complex visual reasoning tasks. S KETCH- PAD substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with S KETCHPAD sets anew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%).All codes and data are in https://visualsketchpad.github.io/.																																	2024-12-19	PPRN:89302732		
J	Ugare, Shubham; Suresh, Tarun; Kang, Hangoo; Misailovic, Sasa; Singh, Gagandeep										SynCode: LLM Generation with Grammar Augmentation								Arxiv											4	4;2024-11-06;https://www.arxiv.org/abs/2403.01632v4| 3;2024-07-14;https://www.arxiv.org/abs/2403.01632v3| 2;2024-04-29;https://www.arxiv.org/abs/2403.01632v2| 1;2024-03-03;https://www.arxiv.org/abs/2403.01632v1	arXiv:2403.01632			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 06 2024	2024	LLMs are widely used in complex AI applications. These applications underscore the need for LLM outputs to adhere to a specific format, for their integration with other components in the systems. Typically the format rules – e.g., data serialization formats such as JSON, YAML, or Code in Programming Language – are expressed as context-free grammar (CFG). Due to the hallucinations and unreliability of LLMs, instructing LLMs to adhere to specified syntax becomes an increasingly important challenge. We present SYNCODE, a novel framework for efficient and general syntactical decoding with LLMs, to address this challenge. SYNCODE ensures soundness and completeness with respect to the CFG of a formal language, effectively retaining valid tokens while filtering out invalid ones. SYNCODE uses an offline-constructed, efficient lookup table, the DFA mask store, created from the DFA (Deterministic Finite Automaton) of the language’s grammar for efficient generation. SYNCODE seamlessly integrates with any language defined by CFG, as evidenced by experiments focusing on generating JSON, SQL, Python, and Go outputs. Our experiments evaluating the effectiveness of SYNCODE for JSON generation demonstrate that SYNCODE eliminates all syntax errors and significantly outperforms state-of-the-art baselines. Furthermore, our results underscore how SYNCODE significantly reduces 96.07% of syntax errors in generated Python and Go code, showcasing its substantial impact on enhancing syntactical precision in LLM generation.																																	2024-11-30	PPRN:88021845		
J	Wang, Zora Zhiruo; Mao, Jiayuan; Fried, Daniel; Neubig, Graham										Agent Workflow Memory								Arxiv											1	1;2024-09-11;https://www.arxiv.org/abs/2409.07429v1	arXiv:2409.07429			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Sep 11 2024	2024	Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.																																	2024-09-26	PPRN:91834119		
J	Greenblatt, Ryan; Shlegeris, Buck; Sachan, Kshitij; Roger, Fabien				roger, frederic/G-2263-2014						AI Control: Improving Safety Despite Intentional Subversion								Arxiv											5	5;2024-07-23;https://www.arxiv.org/abs/2312.06942v5| 4;2024-07-17;https://www.arxiv.org/abs/2312.06942v4| 3;2024-01-05;https://www.arxiv.org/abs/2312.06942v3| 2;2023-12-14;https://www.arxiv.org/abs/2312.06942v2| 1;2023-12-12;https://www.arxiv.org/abs/2312.06942v1	arXiv:2312.06942			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 23 2024	2024	As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or redteaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques (“protocols”) that are robust to intentional subversion. We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases. We investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call trusted editing . This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is untrusted monitoring . This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines. 1																																	2024-07-30	PPRN:86554822		
J	Zhang, Ziyin; Chen, Chaoyu; Liu, Bingchang; Liao, Cong; Gong, Zi; Yu, Hang; Li, Jianguo; Wang, Rui				Liu, Bingchang/AEL-0100-2022; Zhang, Haiyan/KWT-8071-2024						Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code								Arxiv											6	6;2024-06-26;https://www.arxiv.org/abs/2311.07989v7| 5;2024-04-16;https://www.arxiv.org/abs/2311.07989v5| 4;2024-01-22;https://www.arxiv.org/abs/2311.07989v4| 3;2023-12-05;https://www.arxiv.org/abs/2311.07989v3| 2;2023-11-19;https://www.arxiv.org/abs/2311.07989v2| 1;2023-11-14;https://www.arxiv.org/abs/2311.07989v1	arXiv:2311.07989			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 26 2024	2024	In this work we systematically review the recent advancements in software engineering with language models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 related works. Unlike previous works, we integrate software engineering (SE) with natural language processing (NLP) by discussing the perspectives of both sides: SE applies language models for development automation, while NLP adopts SE tasks for language model evaluation. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also go beyond programming and review LLMs' application in other software engineering activities including requirement engineering, testing, deployment, and operations in an endeavor to provide a global view of NLP in SE, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.																																	2024-09-18	PPRN:86201038		
J	Oren, Matanel; Hassid, Michael; Yarden, Nir; Adi, Yossi; Schwartz, Roy				Adi, Yossi/HLG-5748-2023						Transformers are Multi-State RNNs								Arxiv											2	2;2024-06-18;https://www.arxiv.org/abs/2401.06104v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.06104v1	arXiv:2401.06104			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 18 2024	2024	Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models—recurrent neural networks (RNNs). In this work, we demonstrate that decoder -only transformers can in fact be conceptualized as unbounded multistate RNNs—an RNN variant with unlimited hidden state size. We further show that transformers can be converted into bounded multistate RNNs by fixing the size of their hidden state, effectively compressing their keyvalue cache. We introduce a novel, trainingfree compression policy—Token O mission Via A ttention (TOVA).1 1 Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only 1 / 8 of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs’ most painful computational bottlenecks—the size of their key -value cache.2																																	2024-07-04	PPRN:87128008		
J	Jiang, Fengqing; Xu, Zhangchen; Niu, Luyao; Xiang, Zhen; Ramasubramanian, Bhaskar; Li, Bo; Poovendran, Radha				Niu, Luyao/AEN-7350-2022; Xu, Zhangchen/IYJ-6907-2023						ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2402.11753v4| 2;2024-04-19;https://www.arxiv.org/abs/2402.11753v3| 1;2024-02-19;https://www.arxiv.org/abs/2402.11753v1	arXiv:2402.11753			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs. 																																	2024-06-22	PPRN:87760717		
J	Bordes, Florian; Pang, Richard Yuanzhe; Ajay, Anurag; Li, Alexander C.; Bardes, Adrien; Petryk, Suzanne; Manas, Oscar; Lin, Zhiqiu; Mahmoud, Anas; Jayaraman, Bargav; Ibrahim, Mark; Hall, Melissa; Xiong, Yunyang; Lebensold, Jonathan; Ross, Candace; Jayakumar, Srihari; Guo, Chuan; Bouchacourt, Diane; Al-Tahan, Haider; Padthe, Karthik; Sharma, Vasu; Xu, Hu; Tan, Xiaoqing Ellen; Richards, Megan; Lavoie, Samuel; Astolfi, Pietro; Hemmat, Reyhane Askari; Chen, Jun; Tirumala, Kushal; Assouel, Rim; Moayeri, Mazda; Talattof, Arjang; Chaudhuri, Kamalika; Liu, Zechun; Chen, Xilun; Garrido, Quentin; Ullrich, Karen; Agrawal, Aishwarya; Saenko, Kate; Celikyilmaz, Asli; Chandra, Vikas				liu, zechun/LBH-4471-2024; Xiong, Yunyang/HHZ-6012-2022; Alaa, Mahmoud/AGZ-3537-2022						An Introduction to Vision-Language Modeling								Arxiv											1	1;2024-05-27;https://www.arxiv.org/abs/2405.17247v1	arXiv:2405.17247			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 27 2024	2024	Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.																																	2024-06-09	PPRN:89057721		
J	Patel, Pratyush; Choukse, Esha; Zhang, Chaojie; Shah, Aashaka; Goiri, Inigo; Maleki, Saeed; Bianchini, Ricardo				Maleki, Saeed/HNQ-7316-2023; Patel, Pratyush/JRW-5468-2023						Splitwise: Efficient generative LLM inference using phase splitting								Arxiv											2	2;2024-05-20;https://www.arxiv.org/abs/2311.18677v2| 1;2023-11-30;https://www.arxiv.org/abs/2311.18677v1	arXiv:2311.18677			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 20 2024	2024	Generative large language model (LLM) applications are growing rapidly, leading to large-scale deployments of expensive and power-hungry GPUs. Our characterization of LLM inference shows that each inference request undergoes two phases: a compute -intensive prompt computation phase and a memoryintensive token generation phase, each with distinct latency, throughput, memory, and power characteristics. Despite stateof-the-art batching and scheduling, the token generation phase underutilizes compute resources. Unlike prompt computation, token generation does not need the compute capability of the latest GPUs and can be run with lower power and cost. Based on these insights, we propose Splitwise, a model deployment and scheduling technique that splits the two phases of LLM inference requests on to separate machines. Splitwise enables phase -specific resource management using hardware that is well suited for each phase. Request state is transferred efficiently between machines using optimized network libraries on the fast back -plane interconnects available in today’s GPU clusters. Using Splitwise, we design homogeneous and heterogeneous LLM inference clusters optimized for throughput, cost, and power. Compared to current designs, Splitwise clusters achieve up to 1.4× . 4 × higher throughput at 20% lower cost. Alternatively, they can deliver 2.35× . 35 × more throughput under the same power and cost budgets.																																	2024-06-01	PPRN:86336761		
J	Yang, Lin; Xu, Shawn; Sellergren, Andrew; Kohlberger, Timo; Zhou, Yuchen; Ktena, Ira; Kiraly, Atilla; Ahmed, Faruk; Hormozdiari, Farhad; Jaroensri, Tiam; Wang, Eric; Wulczyn, Ellery; Jamil, Fayaz; Guidroz, Theo; Lau, Chuck; Qiao, Siyuan; Liu, Yun; Goel, Akshay; Park, Kendall; Agharwal, Arnav; George, Nick; Wang, Yang; Tanno, Ryutaro; Barrett, David G.T.; Weng, Wei-Hung; Sara Mahdavi, S.; Saab, Khaled; Tu, Tao; Kalidindi, Sreenivasa Raju; Etemadi, Mozziyar; Cuadros, Jorge; Sorensen, Gregory; Matias, Yossi; Chou, Katherine; Corrado, Greg; Barral, Joelle; Shetty, Shravya; Fleet, David; Ali Eslami, S.M.; Tse, Daniel; Prabhakara, Shruthi; Mclean, Cory; Steiner, Dave; Pilgrim, Rory; Kelly, Christopher; Azizi, Shekoofeh; Golden, Daniel				Park, Kendall/LFS-4344-2024; Goel, Akshay/ABD-2916-2021; Azizi, Shekoofeh/T-5465-2019; Qiao, Siyuan/LYP-0410-2024; tu, tao/KVB-7209-2024						Advancing Multimodal Medical Capabilities of Gemini								Arxiv											1	1;2024-05-06;https://www.arxiv.org/abs/2405.03162v1	arXiv:2405.03162			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	Many clinical tasks require an understanding of specialized data, such as medical images and genomics, which is not typically found in general-purpose large multimodal models. Building upon Gemini's multimodal models, we develop several models within the new Med-Gemini family that inherit core capabilities of Gemini and are optimized for medical use via fine-tuning with 2D and 3D radiology, histopathology, ophthalmology, dermatology and genomic data. Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results across two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as "equivalent or better" than the original radiologists' reports. We demonstrate the first ever large multimodal model-based report generation for 3D computed tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered clinically acceptable, although additional research is needed to meet expert radiologist reporting quality. Beyond report generation, Med-Gemini-2D surpasses the previous best performance in CXR visual question answering (VQA) and performs well in CXR classification and radiology VQA, exceeding SoTA or baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology image classification, Med-Gemini-2D surpasses baselines across 18 out of 20 tasks and approaches task-specific model performance. Beyond imaging, Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based approach for disease risk prediction and generalizes to genetically correlated diseases for which it has never been trained. Although further development and evaluation are necessary in the safety-critical medical domain, our results highlight the potential of Med-Gemini across a wide range of medical tasks.																																	2024-05-25	PPRN:88790100		
J	Zhou, Yupeng; Zhou, Daquan; Cheng, Ming-Ming; Feng, Jiashi; Hou, Qibin				Feng, Jiashi/AGX-6209-2022; ZHOU, YUPENG/MBH-9347-2025; Zhou, Daquan/ACT-7390-2022						StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation								Arxiv											1	1;2024-05-02;https://www.arxiv.org/abs/2405.01434v1	arXiv:2405.01434			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 02 2024	2024	For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner. To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications. Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion.																																	2024-05-25	PPRN:88796520		
J	Jin, Zhijing; Liu, Jiarui; Lyu, Zhiheng; Poff, Spencer; Sachan, Mrinmaya; Mihalcea, Rada; Diab, Mona; Scholkopf, Bernhard				Schölkopf, Bernhard/A-7570-2013						Can Large Language Models Infer Causation from Correlation?								Arxiv											3	3;2024-04-17;https://www.arxiv.org/abs/2306.05836v3| 2;2023-12-31;https://www.arxiv.org/abs/2306.05836v2| 1;2023-06-09;https://www.arxiv.org/abs/2306.05836v1	arXiv:2306.05836			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 17 2024	2024	Causal inference is one of the hallmarks of human intelligence. While the field of Causal NLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task CORR2CAUSE, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re -purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform causal inference in in -distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out -of -distribution settings generated by perturbing these queries. CORR2CAUSE is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs’ pure reasoning skills and generalizability.1																																	2024-04-27	PPRN:73261794		
J	Masterman, Tula; Besen, Sandi; Sawtell, Mason; Chao, Alex										The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey								Arxiv											1	1;2024-04-17;https://www.arxiv.org/abs/2404.11584v1	arXiv:2404.11584			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 17 2024	2024	This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.																																	2024-04-25	PPRN:88557649		
J	Merrill, William; Sabharwal, Ashish				Merrill, William/HMV-2296-2023						The Expressive Power of Transformers with Chain of Thought								Arxiv											5	5;2024-04-11;https://www.arxiv.org/abs/2310.07923v5| 4;2024-03-20;https://www.arxiv.org/abs/2310.07923v4| 3;2023-10-18;https://www.arxiv.org/abs/2310.07923v3| 2;2023-10-16;https://www.arxiv.org/abs/2310.07923v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07923v1	arXiv:2310.07923			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 11 2024	2024	Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite -state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers’ reasoning can be improved by allowing them to use a “chain of thought” or “scratchpad”, i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder -only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps, assuming projected pre -norm (a slight generalization of standard pre -norm), adds a clear new ability (under standard complexity conjectures): recognizing all regular languages. Our results also imply that linear steps keep transformer decoders within context -sensitive languages, and polynomial steps with generalized pre -norm make them recognize exactly the class of polynomial -time solvable problems—the first exact characterization of a type of transformers in terms of standard complexity classes. Together, this provides a nuanced framework for understanding how the length of a transformer’s chain of thought or scratchpad impacts its reasoning power.																																	2024-04-26	PPRN:85604256		
J	Bolton, Elliot; Venigalla, Abhinav; Yasunaga, Michihiro; Hall, David; Xiong, Betty; Lee, Tony; Daneshjou, Roxana; Frankle, Jonathan; Liang, Percy; Carbin, Michael; Manning, Christopher D.				Manning, Christopher/A-1358-2007; Hall, David/KHD-4359-2024; Daneshjou, Roxana/ABE-7764-2021; Yasunaga, Michihiro/GPW-9499-2022						BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text								Arxiv											1	1;2024-03-27;https://www.arxiv.org/abs/2403.18421v1	arXiv:2403.18421			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 27 2024	2024	Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-preserving, economical and environmentally friendly foundations for particular NLP applications, such as in biomedicine. 																																	2024-04-14	PPRN:88334041		
J	Li, Jiarui; Yuan, Ye; Zhang, Zehua				Li, Jiarui/KBB-3765-2024						Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases								Arxiv											1	1;2024-03-15;https://www.arxiv.org/abs/2403.10446v1	arXiv:2403.10446			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 15 2024	2024	We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.																																	2024-04-11	PPRN:88167314		
J	Ning, Xuefei; Lin, Zinan; Zhou, Zixuan; Wang, Zifu; Yang, Huazhong; Wang, Yu				yang, huazhong/ACF-0711-2022; WANG, Yu/B-7985-2011; Lin, Zinan/NGS-1685-2025						Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation								Arxiv											3	3;2024-03-02;https://www.arxiv.org/abs/2307.15337v3| 2;2023-10-08;https://www.arxiv.org/abs/2307.15337v2| 1;2023-07-28;https://www.arxiv.org/abs/2307.15337v1	arXiv:2307.15337			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 02 2024	2024	This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and showcases the potential of eliciting high-quality answers by explicitly planning the answer structure in language.																																	2024-03-31	PPRN:74169133		
J	Liu, Yang; Cao, Jiahuan; Liu, Chongyu; Ding, Kai; Jin, Lianwen				Jin, Lianwen/AAJ-6536-2020						Datasets for Large Language Models: A Comprehensive Survey								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18041v1	arXiv:2402.18041			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre -training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pretraining corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.																																	2024-11-10	PPRN:88010997		
J	Bardes, Adrien; Garrido, Quentin; Ponce, Jean; Chen, Xinlei; Rabbat, Michael; Lecun, Yann; Assran, Mahmoud; Ballas, Nicolas				Rabbat, Michael/G-4582-2012						Revisiting Feature Prediction for Learning Visual Representations from Video								Arxiv											1	1;2024-02-15;https://www.arxiv.org/abs/2404.08471v1	arXiv:2404.08471			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 15 2024	2024	This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.																																	2024-04-26	PPRN:88550254		
J	Terven, Juan R; Cordova-Esparza, Diana M				Cordova, Diana/GQZ-7591-2022; Terven, Juan Ramon/IAN-0006-2023						A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS								Arxiv											4	4;2024-02-04;https://www.arxiv.org/abs/2304.00501v7| 3;2024-01-07;https://www.arxiv.org/abs/2304.00501v6| 2;2023-10-08;https://www.arxiv.org/abs/2304.00501v5| 1;2023-04-02;https://www.arxiv.org/abs/2304.00501v1	arXiv:2304.00501			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 04 2024	2024	YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with Transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.																																	2024-05-25	PPRN:53669679		
J	Faiz, Ahmad; Kaneda, Sotaro; Wang, Ruhan; Osi, Rita; Sharma, Prateek; Chen, Fan; Jiang, Lei										LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models								Arxiv											2	2;2024-01-19;https://www.arxiv.org/abs/2309.14393v2| 1;2023-09-25;https://www.arxiv.org/abs/2309.14393v1	arXiv:2309.14393			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 19 2024	2024	The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce textit{carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. 																																	2024-05-25	PPRN:85227544		
J	Hao, Shibo; Liu, Tianyang; Wang, Zhen; Hu, Zhiting				Liu, Tianyang/HOC-6916-2023						ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings								Arxiv											3	3;2024-01-15;https://www.arxiv.org/abs/2305.11554v4| 2;2023-10-30;https://www.arxiv.org/abs/2305.11554v3| 1;2023-05-19;https://www.arxiv.org/abs/2305.11554v1	arXiv:2305.11554			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 15 2024	2024	Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which fine-tune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, ToolkenGPT, which combines the benefits of both sides. Our approach represents each tool as a token (“toolken”) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.1																																	2024-05-25	PPRN:70569531		
J	Ma, Yue; He, Yingqing; Cun, Xiaodong; Wang, Xintao; Chen, Siran; Li, Xiu; Chen, Qifeng				Cun, Xiaodong/AAA-4674-2022						Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos								Arxiv											2	2;2024-01-03;https://www.arxiv.org/abs/2304.01186v2| 1;2023-04-03;https://www.arxiv.org/abs/2304.01186v1	arXiv:2304.01186			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 03 2024	2024	Generating text-editable and pose-controllable character videos have an imperious demand in creating various digital human. Nevertheless, this task has been restricted by the absence of a comprehensive dataset featuring paired video-pose captions and the generative prior models for videos. In this work, we design a novel two-stage training scheme that can utilize easily obtained datasets (i.e.,image pose pair and pose-free video) and the pre-trained text-to-image (T2I) model to obtain the pose-controllable character videos. Specifically, in the first stage, only the keypoint-image pairs are used only for a controllable text-to-image generation. We learn a zero-initialized convolutional encoder to encode the pose information. In the second stage, we finetune the motion of the above network via a pose-free video dataset by adding the learnable temporal self-attention and reformed cross-frame self-attention blocks. Powered by our new designs, our method successfully generates continuously pose-controllable character videos while keeps the editing and concept composition ability of the pre-trained T2I model. 																																	2024-01-11	PPRN:53717921		
J	Zhao, Tony Z.; Tompson, Jonathan; Driess, Danny; Florence, Pete; Ghasemipour, Kamyar; Finn, Chelsea; Wahid, Ayzaan										ALOHA Unleashed: A Simple Recipe for Robot Dexterity								Arxiv											1	1;2024-10-17;https://www.arxiv.org/abs/2410.13126v1	arXiv:2410.13126			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	Recent work has shown promising results for learning end-to-end robot policies using imitation learning. In this work we address the question of how far can we push imitation learning for challenging dexterous manipulation tasks. We show that a simple recipe of large scale data collection on the ALOHA 2 platform, combined with expressive models such as Diffusion Policies, can be effective in learning challenging bimanual manipulation tasks involving deformable objects and complex contact rich dynamics. We demonstrate our recipe on 5 challenging real-world and 3 simulated tasks and demonstrate improved performance over state-of-the-art baselines. 																																	2024-11-13	PPRN:115453775		
J	Wu, Junde; Zhu, Jiayuan; Qi, Yunli; Chen, Jingkun; Xu, Min; Menolascina, Filippo; Grau, Vicente				王, 仁杰/IUO-6250-2023; Zhu, Jiayuan/JAO-3713-2023						Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation								Arxiv											2	2;2024-10-15;https://www.arxiv.org/abs/2408.04187v2| 1;2024-08-08;https://www.arxiv.org/abs/2408.04187v1	arXiv:2408.04187			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 15 2024	2024	We introduce a novel graph-based Retrieval-Augmented Generation (RAG) framework specifically designed for the medical domain, called textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM) capabilities for generating evidence-based medical responses, thereby improving safety and reliability when handling private medical data. Graph-based RAG (GraphRAG) leverages LLMs to organize RAG data into graphs, showing strong potential for gaining holistic insights from long-form documents. However, its standard implementation is overly complex for general use and lacks the ability to generate evidence-based responses, limiting its effectiveness in the medical field. To extend the capabilities of GraphRAG to the medical domain, we propose unique Triple Graph Construction and U-Retrieval techniques over it. In our graph construction, we create a triple-linked structure that connects user documents to credible medical sources and controlled vocabularies. In the retrieval process, we propose U-Retrieval which combines Top-down Precise Retrieval with Bottom-up Response Refinement to balance global context awareness with precise indexing. These effort enable both source information retrieval and comprehensive response generation. Our approach is validated on 9 medical Q&A benchmarks, 2 health fact-checking benchmarks, and one collected dataset testing long-form generation. The results show that MedGraphRAG consistently outperforms state-of-the-art models across all benchmarks, while also ensuring that responses include credible source documentation and definitions. 																																	2024-11-12	PPRN:91296938		
J	Ye, Tianzhu; Dong, Li; Xia, Yuqing; Sun, Yutao; Zhu, Yi; Huang, Gao; Wei, Furu				Xia, Yuqing/LWH-5429-2024						Differential Transformer								Arxiv											1	1;2024-10-07;https://www.arxiv.org/abs/2410.05258v1	arXiv:2410.05258			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 07 2024	2024	Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.																																	2024-10-27	PPRN:104403317		
J	Zhai, Yuexiang; Bai, Hao; Lin, Zipeng; Pan, Jiayi; Tong, Shengbang; Zhou, Yifei; Suhr, Alane; Xie, Saining; LeCun, Yann; Ma, Yi; Levine, Sergey				pan, jiayi/ABR-2644-2022; Lin, Zipeng/HTO-1205-2023						Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning								Arxiv											2	2;2024-10-07;https://www.arxiv.org/abs/2405.10292v3| 1;2024-05-17;https://www.arxiv.org/abs/2405.10292v2	arXiv:2405.10292			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 07 2024	2024	Large vision-language models (VLMs) fine-tuned on specialized visual instructionfollowing data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.																																	2024-10-30	PPRN:88929699		
J	Abbasian, Mahyar; Azimi, Iman; Rahmani, Amir M.; Jain, Ramesh				Rahmani, AmirMohammad/KIC-4428-2024; azimi, iman/ADF-7907-2022						Conversational Health Agents: A Personalized LLM-Powered Agent Framework								Arxiv											5	5;2024-09-25;https://www.arxiv.org/abs/2310.02374v5| 4;2024-01-23;https://www.arxiv.org/abs/2310.02374v4| 3;2023-12-07;https://www.arxiv.org/abs/2310.02374v3| 2;2023-10-21;https://www.arxiv.org/abs/2310.02374v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.02374v1	arXiv:2310.02374			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 25 2024	2024	Conversational Health Agents (CHAs) are interactive systems that provide healthcare services, such as assistance and diagnosis. Current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation aspects. However, they offer limited agent capabilities, specifically lacking multi-step problem-solving, personalized conversations, and multimodal data analysis. Our aim is to overcome these limitations. We propose openCHA, an open-source LLM-powered framework, to empower conversational agents to generate a personalized response for users’ healthcare queries. This framework enables developers to integrate external sources including data sources, knowledge bases, and analysis models, into their LLM-based solutions. openCHA includes an orchestrator to plan and execute actions for gathering information from external sources, essential for formulating responses to user inquiries. It facilitates knowledge acquisition, problemsolving capabilities, multilingual and multimodal conversations, and fosters interaction with various AI platforms. We illustrate the framework’s proficiency in handling complex healthcare tasks via two demonstrations and four use cases. Moreover, we release openCHA as open source available to the community via GitHub⋄,⋄. ⋄,⋄																																	2024-10-08	PPRN:85399079		
J	Zeng, Yongcheng; Liu, Guoqing; Ma, Weiyu; Yang, Ning; Zhang, Haifeng; Wang, Jun				Ma, Weiyu/NJS-4865-2025						Token-level Direct Preference Optimization								Arxiv											5	5;2024-08-30;https://www.arxiv.org/abs/2404.11999v5| 4;2024-06-27;https://www.arxiv.org/abs/2404.11999v4| 3;2024-06-02;https://www.arxiv.org/abs/2404.11999v3| 2;2024-05-28;https://www.arxiv.org/abs/2404.11999v2| 1;2024-04-18;https://www.arxiv.org/abs/2404.11999v1	arXiv:2404.11999			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 30 2024	2024	Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO’s superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods. 																																	2024-09-07	PPRN:88564768		
J	Bai, Yushi; Zhang, Jiajie; Lv, Xin; Zheng, Linzhi; Zhu, Siqi; Hou, Lei; Dong, Yuxiao; Tang, Jie; Li, Juanzi				Li, Zhiyuan/ESQ-7168-2022						LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs								Arxiv											1	1;2024-08-13;https://www.arxiv.org/abs/2408.07055v1	arXiv:2408.07055			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 13 2024	2024	Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT). In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets. To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words. By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality. We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities. Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models. In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability. 																																	2024-08-22	PPRN:91364919		
J	Lovenia, Holy; Dai, Wenliang; Cahyawijaya, Samuel; Ji, Ziwei; Fung, Pascale										Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models								Arxiv											2	2;2024-08-13;https://www.arxiv.org/abs/2310.05338v2| 1;2023-10-09;https://www.arxiv.org/abs/2310.05338v1	arXiv:2310.05338			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 13 2024	2024	Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects. However, the absence of a general measurement for evaluating object hallucination in VL models has hindered our understanding and ability to mitigate this issue. In this work, we present NOPE (Negative Object Presence Evaluation), a novel benchmark designed to assess object hallucination in VL models through visual question answering (VQA). We propose a cost-effective and scalable approach utilizing large language models to generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE. We extensively investigate the performance of 10 state-of-the-art VL models in discerning the non-existence of objects in visual questions, where the ground truth answers are denoted as NegP (e.g., "none"). Additionally, we evaluate their standard performance on visual questions on 9 other VQA datasets. Through our experiments, we demonstrate that no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy below 10% on NegP. Furthermore, we uncover that lexically diverse visual questions, question types with large scopes, and scene-relevant objects capitalize the risk of object hallucination in VL models.																																	2024-08-22	PPRN:85572994		
J	Choudhury, Sayantan; Gangopadhyay, Mayukh R.; Sami, M.										No-go for the formation of heavy mass Primordial Black Holes in Single Field Inflation								Arxiv											3	3;2024-08-10;https://www.arxiv.org/abs/2301.10000v6| 2;2023-12-08;https://www.arxiv.org/abs/2301.10000v5| 1;2023-01-24;https://www.arxiv.org/abs/2301.10000v1	arXiv:2301.10000			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 10 2024	2024	We examine the possibility of Primordial Black Holes (PBHs) formation in single field models of inflation. Using the adiabatic or wave function renormalization scheme in the short range modes, we show that one-loop correction to the power spectrum is free from quadratic UV divergence. We consider a framework in which PBHs are produced during the transition from Slow Roll (SR) to Ultra Slow Roll (USR) followed by the end of inflation. We demonstrate that the renormalized power spectrum soften the contribution of the logarithmic IR divergence and severely restricts the possible mass range of produced PBHs in the said transition, namely, MPBH ∼ 102 gm ala a no-go theorem. In particular, we find that the produced PBHs are short lived (t evap PBH ∼ 10−20sec) and the corresponding number of e-folds in the USR region is restricted to ∆NUSR ≈ 2.																																	2024-08-22	PPRN:35956590		
J	Song, Yifan; Yin, Da; Yue, Xiang; Huang, Jie; Li, Sujian; Lin, Bill Yuchen				Liu, Jiumeng/G-3719-2019						Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents								Arxiv											2	2;2024-07-10;https://www.arxiv.org/abs/2403.02502v2| 1;2024-03-04;https://www.arxiv.org/abs/2403.02502v1	arXiv:2403.02502			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 10 2024	2024	Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO (Rafailov et al., 2023). This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach. 																																	2024-07-21	PPRN:88032363		
J	Li, Yuhui; Wei, Fangyun; Zhang, Chao; Zhang, Hongyang										EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees								Arxiv											2	2;2024-06-30;https://www.arxiv.org/abs/2406.16858v2| 1;2024-06-24;https://www.arxiv.org/abs/2406.16858v1	arXiv:2406.16858			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 30 2024	2024	Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm.																																	2024-07-18	PPRN:89417276		
J	Zhang, Mingyang; Chen, Hao; Shen, Chunhua; Yang, Zhen; Ou, Linlin; Yu, Xinyi; Zhuang, Bohan				ZHANG, JING/HKF-4837-2023; ou, linlin/G-7964-2012						LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning								Arxiv											4	4;2024-08-07;https://www.arxiv.org/abs/2305.18403v5| 3;2024-06-20;https://www.arxiv.org/abs/2305.18403v4| 2;2023-10-03;https://www.arxiv.org/abs/2305.18403v3| 1;2023-05-28;https://www.arxiv.org/abs/2305.18403v2	arXiv:2305.18403			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead. To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We subsequently integrate this criterion into an iterative pruning process, effectively removing redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models. At a 50% compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner, achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while also decreasing memory usage by 52.6%. Besides, LoRAPrune also matches semi-structural pruning across multiple LLMs, proving its wide applicability. The code is available at https://github.com/aim-uofa/LoRAPrune.																																	2025-08-07	PPRN:72767247		
J	Yuan, Jianhao; Sun, Shuyang; Omeiza, Daniel; Zhao, Bo; Newman, Paul; Kunze, Lars; Gadd, Matthew				Kunze, Lars/HKF-7344-2023; Sun, Shuyang/JXL-8560-2024						RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2402.10828v2| 1;2024-02-16;https://www.arxiv.org/abs/2402.10828v1	arXiv:2402.10828			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 29 2024	2024	We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in MultiModal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAGDriver , a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.																																	2024-11-09	PPRN:87754337		
J	Polo, Felipe Maia; Weber, Lucas; Choshen, Leshem; Sun, Yuekai; Xu, Gongjun; Yurochkin, Mikhail										tinyBenchmarks: evaluating LLMs with fewer examples								Arxiv											2	2;2024-05-26;https://www.arxiv.org/abs/2402.14992v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.14992v1	arXiv:2402.14992			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 26 2024	2024	The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models’ abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results 1 .																																	2024-06-11	PPRN:87871108		
J	Wang, Shiyu; Wu, Haixu; Shi, Xiaoming; Hu, Tengge; Luo, Huakun; Ma, Lintao; Zhang, James Y.; Zhou, Jun				Shi, Xiaoming/AAU-4105-2021						TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14616v1	arXiv:2405.14616			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Time series forecasting is widely used in extensive applications, such as traffic planning and weather forecasting. However, real -world time series usually present intricate temporal variations, making forecasting extremely challenging. Going beyond the mainstream paradigms of plain decomposition and multiperiodicity analysis, we analyze temporal variations in a novel view of multiscale-mixing, which is based on an intuitive but important observation that time series present distinct patterns in different sampling scales. The microscopic and the macroscopic information are reflected in fine and coarse scales respectively, and thereby complex variations can be inherently disentangled. Based on this observation, we propose TimeMixer as a fully MLP-based architecture with Past-Decomposable-Mixing (PDM) and Future-Multipredictor-Mixing (FMM) blocks to take full advantage of disentangled multiscale series in both past extraction and future prediction phases. Concretely, PDM applies the decomposition to multiscale series and further mixes the decomposed seasonal and trend components in fine-to-coarse and coarse-to-fine directions separately, which successively aggregates the microscopic seasonal and macroscopic trend information. FMM further ensembles multiple predictors to utilize complementary forecasting capabilities in multiscale observations. Consequently, TimeMixer is able to achieve consistent state -of -the -art performances in both long-term and short-term forecasting tasks with favorable run -time efficiency.																																	2024-06-06	PPRN:88990473		
J	Wu, Zhengxuan; Arora, Aryaman; Wang, Zheng; Geiger, Atticus; Jurafsky, Dan; Manning, Christopher D.; Potts, Christopher				Manning, Christopher/A-1358-2007						ReFT: Representation Finetuning for Language Models								Arxiv											3	3;2024-05-22;https://www.arxiv.org/abs/2404.03592v3| 2;2024-04-08;https://www.arxiv.org/abs/2404.03592v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.03592v1	arXiv:2404.03592			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 22 2024	2024	Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights . However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15 × –65 × more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft .																																	2024-06-06	PPRN:88405561		
J	Thanasilp, Supanut; Wang, Samson; Cerezo, M.; Holmes, Zoe				Cerezo, Marco/ABD-9254-2020						Exponential concentration in quantum kernel methods								Arxiv											2	2;2024-04-14;https://www.arxiv.org/abs/2208.11060v2| 1;2022-08-23;https://www.arxiv.org/abs/2208.11060v1	arXiv:2208.11060			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 14 2024	2024	Kernel methods in Quantum Machine Learning (QML) have recently gained significant attention as a potential candidate for achieving a quantum advantage in data analysis. Among other attractive properties, when training a kernel-based model one is guaranteed to find the optimal model's parameters due to the convexity of the training landscape. However, this is based on the assumption that the quantum kernel can be efficiently obtained from quantum hardware. In this work we study the performance of quantum kernel models from the perspective of the resources needed to accurately estimate kernel values. We show that, under certain conditions, values of quantum kernels over different input data can be exponentially concentrated (in the number of qubits) towards some fixed value. Thus on training with a polynomial number of measurements, one ends up with a trivial model where the predictions on unseen inputs are independent of the input data. We identify four sources that can lead to concentration including: expressivity of data embedding, global measurements, entanglement and noise. For each source, an associated concentration bound of quantum kernels is analytically derived. Lastly, we show that when dealing with classical data, training a parametrized data embedding with a kernel alignment method is also susceptible to exponential concentration. Our results are verified through numerical simulations for several QML tasks. Altogether, we provide guidelines indicating that certain features should be avoided to ensure the efficient evaluation of quantum kernels and so the performance of quantum kernel methods.																																	2024-04-25	PPRN:12351546		
J	You, Keen; Zhang, Haotian; Schoop, Eldon; Weers, Floris; Swearngin, Amanda; Nichols, Jeffrey; Yang, Yinfei; Gan, Zhe				Zhang, Haotian/CAH-0725-2022						Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs								Arxiv											1	1;2024-04-08;https://www.arxiv.org/abs/2404.05719v1	arXiv:2404.05719			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 08 2024	2024	Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate "any resolution" on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio (i.e., horizontal division for portrait screens and vertical division for landscape screens). Both sub-images are encoded separately before being sent to LLMs. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model's reasoning ability, we further compile a dataset for advanced tasks, including detailed description, perception/interaction conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.																																	2024-04-21	PPRN:88444909		
J	Sun, Ruoxi; Arik, Sercan O.; Muzio, Alex; Miculicich, Lesly; Gundabathula, Satya; Yin, Pengcheng; Dai, Hanjun; Nakhost, Hootan; Sinha, Rajarishi; Wang, Zifeng; Pfister, Tomas				Dai, Hanjun/AAQ-8943-2021						SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended)								Arxiv											2	2;2024-03-30;https://www.arxiv.org/abs/2306.00739v4| 1;2023-05-26;https://www.arxiv.org/abs/2306.00739v1	arXiv:2306.00739			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 30 2024	2024	Text-to-SQL, the process of translating natural language into Structured Query Language (SQL), represents a transformative application of large language models (LLMs), potentially revolutionizing how humans interact with data. This paper introduces the SQL-PaLM framework, a comprehensive solution for understanding and enhancing Text-to-SQL using LLMs, using in the learning regimes of few-shot prompting and instruction fine-tuning. With few-shot prompting, we explore the effectiveness of consistency decoding with execution-based error filtering. With instruction fine-tuning, we delve deep in understanding the critical paradigms that influence the performance of tuned LLMs. In particular, we investigate how performance can be improved through expanded training data coverage and diversity, synthetic data augmentation, and integrating query-specific database content. We propose a test-time selection method to further refine accuracy by integrating SQL outputs from multiple paradigms with execution feedback as guidance. Additionally, we tackle the practical challenge of navigating intricate databases with a significant number of tables and columns, proposing efficient techniques for accurately selecting relevant database elements to enhance Text-to-SQL performance. Our holistic approach yields substantial advancements in Text-to-SQL, as demonstrated on two key public benchmarks, Spider and BIRD. Through comprehensive ablations and error analyses, we shed light on the strengths and weaknesses of our framework, offering valuable insights into Text-to-SQL's future work.																																	2024-04-17	PPRN:72812865		
J	Zhang, Shu; Yang, Xinyi; Feng, Yihao; Qin, Can; Chen, Chia-Chih; Yu, Ning; Chen, Zeyuan; Wang, Huan; Savarese, Silvio; Ermon, Stefano; Xiong, Caiming; Xu, Ran				Feng, Yihao/LVR-7524-2024; Yang, Xinyi/IVH-6916-2023; Xu, Ran/KMY-2774-2024						HIVE: Harnessing Human Feedback for Instructional Visual Editing								Arxiv											2	2;2024-03-26;https://www.arxiv.org/abs/2303.09618v2| 1;2023-03-16;https://www.arxiv.org/abs/2303.09618v1	arXiv:2303.09618			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 26 2024	2024	Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.																																	2024-04-15	PPRN:46916409		
J	Wang, Jiaqi; Shi, Enze; Yu, Sigang; Wu, Zihao; Ma, Chong; Dai, Haixing; Yang, Qiushi; Kang, Yanqing; Wu, Jinru; Hu, Huawen; Yue, Chenxi; Zhang, Haiyang; Liu, Yiheng; Pan, Yi; Liu, Zhengliang; Sun, Lichao; Li, Xiang; Ge, Bao; Jiang, Xi; Zhu, Dajiang; Yuan, Yixuan; Shen, Dinggang; Liu, Tianming; Zhang, Shu				Hu, Huawen/LIH-1776-2024; Shi, Enze/IWD-9102-2023; Li, Xiang/J-6924-2019; wu, zihao/R-8745-2019; zhang, haiyang/JOZ-2105-2023; Liu, Tianming/GLS-1211-2022; wang, jiaqi/HHS-0123-2022; Zhao, Lin/ABM-7665-2022; yuan, yixuan/KLZ-6092-2024; Ma, Chong/MIT-9373-2025						Prompt Engineering for Healthcare: Methodologies and Applications								Arxiv											2	2;2024-03-23;https://www.arxiv.org/abs/2304.14670v2| 1;2023-04-28;https://www.arxiv.org/abs/2304.14670v1	arXiv:2304.14670			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 23 2024	2024	Prompt engineering is a critical technique in the field of natural language processing that involves designing and optimizing the prompts used to input information into models, aiming to enhance their performance on specific tasks. With the recent advancements in large language models, prompt engineering has shown significant superiority across various domains and has become increasingly important in the healthcare domain. However, there is a lack of comprehensive reviews specifically focusing on prompt engineering in the medical field. This review will introduce the latest advances in prompt engineering in the field of natural language processing for the medical field. First, we will provide the development of prompt engineering and emphasize its significant contributions to healthcare natural language processing applications such as question-answering systems, text summarization, and machine translation. With the continuous improvement of general large language models, the importance of prompt engineering in the healthcare domain is becoming increasingly prominent. The aim of this article is to provide useful resources and bridges for healthcare natural language processing researchers to better explore the application of prompt engineering in this field. We hope that this review can provide new ideas and inspire for research and application in medical natural language processing.																																	2024-04-14	PPRN:66258734		
J	Borhanian, Ssohrab; Sathyaprakash, B.S.										Listening to the Universe with Next Generation Ground-Based Gravitational-Wave Detectors								Arxiv											1	1;2024-03-22;https://www.arxiv.org/abs/2202.11048v2	arXiv:2202.11048			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 22 2024	2024	In this study, we use simple performance metrics to assess the science capabilities of future ground-based gravitational-wave detector networks—composed of A+ or Voyager upgrades to the LIGO, Virgo, and KAGRA observatories and proposed next generation observatories such as Cosmic Explorer and Einstein Telescope. These metrics refer to coalescences of binary neutron stars (BNSs) and binary black holes (BBHs) and include: (i) network detection efficiency and detection rate of cosmological sources as a function of redshift, (ii) signalto-noise ratios and the accuracy with which intrinsic and extrinsic parameters would be measured, and (iii) enabling multimessenger astronomy with gravitational waves by accurate 3D localization and early warning alerts. We further discuss the science enabled by the small population of rare and extremely loud events. While imminent upgrades will provide impressive advances in all these metrics, next generation observatories will deliver an improvement of an order-of-magnitude or more in most metrics. In fact, a network containing two or three such facilities will detect half of all the BNS and BBH mergers up to a redshift of z = 1 and z = 20, respectively, give access to hundreds of BNSs and ten thousand BBHs with signal-to-noise ratios exceeding 100, readily localize hundreds to thousands of mergers to within 1 deg2 on the sky and better than 10% in luminosity distance, respectively, and consequently, enable mutlimessenger astronomy through follow-up surveys in the electromagnetic spectrum several times a week. Such networks will further shed light on potential cosmological merger populations and detect an abundance of high-fidelity BNS and BBH signals which will allow investigations of the high-density regime of matter at an unprecedented level and enable precision tests of general relativity in the strong-filed regime, respectively.																																	2024-04-13	PPRN:88261847		
J	Jin, Yang; Xu, Kun; Chen, Liwei; Liao, Chao; Tan, Jianchao; Huang, Quzhe; Chen, Bin; Lei, Chenyi; Liu, An; Song, Chengru; Lei, Xiaoqiang; Zhang, Di; Ou, Wenwu; Gai, Kun; Mu, Yadong				Chen, Li-Wei/GVT-8217-2022; Lei, Xiaoqiang/OTI-4287-2025						Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization								Arxiv											3	3;2024-03-22;https://www.arxiv.org/abs/2309.04669v3| 2;2023-09-29;https://www.arxiv.org/abs/2309.04669v2| 1;2023-09-09;https://www.arxiv.org/abs/2309.04669v1	arXiv:2309.04669			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 22 2024	2024	Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model’s potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well -designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi -modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision -language tasks. 																																	2024-04-13	PPRN:84950297		
J	He, Tairan; Luo, Zhengyi; Xiao, Wenli; Zhang, Chong; Kitani, Kris; Liu, Changliu; Shi, Guanya										Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.04436v1	arXiv:2403.04436			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable "sim-to-data" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.																																	2024-04-05	PPRN:88062984		
J	Xu, Yuhao; Gu, Tao; Chen, Weifeng; Chen, Chengcai				xu, yuhao/OIU-9639-2025						OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on								Arxiv											2	2;2024-03-07;https://www.arxiv.org/abs/2403.01779v2| 1;2024-03-04;https://www.arxiv.org/abs/2403.01779v1	arXiv:2403.01779			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 07 2024	2024	We present OOT Diffusion, a novel network architecture for realistic and controllable image -based virtual try -on (VTON). We leverage the power of pretrained latent diffusion models, designing an outfitting UNet to learn the garment detail features. Without a redundant warping process, the garment features are precisely aligned with the target human body via the proposed outfitting fusion in the self -attention layers of the denoising UNet. In order to further enhance the controllability, we introduce outfitting dropout to the training process, which enables us to adjust the strength of the garment features through classifier -free guidance. Our comprehensive experiments on the VITON-HD and Dress Code datasets demonstrate that OOT Diffusion efficiently generates high quality try -on results for arbitrary human and garment images, which outperforms other VTON methods in both realism and controllability, indicating an impressive breakthrough in virtual try -on. 																																	2024-04-05	PPRN:88022668		
J	Tarzanagh, Davoud Ataee; Li, Yingcong; Thrampoulidis, Christos; Oymak, Samet				Tryfonopoulos, Christos/AAL-8960-2021						Transformers as Support Vector Machines								Arxiv											3	3;2024-02-22;https://www.arxiv.org/abs/2308.16898v3| 2;2023-09-07;https://www.arxiv.org/abs/2308.16898v2| 1;2023-08-31;https://www.arxiv.org/abs/2308.16898v1	arXiv:2308.16898			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	Since its inception in “Attention Is All You Need”, the transformer architecture has led to revolutionary advancements in natural language processing. The attention layer within the transformer admits a sequence of input tokens X and makes them interact through pairwise similarities computed as softmax(XQK⊤X⊤), where (K, Q) are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard -margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent, as follows. (1) Optimizing the attention layer, parameterized by (K, Q), with vanishing regularization, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter W := KQ⊤. Instead, directly parameterizing by W minimizes a Frobenius norm SVM objective. We characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones. (2) Complementing this, for W-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. Our findings apply to general datasets, trivially extend to cross-attention layer, and their practical validity is verified via thorough numerical experiments. We also introduce open problems and future research directions. We believe these findings inspire a new perspective, interpreting multilayer transformers as a hierarchy of SVMs that separates and selects optimal tokens.																																	2024-03-21	PPRN:84617374		
J	Gao, Chongyang; Chen, Kezhen; Rao, Jinmeng; Sun, Baochen; Liu, Ruibo; Peng, Daiyi; Zhang, Yawen; Guo, Xiaoyuan; Yang, Jie; Subrahmanian, V S				Sun, Baochen/W-9863-2019; Subrahmanian, Venkatramanan/ABA-7399-2021; Guo, Xiaoyuan/AAP-1101-2021						Higher Layers Need More LoRA Experts								Arxiv											1	1;2024-02-13;https://www.arxiv.org/abs/2402.08562v1	arXiv:2402.08562			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 13 2024	2024	Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, textit{textbf{M}oE-Ltextbf{o}RA with textbf{L}ayer-wise Expert textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines. We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code is available at https://github.com/GCYZSL/MoLA.																																	2024-05-25	PPRN:87675399		
J	Ma, Wei; Liu, Shangqing; Lin, Zhihao; Wang, Wenhan; Hu, Qiang; Zhang, Cen; Liu, Ye; Li, Li; Nie, Liming; Liu, Yang				w, wh/IAP-2639-2023; Lin, Zhi-Hao/HQZ-9228-2023; Liu, Yang/D-2306-2013; Ma, Wei/HJY-8389-2023; Hu, Qiang/AAJ-9438-2020						LMs: Understanding Code Syntax and Semantics for Code Analysis								Arxiv											4	4;2024-02-13;https://www.arxiv.org/abs/2305.12138v4| 3;2024-02-07;https://www.arxiv.org/abs/2305.12138v3| 2;2023-10-20;https://www.arxiv.org/abs/2305.12138v2| 1;2023-05-20;https://www.arxiv.org/abs/2305.12138v1	arXiv:2305.12138			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 13 2024	2024	Large language models~(LLMs) demonstrate significant potential to revolutionize software engineering (SE) by exhibiting outstanding performance in SE tasks such as code and document generation. However, the high reliability and risk control requirements in software engineering raise concerns about the lack of interpretability of LLMs. To address this concern, we conducted a study to evaluate the capabilities of LLMs and their limitations for code analysis in SE. We break down the abilities needed for artificial intelligence~(AI) models to address SE tasks related to code analysis into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on the ability of LLMs to comprehend code syntax and semantic structures, which include abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We employed four state-of-the-art foundational models, GPT4, GPT3.5, StarCoder and CodeLlama-13b-instruct. We assessed the performance of LLMs on cross-language tasks involving C, Java, Python, and Solidity.   Our findings revealed that while LLMs have a talent for understanding code syntax, they struggle with comprehending code semantics, particularly dynamic semantics. We conclude that LLMs possess capabilities similar to an Abstract Syntax Tree (AST) parser, demonstrating initial competencies in static code analysis. Furthermore, our study highlights that LLMs are susceptible to hallucinations when interpreting code semantic structures and fabricating nonexistent facts. These results indicate the need to explore methods to verify the correctness of LLM output to ensure its dependability in SE. More importantly, our study provides an initial answer to why the codes generated by LLM are usually syntax-correct but vulnerable.																																	2024-02-29	PPRN:70795461		
J	Zhang, Fred; Nanda, Neel										Towards Best Practices of Activation Patching in Language Models: Metrics and Methods								Arxiv											2	2;2024-01-17;https://www.arxiv.org/abs/2309.16042v2| 1;2023-09-27;https://www.arxiv.org/abs/2309.16042v1	arXiv:2309.16042			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 17 2024	2024	Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization—identifying the important model components—is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyper parameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.																																	2024-05-25	PPRN:85323811		
J	Han, Haoyu; Wang, Yu; Shomer, Harry; Guo, Kai; Ding, Jiayuan; Lei, Yongjia; Halappanavar, Mahantesh; Rossi, Ryan A.; Mukherjee, Subhabrata; Tang, Xianfeng; He, Qi; Hua, Zhigang; Long, Bo; Zhao, Tong; Shah, Neil; Javari, Amin; Xia, Yinglong; Tang, Jiliang				Lei, Yongjia/ABC-2357-2020; Tang, Xianfeng/IWM-0393-2023						Retrieval-Augmented Generation with Graphs (GraphRAG)								Arxiv											1	1;2024-12-31;https://www.arxiv.org/abs/2501.00309v1	arXiv:2501.00309			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 31 2024	2024	Retrieval-augmented generation (RAG) is a powerful technique that enhances downstream task execution by retrieving additional information, such as knowledge, skills, and tools from external sources. Graph, by its intrinsic "nodes connected by edges" nature, encodes massive heterogeneous and relational information, making it a golden resource for RAG in tremendous real-world applications. As a result, we have recently witnessed increasing attention on equipping RAG with Graph, i.e., GraphRAG. However, unlike conventional RAG, where the retriever, generator, and external data sources can be uniformly designed in the neural-embedding space, the uniqueness of graph-structured data, such as diverse-formatted and domain-specific relational knowledge, poses unique and significant challenges when designing GraphRAG for different domains. Given the broad applicability, the associated design challenges, and the recent surge in GraphRAG, a systematic and up-to-date survey of its key concepts and techniques is urgently desired. Following this motivation, we present a comprehensive and up-to-date survey on GraphRAG. Our survey first proposes a holistic GraphRAG framework by defining its key components, including query processor, retriever, organizer, generator, and data source. Furthermore, recognizing that graphs in different domains exhibit distinct relational patterns and require dedicated designs, we review GraphRAG techniques uniquely tailored to each domain. Finally, we discuss research challenges and brainstorm directions to inspire cross-disciplinary opportunities. 																																	2025-01-24	PPRN:120258092		
J	Chakraborty, Souradip; Qiu, Jiahao; Yuan, Hui; Koppel, Alec; Manocha, Dinesh; Huang, Furong; Bedi, Amrit Singh; Wang, Mengdi				Koppel, Alec/ABC-5438-2020						MaxMin-RLHF: Alignment with Diverse Human Preferences								Arxiv											2	2;2024-12-26;https://www.arxiv.org/abs/2402.08925v2| 1;2024-02-14;https://www.arxiv.org/abs/2402.08925v1	arXiv:2402.08925			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 26 2024	2024	Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.																																	2025-02-15	PPRN:87684195		
J	He, Jia; Rungta, Mukund; Koleczek, David; Sekhon, Arshdeep; Wang, Franklin X; Hasan, Sadid										Does Prompt Formatting Have Any Impact on LLM Performance?								Arxiv											1	1;2024-11-15;https://www.arxiv.org/abs/2411.10541v1	arXiv:2411.10541			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 15 2024	2024	In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI's GPT models. Experiments show that GPT-3.5-turbo's performance varies by up to 40% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.																																	2024-12-28	PPRN:119262153		
J	Cortes, Marina; Liddle, Andrew R										Interpreting DESI's evidence for evolving dark energy								Arxiv											3	3;2024-11-08;https://www.arxiv.org/abs/2404.08056v3| 2;2024-04-28;https://www.arxiv.org/abs/2404.08056v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.08056v1	arXiv:2404.08056			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 08 2024	2024	The latest results on baryon acoustic oscillations from DESI (Dark Energy Spectroscopic Instrument), when combined with cosmic microwave background and supernova data, show indications of a deviation from a cosmological constant in favour of evolving dark energy. Use of a pivot scale for the equation of state w shows that this evidence is concentrated in the derivative of w rather than its mean offset from −1, indicating a new cosmic coincidence where the mean equation of state matches that of the Λ CDM model precisely in the region probed by the observations. An equivalent way to express this is to say that the dark energy hits the maximum value that it will ever achieve within the observed window. We argue that conclusions on dark energy evolution are strongly driven by the assumed parameter priors and that this coincidence, which we are naming the PhantomX coincidence (where X stands for crossing), may be a signature of this.																																	2024-12-18	PPRN:88529259		
J	Wang, Bin; Xu, Chao; Zhao, Xiaomeng; Ouyang, Linke; Wu, Fan; Zhao, Zhiyuan; Xu, Rui; Liu, Kaiwen; Qu, Yuan; Shang, Fukai; Zhang, Bo; Wei, Liqun; Sui, Zhihao; Li, Wei; Shi, Botian; Qiao, Yu; Lin, Dahua; He, Conghui				Wang, Bin/MVU-8917-2025; He, Conghui/AAZ-3323-2021; Qiao, Yu/ABD-5787-2021; Lin, Dahua/W-6576-2019; LIQUN, WEI/ABF-2938-2020; Shi, Botian/HTT-0363-2023; Liu, Kaiwen/HPC-0015-2023						MinerU: An Open-Source Solution for Precise Document Content Extraction								Arxiv											1	1;2024-09-27;https://www.arxiv.org/abs/2409.18839v1	arXiv:2409.18839			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 27 2024	2024	Document content analysis has been a crucial research area in computer vision. Despite significant advancements in methods such as OCR, layout detection, and formula recognition, existing open-source solutions struggle to consistently deliver high-quality content extraction due to the diversity in document types and content. To address these challenges, we present MinerU, an open-source solution for high-precision document content extraction. MinerU leverages the sophisticated PDF-Extract-Kit models to extract content from diverse documents effectively and employs finely-tuned preprocessing and postprocessing rules to ensure the accuracy of the final results. Experimental results demonstrate that MinerU consistently achieves high performance across various document types, significantly enhancing the quality and consistency of content extraction.																																	2024-10-09	PPRN:100701902		
J	Ankner, Zachary; Paul, Mansheej; Cui, Brandon; Chang, Jonathan D.; Ammanabrolu, Prithviraj										Critique-out-Loud Reward Models								Arxiv											1	1;2024-08-21;https://www.arxiv.org/abs/2408.11791v1	arXiv:2408.11791			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 21 2024	2024	Traditionally, reward models used for reinforcement learning from human feedback (RLHF) are trained to directly predict preference scores without leveraging the generation capabilities of the underlying large language model (LLM). This limits the capabilities of reward models as they must reason implicitly about the quality of a response, i.e., preference modeling must be performed in a single forward pass through the model. To enable reward models to reason explicitly about the quality of a response, we introduce Critique-out-Loud (CLoud) reward models. CLoud reward models operate by first generating a natural language critique of the assistant's response that is then used to predict a scalar reward for the quality of the response. We demonstrate the success of CLoud reward models for both Llama-3-8B and 70B base models: compared to classic reward models CLoud reward models improve pairwise preference classification accuracy on RewardBench by 4.65 and 5.84 percentage points for the 8B and 70B base models respectively. Furthermore, CLoud reward models lead to a Pareto improvement for win rate on ArenaHard when used as the scoring model for Best-of-N. Finally, we explore how to exploit the dynamic inference compute capabilities of CLoud reward models by performing self-consistency decoding for reward prediction.																																	2024-08-31	PPRN:91504044		
J	Mishra, Abhika; Asai, Akari; Balachandran, Vidhisha; Wang, Yizhong; Neubig, Graham; Tsvetkov, Yulia; Hajishirzi, Hannaneh										Fine-grained Hallucination Detection and Editing for Language Models								Arxiv											3	3;2024-08-12;https://www.arxiv.org/abs/2401.06855v4| 2;2024-02-21;https://www.arxiv.org/abs/2401.06855v3| 1;2024-01-17;https://www.arxiv.org/abs/2401.06855v2	arXiv:2401.06855			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 12 2024	2024	Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality 1																																	2024-08-22	PPRN:87209436		
J	Lang, Simon; Alexe, Mihai; Chantry, Matthew; Dramsch, Jesper; Pinault, Florian; Raoult, Baudouin; Clare, Mariana C.A.; Lessig, Christian; Maier-Gerber, Michael; Magnusson, Linus; Ben Bouallegue, Zied; Nemesio, Ana Prieto; Dueben, Peter D.; Brown, Andrew; Pappenberger, Florian; Rabier, Florence				Clare, Mariana/MGV-8166-2025; Pappenberger, Florian/A-2839-2009						AIFS -- ECMWF's data-driven forecasting system								Arxiv											2	2;2024-08-07;https://www.arxiv.org/abs/2406.01465v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01465v1	arXiv:2406.01465			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 07 2024	2024	Machine learning-based weather forecasting models have quickly emerged as a promising methodology for accurate medium-range global weather forecasting. Here, we introduce the Artificial Intelligence Forecasting System (AIFS), a data driven forecast model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS is based on a graph neural network (GNN) encoder and decoder, and a sliding window transformer processor, and is trained on ECMWF's ERA5 re-analysis and ECMWF's operational numerical weather prediction (NWP) analyses. It has a flexible and modular design and supports several levels of parallelism to enable training on high-resolution input data. AIFS forecast skill is assessed by comparing its forecasts to NWP analyses and direct observational data. We show that AIFS produces highly skilled forecasts for upper-air variables, surface weather parameters and tropical cyclone tracks. AIFS is run four times daily alongside ECMWF's physics-based NWP model and forecasts are available to the public under ECMWF's open data policy.																																	2024-08-17	PPRN:89154979		
J	Sheshadri, Abhay; Ewart, Aidan; Guo, Phillip; Lynch, Aengus; Wu, Cindy; Hebbar, Vivek; Sleight, Henry; Stickland, Asa Cooper; Perez, Ethan; Hadfield-Menell, Dylan; Casper, Stephen										Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs								Arxiv											1	1;2024-07-22;https://www.arxiv.org/abs/2407.15549v1	arXiv:2407.15549			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 22 2024	2024	Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of `jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.2																																	2024-07-28	PPRN:91026218		
J	Nguyen, Xuan-Phi; Zhang, Wenxuan; Li, Xin; Aljunied, Mahani; Hu, Zhiqiang; Shen, Chenhui; Chia, Yew Ken; Li, Xingxuan; Wang, Jianyu; Tan, Qingyu; Cheng, Liying; Chen, Guanzheng; Deng, Yue; Yang, Sen; Liu, Chaoqun; Zhang, Hang; Bing, Lidong				Hu, Zhiqiang/HIR-5043-2022; Wang, Yilong/HQZ-1949-2023; Zhang, Wenxuan/LKN-9746-2024; Cheng, Liying/KOC-6988-2024						SeaLLMs -- Large Language Models for Southeast Asia								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2312.00738v2| 1;2023-12-01;https://www.arxiv.org/abs/2312.00738v1	arXiv:2312.00738			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.																																	2024-07-18	PPRN:86357292		
J	Xu, Rui; Yang, Shu; Wang, Yihui; Cai, Yu; Du, Bo; Chen, Hao				Wang, Yihui/OLQ-4655-2025; Xu, Rui/AAD-7507-2020						Visual Mamba: A Survey and New Outlooks								Arxiv											2	2;2024-04-29;https://www.arxiv.org/abs/2404.18861v1| 1;2024-07-01;	arXiv:2404.18861			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Mamba, a recent selective structured state space model, excels in long sequence modeling, which is vital in the large model era. Long sequence modeling poses significant challenges, including capturing long-range dependencies within the data and handling the computational demands caused by their extensive length. Mamba addresses these challenges by overcoming the local perception limitations of convolutional neural networks and the quadratic computational complexity of Transformers. Given its advantages over these mainstream foundation architectures, Mamba exhibits great potential to be a visual foundation architecture. Since January 2024, Mamba has been actively applied to diverse computer vision tasks, yielding numerous contributions. To help keep pace with the rapid advancements, this paper reviews visual Mamba approaches, analyzing over 200 papers. This paper begins by delineating the formulation of the original Mamba model. Subsequently, it delves into representative backbone networks, and applications categorized using different modalities, including image, video, point cloud, and multi-modal. Particularly, we identify scanning techniques as critical for adapting Mamba to vision tasks, and decouple these scanning techniques to clarify their functionality and enhance their flexibility across various applications. Finally, we discuss the challenges and future directions, providing insights into new outlooks in this fast evolving area. 																																	2024-11-17	PPRN:88697074		
J	Chen, Zhaorun; Zhao, Zhuokai; Luo, Hongyin; Yao, Huaxiu; Li, Bo; Zhou, Jiawei				Yao, Huaxiu/V-3516-2019; Chen, Zhaorun/AAT-1611-2021; Zhao, Zhuokai/JLL-0434-2023						HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2403.00425v2| 1;2024-03-01;https://www.arxiv.org/abs/2403.00425v1	arXiv:2403.00425			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.																																	2024-07-04	PPRN:87998034		
J	Rasheed, Hanoona; Maaz, Muhammad; Shaji, Sahal; Shaker, Abdelrahman; Khan, Salman; Cholakkal, Hisham; Anwer, Rao M.; Xing, Erix; Yang, Ming-Hsuan; Khan, Fahad S.				Maaz, Muhammad/GOK-1100-2022; Khan, Salman/M-4834-2016; Cholakkal, Hisham/AAC-2122-2022; Yang, Ming-Hsuan/T-9533-2019; Khan, Fahad Shahbaz/ABD-6646-2021; Shaker, Abdelrahman/AAA-8435-2021						GLaMM: Pixel Grounding Large Multimodal Model								Arxiv											3	3;2024-06-02;https://www.arxiv.org/abs/2311.03356v3| 2;2023-12-29;https://www.arxiv.org/abs/2311.03356v2| 1;2023-11-06;https://www.arxiv.org/abs/2311.03356v1	arXiv:2311.03356			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 02 2024	2024	Large Multimodal Models (LMMs) extend Large Language Models to the vision domain. Initial LMMs used holistic images and text prompts to generate ungrounded textual responses. Recently, region-level LMMs have been used to generate visually grounded responses. However, they are limited to only referring to a single object category at a time, require users to specify the regions, or cannot offer dense pixel-wise object grounding. In this work, we present Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks. GLaMM not only grounds objects appearing in the conversations but is flexible enough to accept both textual and optional visual prompts (region of interest) as input. This empowers users to interact with the model at various levels of granularity, both in textual and visual domains. Due to the lack of standard benchmarks for the novel setting of visually Grounded Conversation Generation (GCG), we introduce a comprehensive evaluation protocol with our curated grounded conversations. Our proposed GCG task requires densely grounded concepts in natural scenes at a large-scale. To this end, we propose a densely annotated Grounding-anything Dataset (GranD) using our proposed automated annotation pipeline that encompasses 7.5M unique concepts grounded in a total of 810M regions available with segmentation masks. Besides GCG, GLaMM also performs effectively on several downstream tasks, e.g., referring expression segmentation, image and region-level captioning and vision-language conversations.																																	2024-06-19	PPRN:86053830		
J	Kim, Yubin; Xu, Xuhai; McDuff, Daniel; Breazeal, Cynthia; Park, Hae Won				Xu, Xuhai/JQK-5168-2023						Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data								Arxiv											2	2;2024-04-27;https://www.arxiv.org/abs/2401.06866v2| 1;2024-01-12;https://www.arxiv.org/abs/2401.06866v1	arXiv:2401.06866			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 27 2024	2024	Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is crucial. This paper investigates the capacity of LLMs to make inferences about health based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of 12 state-of-the-art LLMs with prompting and fine-tuning techniques on four public health datasets (PMData, LifeSnaps, GLOBEM and AW_FB). Our experiments cover 10 consumer health prediction tasks in mental health, activity, metabolic, and sleep assessment. Our fine-tuned model, HealthAlpaca exhibits comparable performance to much larger models (GPT-3.5, GPT-4 and Gemini-Pro), achieving the best performance in 8 out of 10 tasks. Ablation studies highlight the effectiveness of context enhancement strategies. Notably, we observe that our context enhancement can yield up to 23.8% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance.																																	2024-05-15	PPRN:87196220		
J	Rao, Abhinav; Vashistha, Sachin; Naik, Atharva; Aditya, Somak; Choudhury, Monojit				Rao, Abhinav/LSL-1984-2024						Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks								Arxiv											4	4;2024-03-27;https://www.arxiv.org/abs/2305.14965v4| 3;2024-03-25;https://www.arxiv.org/abs/2305.14965v3| 2;2024-02-26;https://www.arxiv.org/abs/2305.14965v2| 1;2023-05-24;https://www.arxiv.org/abs/2305.14965v1	arXiv:2305.14965			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 27 2024	2024	Recent explorations with commercial Large Language Models (LLMs) have shown that non -expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open -source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For further analysis, we release a dataset of model outputs across 3700 jailbreak prompts over 4 tasks.																																	2024-04-14	PPRN:72716810		
J	Luo, Ziwei; Gustafsson, Fredrik K.; Zhao, Zheng; Sjolund, Jens; Schon, Thomas B.				Zhao, Zheng/LGZ-3452-2024; Schön, Thomas/D-4169-2009; Sjolund, Jens/OML-6870-2025						Controlling Vision-Language Models for Multi-Task Image Restoration								Arxiv											2	2;2024-02-28;https://www.arxiv.org/abs/2310.01018v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01018v1	arXiv:2310.01018			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a universal framework for image restoration. More specifically, DA-CLIP trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings. By integrating the embedding into an image restoration network via cross-attention, we are able to pilot the model to learn a high-fidelity image reconstruction. The controller itself will also output a degradation feature that matches the real corruptions of the input, yielding a natural classifier for different degradation types. In addition, we construct a mixed degradation dataset with synthetic captions for DA-CLIP training. Our approach advances state-of-the-art performance on both degradation-specific and unified image restoration tasks, showing a promising direction of prompting image restoration with large-scale pretrained vision-language models.																																	2024-03-28	PPRN:85350254		
J	Maharana, Adyasha; Lee, Dong-Ho; Tulyakov, Sergey; Bansal, Mohit; Barbieri, Francesco; Fang, Yuwei				Fang, Yuwei/AAX-5314-2020; Bansal, Mohit/Q-9105-2016						Evaluating Very Long-Term Conversational Memory of LLM Agents								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17753v1	arXiv:2402.17753			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 27 2024	2024	Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.																																	2024-11-09	PPRN:87920843		
J	Singh, Aaditya K.; Strouse, Dj										Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs								Arxiv											1	1;2024-02-22;https://www.arxiv.org/abs/2402.14903v1	arXiv:2402.14903			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.																																	2024-11-09	PPRN:87871033		
J	Tseng, Gabriel; Cartuyvels, Ruben; Zvonkov, Ivan; Purohit, Mirali; Rolnick, David; Kerner, Hannah										Lightweight, Pre-trained Transformers for Remote Sensing Timeseries								Arxiv											3	3;2024-02-05;https://www.arxiv.org/abs/2304.14065v4| 2;2023-09-30;https://www.arxiv.org/abs/2304.14065v3| 1;2023-04-27;https://www.arxiv.org/abs/2304.14065v1	arXiv:2304.14065			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 05 2024	2024	Machine learning methods for satellite data have a range of societally relevant applications, but labels used to train models can be difficult or impossible to acquire. Self-supervision is a natural solution in settings with limited labeled data, but current self-supervised models for satellite data fail to take advantage of the characteristics of that data, including the temporal dimension (which is critical for many applications, such as monitoring crop growth) and availability of data from many complementary sensors (which can significantly improve a model's predictive performance). We present Presto (the Pretrained Remote Sensing Transformer), a model pre-trained on remote sensing pixel-timeseries data. By designing Presto specifically for remote sensing data, we can create a significantly smaller but performant model. Presto excels at a wide variety of globally distributed remote sensing tasks and performs competitively with much larger models while requiring far less compute. Presto can be used for transfer learning or as a feature extractor for simple models, enabling efficient deployment at scale.																																	2024-05-25	PPRN:65711627		
J	Lyth, Dan; King, Simon										Natural language guidance of high-fidelity text-to-speech with synthetic annotations								Arxiv											2	2;2024-02-02;https://www.arxiv.org/abs/2402.01912v1| 1;2024-02-02;https://www.arxiv.org/abs/2402.01912v1	arXiv:2402.01912			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Feb 02 2024	2024	Text -to -speech models trained on large-scale datasets have demonstrated impressive in -context learning capabilities and naturalness. However, control of speaker identity and style in these models typically requires conditioning on reference speech recordings, limiting creative applications. Alternatively, natural language prompting of speaker identity and style has demonstrated promising results and provides an intuitive method of control. However, reliance on human -labeled descriptions prevents scaling to large datasets. Our work bridges the gap between these two approaches. We propose a scalable method for labeling various aspects of speaker identity, style, and recording conditions. We then apply this method to a 45k hour dataset, which we use to train a speech language model. Furthermore, we propose simple methods for increasing audio fidelity, significantly outperforming recent work despite relying entirely on found data. Our results demonstrate high-fidelity speech generation in a diverse range of accents, prosodic styles, channel conditions, and acoustic conditions, all accomplished with a single model and intuitive natural language conditioning. Audio samples can be heard at https:// text-description-to-speech.com/.																																	2024-02-23	PPRN:87523916		
J	Sun, Jiankai; Zheng, Chuanyang; Xie, Enze; Liu, Zhengying; Chu, Ruihang; Qiu, Jianing; Xu, Jiaqi; Ding, Mingyu; Li, Hongyang; Geng, Mengzhe; Wu, Yue; Wang, Wenhai; Chen, Junsong; Yin, Zhangyue; Ren, Xiaozhe; Fu, Jie; He, Junxian; Yuan, Wu; Liu, Qi; Liu, Xihui; Li, Yu; Dong, Hao; Cheng, Yu; Zhang, Ming; Heng, Pheng Ann; Dai, Jifeng; Luo, Ping; Wang, Jingdong; Wen, Ji-Rong; Qiu, Xipeng; Guo, Yike; Xiong, Hui; Liu, Qun; Li, Zhenguo				LIU, Qi/I-4900-2017; HE, Junxian/OHV-2278-2025; Chen, Junsong/HII-4662-2022; Dong, Hao/LWZ-8645-2024; Dai, Jifeng/HGU-8741-2022; Qiu, Jianing/ISV-1514-2023; pluo/GPG-2707-2022; Li, Hongyang/ABD-7455-2020; Yuan, Wu/E-8847-2010; Liu, Xihui/LHA-5141-2024; Zhang, Yinyan/S-7675-2019						A Survey of Reasoning with Foundation Models								Arxiv											3	3;2024-01-25;https://www.arxiv.org/abs/2312.11562v5| 2;2023-12-26;https://www.arxiv.org/abs/2312.11562v4| 1;2023-12-21;https://www.arxiv.org/abs/2312.11562v3	arXiv:2312.11562			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.																																	2024-02-24	PPRN:86786969		
J	Zhou, Wenxuan; Zhang, Sheng; Gu, Yu; Chen, Muhao; Poon, Hoifung				Chen, Muhao/AAA-3634-2021; Gu, Yu/LEL-7846-2024; Zhou, Wenxuan/AAN-4529-2020						UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition								Arxiv											2	2;2024-01-19;https://www.arxiv.org/abs/2308.03279v2| 1;2023-08-07;https://www.arxiv.org/abs/2308.03279v1	arXiv:2308.03279			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 19 2024	2024	Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.																																	2024-05-25	PPRN:74299835		
J	Li, Junyi; Chen, Jie; Ren, Ruiyang; Cheng, Xiaoxue; Zhao, Wayne Xin; Nie, Jian-Yun; Wen, Ji-Rong				Xia, Lianghao/IWV-0954-2023						The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models								Arxiv											1	1;2024-01-06;https://www.arxiv.org/abs/2401.03205v1	arXiv:2401.03205			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 06 2024	2024	In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs. Our code and data can be accessed at https://github.com/ RUCAIBox/HaluEval-2.0.																																	2024-01-23	PPRN:87032938		
J	Zhu, Zhaocheng; Xue, Yuan; Chen, Xinyun; Zhou, Denny; Tang, Jian; Schuurmans, Dale; Dai, Hanjun				Dai, Hanjun/AAQ-8943-2021; Chen, Xinyun/ABZ-9877-2022						Large Language Models can Learn Rules								Arxiv											3	3;2024-12-19;https://www.arxiv.org/abs/2310.07064v3| 2;2024-04-24;https://www.arxiv.org/abs/2310.07064v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.07064v1	arXiv:2310.07064			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 19 2024	2024	When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often generate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on relational reasoning, numerical reasoning and concept learning problems show that HtT improves existing prompting methods, with an absolute gain of 10-30% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem. [Graphics]																																	2025-01-29	PPRN:85568711		
J	Huang, Ziqi; Wu, Tianxing; Jiang, Yuming; Chan, Kelvin C.K.; Liu, Ziwei				Liu, Ziwei/AAG-6939-2021						ReVersion: Diffusion-Based Relation Inversion from Images								Arxiv											2	2;2024-12-01;https://www.arxiv.org/abs/2303.13495v2| 1;2023-03-23;https://www.arxiv.org/abs/2303.13495v1	arXiv:2303.13495			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 01 2024	2024	Diffusion models gain increasing popularity for their generative capabilities. Recently, there have been surging needs to generate customized images by inverting diffusion models from exemplar images, and existing inversion methods mainly focus on capturing object appearances (i.e., the "look"). However, how to invert object relations, another important pillar in the visual world, remains unexplored. In this work, we propose the Relation Inversion task, which aims to learn a specific relation (represented as "relation prompt") from exemplar images. Specifically, we learn a relation prompt with a frozen pre-trained text-to-image diffusion model. The learned relation prompt can then be applied to generate relation-specific images with new objects, backgrounds, and styles. To tackle the Relation Inversion task, we propose the ReVersion Framework. Specifically, we propose a novel "relation-steering contrastive learning" scheme to steer the relation prompt towards relation-dense regions, and disentangle it away from object appearances. We further devise "relation-focal importance sampling" to emphasize high-level interactions over low-level appearances (e.g., texture, color). To comprehensively evaluate this new task, we contribute the ReVersion Benchmark, which provides various exemplar images with diverse relations. Extensive experiments validate the superiority of our approach over existing methods across a wide range of visual relations. Our proposed task and method could be good inspirations for future research in various domains like generative inversion, few-shot learning, and visual relation detection.																																	2025-01-11	PPRN:47328845		
J	Kynkaanniemi, Tuomas; Aittala, Miika; Karras, Tero; Laine, Samuli; Aila, Timo; Lehtinen, Jaakko				Lehtinen, Jaakko/G-2328-2013						Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models								Arxiv											2	2;2024-11-06;https://www.arxiv.org/abs/2404.07724v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.07724v1	arXiv:2404.07724			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 06 2024	2024	Guidance is a crucial technique for extracting the best performance out of image- generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.																																	2024-12-16	PPRN:88503197		
J	Shu, Yan; Zhang, Peitian; Liu, Zheng; Qin, Minghao; Zhou, Junjie; Huang, Tiejun; Zhao, Bo				Shu, Yan/OAJ-4823-2025; qin, minghao/JLM-4650-2023; Liu, Zheng/AHI-3660-2022; Z, J/GZG-3471-2022						Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding								Arxiv											2	2;2024-10-18;https://www.arxiv.org/abs/2409.14485v3| 1;2024-09-24;https://www.arxiv.org/abs/2409.14485v2	arXiv:2409.14485			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 18 2024	2024	Although current Multi-modal Large Language Models (MLLMs) demonstrate promising results in video understanding, processing extremely long videos remains an ongoing challenge. Typically, MLLMs struggle with handling thousands of visual tokens that exceed the maximum context length, and they suffer from the information decay due to token aggregation. Another challenge is the high computational cost stemming from the large number of video tokens. To tackle these issues, we propose Video-XL, an extra-long vision language model designed for efficient hour-scale video understanding. Specifically, we argue that LLMs can be adapted as effective visual condensers and propose Visual Context Latent Summarization which condenses visual contexts into highly compact forms. Extensive experiments demonstrate that our model achieves promising results on popular long video understanding benchmarks. For example, Video-XL outperforms the current state-of-the-art method on VNBench by nearly 10% in accuracy. Moreover, Video-XL presents an impressive balance between efficiency and effectiveness, processing 2048 frames on a single 80GB GPU while achieving nearly 95% accuracy in the Needle-in-a-Haystack evaluation.																																	2024-11-17	PPRN:98864265		
J	Carta, Thomas; Romac, Clement; Wolf, Thomas; Lamprier, Sylvain; Sigaud, Olivier; Oudeyer, Pierre-Yves										Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning								Arxiv											3	3;2024-10-17;https://www.arxiv.org/abs/2302.02662v4| 2;2023-09-06;https://www.arxiv.org/abs/2302.02662v3| 1;2023-05-12;https://www.arxiv.org/abs/2302.02662v2	arXiv:2302.02662			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 17 2024	2024	Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.																																	2024-11-13	PPRN:69155359		
J	Zhang, Peitian; Liu, Zheng; Xiao, Shitao; Shao, Ninglu; Ye, Qiwei; Dou, Zhicheng				Dou, Zhicheng/OBO-6932-2025; Liu, Zheng/AHI-3660-2022						Long Context Compression with Activation Beacon								Arxiv											3	3;2024-10-11;https://www.arxiv.org/abs/2401.03462v3| 2;2024-02-02;https://www.arxiv.org/abs/2401.03462v2| 1;2024-01-07;https://www.arxiv.org/abs/2401.03462v1	arXiv:2401.03462			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 11 2024	2024	Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. 1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts). 2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. 3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance. 4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, achieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache. 																																	2024-11-04	PPRN:87070845		
J	Xiong, Siheng; Payani, Ali; Kompella, Ramana; Fekri, Faramarz										Large Language Models Can Learn Temporal Reasoning								Arxiv											6	6;2024-10-08;https://www.arxiv.org/abs/2401.06853v6| 5;2024-06-11;https://www.arxiv.org/abs/2401.06853v5| 4;2024-06-01;https://www.arxiv.org/abs/2401.06853v4| 3;2024-04-22;https://www.arxiv.org/abs/2401.06853v3| 2;2024-02-20;https://www.arxiv.org/abs/2401.06853v2| 1;2024-01-12;https://www.arxiv.org/abs/2401.06853v1	arXiv:2401.06853			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 08 2024	2024	While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards languagebased TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation. 1																																	2024-10-30	PPRN:87196186		
J	Wang, Xidong; Song, Dingjie; Chen, Shunian; Zhang, Chen; Wang, Benyou				Wang, Benyou/Y-5146-2019; Wang, Xidong/IZD-5718-2023						Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture								Arxiv											2	2;2024-10-03;https://www.arxiv.org/abs/2409.02889v2| 1;2024-09-04;https://www.arxiv.org/abs/2409.02889v1	arXiv:2409.02889			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 03 2024	2024	Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as textit{degraded performance with more images} and textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model textbf{LongLLaVA}~(textbf{Long}-Context textbf{L}arge textbf{L}anguage textbf{a}nd textbf{V}ision textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks.																																	2024-10-19	PPRN:91736611		
J	Yue, Yubiao; Li, Zhenzhang				Li, Zhenzhang/JVN-8271-2024; Yue, Yubiao/LUZ-6383-2024						MedMamba: Vision Mamba for Medical Image Classification								Arxiv											5	5;2024-09-29;https://www.arxiv.org/abs/2403.03849v5| 4;2024-06-10;https://www.arxiv.org/abs/2403.03849v4| 3;2024-04-02;https://www.arxiv.org/abs/2403.03849v3| 2;2024-03-21;https://www.arxiv.org/abs/2403.03849v2| 1;2024-03-06;https://www.arxiv.org/abs/2403.03849v1	arXiv:2403.03849			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 29 2024	2024	Since the era of deep learning, convolutional neural networks (CNNs) and vision transformers (ViTs) have been extensively studied and widely used in medical image classification tasks. Unfortunately, CNN's limitations in modeling long-range dependencies result in poor classification performances. In contrast, ViTs are hampered by the quadratic computational complexity of their self-attention mechanism, making them difficult to deploy in real-world settings with limited computational resources. Recent studies have shown that state space models (SSMs) represented by Mamba can effectively model long-range dependencies while maintaining linear computational complexity. Inspired by it, we proposed MedMamba, the first Vision Mamba for generalized medical image classification. Concretely, we introduced a novel hybrid basic block named SS-Conv-SSM, which purely integrates the convolutional layers for extracting local features with the abilities of SSM to capture long-range dependencies, aiming to model medical images from different image modalities efficiently. By employing the grouped convolution strategy and channel-shuffle operation, MedMamba successfully provides fewer model parameters and a lower computational burden for efficient applications without sacrificing accuracy. We thoroughly evaluated MedMamba using 16 datasets containing ten imaging modalities and 411,007 images. Experimental results show that MedMamba demonstrates competitive performance on most tasks compared with the state-of-the-art methods. This work aims to explore the potential of Vision Mamba and establish a new baseline for medical image classification, thereby providing valuable insights for developing more powerful Mamba-based artificial intelligence algorithms and applications in medicine. The source codes and all pre-trained weights of MedMamba are available at https://github.com/YubiaoYue/MedMamba.																																	2024-10-10	PPRN:88046882		
J	Bhardwaj, Lakshya; Bottini, Lea E.; Pajer, Daniel; Schafer-Nameki, Sakura				Bhardwaj, Lakshya/ISU-5186-2023; Filho, Luciano/KDN-5303-2024						Gapped Phases with Non-Invertible Symmetries: (1+1)d								Arxiv											6	6;2024-09-27;https://www.arxiv.org/abs/2310.03784v5| 5;2024-07-25;https://www.arxiv.org/abs/2310.03784v4| 4;2024-03-01;https://www.arxiv.org/abs/2310.03784v3| 3;2023-10-22;https://www.arxiv.org/abs/2310.03784v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03784v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03784v1	arXiv:2310.03784			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 27 2024	2024	We propose a general framework to characterize gapped infra-red (IR) phases of theories with non-invertible (or categorical) symmetries. In this paper we focus on (1+1)d gapped phases with fusion category symmetries. The approach that we propose uses the Symmetry Topological Field Theory (SymTFT) as a key input: associated to a field theory in d spacetime dimensions, the SymTFT lives in one dimension higher and admits a gapped boundary, which realizes the categorical symmetries. It also admits a second, physical, boundary, which is generically not gapped. Upon interval compactification of the SymTFT by colliding the gapped and physical boundaries, we regain the original theory. In this paper, we realize gapped symmetric phases by choosing the physical boundary to be a gapped boundary condition as well. This set-up provides computational power to determine the number of vacua, the symmetry breaking pattern, and the action of the symmetry on the vacua. The SymTFT also manifestly encodes the order parameters for these gapped phases, thus providing a generalized, categorical Landau paradigm for (1+1)d gapped phases. We find that for non-invertible symmetries the order parameters involve multiplets containing both untwisted and twisted sector local operators, and hence can be interpreted as mixtures of conventional and string order parameters. We also observe that spontaneous breaking of non-invertible symmetries can lead to vacua that are physically distinguishable: unlike the standard symmetries described by groups, non-invertible symmetries can have different actions on different vacua of an irreducible gapped phase. This leads to the presence of relative Euler terms between physically distinct vacua. We also provide a mathematical description of symmetric gapped phases as 2-functors from delooping of fusion category characterizing the symmetry to Euler completion of 2-vector spaces.																																	2025-03-22	PPRN:85520816		
J	Bonatti, Rogerio; Zhao, Dan; Bonacci, Francesco; Dupont, Dillon; Abdali, Sara; Li, Yinheng; Lu, Yadong; Wagle, Justin; Koishida, Kazuhito; Bucker, Arthur; Jang, Lawrence; Hui, Zack				Lu, Yadong/JGL-6374-2023						Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale								Arxiv											2	2;2024-09-13;https://www.arxiv.org/abs/2409.08264v2| 1;2024-09-12;https://www.arxiv.org/abs/2409.08264v1	arXiv:2409.08264			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 13 2024	2024	Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the W INDOWS AGENT ARENA : a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate W INDOWS A GENT A RENA ’s capabilities, we also introduce anew multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi’s performance, and provide insights into the opportunities for future research in agent development and data generation using W INDOWS AGENTARENA.																																	2024-12-24	PPRN:91860509		
J	Xin, Huajian; Ren, Z.Z.; Song, Junxiao; Shao, Zhihong; Zhao, Wanjia; Wang, Haocheng; Liu, Bo; Zhang, Liyue; Lu, Xuan; Du, Qiushi; Gao, Wenjun; Zhu, Qihao; Yang, Dejian; Gou, Zhibin; Wu, Z.F.; Luo, Fuli; Ruan, Chong				Zhu, Qihao/JWO-8071-2024; zhang, liyue/AAU-6549-2021; gao, wenjun/KIA-6581-2024						DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search								Arxiv											1	1;2024-08-15;https://www.arxiv.org/abs/2408.08152v1	arXiv:2408.08152			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 15 2024	2024	We introduce DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark ($63.5%$) and the undergraduate level ProofNet benchmark ($25.3%$).																																	2024-08-23	PPRN:91414474		
J	Mueller, Samuel; Hollmann, Noah; Arango, Sebastian Pineda; Grabocka, Josif; Hutter, Frank				Pineda Arango, Sebastian/KIB-2459-2024						Transformers Can Do Bayesian Inference								Arxiv											2	2;2024-08-13;https://www.arxiv.org/abs/2112.10510v7| 1;2021-12-20;https://www.arxiv.org/abs/2112.10510v5	arXiv:2112.10510			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 13 2024	2024	Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage in-context learning in large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. 																																	2024-08-22	PPRN:12128413		
J	Liu, Hao; Feng, Jiarui; Kong, Lecheng; Liang, Ningyue; Tao, Dacheng; Chen, Yixin; Zhang, Muhan				chen, yixin/NGS-5073-2025; Shen, Li/AEZ-9528-2022						One for All: Towards Training One Graph Model for All Classification Tasks								Arxiv											3	3;2024-07-12;https://www.arxiv.org/abs/2310.00149v3| 2;2023-12-18;https://www.arxiv.org/abs/2310.00149v2| 1;2023-09-29;https://www.arxiv.org/abs/2310.00149v1	arXiv:2310.00149			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain. However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. We propose textbf{One for All (OFA)}, the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space. Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation. For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs well across different tasks, making it the first general-purpose across-domains classification model on graphs.																																	2024-07-23	PPRN:85348877		
J	He, Xuanhua; Liu, Quande; Qian, Shengju; Wang, Xin; Hu, Tao; Cao, Ke; Yan, Keyu; Zhang, Jie				yan, keyu/GXW-2126-2022						ID-Animator: Zero-Shot Identity-Preserving Human Video Generation								Arxiv											3	3;2024-06-25;https://www.arxiv.org/abs/2404.15275v3| 2;2024-05-14;https://www.arxiv.org/abs/2404.15275v2| 1;2024-04-23;https://www.arxiv.org/abs/2404.15275v1	arXiv:2404.15275			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 25 2024	2024	Generating high-fidelity human video with specified identities has attracted significant attentions in the content generation community. However, existing techniques struggle to strike a balance between training efficiency and identity preservation, either requiring tedious case-by-case fine-tuning or usually missing identity details in the video generation process. In this study, we present ID-Animator , a zero-shot human-video generation approach that can perform personalized video generation given a single reference facial image without further training. ID-Animator inherits existing diffusion-based video generation backbones with a face adapter to encode the ID-relevant embeddings from learnable facial latent queries. To facilitate the extraction of identity information in video generation, we introduce an ID-oriented dataset construction pipeline that incorporates unified human attributes and action captioning techniques from a constructed facial image pool. Based on this pipeline, a random reference training strategy is further devised to precisely capture the ID-relevant embeddings with an ID-preserving loss, thus improving the fidelity and generalization capacity of our model for ID-specific video generation. Extensive experiments demonstrate the superiority of ID-Animator to generate personalized human videos over previous models. Moreover, our method is highly compatible with popular pre-trained T2V models like animatediff and various community backbone models, showing high extendability in real-world applications for video generation where identity preservation is highly desired. Our codes and checkpoints are released.																																	2024-07-15	PPRN:88627998		
J	Zhang, Baowen; Fang, Chuan; Shrestha, Rakesh; Liang, Yixun; Long, Xiaoxiao; Tan, Ping										RaDe-GS: Rasterizing Depth in Gaussian Splatting								Arxiv											2	2;2024-06-24;https://www.arxiv.org/abs/2406.01467v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01467v1	arXiv:2406.01467			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 24 2024	2024	Gaussian Splatting (GS) has proven to be highly effective in novel view synthesis, achieving high-quality and real-time rendering. However, its potential for reconstructing detailed 3D shapes has not been fully explored. Existing methods often suffer from limited shape accuracy due to the discrete and unstructured nature of Gaussian splats, which complicates the shape extraction. While recent techniques like 2D GS have attempted to improve shape reconstruction, they often reformulate the Gaussian primitives in ways that reduce both rendering quality and computational efficiency. To address these problems, our work introduces a rasterized approach to render the depth maps and surface normal maps of general 3D Gaussian splats. Our method not only significantly enhances shape reconstruction accuracy but also maintains the computational efficiency intrinsic to Gaussian Splatting. It achieves a Chamfer distance error comparable to NeuraLangelo on the DTU dataset and maintains similar computational efficiency as the original 3D GS methods. Our method is a significant advancement in Gaussian Splatting and can be directly integrated into existing Gaussian Splatting-based methods.																																	2024-07-15	PPRN:89163550		
J	Amini, Afra; Vieira, Tim; Cotterell, Ryan										Direct Preference Optimization with an Offset								Arxiv											1	1;2024-06-06;https://www.arxiv.org/abs/2402.10571v2	arXiv:2402.10571			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.																																	2024-11-20	PPRN:89259853		
J	Tian, Runchu; Ye, Yining; Qin, Yujia; Cong, Xin; Lin, Yankai; Pan, Yinxu; Wu, Yesai; Hui, Haotian; Liu, Weichuan; Liu, Zhiyuan; Sun, Maosong				Ye, Yining/JPL-4050-2023; Liu, Zhiyuan/I-2233-2014						DebugBench: Evaluating Debugging Capability of Large Language Models								Arxiv											3	3;2024-06-06;https://www.arxiv.org/abs/2401.04621v3| 2;2024-01-11;https://www.arxiv.org/abs/2401.04621v2| 1;2024-01-09;https://www.arxiv.org/abs/2401.04621v1	arXiv:2401.04621			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.																																	2024-07-11	PPRN:87081632		
J	Siuzdak, Hubert										Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis								Arxiv											3	3;2024-05-29;https://www.arxiv.org/abs/2306.00814v3| 2;2023-10-03;https://www.arxiv.org/abs/2306.00814v2| 1;2023-06-01;https://www.arxiv.org/abs/2306.00814v1	arXiv:2306.00814			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. 																																	2024-06-16	PPRN:72810753		
J	Duan, Jinhao; Cheng, Hao; Wang, Shiqi; Zavalny, Alex; Wang, Chenan; Xu, Renjing; Kailkhura, Bhavya; Xu, Kaidi				Cheng, hao/LQI-9241-2024; Wang, Shiqi/AAR-5013-2020						Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models								Arxiv											3	3;2024-05-28;https://www.arxiv.org/abs/2307.01379v3| 2;2023-10-09;https://www.arxiv.org/abs/2307.01379v2| 1;2023-07-03;https://www.arxiv.org/abs/2307.01379v1	arXiv:2307.01379			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	Large Language Models (LLMs) show promising results in language generation and instruction following but frequently “hallucinate”, making their outputs less reliable. Despite Uncertainty Quantification’s (UQ) potential solutions, implementing it accurately within LLMs is challenging. Our research introduces a simple heuristic: not all tokens in auto-regressive LLM text equally represent the underlying meaning, as “linguistic redundancy” often allows a few keywords to convey the essence of long sentences. However, current methods underestimate this inequality when assessing uncertainty, causing tokens with limited semantics to be equally or excessively weighted in UQ. To correct this, we propose S hifting A ttention to more R elevant ( SAR ) components at both token-and sentence-levels for better UQ. We conduct extensive experiments involving a range of popular “off-the-shelf” LLMs, such as Vicuna, WizardLM, and LLaMA-2chat, with model sizes extending up to 33B parameters. We evaluate various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results, coupled with a comprehensive demographic analysis, demonstrate the superior performance of SAR . The code is available at https://github.com/jinhaoduan/SAR .																																	2024-06-16	PPRN:73802386		
J	Chang, Di; Shi, Yichun; Gao, Quankai; Fu, Jessica; Xu, Hongyi; Song, Guoxian; Yan, Qing; Zhu, Yizhe; Yang, Xiao; Soleymani, Mohammad				Song, Guoxian/AAS-8496-2021; XU, Hongyi/F-5026-2013; Shi, Yichun/AGV-8080-2022; Zhu, Yizhe/AAP-4964-2021; Gao, Quankai/GYD-4954-2022						MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion								Arxiv											3	3;2024-05-05;https://www.arxiv.org/abs/2311.12052v3| 2;2024-02-02;https://www.arxiv.org/abs/2311.12052v2| 1;2023-11-18;https://www.arxiv.org/abs/2311.12052v1	arXiv:2311.12052			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 05 2024	2024	In this work, we propose MagicPose, a diffusionbased model for 2D human pose and facial expression retargeting. Specifically, given a reference image, we aim to generate a person’s new images by controlling the poses and facial expressions while keeping the identity unchanged. To this end, we propose a two-stage training strategy to disentangle human motions and appearance (e.g., facial expressions, skin tone and dressing), consisting of (1) the pre-training of an appearance-control block and (2) learning appearance-disentangled pose control. Our novel design enables robust appearance control over generated human images, including body, facial attributes, and even background. By leveraging the prior knowledge of image diffusion models, MagicPose generalizes well to unseen human identities and complex poses without the need for additional fine-tuning. Moreover, the proposed model is easy to use and can be considered as a plug-in module/extension to Stable Diffusion. The code is available at https:// github.com/Boese0601/MagicDance .																																	2024-05-25	PPRN:86224033		
J	Keetha, Nikhil; Karhade, Jay; Jatavallabhula, Krishna Murthy; Yang, Gengshan; Scherer, Sebastian; Ramanan, Deva; Luiten, Jonathon										<italic>SplaTAM</italic>: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM								Arxiv											3	3;2024-04-16;https://www.arxiv.org/abs/2312.02126v3| 2;2024-03-26;https://www.arxiv.org/abs/2312.02126v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02126v1	arXiv:2312.02126			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 16 2024	2024	Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM achieves up to 2x superior performance in camera pose estimation, map construction, and novel-view synthesis over existing methods, paving the way for more immersive high-fidelity SLAM applications.																																	2024-04-26	PPRN:86378074		
J	Matsuki, Hidenobu; Murai, Riku; Kelly, Paul H.J.; Davison, Andrew J.										Gaussian Splatting SLAM								Arxiv											2	2;2024-04-14;https://www.arxiv.org/abs/2312.06741v2| 1;2023-12-11;https://www.arxiv.org/abs/2312.06741v1	arXiv:2312.06741			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 14 2024	2024	We present the first application of 3D Gaussian Splatting in monocular SLAM, the most fundamental but the hardest setup for Visual SLAM. Our method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Designed for challenging monocular settings, our approach is seamlessly extendable to RGB-D SLAM when an external depth sensor is available. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation but also reconstruction of tiny and even transparent objects.																																	2024-04-25	PPRN:86556615		
J	Klemmer, Konstantin; Rolf, Esther; Robinson, Caleb; Mackey, Lester; Russwurm, Marc				Rußwurm, Marc/KMY-6161-2024; Klemmer, Konstantin/GZM-9026-2022						SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery								Arxiv											2	2;2024-04-12;https://www.arxiv.org/abs/2311.17179v3| 1;2023-11-30;https://www.arxiv.org/abs/2311.17179v2	arXiv:2311.17179			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 12 2024	2024	Geographic information is essential for modeling tasks in fields ranging from ecology to epidemiology. However, extracting relevant location characteristics for a given task can be challenging, often requiring expensive data fusion or distillation from massive global imagery datasets. To address this challenge, we introduce Satellite Contrastive Location-Image Pretraining (SatCLIP). This global, general-purpose geographic location encoder learns an implicit representation of locations by matching CNN and ViT inferred visual patterns of openly available satellite imagery with their geographic coordinates. The resulting SatCLIP location encoder efficiently summarizes the characteristics of any given location for convenient use in downstream tasks. In our experiments, we use SatCLIP embeddings to improve prediction performance on nine diverse location-dependent tasks including temperature prediction, animal recognition, and population density estimation. Across tasks, SatCLIP consistently outperforms alternative location encoders and improves geographic generalization by encoding visual similarities of spatially distant environments. These results demonstrate the potential of vision-location models to learn meaningful representations of our planet from the vast, varied, and largely untapped modalities of geospatial data.																																	2024-04-25	PPRN:86359723		
J	Deng, Chunyuan; Zhao, Yilun; Tang, Xiangru; Gerstein, Mark; Cohan, Arman				Gerstein, Mark/HSC-3904-2023; Zhao, Ziang/IAR-5845-2023						Investigating Data Contamination in Modern Benchmarks for Large Language Models								Arxiv											2	2;2024-04-03;https://www.arxiv.org/abs/2311.09783v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09783v1	arXiv:2311.09783			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 03 2024	2024	Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.																																	2024-04-19	PPRN:86177614		
J	Liu, Zhe; Huo, Mengwu; Li, Jie; Li, Qing; Liu, Yuecong; Dai, Yaomin; Zhou, Xiaoxiang; Hao, Jiahao; Lu, Yi; Wang, Meng; Wen, Hai-Hu				Lu, Yi/G-9881-2014; Li, Qing/GYD-5800-2022; Hao, Jiahao/ITV-6037-2023; WANG, MENG/E-6595-2012						Electronic correlations and partial gap in the bilayer nickelate La3Ni2O7								Arxiv											3	3;2024-04-03;https://www.arxiv.org/abs/2307.02950v3| 2;2024-01-31;https://www.arxiv.org/abs/2307.02950v2| 1;2023-07-06;https://www.arxiv.org/abs/2307.02950v1	arXiv:2307.02950			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	The discovery of superconductivity with a critical temperature of about 80 K in La3Ni2O7 single crystals under pressure has received enormous attention. La3Ni2O7is not superconducting under ambient pressure but exhibits a transition at T* ≃ 115 K. Understanding the electronic correlations and charge dynamics is an important step towards the origin of superconductivity and other instabilities. Here, our optical study shows that La3Ni2O7 features strong electronic correlations which significantly reduce the electron’s kinetic energy and place this system in the proximity of the Mott phase. The low-frequency optical conductivity reveals two Drude components arising from multiple bands at the Fermi level. The transition at T* removes the Drude component exhibiting non-Fermi liquid behavior, whereas the one with Fermi-liquid behavior is barely affected. These observations in combination with theoretical results suggest that the Fermi surface dominated by the Ni-d3z2−r2 orbital is removed due to the transition at T*. Our experimental results provide pivotal information for understanding the transition at T* and superconductivity in La3Ni2O7.																																	2024-04-18	PPRN:73806740		
J	Benton, Joe; De Bortoli, Valentin; Doucet, Arnaud; Deligiannidis, George				Doucet, Arnaud/B-2473-2013						NEARLY d-LINEAR CONVERGENCE BOUNDS FOR DIFFUSION MODELS VIA STOCHASTIC LOCALIZATION								Arxiv											3	3;2024-03-06;https://www.arxiv.org/abs/2308.03686v3| 2;2024-01-18;https://www.arxiv.org/abs/2308.03686v2| 1;2023-08-07;https://www.arxiv.org/abs/2308.03686v1	arXiv:2308.03686			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 06 2024	2024	Denoising diffusions are a powerful method to generate approximate samples from high -dimensional data distributions. Recent results provide polynomial bounds on their convergence rate, assuming L2 -accurate scores. Until now, the tightest bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most O( d log2(1/δ)/ ε2 ) steps to approximate an arbitrary distribution on Rd corrupted with Gaussian noise of variance 6 to within ε2 in KL divergence. Our proof extends the Girsanov-based methods of previous works. We introduce a refined treatment of the error from discretizing the reverse SDE inspired by stochastic localization.																																	2024-04-03	PPRN:74300843		
J	Zhou, Yifei; Zanette, Andrea; Pan, Jiayi; Levine, Sergey; Kumar, Aviral				pan, jiayi/ABR-2644-2022						ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19446v1	arXiv:2402.19446			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions - all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we test on in our experiments).																																	2024-03-28	PPRN:87985935		
J	Wang, Wenxiao; Chen, Wei; Luo, Yicong; Long, Yongliu; Lin, Zhengkai; Zhang, Liye; Lin, Binbin; Cai, Deng; He, Xiaofei				lin, binbin/AAA-3162-2021; Wang, Wenxiao/B-9571-2015; Zhang, Liye/I-2722-2019; He, Xiaofei/E-1072-2017						Model Compression and Efficient Inference for Large Language Models: A Survey								Arxiv											1	1;2024-02-15;https://www.arxiv.org/abs/2402.09748v1	arXiv:2402.09748			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 15 2024	2024	Trnsformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource -constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning -free algorithms. (2) Large models emphasize versatility and generalization rather than performance on a single task. Hence, many algorithms, such as knowledge distillation, focus on how to preserving their versatility and generalization after compression. Since these two characteristics were not very pronounced in early large models, we further distinguish large language models into medium models and “real” large models. Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can support basic compression or acceleration algorithms, greatly facilitating model deployment for users.																																	2024-03-12	PPRN:87703599		
J	Tian, Yu; Yang, Xiao; Zhang, Jingyuan; Dong, Yinpeng; Su, Hang				Dong, Yinpeng/KBA-4751-2024; Su, Hang/AAR-7262-2020						Evil Geniuses: Delving into the Safety of LLM-based Agents								Arxiv											2	2;2024-02-02;https://www.arxiv.org/abs/2311.11855v2| 1;2023-11-20;https://www.arxiv.org/abs/2311.11855v1	arXiv:2311.11855			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our evaluations on CAMEL, Metagpt and ChatDev based on GPT-3.5 and GPT-4, demonstrate high success rates. Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research. 																																	2024-02-19	PPRN:86210887		
J	Gupta, Shashank; Shrivastava, Vaishnavi; Deshpande, Ameet; Kalyan, Ashwin; Clark, Peter; Sabharwal, Ashish; Khot, Tushar										Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs								Arxiv											2	2;2024-01-27;https://www.arxiv.org/abs/2311.04892v2| 1;2023-11-08;https://www.arxiv.org/abs/2311.04892v1	arXiv:2311.04892			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 27 2024	2024	Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.																																	2024-05-25	PPRN:86097099		
J	Zhao, Andrew; Huang, Daniel; Xu, Quentin; Lin, Matthieu; Liu, Yong-Jin; Huang, Gao										ExpeL: LLM Agents Are Experiential Learners								Arxiv											3	3;2024-12-20;https://www.arxiv.org/abs/2308.10144v3| 2;2023-12-18;https://www.arxiv.org/abs/2308.10144v2| 1;2023-08-20;https://www.arxiv.org/abs/2308.10144v1	arXiv:2308.10144			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 20 2024	2024	The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.1																																	2025-01-29	PPRN:81968802		
J	Tian, Ye; Peng, Baolin; Song, Linfeng; Jin, Lifeng; Yu, Dian; Han, Lei; Mi, Haitao; Yu, Dong				Peng, Baolin/F-2278-2019; Song, Linfeng/JOH-3221-2023						Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing								Arxiv											2	2;2024-12-10;https://www.arxiv.org/abs/2404.12253v2| 1;2024-04-18;https://www.arxiv.org/abs/2404.12253v1	arXiv:2404.12253			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 10 2024	2024	Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.																																	2025-01-19	PPRN:88565291		
J	Huang, Zhen; Zou, Haoyang; Li, Xuefeng; Liu, Yixiu; Zheng, Yuxiang; Chern, Ethan; Xia, Shijie; Qin, Yiwei; Yuan, Weizhe; Liu, Pengfei				Liu, Pengfei/JUV-0307-2023; zheng, yuxiang/KEI-0799-2024						O1 Replication Journey – Part 2: Surpassing O1-preview through Simple Distillation or Bitter Lesson?								Arxiv											1	1;2024-11-25;https://www.arxiv.org/abs/2411.16489v1	arXiv:2411.16489			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 25 2024	2024	This paper presents a critical examination of current approaches to replicating OpenAI’s O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work (Part 1 (Qin et al., 2024)) explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1’s API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA . Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial “ bitter lesson”: while the pursuit of more capable AI systems is important, the development of researchers grounded in first- principles thinking is paramount. This educational imperative represents not just a technical consideration, but a fundamental human mission that will shape the future of AI innovation.1  [GRAPHICS]																																	2025-01-08	PPRN:119393854		
J	Qi, Zehan; Liu, Xiao; Iong, Iat Long; Lai, Hanyu; Sun, Xueqiao; Yang, Xinyue; Sun, Jiadai; Yang, Yu; Yao, Shuntian; Zhang, Tianjie; Xu, Wei; Tang, Jie; Dong, Yuxiao										WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning								Arxiv											1	1;2024-11-04;https://www.arxiv.org/abs/2411.02337v1	arXiv:2411.02337			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 04 2024	2024	Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WEBRL, a selfevolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WEBRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WEBRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WEBRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WEBRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WE-B RL’s effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.  [GEAPHICS]																																	2024-12-09	PPRN:119022657		
J	Navaneet, K L; Meibodi, Kossar Pourahmadi; Koohpayegani, Soroush Abbasi; Pirsiavash, Hamed										CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization								Arxiv											3	3;2024-09-26;https://www.arxiv.org/abs/2311.18159v3| 2;2024-06-11;https://www.arxiv.org/abs/2311.18159v2| 1;2023-11-30;https://www.arxiv.org/abs/2311.18159v1	arXiv:2311.18159			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 26 2024	2024	3D Gaussian Splatting (3DGS) is a new method for modeling and rendering 3D radiance fields that achieves much faster learning and rendering time compared to SOTA NeRF methods. However, it comes with a drawback in the much larger storage demand compared to NeRF methods since it needs to store the parameters for several 3D Gaussians. We notice that many Gaussians may share similar parameters, so we introduce a simple vector quantization method based on K-means to quantize the Gaussian parameters while optimizing them. Then, we store the small codebook along with the index of the code for each Gaussian. We compress the indices further by sorting them and using a method similar to run-length encoding. Moreover, we use a simple regularizer to encourage zero opacity (invisible Gaussians) to reduce the storage and rendering time by a large factor through reducing the number of Gaussians. We do extensive experiments on standard benchmarks as well as an existing 3D dataset that is an order of magnitude larger than the standard benchmarks used in this field. We show that our simple yet effective method can reduce the storage cost for 3DGS by 40 to 50x and rendering time by 2 to 3x with a very small drop in the quality of rendered images.																																	2024-10-09	PPRN:86342502		
J	Ibrahim, Adam; Therien, Benjamin; Gupta, Kshitij; Richter, Mats L.; Anthony, Quentin; Lesort, Timothee; Belilovsky, Eugene; Rish, Irina										Simple and Scalable Strategies to Continually Pre-train Large Language Models								Arxiv											4	4;2024-09-04;https://www.arxiv.org/abs/2403.08763v4| 3;2024-03-26;https://www.arxiv.org/abs/2403.08763v3| 2;2024-03-22;https://www.arxiv.org/abs/2403.08763v2| 1;2024-03-13;https://www.arxiv.org/abs/2403.08763v1	arXiv:2403.08763			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 04 2024	2024	Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models—saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English→English) and a stronger distribution shift (English→German) at the 405M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that autoregressive transformer-based LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.																																	2024-09-12	PPRN:88128063		
J	Chen, Xuejiao; Jiang, Peiheng; Li, Jie; Zhong, Zhicheng; Lu, Yi				Zhong, Zhicheng/ABC-6627-2021; Lu, Yi/G-9881-2014						Charge and spin instabilities in superconducting La3Ni2O7								Arxiv											2	2;2024-09-02;https://www.arxiv.org/abs/2307.07154v2| 1;2023-07-14;https://www.arxiv.org/abs/2307.07154v1	arXiv:2307.07154			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 02 2024	2024	Motivated by the recent discovery of superconductivity in La3Ni2O7 under high pressure, we explore its potential charge and spin instabilities through combined model analysis and first-principles calculations. Taking into account the small charge-transfer nature of high valence nickel, a fully correlated two-cluster model identifies a lattice-coupled charge instability characterized by substantial short-range fluctuations of oxygen holes. This instability is corroborated by density-functional-theory plus U calculations that also reveal a strong tendency towards concurrent antiferromagnetic ordering. The charge, spin, and associated lattice instabilities are significantly suppressed with increasing external pressure, contributing to the emergence of superconductivity in pressurized La3Ni2O7. Carrier doping is found to effectively suppress these instabilities, suggesting a viable strategy to stabilize a superconducting phase under ambient pressure.																																	2025-03-15	PPRN:73931249		
J	Jiang, Ziyan; Ma, Xueguang; Chen, Wenhu				Ma, Xueguang/GPF-7396-2022						LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs								Arxiv											3	3;2024-09-01;https://www.arxiv.org/abs/2406.15319v3| 2;2024-06-30;https://www.arxiv.org/abs/2406.15319v2| 1;2024-06-21;https://www.arxiv.org/abs/2406.15319v1	arXiv:2406.15319			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 01 2024	2024	In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the “needle” unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced “heavy” retriever and “light” reader design can lead to sub-optimal performance. The loss of contextual information in the short, chunked units may increase the likelihood of introducing hard negatives during the retrieval stage. Additionally, the reader might not fully leverage the capabilities of recent advancements in LLMs. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a “long retriever” and a “long reader”. . In the two Wikipedia-based datasets, NQ and HotpotQA, where the average document size is less than 1K tokens, LongRAG processes the entire Wikipedia corpus into 4K-token units by grouping related documents, making these units 30 times longer than before. By increasing the unit size, we significantly reduce the total number of units from 22M to 600K. This greatly reduces the burden on the retriever, resulting in strong retrieval performance with only a few (less than 8) top units. Compared to traditional RAG, which may require hundreds of short units to achieve similar retrieval performance, our approach minimizes the likelihood of retrieving hard negatives while maintaining semantic integrity of each unit. Then we feed these retrieved units (≈ 30K tokens) to an existing long-context LLM to perform zero-shot answer generation. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furthermore, we test on two non-Wikipedia-based datasets, Qasper and MultiFieldQA-en, where the average document length is already above 4K tokens. LongRAG processes each individual document as a single (long) unit rather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). Our study offers insights into the future roadmap for combining RAG with long-context LLMs.																																	2024-09-12	PPRN:89401305		
J	Lv, Tengchao; Huang, Yupan; Chen, Jingye; Zhao, Yuzhong; Jia, Yilin; Cui, Lei; Ma, Shuming; Chang, Yaoyao; Huang, Shaohan; Wang, Wenhui; Dong, Li; Luo, Weiyao; Wu, Shaoxiang; Wang, Guoxin; Zhang, Cha; Wei, Furu				Jia, Yilin/C-2153-2009; Huang, Shaohan/LDF-3300-2024; wang, wenhui/HLX-0929-2023; Lv, Teng/LSL-4339-2024						KOSMOS-2.5: A Multimodal Literate Model								Arxiv											2	2;2024-08-21;https://www.arxiv.org/abs/2309.11419v2| 1;2023-09-20;https://www.arxiv.org/abs/2309.11419v1	arXiv:2309.11419			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 21 2024	2024	The automatic reading of text-intensive images represents a significant advancement toward achieving Artificial General Intelligence (AGI). In this paper we present KOSMOS-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on a large-scale corpus of text-intensive images, KOSMOS-2.5 excels in two distinct yet complementary transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned spatial coordinates within the image, and (2) producing structured text output that captures both style and structure in markdown format. This unified multimodal literate capability is achieved through a shared decoder-only autoregressive Transformer architecture and task-specific prompts. Building on this foundation, we fine-tune KOSMOS-2.5 for document understanding tasks, resulting in a document understanding generalist named KOSMOS-2.5-CHAT. Additionally, a large corpus of 357.4 million document pages spanning diverse domains was curated for pre-training. We evaluate KOSMOS-2.5 on two newly proposed benchmarks, OCREval and MarkdownEval, for document-level text recognition and image-to-markdown generation, demonstrating impressive literate capabilities comparable to GPT-4o. KOSMOS-2.5-CHAT achieves performance comparable to other state-of-the-art generalists that are five times larger (1.3B vs. 7B) across nine text-rich visual question answering benchmarks. 																																	2024-08-31	PPRN:85076875		
J	Hellenes, Anna Birk; Jungwirth, Tomas; Jaeschke-Ubiergo, Rodrigo; Chakraborty, Atasi; Sinova, Jairo; Smejkal, Libor				Jungwirth, Tomas/G-8952-2014; Jaeschke-Ubiergo, Rodrigo/LRT-3373-2024; Sinova, Jairo/G-9071-2014; Šmejkal, Libor/G-8927-2014						P-wave magnets								Arxiv											3	3;2024-07-31;https://www.arxiv.org/abs/2309.01607v3| 2;2024-03-04;https://www.arxiv.org/abs/2309.01607v2| 1;2023-09-04;https://www.arxiv.org/abs/2309.01607v1	arXiv:2309.01607			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 31 2024	2024	The p-wave Cooper-pairing instability in superfluid $^{3}$He, characterized by a parity-breaking excitation gap, is regarded as one of the most rich and complex phenomena in physics. The possibility of a counterpart unconventional p-wave ordering of interacting fermions, in which a Fermi surface spontaneously breaks the parity symmetry, has been an open problem for many decades. Here we identify the realization of the counterpart of p-wave superfluidity in magnetism. We demonstrate a strong parity-breaking and anisotropic symmetry lowering of spin-polarized and time-reversal symmetric Fermi surfaces in a representative p-wave magnet CeNiAsO. As a direct experimental signature we predict a large spontaneous anisotropy of the resistivity. Abundant and robust realizations of the unconventional p-wave magnetism can be identified from suitable non-relativistic crystal-lattice and spin symmetries, without requiring strong correlations and extreme external conditions. This opens new prospects in fields ranging from topological phenomena to spintronics.																																	2024-08-08	PPRN:84764077		
J	Lee, Taehyun; Hong, Seokhee; Ahn, Jaewoo; Hong, Ilgee; Lee, Hwaran; Yun, Sangdoo; Shin, Jamin; Kim, Gunhee				Lee, Hwaran/GVR-7438-2022; Hong, Seokhee/LQK-2595-2024						Who Wrote this Code? Watermarking for Code Generation								Arxiv											4	4;2024-07-03;https://www.arxiv.org/abs/2305.15060v4| 3;2024-02-23;https://www.arxiv.org/abs/2305.15060v3| 2;2023-11-17;https://www.arxiv.org/abs/2305.15060v2| 1;2023-05-24;https://www.arxiv.org/abs/2305.15060v1	arXiv:2305.15060			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 03 2024	2024	Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed. However, we discover that the existing works fail to function appropriately in code generation tasks due to the task's nature of having low entropy. Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks. Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text. 																																	2024-07-20	PPRN:72713426		
J	Duan, Jinhao; Zhang, Renming; Diffenderfer, James; Kailkhura, Bhavya; Sun, Lichao; Stengel-Eskin, Elias; Bansal, Mohit; Chen, Tianlong; Xu, Kaidi				Bansal, Mohit/Q-9105-2016						GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2402.12348v2| 1;2024-02-19;https://www.arxiv.org/abs/2402.12348v1	arXiv:2402.12348			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and (2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs' behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.																																	2024-07-04	PPRN:87762878		
J	Casanova, Edresson; Davis, Kelly; Goelge, Eren; Goeknar, Goerkem; Gulea, Iulian; Hart, Logan; Aljafari, Aya; Meyer, Joshua; Morais, Reuben; Olayemi, Samuel; Weber, Julian				Casanova, Edresson/JQJ-3117-2023						XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model								Arxiv											1	1;2024-06-07;https://www.arxiv.org/abs/2406.04904v1	arXiv:2406.04904			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	Most Zero-shot Multi-speaker TTS (ZS-TTS) systems support only a single language. Although models like YourTTS, VALL-E X, Mega-TTS 2, and Voicebox explored Multilingual ZS-TTS they are limited to just a few high/medium resource languages, limiting the applications of these models in most of the low/medium resource languages. In this paper, we aim to alleviate this issue by proposing and making publicly available the XTTS system. Our method builds upon the Tortoise model and adds several novel modifications to enable multilingual training, improve voice cloning, and enable faster training and inference. XTTS was trained in 16 languages and achieved state-of-the-art (SOTA) results in most of them.																																	2024-11-20	PPRN:89243913		
J	Agrawal, Amey; Kedia, Nitin; Panwar, Ashish; Mohan, Jayashree; Kwatra, Nipun; Gulavani, Bhargav S.; Tumanov, Alexey; Ramjee, Ramachandran				Panwar, Ashish/GRS-2557-2022						Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve								Arxiv											3	3;2024-06-12;https://www.arxiv.org/abs/2403.02310v2| 2;2024-03-04;https://www.arxiv.org/abs/2403.02310v1| 1;2024-06-01;	arXiv:2403.02310			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt and produces the first output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency. We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address this throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Furthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between iterations resulting in minimal pipeline bubbles. Our techniques yield significant improvements in inference performance across models and hardware under tail latency constraints. For Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher serving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM. When used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up to 5.6x gain in the end-to-end serving capacity. The source code for Sarathi-Serve is available at https://github.com/microsoft/sarathi-serve.																																	2025-11-07	PPRN:88022736		
J	Wang, Xindi; Salmani, Mahsa; Omidi, Parsa; Ren, Xiangyu; Rezagholizadeh, Mehdi; Eshaghi, Armaghan				Ren, Xiangyu/AAR-3691-2020						Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models								Arxiv											3	3;2024-05-29;https://www.arxiv.org/abs/2402.02244v3| 2;2024-05-22;https://www.arxiv.org/abs/2402.02244v2| 1;2024-02-03;https://www.arxiv.org/abs/2402.02244v1	arXiv:2402.02244			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 29 2024	2024	Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.																																	2024-06-16	PPRN:87522506		
J	Niu, Cheng; Wu, Yuanhao; Zhu, Juno; Xu, Siliang; Shum, Kashun; Zhong, Randy; Song, Juntong; Zhang, Tong										RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models								Arxiv											2	2;2024-05-17;https://www.arxiv.org/abs/2401.00396v2| 1;2023-12-31;https://www.arxiv.org/abs/2401.00396v1	arXiv:2401.00396			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 17 2024	2024	Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.																																	2024-06-01	PPRN:86901505		
J	Evans, Zach; Carr, CJ; Taylor, Josiah; Hawley, Scott H.; Pons, Jordi										Fast Timing-Conditioned Latent Audio Diffusion								Arxiv											2	2;2024-05-13;https://www.arxiv.org/abs/2402.04825v3| 1;2024-02-08;https://www.arxiv.org/abs/2402.04825v2	arXiv:2402.04825			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 13 2024	2024	Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.																																	2024-06-08	PPRN:87568788		
J	Maiolino, Roberto; Risaliti, Guido; Signorini, Matilde; Trefoloni, Bartolomeo; Juodzbalis, Ignas; Scholtz, Jan; Ubler, Hannah; D'Eugenio, Francesco; Carniani, Stefano; Fabian, Andy; Ji, Xihan; Mazzolari, Giovanni; Bertola, Elena; Brusa, Marcella; Bunker, Andrew J.; Charlot, Stephane; Comastri, Andrea; Cresci, Giovanni; DeCoursey, Christa Noel; Egami, Eiichi; Fiore, Fabrizio; Gilli, Roberto; Perna, Michele; Tacchella, Sandro; Venturi, Giacomo				Tacchella, Sandro/AAT-1602-2021; Risaliti, Guido/NJR-3265-2025; Venturi, Giacomo/AAB-4352-2021; D'Eugenio, Francesco/H-2606-2019						JWST meets Chandra: a large population of Compton thick, feedback-free, and X-ray weak AGN, with a sprinkle of SNe								Arxiv											1	1;2024-05-01;https://www.arxiv.org/abs/2405.00504v1	arXiv:2405.00504			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	We investigate the X-ray properties of a large sample of 71 broad line and narrow line AGN at 2<z<11 discovered by JWST in the GOODS fields, which have the deepest Chandra observations ever obtained. Despite the widespread presence of AGN signatures in their rest-optical and -UV spectra, the vast majority of them is X-ray undetected. The stacked X-ray data of the non-detected sources also results in a non-detection. The upper limit on the X-ray emission for many of these AGN is one or even two orders of magnitude lower than expected from a standard AGN SED. Heavy X-ray absorption by clouds with large (Compton thick) column density and low dust content, such as the Broad Line Region (BLR) clouds, can explain the X-ray weakness. In this scenario the BLR covering factor should be much larger than in low-z AGN or luminous quasar; this is supported by the larger equivalent width of the broad component of Halpha in JWST-selected AGN. We also find that the JWST-discovered AGN lack the prominent, fast outflows characterizing low-z AGN and luminous quasars, suggesting that, in JWST-selected AGN, dense gas lingers in the nuclear region, resulting in large covering factors. We also note that a large fraction of JWST-selected AGN match the definition of NLSy1, typically characterized by a steep X-ray spectrum, and this can further contribute to their observed weakness at high-z. Finally, we discuss that the broad Balmer lines used to identify type 1 AGN cannot be ascribed to Very Massive Stars, Tidal Disruption Events, or Supernovae, although we show that a minority of the faintest broad lines could potentially be associated with the echo of superluminous SNe or TDE. Scenarios in which the broad lines are ascribed to galactic outflows are also untenable. We emphasize that confirming any of the scenarios discussed above will require X-ray missions more sensitive than Chandra.																																	2024-05-21	PPRN:88711631		
J	Gabriel, Iason; Manzini, Arianna; Keeling, Geoff; Hendricks, Lisa Anne; Rieser, Verena; Iqbal, Hasan; Tomasev, Nenad; Ktena, Ira; Kenton, Zachary; Rodriguez, Mikel; El-Sayed, Seliem; Brown, Sasha; Akbulut, Canfer; Trask, Andrew; Hughes, Edward; Stevie Bergman, A.; Shelby, Renee; Marchal, Nahema; Griffin, Conor; Mateos-Garcia, Juan; Weidinger, Laura; Street, Winnie; Lange, Benjamin; Ingerman, Alex; Lentz, Alison; Enger, Reed; Barakat, Andrew; Krakovna, Victoria; Siy, John Oliver; Kurth-Nelson, Zeb; Mccroskery, Amanda; Bolina, Vijay; Law, Harry; Shanahan, Murray; Alberts, Lize; Balle, Borja; Haas, Sarah de; Ibitoye, Yetunde; Dafoe, Allan; Goldberg, Beth; Krier, Sebastien; Reese, Alexander; Witherspoon, Sims; Hawkins, Will; Rauh, Maribeth; Wallace, Don; Franklin, Matija; Goldstein, Josh A.; Lehman, Joel; Klenk, Michael; Vallor, Shannon; Biles, Courtney; Morris, Meredith Ringel; King, Helen; Arcas, Blaise Agueera y; Isaac, William; Manyika, James				Klenk, Michael/AAE-8063-2020; Dafoe, Allan/G-2505-2014; Gabriel, Iason/IST-7093-2023; Shelby, Renee/LHA-1194-2024; Alberts, Lize/AAJ-3618-2020						The Ethics of Advanced AI Assistants								Arxiv											2	2;2024-04-28;https://www.arxiv.org/abs/2404.16244v2| 1;2024-04-24;https://www.arxiv.org/abs/2404.16244v1	arXiv:2404.16244			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 28 2024	2024	This paper focuses on the opportunities and the ethical and societal risks posed by advanced AI assistants. We define advanced AI assistants as artificial agents with natural language interfaces, whose function is to plan and execute sequences of actions on behalf of a user – across one or more domains – in line with the user’s expectations. The paper starts by considering the technology itself, providing an overview of AI assistants, their technical foundations and potential range of applications. It then explores questions around AI value alignment, well-being, safety and malicious uses. Extending the circle of inquiry further, we next consider the relationship between advanced AI assistants and individual users in more detail, exploring topics such as manipulation and persuasion, anthropomorphism, appropriate relationships, trust and privacy. With this analysis in place, we consider the deployment of advanced assistants at a societal scale, focusing on cooperation, equity and access, misinformation, economic impact, the environment and how best to evaluate advanced AI assistants. Finally, we conclude by providing a range of recommendations for researchers, developers, policymakers and public stakeholders.   Our analysis suggests that advanced AI assistants are likely to have a profound impact on our individual and collective lives. To be beneficial and value-aligned, we argue that assistants must be appropriately responsive to the competing claims and needs of users, developers and society. Features such as increased agency, the capacity to interact in natural language and high degrees of personalisation could make AI assistants especially helpful to users. However, these features also make people vulnerable to inappropriate influence by the technology, so robust safeguards are needed. Moreover, when AI assistants are deployed at scale, knock-on effects that arise from interaction between them and questions about their overall impact on wider institutions and social processes rise to the fore. These dynamics likely require technical and policy interventions in order to foster beneficial cooperation and to achieve broad, inclusive and equitable outcomes. Finally, given that the current landscape of AI evaluation focuses primarily on the technical components of AI systems, it is important to invest in the holistic sociotechnical evaluations of AI assistants, including human–AI interaction, multi-agent and societal level research, to support responsible decision-making and deployment in this domain.																																	2024-05-15	PPRN:88651355		
J	Hao, Shibo; Gu, Yi; Luo, Haotian; Liu, Tianyang; Shao, Xiyan; Wang, Xinyuan; Xie, Shuhua; Ma, Haodi; Samavedhi, Adithya; Gao, Qiyue; Wang, Zhen; Hu, Zhiting				Ma, Haodi/LZE-7039-2025; Gu, Yi/JRX-8093-2023; Liu, Tianyang/JOZ-0867-2023; Luo, Haotian/IVM-4849-2023						New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models								Arxiv											1	1;2024-04-08;https://www.arxiv.org/abs/2404.05221v1	arXiv:2404.05221			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 08 2024	2024	Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.																																	2024-04-21	PPRN:88441392		
J	Yuan, Siyu; Song, Kaitao; Chen, Jiangjie; Tan, Xu; Shen, Yongliang; Ren, Kan; Li, Dongsheng; Yang, Deqing				Song, Kaitao/JKJ-5832-2023; Shen, Yongliang/GWC-1883-2022; Chen, Jiangjie/JCE-5486-2023						EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction								Arxiv											3	3;2024-03-27;https://www.arxiv.org/abs/2401.06201v3| 2;2024-02-18;https://www.arxiv.org/abs/2401.06201v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.06201v1	arXiv:2401.06201			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 27 2024	2024	There has been a rising interest in utilizing tools in applications of autonomous agents based on large language models (LLMs) to address intricate real-world tasks. To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentations. However, these documentations could be diverse, redundant, or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EASYTOOL purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EASYTOOL can significantly reduce token consumption and improve the performance of LLM-based agents on tool utilization in real-world scenarios.																																	2024-04-15	PPRN:87153493		
J	Angelopoulos, Anastasios N.; Duchi, John C.; Zrnic, Tijana				Angelopoulos, Anastasios/AAT-5355-2020						PPI++: Efficient Prediction-Powered Inference								Arxiv											2	2;2024-03-26;https://www.arxiv.org/abs/2311.01453v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01453v1	arXiv:2311.01453			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 26 2024	2024	We present PPI++: a computationally lightweight methodology for estimation and inference based on a small labeled dataset and a typically much larger dataset of machine -learning predictions. The methods automatically adapt to the quality of available predictions, yielding easy -to -compute confidence sets—for parameters of any dimensionality—that always improve on classical intervals using only the labeled data. PPI++ builds on prediction -powered inference (PPI), which targets the same problem setting, improving its computational and statistical efficiency. Real and synthetic experiments demonstrate the benefits of the proposed adaptations.																																	2024-04-14	PPRN:85982849		
J	Sprague, Zayne; Ye, Xi; Bostrom, Kaj; Chaudhuri, Swarat; Durrett, Greg				ye, xi/KTH-8756-2024						MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning								Arxiv											2	2;2024-03-23;https://www.arxiv.org/abs/2310.16049v2| 1;2023-10-24;https://www.arxiv.org/abs/2310.16049v1	arXiv:2310.16049			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 23 2024	2024	While large language models (LLMs) equipped with techniques like chain-ofthought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other syntheticallycrafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.1																																	2024-04-14	PPRN:85767572		
J	Vehtari, Aki; Simpson, Daniel; Gelman, Andrew; Yao, Yuling; Gabry, Jonah				Yao, Yuling/ABD-3881-2021; Vehtari, Aki/A-7584-2008						Pareto Smoothed Importance Sampling								Arxiv											1	1;2024-03-13;https://www.arxiv.org/abs/1507.02646v9	arXiv:1507.02646			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 13 2024	2024	Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance ampling estimates, includes stabilized effective sample size estimates, Monte Carlo errores timates, and convergence diagnostics. The presented Paretokfinite sample convergence rate diagnostic is useful for any Monte Carlo estimator																																	2024-04-08	PPRN:88127159		
J	Wu, Minghao; Waheed, Abdul; Zhang, Chiyu; Abdul-Mageed, Muhammad; Aji, Alham Fikri				Zhang, Chiyu/ABG-2731-2021						LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions								Arxiv											2	2;2024-01-29;https://www.arxiv.org/abs/2304.14402v3| 1;2023-04-27;https://www.arxiv.org/abs/2304.14402v1	arXiv:2304.14402			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs into much smaller ones. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizable, we design our instructions to cover a broad set of topics to ensure diversity. Extensive analysis of our instruction dataset confirms its diversity, and we generate responses for these instructions using gpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of models, collectively referred to as LaMini-LM, which includes models from both the encoder-decoder and decoder-only families, with varying sizes. We evaluate the performance of our models using automatic metrics on 15 different natural language processing (NLP) benchmarks, as well as through human assessment. The results demonstrate that our proposed LaMini-LM models are comparable to competitive baselines, while being much smaller in size.																																	2024-05-25	PPRN:65759705		
J	Wang, Xiyao; Zhou, Yuhang; Liu, Xiaoyu; Lu, Hongjin; Xu, Yuancheng; He, Feihong; Yoon, Jaehong; Lu, Taixi; Bertasius, Gedas; Bansal, Mohit; Yao, Huaxiu; Huang, Furong				Zhou, Yuhang/HNC-1322-2023; Bansal, Mohit/Q-9105-2016; Wang, Xiyao/AFN-9739-2022; Yao, Huaxiu/V-3516-2019						Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences								Arxiv											2	2;2024-01-25;https://www.arxiv.org/abs/2401.10529v2| 1;2024-01-19;https://www.arxiv.org/abs/2401.10529v1	arXiv:2401.10529			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs' sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of cooccurring behaviors, and the compounding impact of behavioral hallucinations. 																																	2024-05-25	PPRN:87309883		
J	Liu, Lang; Chen, Zu-Cheng; Huang, Qing-Guo				陈, 祖成/ABG-2281-2020; Liu, Lang/OJS-8538-2025						Implications for the non-Gaussianity of curvature perturbation from pulsar timing arrays								Arxiv											2	2;2024-01-18;https://www.arxiv.org/abs/2307.01102v3| 1;2023-07-31;https://www.arxiv.org/abs/2307.01102v2	arXiv:2307.01102			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 18 2024	2024	The recently released data by pulsar timing array (PTA) collaborations present strong evidence for a stochastic signal consistent with a gravitational-wave background. Assuming this signal originates from scalar-induced gravitational waves, we jointly use the PTA data from the NANOGrav 15-yr data set, PPTA DR3, and EPTA DR2 to probe the small-scale non-Gaussianity. We put the first-ever constraint on the non-Gaussianity parameter, finding |FNL| (sic) 13.9 for a lognormal power spectrum of the curvature perturbations. Furthermore, we obtain −13.9 (sic) FNL (sic) −0.1 to prevent excessive production of primordial black holes. Moreover, the multi-band observations with the space-borne gravitational-wave detectors, such as LISA/Taiji/TianQin, will provide a complementary investigation of primordial non-Gaussianity. Our findings pave the way to constrain inflation models with PTAs.																																	2024-02-03	PPRN:74181905		
J	Vorontsov, Eugene; Bozkurt, Alican; Casson, Adam; Shaikovski, George; Zelechowski, Michal; Liu, Siqi; Severson, Kristen; Zimmermann, Eric; Hall, James; Tenenholtz, Neil; Fusi, Nicolo; Mathieu, Philippe; van Eck, Alexander; Lee, Donghun; Viret, Julian; Robert, Eric; Wang, Yi Kan; Kunz, Jeremy D.; Lee, Matthew C.H.; Bernhard, Jan; Godrich, Ran A.; Oakley, Gerard; Millar, Ewan; Hanna, Matthew; Retamero, Juan; Moye, William A.; Yousfi, Razik; Kanan, Christopher; Klimstra, David; Rothrock, Brandon; Fuchs, Thomas J.				robert, eric/E-7992-2019; liu, siqi/HPF-1938-2023						Virchow: A Million-Slide Digital Pathology Foundation Model								Arxiv											5	5;2024-01-18;https://www.arxiv.org/abs/2309.07778v5| 4;2023-10-28;https://www.arxiv.org/abs/2309.07778v4| 3;2023-09-21;https://www.arxiv.org/abs/2309.07778v3| 2;2023-09-15;https://www.arxiv.org/abs/2309.07778v2| 1;2023-09-14;https://www.arxiv.org/abs/2309.07778v1	arXiv:2309.07778			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jan 18 2024	2024	The use of artificial intelligence to enable precision medicine and decision support systems through the analysis of pathology images has the potential to revolutionize the diagnosis and treatment of cancer. Such applications will depend on models' abilities to capture the diverse patterns observed in pathology images. To address this challenge, we present Virchow, a foundation model for computational pathology. Using self-supervised learning empowered by the DINOv2 algorithm, Virchow is a vision transformer model with 632 million parameters trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue and specimen types, which is orders of magnitude more data than previous works. The Virchow model enables the development of a pan-cancer detection system with 0.949 overall specimen-level AUC across 17 different cancer types, while also achieving 0.937 AUC on 7 rare cancer types. The Virchow model sets the state-of-the-art on the internal and external image tile level benchmarks and slide level biomarker prediction tasks. The gains in performance highlight the importance of training on massive pathology image datasets, suggesting scaling up the data and network architecture can improve the accuracy for many high-impact computational pathology applications where limited amounts of training data are available.																																	2024-02-03	PPRN:85016185		
J	Glazer, Elliot; Erdil, Ege; Besiroglu, Tamay; Chicharro, Diego; Chen, Evan; Gunning, Alex; Olsson, Caroline Falkman; Denain, Jean-Stanislas; Ho, Anson; Santos, Emily de Oliveira; Jarviniemi, Olli; Barnett, Matthew; Sandler, Robert; Vrzala, Matej; Sevilla, Jaime; Ren, Qiuyu; Pratt, Elizabeth; Levine, Lionel; Barkley, Grant; Stewart, Natalie; Grechuk, Bogdan; Grechuk, Tetiana; Enugandla, Shreepranav Varma; Wildon, Mark				Erdil, Erkan/D-2031-2010; Sevilla, Jaime/OHV-3069-2025; Ren, Qiuyu/OHV-3816-2025						FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI								Arxiv											3	3;2024-12-20;https://www.arxiv.org/abs/2411.04872v5| 2;2024-11-22;https://www.arxiv.org/abs/2411.04872v4| 1;2024-11-14;https://www.arxiv.org/abs/2411.04872v3	arXiv:2411.04872			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 20 2024	2024	We introduce FrontierMath, a benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians. The questions cover most major branches of modern mathematics—from computationally intensive problems in number theory and real analysis to abstract questions in algebraic geometry and category theory. Solving a typical problem requires multiple hours of effort from a researcher in the relevant branch of mathematics, and for the upper end questions, multiple days. FrontierMath uses new, unpublished problems and automated verification to reliably evaluate models while minimizing risk of data contamination. Current state-of-the-art AI models solve under 2% of problems, revealing a vast gap between AI capabilities and the prowess of the mathematical community. As AI systems advance toward expert-level mathematical abilities, FrontierMath offers a rigorous testbed that quantifies their progress.																																	2025-01-29	PPRN:119219935		
J	Zhang, Zheyuan; Zhang-Li, Daniel; Yu, Jifan; Gong, Linlu; Zhou, Jinchang; Hao, Zhanxin; Jiang, Jianxiao; Cao, Jie; Liu, Huiqin; Liu, Zhiyuan; Hou, Lei; Li, Juanzi				Liu, Zhiyuan/I-2233-2014; liu, huiqin/LDF-6712-2024; Jiang, Jianxiao/NEU-0004-2025; Zhang-Li, Daniel/OJT-2553-2025; Li, Zhiyuan/ESQ-7168-2022						Simulating Classroom Education with LLM-Empowered Agents								Arxiv											2	2;2024-11-27;https://www.arxiv.org/abs/2406.19226v2| 1;2024-06-27;https://www.arxiv.org/abs/2406.19226v1	arXiv:2406.19226			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	Large language models (LLMs) have been applied across various intelligent educational tasks to assist teaching. While preliminary studies have focused on task-specific, independent LLM-empowered agents, the potential of LLMs within a multi-agent collaborative framework for classroom simulation with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation teaching framework. We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses. Using the Flanders Interactive Analysis System and Community of Inquiry theoretical frameworks from educational analysis, we demonstrate that LLMs can simulate a dynamic learning environment for users with active teacher-student and student-student interactions. We also observe group behaviors among agents in SimClass, where agents collaborate to create enlivening interactions in classrooms to improve user learning process. We hope this work pioneers the application of LLM-empowered multi-agent systems in virtual classroom teaching.																																	2025-01-08	PPRN:90137584		
J	Ku, Max; Wei, Cong; Ren, Weiming; Yang, Harry; Chen, Wenhu										AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks								Arxiv											4	4;2024-11-03;https://www.arxiv.org/abs/2403.14468v4| 3;2024-06-10;https://www.arxiv.org/abs/2403.14468v3| 2;2024-03-22;https://www.arxiv.org/abs/2403.14468v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14468v1	arXiv:2403.14468			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 03 2024	2024	In the dynamic field of digital content creation using generative models, state-of-the-art video editing models still do not offer the level of quality and control that users desire. Previous works on video editing either extended from image-based generative models in a zero-shot manner or necessitated extensive fine-tuning, which can hinder the production of fluid video edits. Furthermore, these methods frequently rely on textual input as the editing guidance, leading to ambiguities and limiting the types of edits they can perform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free paradigm designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model to modify the first frame, (2) utilizing an existing image-to-video generation model to generate the edited video through temporal feature injection. AnyV2V can leverage any existing image editing tools to support an extensive array of video editing tasks, including prompt-based editing, reference-based style transfer, subject-driven editing, and identity manipulation, which were unattainable by previous methods. AnyV2V can also support any video length. Our evaluation shows that AnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore, AnyV2V significantly outperformed these baselines in human evaluations, demonstrating notable improvements in visual consistency with the source video while producing high-quality edits across all editing tasks. The code is available at https://github.com/TIGER-AI-Lab/AnyV2V.																																	2024-12-16	PPRN:88261426		
J	Rawal, Ruchit; Saifullah, Khalid; Farre, Miquel; Basri, Ronen; Jacobs, David; Somepalli, Gowthami; Goldstein, Tom										CinePile: A Long Video Question Answering Dataset and Benchmark								Arxiv											3	3;2024-10-21;https://www.arxiv.org/abs/2405.08813v3| 2;2024-06-14;https://www.arxiv.org/abs/2405.08813v2| 1;2024-05-14;https://www.arxiv.org/abs/2405.08813v1	arXiv:2405.08813			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 21 2024	2024	Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video. To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding. This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data. Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene. Additionally, we fine-tuned open-source Video-LLMs on the training split and evaluated both open-source and proprietary video-centric LLMs on the test split of our dataset. The findings indicate that although current models underperform compared to humans, fine-tuning these models can lead to significant improvements in their performance.																																	2024-11-20	PPRN:89047373		
J	Yin, Tianwei; Gharbi, Michael; Zhang, Richard; Shechtman, Eli; Durand, Fredo; Freeman, William T.; Park, Taesung				Shechtman, Eli/B-2736-2012						One-step Diffusion with Distribution Matching Distillation								Arxiv											3	3;2024-10-04;https://www.arxiv.org/abs/2311.18828v4| 2;2023-12-05;https://www.arxiv.org/abs/2311.18828v3| 1;2023-11-30;https://www.arxiv.org/abs/2311.18828v1	arXiv:2311.18828			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 04 2024	2024	Diffusion models generate high-quality images but require dozens of forward passes. We introduce Distribution Matching Distillation (DMD), a procedure to transform a diffusion model into a one-step image generator with minimal impact on image quality. We enforce the one-step image generator match the diffusion model at distribution level, by minimizing an approximate KL divergence whose gradient can be expressed as the difference between 2 score functions, one of the target distribution and the other of the synthetic distribution being produced by our one-step generator. The score functions are parameterized as two diffusion models trained separately on each distribution. Combined with a simple regression loss matching the large-scale structure of the multi-step diffusion outputs, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.																																	2024-10-27	PPRN:86340511		
J	Banerjee, Sourav; Agarwal, Ayushi; Singla, Saloni										LLMs Will Always Hallucinate, and We Need to Live With This								Arxiv											1	1;2024-09-09;https://www.arxiv.org/abs/2409.05746v1	arXiv:2409.05746			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Sep 09 2024	2024	As Large Language Models become more ubiquitous across domains, it becomes important to examine their inherent limitations critically. This work argues that hallucinations in language models are not just occasional errors but an inevitable feature of these systems. We demonstrate that hallucinations stem from the fundamental mathematical and logical structure of LLMs. It is, therefore, impossible to eliminate them through architectural improvements, dataset enhancements, or factchecking mechanisms. Our analysis draws on computational theory and Gödel’s First Incompleteness Theorem, which references the undecidability of problems like the Halting, Emptiness, and Acceptance Problems. We demonstrate that every stage of the LLM process—from training data compilation to fact retrieval, intent classification, and text generation—will have a non-zero probability of producing hallucinations. This work introduces the concept of "Structural Hallucinations" as an intrinsic nature of these systems. By establishing the mathematical certainty of hallucinations, we challenge the prevailing notion that they can be fully mitigated.																																	2024-09-24	PPRN:91805295		
J	Li, Nathaniel; Han, Ziwen; Steneker, Ian; Primack, Willow; Goodside, Riley; Zhang, Hugh; Wang, Zifan; Menghini, Cristina; Yue, Summer				wang, zifan/HHS-5709-2022						LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet								Arxiv											1	1;2024-09-04;https://www.arxiv.org/abs/2408.15221v2	arXiv:2408.15221			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 04 2024	2024	Recent large language model (LLM) defenses have greatly improved models' ability to refuse harmful queries, even when adversarially attacked. However, LLM defenses are primarily evaluated against automated adversarial attacks in a single turn of conversation, an insufficient threat model for real-world malicious use. We demonstrate that multi-turn human jailbreaks uncover significant vulnerabilities, exceeding 70% attack success rate (ASR) on HarmBench against defenses that report single-digit ASRs with automated single-turn attacks. Human jailbreaks also reveal vulnerabilities in machine unlearning defenses, successfully recovering dual-use biosecurity knowledge from unlearned models. We compile these results into Multi-Turn Human Jailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks. We publicly release MHJ alongside a compendium of jailbreak tactics developed across dozens of commercial red teaming engagements, supporting research towards stronger LLM defenses.																																	2024-09-13	PPRN:91564885		
J	Wang, Hengyi; Agapito, Lourdes										3D Reconstruction with Spatial Memory								Arxiv											1	1;2024-08-28;https://www.arxiv.org/abs/2408.16061v1	arXiv:2408.16061			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 28 2024	2024	We present Spann3R, a novel approach for dense 3D reconstruction from ordered or unordered image collections. Built on the DUSt3R paradigm, Spann3R uses a transformer-based architecture to directly regress pointmaps from images without any prior knowledge of the scene or camera parameters. Unlike DUSt3R, which predicts per image-pair pointmaps each expressed in its local coordinate frame, Spann3R can predict per-image pointmaps expressed in a global coordinate system, thus eliminating the need for optimization-based global alignment. The key idea of Spann3R is to manage an external spatial memory that learns to keep track of all previous relevant 3D information. Spann3R then queries this spatial memory to predict the 3D structure of the next frame in a global coordinate system. Taking advantage of DUSt3R's pre-trained weights, and further fine-tuning on a subset of datasets, Spann3R shows competitive performance and generalization ability on various unseen datasets and can process ordered image collections in real time. 																																	2024-09-19	PPRN:91783595		
J	Laurencon, Hugo; Marafioti, Andres; Sanh, Victor; Tronchon, Leo										Building and better understanding vision-language models: insights and future directions								Arxiv											1	1;2024-08-22;https://www.arxiv.org/abs/2408.12637v1	arXiv:2408.12637			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 22 2024	2024	The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.																																	2024-09-03	PPRN:91532343		
J	Zheng, Lin; Yuan, Jianbo; Yu, Lei; Kong, Lingpeng				kong, lingpeng/NHQ-3170-2025; 袁, 剑波/JVO-4766-2024						A Reparameterized Discrete Diffusion Model for Text Generation								Arxiv											2	2;2024-08-02;https://www.arxiv.org/abs/2302.05737v3| 1;2024-02-03;https://www.arxiv.org/abs/2302.05737v2	arXiv:2302.05737			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 02 2024	2024	This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.																																	2024-08-08	PPRN:87523998		
J	Drouin, Alexandre; Gasse, Maxime; Caccia, Massimo; Laradji, Issam H.; Del Verme, Manuel; Marty, Tom; Boisvert, Leo; Thakkar, Megh; Cappart, Quentin; Vazquez, David; Chapados, Nicolas; Lacoste, Alexandre				Caccia, Massimo/C-5407-2012; GASSE, Maxime/M-1444-2013						WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?								Arxiv											4	4;2024-07-23;https://www.arxiv.org/abs/2403.07718v5| 3;2024-06-14;https://www.arxiv.org/abs/2403.07718v3| 2;2024-04-21;https://www.arxiv.org/abs/2403.07718v2| 1;2024-03-12;https://www.arxiv.org/abs/2403.07718v1	arXiv:2403.07718			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 23 2024	2024	We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 33 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.																																	2024-07-29	PPRN:88119187		
J	Zhang, Beichen; Zhang, Pan; Dong, Xiaoyi; Zang, Yuhang; Wang, Jiaqi				Dong, Xiaoyi/AAC-8666-2019; Zhang, Beichen/KIE-6691-2024; WANG, JIAQI/KBB-8837-2024; Zang, Yuhang/AES-3018-2022						Long-CLIP: Unlocking the Long-Text Capability of CLIP								Arxiv											3	3;2024-07-22;https://www.arxiv.org/abs/2403.15378v3| 2;2024-05-23;https://www.arxiv.org/abs/2403.15378v2| 1;2024-03-22;https://www.arxiv.org/abs/2403.15378v1	arXiv:2403.15378			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 22 2024	2024	Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites. To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its zero-shot generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks. Nevertheless, achieving this goal is far from straightforward, as simplistic fine-tuning can result in a significant degradation of CLIP's performance. Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses. Accordingly, Long-CLIP introduces an efficient fine-tuning solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features. With leveraging just one million extra long text-image pairs, Long-CLIP has shown the superiority to CLIP for about 20% in long caption text-image retrieval and 6% in traditional text-image retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner.																																	2024-07-28	PPRN:88263886		
J	Ren, Weiming; Yang, Huan; Zhang, Ge; Wei, Cong; Du, Xinrun; Huang, Wenhao; Chen, Wenhu				Huang, Wenhao/GWU-9337-2022						ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2402.04324v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.04324v1	arXiv:2402.04324			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Image-to-video (I2V) generation aims to use the initial frame (alongside a text prompt) to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative ( cf. Figure 1). To mitigate these issues, we propose CoNsIsTI2V1, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable CoNsIsTI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of CoNsIsTI2V over existing methods.																																	2024-07-18	PPRN:87567592		
J	Lee, Seungjae; Wang, Yibin; Etukuru, Haritheja; Kim, H. Jin; Shafiullah, Nur Muhammad Mahi; Pinto, Lerrel										Behavior Generation with Latent Actions								Arxiv											1	1;2024-06-28;https://www.arxiv.org/abs/2403.03181v2	arXiv:2403.03181			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 28 2024	2024	Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies. Videos and code can be found https://sjlee.cc/vq-bet																																	2025-08-13	PPRN:123423152		
J	Xu, Zhengzhuo; Du, Sinan; Qi, Yiyan; Xu, Chengjin; Yuan, Chun; Guo, Jian										ChartBench: A Benchmark for Complex Visual Reasoning in Charts								Arxiv											3	3;2024-06-19;https://www.arxiv.org/abs/2312.15915v3| 2;2024-01-29;https://www.arxiv.org/abs/2312.15915v2| 1;2023-12-26;https://www.arxiv.org/abs/2312.15915v1	arXiv:2312.15915			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 19 2024	2024	Multimodal Large Language Models (MLLMs) have shown impressive capabilities in image understanding and generation. However, current benchmarks fail to accurately evaluate the chart comprehension of MLLMs due to limited chart types and inappropriate metrics. To address this, we propose ChartBench, a comprehensive benchmark designed to assess chart comprehension and data reliability through complex visual reasoning. ChartBench includes 42 categories, 66.6k charts, and 600k question-answer pairs. Notably, many charts lack data point annotations, which requires MLLMs to derive values similar to human understanding by leveraging inherent chart elements such as color, legends, and coordinate systems. We also design an enhanced evaluation metric, Acc+, to evaluate MLLMs without extensive manual or costly LLM-based evaluations. Furthermore, we propose two baselines based on the chain of thought and supervised fine-tuning to improve model performance on unannotated charts. Extensive experimental evaluations of 18 open-sourced and 3 proprietary MLLMs reveal their limitations in chart comprehension and offer valuable insights for further research. Code and dataset are publicly available at https://chartbench.github.io.																																	2024-07-10	PPRN:86827427		
J	Liang, Weixin; Izzo, Zachary; Zhang, Yaohui; Lepp, Haley; Cao, Hancheng; Zhao, Xuandong; Chen, Lingjiao; Ye, Haotian; Liu, Sheng; Huang, Zhi; McFarland, Daniel A.; Zou, James Y.				Lepp, Haley/MTB-9361-2025; Zhao, Xuandong/LIG-4204-2024						Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews								Arxiv											2	2;2024-06-15;https://www.arxiv.org/abs/2403.07183v2| 1;2024-03-11;https://www.arxiv.org/abs/2403.07183v1	arXiv:2403.07183			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 15 2024	2024	We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.																																	2024-07-04	PPRN:88116746		
J	Zheng, Huaixiu Steven; Mishra, Swaroop; Zhang, Hugh; Chen, Xinyun; Chen, Minmin; Nova, Azade; Hou, Le; Cheng, Heng-Tze; Le, Quoc V.; Chi, Ed H.; Zhou, Denny				Chen, Xinyun/ABZ-9877-2022						NATURAL PLAN: Benchmarking LLMs on Natural Language Planning								Arxiv											1	1;2024-06-06;https://www.arxiv.org/abs/2406.04520v1	arXiv:2406.04520			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 06 2024	2024	We introduce NATURAL PLAN, a realistic planning benchmark in natural language containing 3 key tasks: Trip Planning, Meeting Planning, and Calendar Scheduling. We focus our evaluation on the planning capabilities of LLMs with full information on the task, by providing outputs from tools such as Google Flights, Google Maps, and Google Calendar as contexts to the models. This eliminates the need for a tool-use environment for evaluating LLMs on Planning. We observe that NATURAL PLAN is a challenging benchmark for state of the art models. For example, in Trip Planning, GPT-4 and Gemini 1.5 Pro could only achieve 31.1% and 34.8% solve rate respectively. We find that model performance drops drastically as the complexity of the problem increases: all models perform below 5% when there are 10 cities, highlighting a significant gap in planning in natural language for SoTA LLMs. We also conduct extensive ablation studies on NATURAL PLAN to further shed light on the (in)effectiveness of approaches such as self-correction, few-shot generalization, and in-context planning with long-contexts on improving LLM planning.																																	2024-06-22	PPRN:89237028		
J	Xu, Kunpeng; Chen, Lifei; Wang, Shengrui				Xu, Kunpeng/JXL-9994-2024						Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability								Arxiv											1	1;2024-06-04;https://www.arxiv.org/abs/2406.02496v1	arXiv:2406.02496			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 04 2024	2024	Kolmogorov-Arnold Networks (KAN) is a groundbreaking model recently proposed by the MIT team, representing a revolutionary approach with the potential to be a game-changer in the field. This innovative concept has rapidly garnered worldwide interest within the AI community. Inspired by the Kolmogorov-Arnold representation theorem, KAN utilizes spline-parametrized univariate functions in place of traditional linear weights, enabling them to dynamically learn activation patterns and significantly enhancing interpretability. In this paper, we explore the application of KAN to time series forecasting and propose two variants: T-KAN and MT-KAN. T-KAN is designed to detect concept drift within time series and can explain the nonlinear relationships between predictions and previous time steps through symbolic regression, making it highly interpretable in dynamically changing environments. MT-KAN, on the other hand, improves predictive performance by effectively uncovering and leveraging the complex relationships among variables in multivariate time series. Experiments validate the effectiveness of these approaches, demonstrating that T-KAN and MT-KAN significantly outperform traditional methods in time series forecasting tasks, not only enhancing predictive accuracy but also improving model interpretability. This research opens new avenues for adaptive forecasting models, highlighting the potential of KAN as a powerful and interpretable tool in predictive analytics.																																	2024-06-22	PPRN:89259402		
J	Wu, Haixu; Luo, Huakun; Wang, Haowen; Wang, Jianmin; Long, Mingsheng				Wang, Haowen/LTF-6384-2024; Wang, Jianmin/AAA-2496-2020						Transolver: A Fast Transformer Solver for PDEs on General Geometries								Arxiv											3	3;2024-06-01;https://www.arxiv.org/abs/2402.02366v2| 2;2024-02-04;https://www.arxiv.org/abs/2402.02366v1| 1;2024-02-04;https://www.arxiv.org/abs/2402.02366v1	arXiv:2402.02366			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 01 2024	2024	Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-general modeling capacity and can be efficiently computed in linear complexity. Transolver achieves consistent state-of-the-art with 22% relative gain across six standard benchmarks and also excels in large-scale industrial simulations, including car and airfoil designs. 																																	2024-06-22	PPRN:87523845		
J	Wang, Ruocheng; Zelikman, Eric; Poesia, Gabriel; Pu, Yewen; Haber, Nick; Goodman, Noah D.										Hypothesis Search: Inductive Reasoning with Language Models								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2309.05660v2| 1;2023-09-11;https://www.arxiv.org/abs/2309.05660v1	arXiv:2309.05660			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding “in context learning.” This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline’s effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100 -problem subset of ARC, our automated pipeline using LLM summaries achieves 30% accuracy, outperforming the direct prompting baseline (accuracy of 17%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33%. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks.																																	2024-06-17	PPRN:84948422		
J	Kong, Zhifeng; Goel, Arushi; Badlani, Rohan; Ping, Wei; Valle, Rafael; Catanzaro, Bryan				Ping, Wei/O-4470-2019; Goel, Arushi/MTE-8311-2025						Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities								Arxiv											2	2;2024-05-28;https://www.arxiv.org/abs/2402.01831v3| 1;2024-02-02;https://www.arxiv.org/abs/2402.01831v1	arXiv:2402.01831			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	Augmenting large language models (LLMs) to understand audio – including non-speech sounds and non-verbal speech – is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, , a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via incontext learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https:// github.com/NVIDIA/audio-flamingo. .																																	2024-06-14	PPRN:87524095		
J	Xiao, Chaojun; Zhang, Pengle; Han, Xu; Xiao, Guangxuan; Lin, Yankai; Zhang, Zhengyan; Liu, Zhiyuan; Sun, Maosong				Liu, Zhiyuan/I-2233-2014; zhengyan, zhang/D-2029-2012						InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory								Arxiv											2	2;2024-05-28;https://www.arxiv.org/abs/2402.04617v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.04617v1	arXiv:2402.04617			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to 1,024K, InfLLM still effectively captures long-distance dependencies. 																																	2024-06-12	PPRN:87553196		
J	Bhardwaj, Lakshya; Pajer, Daniel; Schafer-Nameki, Sakura; Warman, Alison				Bhardwaj, Lakshya/ISU-5186-2023						Hasse Diagrams for Gapless SPT and SSB Phases with Non-Invertible Symmetries								Arxiv											2	2;2024-05-13;https://www.arxiv.org/abs/2403.00905v2| 1;2024-03-01;https://www.arxiv.org/abs/2403.00905v1	arXiv:2403.00905			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 13 2024	2024	We discuss (1+1)d gapless phases with non-invertible global symmetries, also referred to as categorical symmetries. This includes gapless phases showing properties analogous to gapped symmetry protected topological (SPT) phases, known as gapless SPT (or gSPT) phases; and gapless phases showing properties analogous to gapped spontaneous symmetry broken (SSB) phases, that we refer to as gapless SSB (or gSSB) phases. We fit these gapless phases, along with gapped SPT and SSB phases, into a phase diagram describing possible deformations connecting them. This phase diagram is partially ordered and defines a so-called Hasse diagram. Based on these deformations, we identify gapless phases exhibiting symmetry protected criticality, that we refer to as intrinsically gapless SPT (igSPT) and intrinsically gapless SSB (igSSB) phases. This includes the first examples of igSPT and igSSB phases with non-invertible symmetries. Central to this analysis is the Symmetry Topological Field Theory (SymTFT), where each phase corresponds to a condensable algebra in the Drinfeld center of the symmetry category. On a mathematical note, gSPT phases are classified by functors between fusion categories, generalizing the fact that gapped SPT phases are classified by fiber functors; and gSSB phases are classified by functors from fusion to multi-fusion categories. Finally, our framework can be applied to understand gauging of trivially acting non-invertible symmetries, including possible patterns of decomposition arising due to such gaugings.																																	2024-06-08	PPRN:88023949		
J	Shen, Zhiqiang; Tao, Tianhua; Ma, Liqun; Neiswanger, Willie; Liu, Zhengzhong; Wang, Hongyi; Tan, Bowen; Hestness, Joel; Vassilieva, Natalia; Soboleva, Daria; Xing, Eric				Tan, Bowen/LDE-8670-2024; Wang, Hongyi/KMA-5952-2024						SlimPajama-DC: Understanding Data Combinations for LLM Training								Arxiv											3	3;2024-05-09;https://www.arxiv.org/abs/2309.10818v3| 2;2023-10-09;https://www.arxiv.org/abs/2309.10818v2| 1;2023-09-19;https://www.arxiv.org/abs/2309.10818v1	arXiv:2309.10818			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 09 2024	2024	This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajamaDC , an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16× × CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) ) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: link1 and the separate SlimPajama-DC datasets are available at: link2.																																	2024-05-28	PPRN:85052919		
J	Li, Liunian Harold; Hessel, Jack; Yu, Youngjae; Ren, Xiang; Chang, Kai-Wei; Choi, Yejin				ren, xiang/HLQ-5068-2023; Chang, Kai-Wei/AAJ-7874-2020						Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2306.14050v2| 1;2023-06-24;https://www.arxiv.org/abs/2306.14050v1	arXiv:2306.14050			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 15 2024	2024	Chain-of-thought prompting (e.g., "Let's think step-by-step") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.																																	2024-04-26	PPRN:73533047		
J	Qian, Shenhan; Kirschstein, Tobias; Schoneveld, Liam; Davoli, Davide; Giebenhain, Simon; Niessner, Matthias				Schoneveld, Liam/AFU-7558-2022; Qian, Shenhan/JXL-1437-2024						GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians								Arxiv											2	2;2024-03-28;https://www.arxiv.org/abs/2312.02069v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02069v1	arXiv:2312.02069			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 28 2024	2024	We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.																																	2024-04-14	PPRN:86376935		
J	Shi, Lucy Xiaoyang; Hu, Zheyuan; Zhao, Tony Z.; Sharma, Archit; Pertsch, Karl; Luo, Jianlan; Levine, Sergey; Finn, Chelsea				王, 枫梅/AFV-7127-2022						Yell At Your Robot: Improving On-the-Fly from Language Corrections								Arxiv											1	1;2024-03-19;https://www.arxiv.org/abs/2403.12910v1	arXiv:2403.12910			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 19 2024	2024	Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements ("move a bit to the left"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation.																																	2024-04-12	PPRN:88244691		
J	Zhou, Weikang; Wang, Xiao; Xiong, Limao; Xia, Han; Gu, Yingshuang; Chai, Mingxu; Zhu, Fukang; Huang, Caishuang; Dou, Shihan; Xi, Zhiheng; Zheng, Rui; Gao, Songyang; Zou, Yicheng; Yan, Hang; Le, Yifan; Wang, Ruohui; Li, Lijun; Shao, Jing; Gui, Tao; Zhang, Qi; Huang, Xuanjing				Gui, Tao/LWI-6783-2024; Xi, Zhiheng/KUD-1665-2024; Zou, Yicheng/ISU-0863-2023						EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.12171v1	arXiv:2403.12171			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs). They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs. It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs. Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks. Notably, even advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success Rates (ASR) of 57% and 33%, respectively. We have released a wealth of resources for researchers, including a web platform, PyPI published package, screencast video, and experimental outputs.																																	2024-04-12	PPRN:88239373		
J	Liu, Fangchen; Fang, Kuan; Abbeel, Pieter; Levine, Sergey										MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting								Arxiv											1	1;2024-03-05;https://www.arxiv.org/abs/2403.03174v1	arXiv:2403.03174			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 05 2024	2024	Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.																																	2024-04-03	PPRN:88035097		
J	Tang, Zhengyang; Zhang, Xingxing; Wang, Benyou; Wei, Furu				Wang, Benyou/Y-5146-2019; Zhang, Xingxing/KYQ-3478-2024						MathScale: Scaling Instruction Tuning for Mathematical Reasoning								Arxiv											1	1;2024-03-05;https://www.arxiv.org/abs/2403.02884v1	arXiv:2403.02884			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 05 2024	2024	Large language models (LLMs) have demonstrated remarkable capabilities in problemsolving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., GPT-3.5). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math questionanswer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct MWPBENCH, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine -tune open -source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on MWPBENCH, MathScale7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9% in micro average accuracy and 43.7% in macro average accuracy, respectively.																																	2024-04-02	PPRN:88028903		
J	Rottger, Paul; Hofmann, Valentin; Pyatkin, Valentina; Hinck, Musashi; Kirk, Hannah Rose; Schuetze, Hinrich; Hovy, Dirk				Hovy, Dirk/MVX-7752-2025						Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2402.16786v1	arXiv:2402.16786			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.																																	2024-07-23	PPRN:87887076		
J	Zhu, Zhengbang; Zhao, Hanye; He, Haoran; Zhong, Yichao; Zhang, Shenyu; Guo, Haoquan; Chen, Tingting; Zhang, Weinan				He, Haoran/JOZ-0743-2023; Zhu, Zhengbang/JJF-0358-2023; Chen, Tingting/AAL-7890-2021						Diffusion Models for Reinforcement Learning: A Survey								Arxiv											4	4;2024-02-23;https://www.arxiv.org/abs/2311.01223v4| 3;2024-02-05;https://www.arxiv.org/abs/2311.01223v3| 2;2023-12-11;https://www.arxiv.org/abs/2311.01223v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01223v1	arXiv:2311.01223			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 23 2024	2024	Diffusion models surpass previous generative models in sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions. This survey aims to provide an overview of this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by RL algorithms. Then, we present a taxonomy of existing methods based on the roles of diffusion models in RL and explore how the preceding challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks. Finally, we conclude the survey and offer insights into future research directions. We are actively maintaining a GitHub repository for papers and other related resources in utilizing diffusion models in RL.																																	2024-03-23	PPRN:85976709		
J	Wan, Fanqi; Huang, Xinting; Cai, Deng; Quan, Xiaojun; Bi, Wei; Shi, Shuming										Knowledge Fusion of Large Language Models								Arxiv											1	1;2024-01-22;https://www.arxiv.org/abs/2401.10491v2	arXiv:2401.10491			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 22 2024	2024	While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures-Llama-2, MPT, and OpenLLaMA-across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. 																																	2024-05-25	PPRN:87277409		
J	Luong, Trung Quoc; Zhang, Xinbo; Jie, Zhanming; Sun, Peng; Jin, Xiaoran; Li, Hang										ReFT: Reasoning with Reinforced Fine-Tuning								Arxiv											3	3;2024-12-13;https://www.arxiv.org/abs/2401.08967v3| 2;2024-06-27;https://www.arxiv.org/abs/2401.08967v2| 1;2024-01-17;https://www.arxiv.org/abs/2401.08967v1	arXiv:2401.08967			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 13 2024	2024	One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT 1 .																																	2025-01-22	PPRN:87211117		
J	Wang, Xiong; Li, Yangze; Fu, Chaoyou; Shen, Yunhang; Xie, Lei; Li, Ke; Sun, Xing; Ma, Long				Li, Ke/KSM-7426-2024; Shen, Yunhang/ADW-0834-2022						Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM								Arxiv											5	5;2024-12-08;https://www.arxiv.org/abs/2411.00774v5| 4;2024-11-29;https://www.arxiv.org/abs/2411.00774v4| 3;2024-11-21;https://www.arxiv.org/abs/2411.00774v3| 2;2024-11-12;https://www.arxiv.org/abs/2411.00774v2| 1;2024-11-01;https://www.arxiv.org/abs/2411.00774v1	arXiv:2411.00774			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 08 2024	2024	Rapidly developing large language models (LLMs) have brought tremendous intelligent applications. Especially, the GPT-4o's excellent duplex speech interaction ability has brought impressive experience to users. Researchers have recently proposed several multi-modal LLMs in this direction that can achieve user-agent speech-to-speech conversations. This paper proposes a novel speech-text multimodal LLM architecture called Freeze-Omni. Our main contribution is that the speech input and output modalities can be easily connected to a textual LLM while keeping the LLM's parameters frozen throughout the training process. We design a three-stage training strategy for modeling both the speech input and output, enabling Freeze-Omni to obtain speech-to-speech conversation ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while achieving low latency end-to-end spoken response. In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, giving Freeze-Omni a more natural style of dialogue ability between users and agents. In summary, Freeze-Omni holds great potential to conduct speech-to-speech dialogue based on a multimodal LLM under the condition of a frozen LLM, avoiding the catastrophic forgetting problem caused by limited data and training resources.																																	2025-01-17	PPRN:119043362		
J	Xie, Xingyu; Zhou, Pan; Li, Huan; Lin, Zhouchen; Yan, Shuicheng				Yan, Shuicheng/HCI-1431-2022; Zhou, Pan/HNC-2442-2023						Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models								Arxiv											2	2;2024-11-29;https://www.arxiv.org/abs/2208.06677v5| 1;2022-08-13;https://www.arxiv.org/abs/2208.06677v2	arXiv:2208.06677			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 29 2024	2024	deep learning, different kinds of deep networks typically need different optimizers, which have to be chosen after multiple trials, making the training process inefficient. To relieve this issue and consistently improve the model training speed across deep networks, we propose the ADAptive Nesterov momentum algorithm, Adan for short. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation (NME) method, which avoids the extra overhead of computing gradient at the extrapolation point. Then Adan adopts NME to estimate the gradient’s first- and second-order moments in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an ϵ-approximate first-order stationary point within O ( ϵ − 3 . 5 ) stochastic gradient complexity on the non-convex stochastic problems ( e.g. deep learning problems), matching the best-known lower bound. Extensive experimental results show that Adan consistently surpasses the corresponding SoTA optimizers on vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g. ResNet, ConvNext, ViT, Swin, MAE, DETR, GPT-2, Transformer-XL, and BERT. More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, GPT-2, MAE, etc, and also shows great tolerance to a large range of minibatch size, e.g. from 1k to 32k. 																																	2025-01-11	PPRN:12901334		
J	Li, Wei; Bishop, William; Li, Alice; Rawles, Chris; Campbell-Ajala, Folawiyo; Tyamagundlu, Divya; Riva, Oriana										On the Effects of Data Scale on UI Control Agents								Arxiv											4	4;2024-11-13;https://www.arxiv.org/abs/2406.03679v6| 3;2024-11-04;https://www.arxiv.org/abs/2406.03679v5| 2;2024-06-11;https://www.arxiv.org/abs/2406.03679v2| 1;2024-06-01;	arXiv:2406.03679			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Nov 13 2024	2024	Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 14,548 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.																																	2024-12-23	PPRN:89287843		
J	Zhou, Andy; Li, Bo; Wang, Haohan				Wang, Haohan/MEO-3902-2025						Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks								Arxiv											5	5;2024-11-08;https://www.arxiv.org/abs/2401.17263v5| 4;2024-07-08;https://www.arxiv.org/abs/2401.17263v4| 3;2024-06-05;https://www.arxiv.org/abs/2401.17263v3| 2;2024-02-02;https://www.arxiv.org/abs/2401.17263v2| 1;2024-01-30;https://www.arxiv.org/abs/2401.17263v1	arXiv:2401.17263			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 08 2024	2024	Despite advances in AI alignment, large language models (LLMs) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries can modify prompts to induce unwanted behavior. While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models. To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO) to create robust system-level defenses. Our approach directly incorporates the adversary into the defensive objective and optimizes a lightweight and transferable suffix, enabling RPO to adapt to worst-case adaptive attacks. Our theoretical and experimental results show improved robustness to both jailbreaks seen during optimization and unknown jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2 to 0% on JailbreakBench, setting the state-of-the-art. 																																	2024-12-16	PPRN:87419478		
J	Wang, Fali; Zhang, Zhiwei; Zhang, Xianren; Wu, Zongyu; Mo, Tzuhao; Lu, Qiuhao; Wang, Wanjing; Li, Rui; Xu, Junjie; Tang, Xianfeng; He, Qi; Ma, Yao; Huang, Ming; Wang, Suhang				Wang, Wanjing/LRT-7010-2024; Wu, Zong Yu/GRX-1989-2022; Tang, Xianfeng/IWM-0393-2023; Wang, Suhang/AAH-1378-2019; Wang, Fali/KOD-5704-2024; Lu, Qiuhao/AEM-9168-2022; 许, 俊杰/HHC-2199-2022; Zhang, Xianren/P-6595-2015						A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness								Arxiv											1	1;2024-11-04;https://www.arxiv.org/abs/2411.03350v1	arXiv:2411.03350			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	Large language models (LLM) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like LaPM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively.																																	2024-12-16	PPRN:119060126		
J	Ye, Fanghua; Yang, Mingming; Pang, Jianhui; Wang, Longyue; Wong, Derek F.; Yilmaz, Emine; Shi, Shuming; Tu, Zhaopeng				Pang, Jianhui/KFQ-6031-2024; Tu, Zhaopeng/AAS-4259-2021; Wong, Derek F/CAI-7740-2022; Yılmaz Arslan, Emine/LZG-8302-2025; yang, mingming/HZH-9027-2023; Ye, Fanghua/KIH-6611-2024						Benchmarking LLMs via Uncertainty Quantification								Arxiv											3	3;2024-10-31;https://www.arxiv.org/abs/2401.12794v3| 2;2024-04-25;https://www.arxiv.org/abs/2401.12794v2| 1;2024-01-23;https://www.arxiv.org/abs/2401.12794v1	arXiv:2401.12794			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 31 2024	2024	The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect – uncertainty , which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty ; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts ; and III) Instruction-finetuning tends to increase the uncertainty of LLMs . These results underscore the significance of incorporating uncertainty into the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench .																																	2024-12-06	PPRN:87300190		
J	Tam, Zhi Rui; Wu, Cheng-Kuang; Tsai, Yi-Lin; Lin, Chieh-Yen; Lee, Hung-yi; Chen, Yun-Nung				Chen, Jung-Chien/J-5386-2015						Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models								Arxiv											3	3;2024-10-14;https://www.arxiv.org/abs/2408.02442v3| 2;2024-09-21;https://www.arxiv.org/abs/2408.02442v2| 1;2024-08-05;https://www.arxiv.org/abs/2408.02442v1	arXiv:2408.02442			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 14 2024	2024	Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs). This study investigates whether such constraints on generation space impact LLMs’ abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs’ performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs’ reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks. Our code and results are available online.1																																	2024-11-05	PPRN:91245972		
J	Chen, Yiwen; He, Tong; Huang, Di; Ye, Weicai; Chen, Sijin; Tang, Jiaxiang; Chen, Xin; Cai, Zhongang; Yang, Lei; Yu, Gang; Lin, Guosheng; Zhang, Chi										MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers								Arxiv											2	2;2024-10-09;https://www.arxiv.org/abs/2406.10163v2| 1;2024-06-14;https://www.arxiv.org/abs/2406.10163v1	arXiv:2406.10163			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 09 2024	2024	Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists. Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality. To address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes. By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry. The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.																																	2024-10-24	PPRN:89328845		
J	Yuan, Tongxin; He, Zhiwei; Dong, Lingzhong; Wang, Yiming; Zhao, Ruijie; Xia, Tian; Xu, Lizhen; Zhou, Binglin; Li, Fangqi; Zhang, Zhuosheng; Wang, Rui; Liu, Gongshen				xu, lizhen/GZM-1151-2022; wang, rui/JAC-6240-2023; Wang, Yiming/T-4946-2017; Zhang, Zhuosheng/AAF-4919-2020; Zhao, RJ/IQS-1681-2023						R-Judge: Benchmarking Safety Risk Awareness for LLM Agents								Arxiv											3	3;2024-10-05;https://www.arxiv.org/abs/2401.10019v3| 2;2024-02-18;https://www.arxiv.org/abs/2401.10019v2| 1;2024-01-18;https://www.arxiv.org/abs/2401.10019v1	arXiv:2401.10019			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 05 2024	2024	Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on the harmlessness of LLM-generated content in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 569 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 5 application categories and 10 risk types. It is of high-quality curation with annotated safety labels and risk descriptions. Evaluation of 11 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models significantly exceed the random. Moreover, we reveal that risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for LLMs. With further experiments, we find that fine-tuning on safety judgment significantly improve model performance while straightforward prompting mechanisms fail. 																																	2024-10-27	PPRN:87222561		
J	Kazemnejad, Amirhossein; Aghajohari, Milad; Portelance, Eva; Sordoni, Alessandro; Reddy, Siva; Courville, Aaron; Roux, Nicolas Le										VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment								Arxiv											1	1;2024-10-02;https://www.arxiv.org/abs/2410.01679v1	arXiv:2410.01679			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 02 2024	2024	Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative.																																	2024-10-16	PPRN:100926317		
J	Renze, Matthew; Guven, Erhan				Güven, Hikmet/AEM-2339-2022						The Effect of Sampling Temperature on Problem Solving in Large Language Models								Arxiv											3	3;2024-10-02;https://www.arxiv.org/abs/2402.05201v3| 2;2024-06-14;https://www.arxiv.org/abs/2402.05201v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.05201v1	arXiv:2402.05201			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 02 2024	2024	In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. 																																	2024-10-18	PPRN:87572504		
J	Li, Haoran; Chen, Yulin; Luo, Jinglong; Wang, Jiecong; Peng, Hao; Kang, Yan; Zhang, Xiaojin; Hu, Qi; Chan, Chunkit; Xu, Zenglin; Hooi, Bryan; Song, Yangqiu				zhang, xiaojin/E-9639-2015; Li, Haoran/KHW-8005-2024; Song, Yangqiu/OHU-0096-2025; Xu, Zenglin/HHZ-8366-2022						Privacy in Large Language Models: Attacks, Defenses and Future Directions								Arxiv											2	2;2024-09-30;https://www.arxiv.org/abs/2310.10383v2| 1;2023-10-16;https://www.arxiv.org/abs/2310.10383v1	arXiv:2310.10383			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 30 2024	2024	With the advancement of deep learning and transformer models, large language models (LLMs) have significantly enhanced the ability to effectively tackle various downstream NLP tasks and unify these tasks into generative pipelines. On the one hand, powerful language models, trained on massive textual data, have brought unparalleled accessibility and usability for both models and users. These LLMs have significantly lowered the entry barrier for application developers and users, as they provide pre-trained language understanding and instruction-following capabilities. The availability of powerful LLMs has opened up new possibilities across various fields, including LLM-enabled agents, virtual assistants, chatbots, and more. On the other hand, unrestricted access to these models can also introduce potential malicious and unintentional privacy risks. The same capabilities that make these models valuable tools can also be exploited for malicious purposes or unintentionally compromise sensitive information. Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved. In this paper, we aim to offer a thorough examination of the current privacy attacks targeting LLMs and categorize them according to the adversary’s assumed capabilities to shed light on the potential vulnerabilities presented in LLMs. Then, we delve into an exploration of prominent defense strategies that have been developed to mitigate the risks of these privacy attacks. In addition to discussing existing works, we also address the upcoming privacy concerns that may arise as these LLMs continue to evolve. Lastly, we conclude our paper by highlighting several promising directions for future research and exploration in the field of LLM privacy. By identifying these research directions, we aim to inspire further advancements in privacy protection for LLMs and contribute to more secure and privacy-aware development of these powerful LLMs. With this survey, we hope to provide valuable insights into the potential vulnerabilities that exist within LLMs, thus highlighting the importance of addressing privacy concerns in their development and applications.																																	2024-10-10	PPRN:85661417		
J	Li, Chengshu; Liang, Jacky; Zeng, Andy; Chen, Xinyun; Hausman, Karol; Sadigh, Dorsa; Levine, Sergey; Fei-Fei, Li; Xia, Fei; Ichter, Brian				Chen, Xinyun/ABZ-9877-2022; Xia, Fei/AAW-8782-2021						Chain of Code: Reasoning with a Language Model-Augmented Code Emulator								Arxiv											3	3;2024-07-29;https://www.arxiv.org/abs/2312.04474v4| 2;2023-12-08;https://www.arxiv.org/abs/2312.04474v2| 1;2023-12-07;https://www.arxiv.org/abs/2312.04474v1	arXiv:2312.04474			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 29 2024	2024	Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter - we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively "emulate" the interpreter by generating the expected output of "detect_sarcasm(string)". In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by "thinking in code".																																	2024-08-06	PPRN:86442594		
J	Isensee, Fabian; Wald, Tassilo; Ulrich, Constantin; Baumgartner, Michael; Roy, Saikat; Maier-Hein, Klaus; Jaeger, Paul F.				Maier-Hein, Klaus/AAF-8487-2020; Roy, Saikat/AAE-6683-2020; Isensee, Fabian/ADD-4248-2022						nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image Segmentation								Arxiv											2	2;2024-07-25;https://www.arxiv.org/abs/2404.09556v2| 1;2024-04-15;https://www.arxiv.org/abs/2404.09556v1	arXiv:2404.09556			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 25 2024	2024	The release of nnU-Net marked a paradigm shift in 3D medical image segmentation, demonstrating that a properly configured U-Net architecture could still achieve state-of-the-art results. Despite this, the pursuit of novel architectures, and the respective claims of superior performance over the U-Net baseline, continued. In this study, we demonstrate that many of these recent claims fail to hold up when scrutinized for common validation shortcomings, such as the use of inadequate baselines, insufficient datasets, and neglected computational resources. By meticulously avoiding these pitfalls, we conduct a thorough and comprehensive benchmarking of current segmentation methods including CNN-based, Transformer-based, and Mamba-based approaches. In contrast to current beliefs, we find that the recipe for state-of-the-art performance is 1) employing CNN-based U-Net models, including ResNet and ConvNeXt variants, 2) using the nnU-Net framework, and 3) scaling models to modern hardware resources. These results indicate an ongoing innovation bias towards novel architectures in the field and underscore the need for more stringent validation standards in the quest for scientific progress.																																	2024-08-02	PPRN:88530712		
J	Wang, Zhouxia; Yuan, Ziyang; Wang, Xintao; Li, Yaowei; Chen, Tianshui; Xia, Menghan; Luo, Ping; Shan, Ying				Luo, Ping/HGE-7623-2022						MotionCtrl: A Unified and Flexible Motion Controller for Video Generation								Arxiv											2	2;2024-07-16;https://www.arxiv.org/abs/2312.03641v2| 1;2023-12-06;https://www.arxiv.org/abs/2312.03641v1	arXiv:2312.03641			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 16 2024	2024	Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods. 																																	2024-07-25	PPRN:86418311		
J	Baechler, Gilles; Sunkara, Srinivas; Wang, Maria; Zubach, Fedir; Mansoor, Hassan; Etter, Vincent; Carbune, Victor; Lin, Jason; Chen, Jindong; Sharma, Abhanshu				陈, 金东/GSI-4116-2022						ScreenAI: A Vision-Language Model for UI and Infographics Understanding								Arxiv											3	3;2024-07-04;https://www.arxiv.org/abs/2402.04615v3| 2;2024-02-19;https://www.arxiv.org/abs/2402.04615v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.04615v1	arXiv:2402.04615			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-art results on UI- and infographics-based tasks (Multipage DocVQA, WebSRC, and MoTIF), and new best-inclass performance on others (ChartQA, DocVQA, and InfographicVQA) compared to models of similar size. Finally, we release three new datasets: one focused on the screen annotation task and two others focused on question answering.																																	2024-07-20	PPRN:87561440		
J	Ye, Qinyuan; Axmed, Maxamed; Pryzant, Reid; Khani, Fereshte										Prompt Engineering a Prompt Engineer								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2311.05661v3| 1;2023-11-09;https://www.arxiv.org/abs/2311.05661v1	arXiv:2311.05661			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, exhibits remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted and highly specific prompt edits, rectify erroneous prompts, and induce multi-step plans for complex tasks.																																	2024-07-20	PPRN:86126349		
J	Bai, Hao; Zhou, Yifei; Cemri, Mert; Pan, Jiayi; Suhr, Alane; Levine, Sergey; Kumar, Aviral				pan, jiayi/ABR-2644-2022						DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning								Arxiv											2	2;2024-06-14;https://www.arxiv.org/abs/2406.11896v1| 1;2024-06-14;https://www.arxiv.org/abs/2406.11896v1	arXiv:2406.11896			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 14 2024	2024	Training corpuses for vision language models (VLMs) typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short for controlling real GUIs due to their failure to deal with real-world stochasticity and non-stationarity not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offline-to-online RL. To do this, we build a scalable and parallelizable Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.3B VLM trained with RL achieves a 49.5% absolute improvement -- from 17.7 to 67.2% success rate -- over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent trained with AitW data (38.5%), but also the prior best autonomous RL approach based on filtered behavior cloning (57.8%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.																																	2024-07-06	PPRN:89361327		
J	Liu, Xiaoxuan; Hu, Lanxiang; Bailis, Peter; Cheung, Alvin; Deng, Zhijie; Stoica, Ion; Zhang, Hao				Liu, Xiaoxuan/Y-2976-2019						Online Speculative Decoding								Arxiv											3	3;2024-06-10;https://www.arxiv.org/abs/2310.07177v4| 2;2023-10-17;https://www.arxiv.org/abs/2310.07177v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07177v1	arXiv:2310.07177			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jun 10 2024	2024	Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding to address this challenge. The main idea is to continuously update the (multiple) draft model(s) on observed user query data. Adapting to query distribution mitigates the shifts between the training distribution of the draft model and the query distribution, enabling the draft model to more accurately predict the target model's outputs. We develop a prototype of online speculative decoding based on knowledge distillation and evaluate it using both synthetic and real query data. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, bringing 1.42x to 2.17x latency reduction.																																	2024-11-20	PPRN:85540681		
J	Bannur, Shruthi; Bouzid, Kenza; Castro, Daniel C.; Schwaighofer, Anton; Bond-Taylor, Sam; Ilse, Maximilian; Perez-Garcia, Fernando; Salvatelli, Valentina; Sharma, Harshita; Meissen, Felix; Ranjit, Mercy; Srivastav, Shaury; Gong, Julia; Falck, Fabian; Oktay, Ozan; Thieme, Anja; Lungren, Matthew P.; Wetscherek, Maria Teodora; Alvarez-Valle, Javier; Hyland, Stephanie L.				Meissen, Felix/JPL-2263-2023; Sharma, Harshita/U-4759-2017; Pérez-García, Fernando/ACR-8536-2022; Bond-Taylor, Sam/AAX-2775-2021						MAIRA-2: Grounded Radiology Report Generation								Arxiv											1	1;2024-06-06;https://www.arxiv.org/abs/2406.04449v1	arXiv:2406.04449			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Radiology reporting is a complex task that requires detailed image understanding, integration of multiple inputs, including comparison with prior imaging, and precise language generation. This makes it ideal for the development and use of generative multimodal models. Here, we extend report generation to include the localisation of individual findings on the image - a task we call grounded report generation. Prior work indicates that grounding is important for clarifying image understanding and interpreting AI-generated text. Therefore, grounded reporting stands to improve the utility and transparency of automated report drafting.   To enable evaluation of grounded reporting, we propose a novel evaluation framework - RadFact - leveraging the reasoning capabilities of large language models (LLMs). RadFact assesses the factuality of individual generated sentences, as well as correctness of generated spatial localisations when present.   We introduce MAIRA-2, a large multimodal model combining a radiology-specific image encoder with a LLM, and trained for the new task of grounded report generation on chest X-rays. MAIRA-2 uses more comprehensive inputs than explored previously: the current frontal image, the current lateral image, the prior frontal image and prior report, as well as the Indication, Technique and Comparison sections of the current report. We demonstrate that these additions significantly improve report quality and reduce hallucinations, establishing a new state of the art on findings generation (without grounding) on MIMIC-CXR while demonstrating the feasibility of grounded reporting as a novel and richer task.																																	2024-07-04	PPRN:89244162		
J	Yu, Qihang; Weber, Mark; Deng, Xueqing; Shen, Xiaohui; Cremers, Daniel; Chen, Liang-Chieh				Yu, Qihang/NXC-6226-2025						An Image is Worth 32 Tokens for Reconstruction and Generation								Arxiv											1	1;2024-06-01;	arXiv:2406.07550			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce Transformer-based 1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a 256 x 256 x 3 image can be reduced to just 32 discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64x, leading to 410x faster generation process. Our best-performing variant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating high-quality samples 74x faster.																																	2024-11-16	PPRN:118645568		
J	Liu, Zihan; Ping, Wei; Roy, Rajarshi; Xu, Peng; Lee, Chankyu; Shoeybi, Mohammad; Catanzaro, Bryan										ChatQA: Surpassing GPT-4 on Conversational QA and RAG								Arxiv											3	3;2024-05-22;https://www.arxiv.org/abs/2401.10225v4| 2;2024-01-23;https://www.arxiv.org/abs/2401.10225v2| 1;2024-01-18;https://www.arxiv.org/abs/2401.10225v1	arXiv:2401.10225			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 22 2024	2024	In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval -augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two -stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the CHATRAG HAT RAG B ENCH , which encompasses ten datasets covering comprehensive evaluations on RAG, table -related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo2024-04-09 (score: 54.03) on the CHATRAG HAT RAG B ENCH , without relying on any synthetic data from OpenAI GPT models. Notably, the Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09, achieving a 4.4% improvement. To advance research in this field, we open -sourced the model weights, instruction tuning data, CHATRAG HAT RAG B ENCH , and retriever for the community: https://chatqa-project.github.io/. .																																	2024-06-05	PPRN:87221210		
J	Zheng, Chujie; Yin, Fan; Zhou, Hao; Meng, Fandong; Zhou, Jie; Chang, Kai-Wei; Huang, Minlie; Peng, Nanyun				Chang, Kai-Wei/AAJ-7874-2020; Zheng, Chujie/CAG-9031-2022						On Prompt-Driven Safeguarding for Large Language Models								Arxiv											4	4;2024-06-03;https://www.arxiv.org/abs/2401.18018v4| 3;2024-05-21;https://www.arxiv.org/abs/2401.18018v3| 2;2024-03-04;https://www.arxiv.org/abs/2401.18018v2| 1;2024-01-31;https://www.arxiv.org/abs/2401.18018v1	arXiv:2401.18018			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 21 2024	2024	Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs’ behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a “higher-refusal” direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries’ representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models’ general performance.																																	2024-08-24	PPRN:87435396		
J	Gao, Quankai; Xu, Qiangeng; Cao, Zhe; Mildenhall, Ben; Ma, Wenchao; Chen, Le; Tang, Danhang; Neumann, Ulrich				Gao, Quankai/GYD-4954-2022; MA, Wenchao/AFK-8637-2022						GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation								Arxiv											2	2;2024-05-13;https://www.arxiv.org/abs/2403.12365v2| 1;2024-03-19;https://www.arxiv.org/abs/2403.12365v1	arXiv:2403.12365			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 13 2024	2024	Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method's effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. 																																	2024-06-08	PPRN:88211340		
J	Pfau, Jacob; Merrill, William; Bowman, Samuel R.				Merrill, William/HMV-2296-2023						Let's Think Dot by Dot: Hidden Computation in Transformer Language Models								Arxiv											1	1;2024-04-24;https://www.arxiv.org/abs/2404.15758v1	arXiv:2404.15758			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 24 2024	2024	Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.1																																	2024-05-04	PPRN:88634647		
J	Gandhi, Kanishk; Lee, Denise; Grand, Gabriel; Liu, Muxin; Cheng, Winson; Sharma, Archit; Goodman, Noah D.										Stream of Search (SoS): Learning to Search in Language								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2404.03683v1	arXiv:2404.03683			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	Language models are rarely shown fruitful mistakes while training. They then struggle to look beyond the next token, suffering from a snowballing of errors and struggling to predict the consequence of their actions several steps ahead. In this paper, we show how language models can be taught to search by representing the process of search in language, as a flattened string — a stream of search (SoS). We propose a unified language for search that captures an array of different symbolic search strategies. We demonstrate our approach using the simple yet difficult game of Countdown, where the goal is to combine input numbers with arithmetic operations to reach a target number. We pretrain a transformer -based language model from scratch on a dataset of streams of search generated by heuristic solvers. We find that SoS pretraining increases search accuracy by 25% over models trained to predict only the optimal search trajectory. We further finetune this model with two policy improvement methods: Advantage -Induced Policy Alignment (APA) and Self -Taught Reasoner (STaR). The finetuned SoS models solve 36% of previously unsolved problems, including problems that cannot be solved by any of the heuristic solvers. Our results indicate that language models can learn to solve problems via search, selfimprove to flexibly use different search strategies, and potentially discover new ones. 1																																	2024-04-19	PPRN:88428990		
J	Ren, Shuhuai; Yao, Linli; Li, Shicheng; Sun, Xu; Hou, Lu				Ren, Shuhuai/KDO-1734-2024						TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding								Arxiv											2	2;2024-03-28;https://www.arxiv.org/abs/2312.02051v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02051v1	arXiv:2312.02051			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 28 2024	2024	This work proposes TimeChat, a time -sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q -Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instructiontuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat’s instruction -following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat’s strong zero -shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long -form video comprehension tasks and satisfy realistic user requirements.1																																	2024-04-15	PPRN:86373274		
J	Zhao, Han; Zhang, Min; Zhao, Wei; Ding, Pengxiang; Huang, Siteng; Wang, Donglin				Wang, Donglin/JXN-0063-2024; Ding, Pengxiang/KFQ-8404-2024						Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference								Arxiv											1	1;2024-03-22;https://www.arxiv.org/abs/2403.14520v2	arXiv:2403.14520			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 22 2024	2024	In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success. However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.																																	2025-08-07	PPRN:123160528		
J	Salvi, Francesco; Ribeiro, Manoel Horta; Gallotti, Riccardo; West, Robert				Gallotti, Riccardo/B-6339-2013; Salvi, Francesco/LTY-8345-2024; West, Robert/B-5414-2009						On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial								Arxiv											1	1;2024-03-21;https://www.arxiv.org/abs/2403.14380v1	arXiv:2403.14380			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about LLMs’ persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this preregistered study, we analyze the effect of AI -driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N = 820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p = 0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.																																	2024-04-13	PPRN:88257890		
J	Mishchenko, Konstantin; Defazio, Aaron				Mishchenko, Konstantin/AAZ-1982-2020						Prodigy: An Expeditiously Adaptive Parameter-Free Learner								Arxiv											4	4;2024-03-19;https://www.arxiv.org/abs/2306.06101v4| 3;2023-10-29;https://www.arxiv.org/abs/2306.06101v3| 2;2023-09-21;https://www.arxiv.org/abs/2306.06101v2| 1;2023-06-09;https://www.arxiv.org/abs/2306.06101v1	arXiv:2306.06101			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	We consider the problem of estimating the learning rate in adaptive methods, such as AdaGrad and Adam. We propose Prodigy, an algorithm that provably estimates the distance to the solution D, which is needed to set the learning rate optimally. At its core, Prodigy is a modification of the D-Adaptation method for learning-rate-free learning. It improves upon the convergence rate of D-Adaptation by a factor of O(√log(D/d0)), where d0 is the initial estimate of D. We test Prodigy on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approach consistently outperforms D-Adaptation and reaches test accuracy values close to that of hand-tuned Adam.																																	2024-04-12	PPRN:73262211		
J	Hang, Tiankai; Gu, Shuyang; Li, Chen; Bao, Jianmin; Chen, Dong; Hu, Han; Geng, Xin; Guo, Baining				Zhang, Bo/AAN-7181-2020; Hang, Tiankai/LQJ-5893-2024; Geng, Xin/OUI-5114-2025						Efficient Diffusion Training via Min-SNR Weighting Strategy								Arxiv											2	2;2024-03-11;https://www.arxiv.org/abs/2303.09556v3| 1;2023-03-16;https://www.arxiv.org/abs/2303.09556v2	arXiv:2303.09556			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 11 2024	2024	Denoising diffusion models have been a mainstream approach for image generation, however, training these models often suffers from slow convergence. In this paper, we discovered that the slow convergence is partly due to conflicting optimization directions between timesteps. To address this issue, we treat the diffusion training as a multi-task learning problem, and introduce a simple yet effective approach referred to as Min-SNR-γ. This method adapts loss weights of timesteps based on clamped signal-to-noise ratios, which effectively balances the conflicts among timesteps. Our results demonstrate a significant improvement in converging speed, 3.4 times faster than previous weighting strategies. It is also more effective, achieving a new record FID score of 2.06 on the ImageNet 256×256 benchmark using smaller architectures than that employed in previous state-of-the-art.																																	2024-05-01	PPRN:46904355		
J	Uehara, Masatoshi; Zhao, Yulai; Black, Kevin; Hajiramezanali, Ehsan; Scalia, Gabriele; Diamant, Nathaniel Lee; Tseng, Alex M; Biancalani, Tommaso; Levine, Sergey				Scalia, Gabriele/AAS-9964-2020; Biancalani, Tommaso/Q-2010-2016; ZHAO, YULAI/JLL-5344-2023						Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.15194v2	arXiv:2402.15194			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 28 2024	2024	Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose a substantial obstacle. To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.																																	2024-03-28	PPRN:87989307		
J	Wu, Fangzhou; Zhang, Ning; Jha, Somesh; Mcdaniel, Patrick; Xiao, Chaowei				Xiao, Chaowei/AAT-8772-2021; Wu, Fangzhou/IST-1642-2023						A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18649v1	arXiv:2402.18649			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4. Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user’s chat history, all without the need to manipulate the user’s input or gain direct access to OpenAI GPT4. We have reported the discovered vulnerabilities to OpenAI and our project demo is placed in the following link: https: //fzwark.github.io/LLM-System-Attack-Demo/																																	2024-03-25	PPRN:87985474		
J	Huang, Yunpeng; Xu, Jingwei; Lai, Junyu; Jiang, Zixu; Chen, Taolue; Li, Zenan; Yao, Yuan; Ma, Xiaoxing; Yang, Lijuan; Chen, Hao; Li, Shupeng; Zhao, Penghao				Xu, Jingwei/V-8512-2019; Li, Shupeng/OML-1429-2025; Ma, Xiaoxing/K-4290-2012						Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/2311.12351v2| 1;2023-11-21;https://www.arxiv.org/abs/2311.12351v1	arXiv:2311.12351			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research.																																	2024-03-24	PPRN:86223918		
J	Bai, Yushi; Lv, Xin; Zhang, Jiajie; He, Yuze; Qi, Ji; Hou, Lei; Tang, Jie; Dong, Yuxiao; Li, Juanzi				Li, Zhiyuan/ESQ-7168-2022						LongAlign: A Recipe for Long Context Alignment of Large Language Models								Arxiv											1	1;2024-01-31;https://www.arxiv.org/abs/2401.18058v1	arXiv:2401.18058			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 31 2024	2024	Extending large language models to effectively handle long contexts requires instruction finetuning on input sequences of similar length. To address this, we present LongAlign—a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction -following dataset using Self -Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction -following capabilities on queries of 10k -100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30%, while also maintaining their proficiency in handling short, generic tasks. The code, data, and longaligned models are open -sourced at https: //github.com/THUDM/LongAlign.																																	2024-02-17	PPRN:87430711		
J	Pfeiffer, Jonas; Ruder, Sebastian; Vulic, Ivan; Ponti, Edoardo Maria										Modular Deep Learning								Arxiv											2	2;2024-01-27;https://www.arxiv.org/abs/2302.11529v2| 1;2023-02-22;https://www.arxiv.org/abs/2302.11529v1	arXiv:2302.11529			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 27 2024	2024	Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. 																																	2024-02-15	PPRN:44128661		
J	Hu, Yan; Chen, Qingyu; Du, Jingcheng; Peng, Xueqing; Keloth, Vipina Kuttichi; Zuo, Xu; Zhou, Yujia; Li, Zehan; Jiang, Xiaoqian; Lu, Zhiyong; Roberts, Kirk; Xu, Hua				Jiang, Xiaoqian/K-6752-2012; Chen, Qingyu/JYQ-2478-2024; Du, Jingcheng/AAQ-5022-2020; Roberts, Kirk/AAZ-4169-2021; Hu, Yan/KFQ-7370-2024; ZUO, XU/OXC-0732-2025; li, zehan/GXZ-9774-2022; Lu, Zhiyong/D-3243-2017						Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering								Arxiv											2	2;2024-01-25;https://www.arxiv.org/abs/2303.16416v3| 1;2023-03-29;https://www.arxiv.org/abs/2303.16416v1	arXiv:2303.16416			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	Objective: This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance. Materials and Methods: We evaluated these models on two clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) identifying nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models’ performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt’s effectiveness and compared the models to BioClinicalBERT. Results: Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples, and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all four components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed. Conclusion: While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models’ feasibility for potential clinical applications.																																	2024-05-25	PPRN:56481012		
J	Lee, Changhun; Jin, Jungyu; Kim, Taesu; Kim, Hyungjun; Park, Eunhyeok										OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models								Arxiv											2	2;2024-01-24;https://www.arxiv.org/abs/2306.02272v4| 1;2023-06-13;https://www.arxiv.org/abs/2306.02272v2	arXiv:2306.02272			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 24 2024	2024	Large language models (LLMs) with hundreds of billions of parameters require powerful server-grade GPUs for inference, limiting their practical deployment. To address this challenge, we introduce the outlier-aware weight quantization (OWQ) method, which aims to minimize LLM's footprint through low-precision representation. OWQ prioritizes a small subset of structured weights sensitive to quantization, storing them in high-precision, while applying highly tuned quantization to the remaining dense weights. This sensitivity-aware mixed-precision scheme reduces the quantization error notably, and extensive experiments demonstrate that 3.1-bit models using OWQ perform comparably to 4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a parameter-efficient fine-tuning for task-specific adaptation, called weak column tuning (WCT), enabling accurate task-specific LLM adaptation with minimal memory overhead in the optimized format. OWQ represents a notable advancement in the flexibility, efficiency, and practicality of LLM optimization literature. 																																	2024-02-10	PPRN:72853646		
J	Ridnik, Tal; Kredo, Dedy; Friedman, Itamar										Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering								Arxiv											1	1;2024-01-16;https://www.arxiv.org/abs/2401.08500v1	arXiv:2401.08500			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 16 2024	2024	Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium																																	2024-02-02	PPRN:87187983		
J	Xu, Xingqian; Wang, Zhangyang; Zhang, Eric; Wang, Kai; Shi, Humphrey				Zhihua, Wang/AFO-5263-2022; Shi, Honghui/V-9259-2019; Wang, Kai/JCO-9763-2023						Versatile Diffusion: Text, Images and Variations All in One Diffusion Model								Arxiv											2	2;2024-01-11;https://www.arxiv.org/abs/2211.08332v4| 1;2022-11-15;https://www.arxiv.org/abs/2211.08332v2	arXiv:2211.08332			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. 																																	2024-05-25	PPRN:23075903		
J	Steiner, Andreas; Pinto, Andre Susano; Tschannen, Michael; Keysers, Daniel; Wang, Xiao; Bitton, Yonatan; Gritsenko, Alexey; Minderer, Matthias; Sherbondy, Anthony; Long, Shangbang; Qin, Siyang; Ingle, Reeve; Bugliarello, Emanuele; Kazemzadeh, Sahar; Mesnard, Thomas; Alabdulmohsin, Ibrahim; Beyer, Lucas; Zhai, Xiaohua				Zhai, Xiaohua/AAQ-1391-2020; QIN, SIYANG/KNB-3229-2024						PaliGemma 2: A Family of Versatile VLMs for Transfer								Arxiv											1	1;2024-12-04;https://www.arxiv.org/abs/2412.03555v1	arXiv:2412.03555			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 04 2024	2024	PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px2, 448px2 and 896px2) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.																																	2025-01-15	PPRN:119697428		
J	Das, Badhan Chandra; Amini, M. Hadi; Wu, Yanzhao				Amini, Hadi/U-8693-2019; Wu, Yanzhao/AAI-5865-2021						Security and Privacy Challenges of Large Language Models: A Survey								Arxiv											2	2;2024-11-14;https://www.arxiv.org/abs/2402.00888v2| 1;2024-01-30;https://www.arxiv.org/abs/2402.00888v1	arXiv:2402.00888			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 14 2024	2024	Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potential defense mechanisms. Additionally, the survey outlines existing research gaps in this domain and highlights future research directions.																																	2024-12-25	PPRN:87510317		
J	Wang, Bin; Zou, Xunlong; Lin, Geyu; Sun, Shuo; Liu, Zhuohan; Zhang, Wenyu; Liu, Zhengyuan; Aw, Aiti; Chen, Nancy F.				Chen, Nancy/GSD-8813-2022; Zhang, Wenyu/AEM-3128-2022						AudioBench: A Universal Benchmark for Audio Large Language Models								Arxiv											3	3;2024-11-06;https://www.arxiv.org/abs/2406.16020v4| 2;2024-09-02;https://www.arxiv.org/abs/2406.16020v3| 1;2024-06-25;https://www.arxiv.org/abs/2406.16020v2	arXiv:2406.16020			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 06 2024	2024	We introduce AudioBench, a universal benchmark designed to evaluate Audio Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among which, 7 are newly proposed datasets. The evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). Despite recent advancements, there lacks a comprehensive benchmark for AudioLLMs on instruction following capabilities conditioned on audio signals. AudioBench addresses this gap by setting up datasets as well as desired evaluation metrics. Besides, we also evaluated the capabilities of five popular models and found that no single model excels consistently across all tasks. We outline the research outlook for AudioLLMs and anticipate that our open-sourced evaluation toolkit, data, and leaderboard will offer a robust testbed for future model developments.1																																	2024-11-30	PPRN:89514777		
J	Bai, Guangji; Chai, Zheng; Ling, Chen; Wang, Shiyu; Lu, Jiaying; Zhang, Nan; Shi, Tingwei; Yu, Ziyang; Zhu, Mengdan; Zhang, Yifei; Yang, Carl; Cheng, Yue; Zhao, Liang				Zhao, Liang/F-2300-2010; Lu, Jiaying/ADT-8968-2022; Zhang, Yifei/LKJ-4465-2024; Yu, Ziyang/NBX-8622-2025						Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models								Arxiv											3	3;2024-10-27;https://www.arxiv.org/abs/2401.00625v3| 2;2024-01-04;https://www.arxiv.org/abs/2401.00625v2| 1;2024-01-01;https://www.arxiv.org/abs/2401.00625v1	arXiv:2401.00625			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 27 2024	2024	The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI’s ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus—covering computational, memory, energy, financial, and network resources—and their applicability across various stages of an LLM’s lifecycle, including architecture design, pre-training, fine-tuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current state-of-the-art and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape. 																																	2024-12-06	PPRN:86904772		
J	Wang, Yuancheng; Zhan, Haoyue; Liu, Liwei; Zeng, Ruihong; Guo, Haotian; Zheng, Jiachen; Zhang, Qiang; Zhang, Xueyao; Zhang, Shunsi; Wu, Zhizheng				GUO, HAOTIAN/HSF-9906-2023; Wang, Yuancheng/GLR-2067-2022; ZHANG, QIANG/AAG-1244-2019						MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer								Arxiv											3	3;2024-10-20;https://www.arxiv.org/abs/2409.00750v3| 2;2024-10-11;https://www.arxiv.org/abs/2409.00750v2| 1;2024-09-01;https://www.arxiv.org/abs/2409.00750v1	arXiv:2409.00750			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 20 2024	2024	The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked G enerative C odec T ransformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-thewild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/ main/models/tts/maskgct.																																	2024-11-20	PPRN:91722869		
J	Yang, Yifei; Cao, Zouying; Zhao, Hai				YANG, Yifei/HKN-6975-2023						LaCo: Large Language Model Pruning via Layer Collapse								Arxiv											2	2;2024-10-15;https://www.arxiv.org/abs/2402.11187v2| 1;2024-02-17;https://www.arxiv.org/abs/2402.11187v1	arXiv:2402.11187			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 15 2024	2024	Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the model internal structure. In this paper, we propose a concise layer-wise structured pruner called Layer Collapse (LaCo), in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80% at pruning ratios of 25-30%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the LaCo effectively inherits the parameters of the original model. Additionally, we perform ablation studies on various settings of LaCo. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios1. 																																	2024-11-10	PPRN:87798997		
J	Pan, Jiayi; Zhang, Yichi; Tomlin, Nicholas; Zhou, Yifei; Levine, Sergey; Suhr, Alane				Zhang, Yichi/AAV-2870-2021; pan, jiayi/ABR-2644-2022						Autonomous Evaluation and Refinement of Digital Agents								Arxiv											2	2;2024-10-07;https://www.arxiv.org/abs/2404.06474v3| 1;2024-04-10;https://www.arxiv.org/abs/2404.06474v2	arXiv:2404.06474			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 07 2024	2024	We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control. We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy. We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics. Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance. Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve around 75% relative improvement in device control settings.																																	2024-10-28	PPRN:88478773		
J	Gu, Jia-Chen; Xu, Hao-Xiang; Ma, Jun-Yu; Lu, Pan; Ling, Zhen-Hua; Chang, Kai-Wei; Peng, Nanyun				Chang, Kai-Wei/M-6055-2016; Ma, Jun-Yu/HPD-0474-2023						Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue								Arxiv											4	4;2024-10-04;https://www.arxiv.org/abs/2401.04700v4| 3;2024-06-16;https://www.arxiv.org/abs/2401.04700v3| 2;2024-02-04;https://www.arxiv.org/abs/2401.04700v2| 1;2024-01-09;https://www.arxiv.org/abs/2401.04700v1	arXiv:2401.04700			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model's behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing's improvements on factuality may come at the cost of a significant degradation of the model's general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.1																																	2024-10-27	PPRN:87077605		
J	Bansal, Hritik; Lin, Zongyu; Xie, Tianyi; Zong, Zeshun; Yarom, Michal; Bitton, Yonatan; Jiang, Chenfanfu; Sun, Yizhou; Chang, Kai-Wei; Grover, Aditya				Xie, Tianyi/MGU-9357-2025; Chang, Kai-Wei/M-6055-2016						VideoPhy: Evaluating Physical Commonsense for Video Generation								Arxiv											2	2;2024-10-03;https://www.arxiv.org/abs/2406.03520v2| 1;2024-06-05;https://www.arxiv.org/abs/2406.03520v1	arXiv:2406.03520			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	Recent advances in internet-scale video data pretraining have led to the development of text-to-video generative models that can create high-quality videos across a broad range of visual concepts, synthesize realistic motions and render complex objects. Hence, these generative models have the potential to become general-purpose simulators of the physical world. However, it is unclear how far we are from this goal with the existing text-to-video generative models. To this end, we present VideoPhy, a benchmark designed to assess whether the generated videos follow physical commonsense for real-world activities (e.g. marbles will roll down when placed on a slanted surface). Specifically, we curate diverse prompts that involve interactions between various material types in the physical world (e.g., solid-solid, solid-fluid, fluid-fluid). We then generate videos conditioned on these captions from diverse state-of-the-art text-to-video generative models, including open models (e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human evaluation reveals that the existing models severely lack the ability to generate videos adhering to the given text prompts, while also lack physical commonsense. Specifically, the best performing model, CogVideoX-5B, generates videos that adhere to the caption and physical laws for 39.6% of the instances. VideoPhy thus highlights that the video generative models are far from accurately simulating the physical world. Finally, we propose an auto-evaluator, VideoCon-Physics, to assess the performance reliably for the newly released models.																																	2024-10-19	PPRN:89249135		
J	Gong, Shizhan; Zhong, Yuan; Ma, Wenao; Li, Jinpeng; Wang, Zhao; Zhang, Jingyang; Heng, Pheng-Ann; Dou, Qi				Qi, Dou/JKJ-3763-2023; Gong, Shizhan/JMD-0190-2023; Zhang, Jingyang/AAU-7348-2021						3DSAM-adapter: Holistic adaptation of SAM from 2D to 3D for promptable tumor segmentation								Arxiv											2	2;2024-10-02;https://www.arxiv.org/abs/2306.13465v2| 1;2023-06-23;https://www.arxiv.org/abs/2306.13465v1	arXiv:2306.13465			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 02 2024	2024	Despite that the segment anything model (SAM) achieved impressive results on general-purpose semantic segmentation with strong generalization ability on daily images, its demonstrated performance on medical image segmentation is less precise and not stable, especially when dealing with tumor segmentation tasks that involve objects of small sizes, irregular shapes, and low contrast. Notably, the original SAM architecture is designed for 2D natural images, therefore would not be able to extract the 3D spatial information from volumetric medical data effectively. In this paper, we propose a novel adaptation method for transferring SAM from 2D to 3D for promptable medical image segmentation. Through a holistically designed scheme for architecture modification, we transfer the SAM to support volumetric inputs while retaining the majority of its pre-trained parameters for reuse. The fine-tuning process is conducted in a parameter-efficient manner, wherein most of the pre-trained parameters remain frozen, and only a few lightweight spatial adapters are introduced and tuned. Regardless of the domain gap between natural and medical data and the disparity in the spatial arrangement between 2D and 3D, the transformer trained on natural images can effectively capture the spatial patterns present in volumetric medical images with only lightweight adaptations. We conduct experiments on four open-source tumor segmentation datasets, and with a single click prompt, our model can outperform domain state-of-the-art medical image segmentation models on 3 out of 4 tasks, specifically by 8.25%, 29.87%, and 10.11% for kidney tumor, pancreas tumor, colon cancer segmentation, and achieve similar performance for liver tumor segmentation. We also compare our adaptation method with existing popular adapters, and observed significant performance improvement on most datasets. Our code and models are available at: https://github.com/med-air/3DSAM-adapter. .																																	2024-10-16	PPRN:73494285		
J	Wang, Jiawei; Yuan, Liping; Zhang, Yuchen; Sun, Haomiao				Wang, Jiawei/HSI-2308-2023; Huang, Liyan/JEZ-6158-2023; Zhang, Ziyue/KSL-8395-2024						Tarsier: Recipes for Training and Evaluating Large Video Description Models								Arxiv											2	2;2024-09-24;https://www.arxiv.org/abs/2407.00634v2| 1;2024-06-30;https://www.arxiv.org/abs/2407.00634v1	arXiv:2407.00634			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 24 2024	2024	Generating fine-grained video descriptions is a fundamental challenge in video understanding. In this work, we introduce Tarsier, a family of large-scale video-language models designed to generate high-quality video descriptions. Tarsier employs CLIP-ViT to encode frames separately and then uses an LLM to model temporal relationships. Despite its simple architecture, we demonstrate that with a meticulously designed two-stage training procedure, the Tarsier models exhibit substantially stronger video description capabilities than any existing open-source model, showing a +51.4% advantage in human side-by-side evaluation over the strongest model. Additionally, they are comparable to state-of-the-art proprietary models, with a +12.3% advantage against GPT-4V and a −6.7% disadvantage against Gemini 1.5 Pro. When upgraded to Tarsier2 by building upon SigLIP and Qwen2-7B, it further improves significantly with a +4.8% advantage against GPT-4o. Besides video description, Tarsier proves to be a versatile generalist model, achieving new state-of-the-art results across nine public benchmarks, including multichoice VQA, open-ended VQA, and zero-shot video captioning. Our second contribution is the introduction of a new benchmark – DREAM-1K (https://tarsier-vlm.github.io/) for evaluating video description models, consisting of a new challenging dataset featuring videos from diverse sources and varying complexity, along with an automatic method specifically designed to assess the quality of fine-grained video descriptions. 																																	2024-10-07	PPRN:90653303		
J	Jin, Yizhang; Li, Jian; Liu, Yexin; Gu, Tianjun; Wu, Kai; Jiang, Zhengkai; He, Muyang; Zhao, Bo; Tan, Xin; Gan, Zhenye; Wang, Yabiao; Wang, Chengjie; Ma, Lizhuang				WU, KAI/AAF-2238-2019; Wang, Yuanhao/KBA-1055-2024						Efficient Multimodal Large Language Models: A Survey								Arxiv											2	2;2024-08-09;https://www.arxiv.org/abs/2405.10739v2| 1;2024-05-17;https://www.arxiv.org/abs/2405.10739v1	arXiv:2405.10739			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 09 2024	2024	In the past year, Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in tasks such as visual question answering, visual understanding and reasoning. However, the extensive model size and high training and inference costs have hindered the widespread application of MLLMs in academia and industry. Thus, studying efficient and lightweight MLLMs has enormous potential, especially in edge computing scenarios. In this survey, we provide a comprehensive and systematic review of the current state of efficient MLLMs. Specifically, we summarize the timeline of representative efficient MLLMs, research state of efficient structures and strategies, and the applications. Finally, we discuss the limitations of current efficient MLLM research and promising future directions. 																																	2024-08-21	PPRN:89091674		
J	Stechly, Kaya; Valmeekam, Karthik; Kambhampati, Subbarao										On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks								Arxiv											2	2;2024-08-03;https://www.arxiv.org/abs/2402.08115v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.08115v1	arXiv:2402.08115			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 03 2024	2024	There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples–ranging from multiplication to simple planning–there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation–a rather classical argument from computational complexity–which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique and significant performance gains with sound external verification. We also note that merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.																																	2024-08-11	PPRN:87675796		
J	An, Keyu; Chen, Qian; Deng, Chong; Du, Zhihao; Gao, Changfeng; Gao, Zhifu; Gu, Yue; He, Ting; Hu, Hangrui; Hu, Kai; Ji, Shengpeng; Li, Yabin; Li, Zerui; Lu, Heng; Lv, Xiang; Ma, Bin; Ma, Ziyang; Ni, Chongjia; Song, Changhe; Shi, Jiaqi; Shi, Xian; Wang, Hao; Wang, Wen; Wang, Yuxuan; Xiao, Zhangyu; Yan, Zhijie; Yang, Yexin; Zhang, Bin; Zhang, Qinglin; Zhang, Shiliang; Zhao, Nan; Zheng, Siqi		Tongyi Speech Team; Alibaba Grp		Zhang, Qinglin/D-9258-2013; Zhang, ShiLiang/AAA-4638-2020; shi, xian/AAL-3936-2021						FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs								Arxiv											1	1;2024-07-09;https://www.arxiv.org/abs/2407.04051v2	arXiv:2407.04051			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 09 2024	2024	This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at https://fun-audio-llm.github.io, and the code can be accessed at https://github.com/FunAudioLLM.																																	2024-07-21	PPRN:90751278		
J	Wang, Fei; Fu, Xingyu; Huang, James Y.; Li, Zekun; Liu, Qin; Liu, Xiaogeng; Ma, Mingyu Derek; Xu, Nan; Zhou, Wenxuan; Zhang, Kai; Yan, Tianyi; Mo, Wenjie; Liu, Hsiang-Hui; Lu, Pan; Li, Chunyuan; Xiao, Chaowei; Chang, Kai-Wei; Roth, Dan; Zhang, Sheng; Poon, Hoifung; Chen, Muhao				li, zekun/KPB-2728-2024; Chang, Kai-Wei/M-6055-2016; Zhang, Kai/KOD-2592-2024; Xiao, Chaowei/AAT-8772-2021; Fu, Xingyu/GZM-3129-2022; Chen, Muhao/AAA-3634-2021; xu, nan/LIG-7697-2024; Liu, Xiaogeng/KIJ-1671-2024; Zhou, Wenxuan/AAN-4529-2020; wang, fei/M-1445-2013						MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding								Arxiv											2	2;2024-07-02;https://www.arxiv.org/abs/2406.09411v2| 1;2024-06-13;https://www.arxiv.org/abs/2406.09411v1	arXiv:2406.09411			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 02 2024	2024	We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations). Comprising 11,264 images and 2,600 multiple-choice questions, MuirBench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy. Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. These results highlight the importance of MuirBench in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements.																																	2024-12-06	PPRN:89294595		
J	Liu, Qian; Zheng, Xiaosen; Muennighoff, Niklas; Zeng, Guangtao; Dou, Longxu; Pang, Tianyu; Jiang, Jing; Lin, Min				Tianyu, Pang/AAW-2653-2020; Wang, Jingjing/B-7476-2016; zeng, guangtao/OPF-2200-2025; Zheng, Xiaosen/KFA-4269-2024						RegMix: Data Mixture as Regression for Language Model Pre-training								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.01492v1	arXiv:2407.01492			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. 																																	2024-07-18	PPRN:90657503		
J	Zhou, Dongruo; Chen, Jinghui; Cao, Yuan; Yang, Ziyan; Gu, Quanquan				Yang, Ziyan/HTQ-4622-2023; Chen, Jinghui/AFT-5065-2022; Zhou, Dongruo/GYJ-3503-2022						On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization								Arxiv											2	2;2020-10-19;https://www.arxiv.org/abs/1808.05671v3| 1;2024-06-01;	arXiv:1808.05671			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	Adaptive gradient methods are workhorses in deep learning. However, the convergence guarantees of adaptive gradient methods for nonconvex optimization have not been thoroughly studied. In this paper, we provide a fine-grained convergence analysis for a general class of adaptive gradient methods including AMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that adaptive gradient methods in expectation converge to a first-order stationary point. Our convergence rate is better than existing results for adaptive gradient methods in terms of dimension. In addition, we also prove high probability bounds on the convergence rates of AMSGrad, RMSProp as well as AdaGrad, which have not been established before. Our analyses shed light on better understanding the mechanism behind adaptive gradient methods in optimizing nonconvex objectives.																																	2025-11-07	PPRN:14759775		
J	Zhou, Yiyang; Fan, Zhiyuan; Cheng, Dongjie; Yang, Sihan; Chen, Zhaorun; Cui, Chenhang; Wang, Xiyao; Li, Yun; Zhang, Linjun; Yao, Huaxiu				Yao, Huaxiu/V-3516-2019; Chen, Zhaorun/AAT-1611-2021; Zhang, Linjun/IUP-2157-2023; Zhou, Yiyang/AAU-7705-2021; Cheng, Dongjie/KBR-0286-2024; Wang, Xiyao/AFN-9739-2022						Calibrated Self-Rewarding Vision Language Models								Arxiv											3	3;2024-05-31;https://www.arxiv.org/abs/2405.14622v3| 2;2024-05-25;https://www.arxiv.org/abs/2405.14622v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14622v1	arXiv:2405.14622			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 31 2024	2024	Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.																																	2024-06-19	PPRN:88983095		
J	Magesh, Varun; Surani, Faiz; Dahl, Matthew; Suzgun, Mirac; Manning, Christopher D.; Ho, Daniel E.				Manning, Christopher/A-1358-2007						Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools								Arxiv											1	1;2024-05-30;https://www.arxiv.org/abs/2405.20362v1	arXiv:2405.20362			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 30 2024	2024	Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to "hallucinate," or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as "eliminating" (Casetext, 2023) or "avoid[ing]" hallucinations (Thomson Reuters, 2023), or guaranteeing "hallucination-free" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.1																																	2024-06-19	PPRN:89125288		
J	Kang, Katie; Wallace, Eric; Tomlin, Claire; Kumar, Aviral; Levine, Sergey										Unfamiliar Finetuning Examples Control How Language Models Hallucinate								Arxiv											2	2;2024-05-28;https://www.arxiv.org/abs/2403.05612v2| 1;2024-03-08;https://www.arxiv.org/abs/2403.05612v1	arXiv:2403.05612			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	Large language models are known to hallucinate when faced with unfamiliar queries, but the underlying mechanism that govern how models hallucinate are not yet fully understood. In this work, we find that unfamiliar examples in the models’ finetuning data – those that introduce concepts beyond the base model’s scope of knowledge – are crucial in shaping these errors. In particular, we find that an LLM’s hallucinated predictions tend to mirror the responses associated with its unfamiliar finetuning examples. This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model’s responses to unfamiliar queries (e.g., say “I don’t know”). We empirically validate this observation in a series of controlled experiments involving SFT, RL, and reward model finetuning on TriviaQA and MMLU. Our work further investigates RL finetuning strategies for improving the factuality of long-form model generations. We find that, while hallucinations from the reward model can significantly undermine the effectiveness of RL factuality finetuning, strategically controlling how reward models hallucinate can minimize these negative effects. Leveraging our previous observations on controlling hallucinations, we propose an approach for learning more reliable reward models, and show that they improve the efficacy of RL factuality finetuning in long-form biography and book/movie plot generation tasks.																																	2024-08-24	PPRN:88104045		
J	Islam, Md.Ashraful; Ali, Mohammed Eunus; Parvez, Md Rizwan				Ali, Muhamad/HTT-0028-2023; Islam, Ashraful/JEP-8161-2023						MapCoder: Multi-Agent Code Generation for Competitive Problem Solving								Arxiv											1	1;2024-05-18;https://www.arxiv.org/abs/2405.11403v1	arXiv:2405.11403			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 18 2024	2024	Code synthesis, which requires a deep understanding of complex natural language problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. While large language models (LLMs) demonstrate impressive proficiency in natural language processing, their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLM ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks, MapCoder showcases remarkable code generation capabilities, achieving new state-of-the-art results (pass@1) on HumanEval (93.9%), MBPP (83.1%), APPS (22.0%), CodeContests (28.5%), and xCodeEval (45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. 																																	2024-08-25	PPRN:91459628		
J	Huang, Wei; Liu, Yangdong; Qin, Haotong; Li, Ying; Zhang, Shiming; Liu, Xianglong; Magno, Michele; Qi, Xiaojuan				Zhang, Shiming/O-5077-2015; Liu, Xianglong/NTQ-2427-2025; Qi, Xiaojuan/MVV-7776-2025; Bian, Sizhen/AAI-8450-2021						BiLLM: Pushing the Limit of Post-Training Quantization for LLMs								Arxiv											2	2;2024-05-15;https://www.arxiv.org/abs/2402.04291v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.04291v1	arXiv:2402.04291			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 15 2024	2024	Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLMs families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins. Moreover, BiLLM enables the binarization process of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency. 																																	2024-06-11	PPRN:87561453		
J	Cui, Yingqian; Ren, Jie; Xu, Han; He, Pengfei; Liu, Hui; Sun, Lichao; Xing, Yue; Tang, Jiliang				Cui, Yingqian/KBA-8229-2024; Jie, Ren/IUO-1535-2023						DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models								Arxiv											4	4;2024-05-10;https://www.arxiv.org/abs/2306.04642v4| 3;2024-05-08;https://www.arxiv.org/abs/2306.04642v3| 2;2023-10-09;https://www.arxiv.org/abs/2306.04642v2| 1;2023-05-25;https://www.arxiv.org/abs/2306.04642v1	arXiv:2306.04642			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 10 2024	2024	Recently, Generative Diffusion Models (GDMs) have shown remarkable abilities in learning and generating images, fostering a large community of GDMs. However, the unrestricted proliferation has raised serious concerns on copyright issues. For example, artists become concerned that GDMs could effortlessly replicate their unique artworks without permission. In response to these challenges, we introduce a novel watermark scheme, DiffusionShield, against GDMs. It protects images from infringement by encoding the ownership message into an imperceptible watermark and injecting it into images. This watermark can be easily learned by GDMs and will be reproduced in generated images. By detecting the watermark in generated images, the infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, DiffusionShield ensures low distortion of the original image, high watermark detection performance, and lengthy encoded messages. We conduct rigorous and comprehensive experiments to show its effectiveness in defending against infringement by GDMs and its superiority over traditional watermark methods.																																	2024-05-29	PPRN:73234741		
J	Mehta, Sachin; Sekhavat, Mohammad Hossein; Cao, Qingqing; Horton, Maxwell; Jin, Yanzi; Sun, Chenfan; Mirzadeh, Iman; Najibi, Mahyar; Belenko, Dmitry; Zatloukal, Peter; Rastegari, Mohammad										OpenELM: An Efficient Language Model Family with Open Training and Inference Framework								Arxiv											2	2;2024-05-02;https://www.arxiv.org/abs/2404.14619v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.14619v1	arXiv:2404.14619			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2 × fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at https: //github. com/apple/corenet . Additionally, OpenELM models can be found on HuggingFace at: https : // huggingface.co/apple/OpenELM .																																	2024-05-19	PPRN:88622805		
J	Heimersheim, Stefan; Nanda, Neel										How to use and interpret activation patching								Arxiv											1	1;2024-04-23;https://www.arxiv.org/abs/2404.15255v1	arXiv:2404.15255			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 23 2024	2024	Activation patching is a popular mechanistic interpretability technique, but has many subtleties regarding how it is applied and how one may interpret the results. We provide a summary of advice and best practices, based on our experience using this technique in practice. We include an overview of the different ways to apply activation patching and a discussion on how to interpret the results. We focus on what evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls.																																	2024-05-02	PPRN:88627059		
J	Ye, Seonghyeon; Kim, Doyoung; Kim, Sungdong; Hwang, Hyeonbin; Kim, Seungone; Jo, Yongrae; Thorne, James; Kim, Juho; Seo, Minjoon										FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets								Arxiv											3	3;2024-04-14;https://www.arxiv.org/abs/2307.10928v4| 2;2024-02-16;https://www.arxiv.org/abs/2307.10928v3| 1;2023-10-04;https://www.arxiv.org/abs/2307.10928v2	arXiv:2307.10928			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 14 2024	2024	Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations1.																																	2024-04-25	PPRN:85397916		
J	Malaviya, Chaitanya; Lee, Subin; Chen, Sihao; Sieber, Elizabeth; Yatskar, Mark; Roth, Dan				Malaviya, Chaitanya/NXC-5110-2025						ExpertQA: Expert-Curated Questions and Attributed Answers								Arxiv											2	2;2024-04-02;https://www.arxiv.org/abs/2309.07852v2| 1;2023-09-14;https://www.arxiv.org/abs/2309.07852v1	arXiv:2309.07852			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	As language models are adopted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying attribution and factuality has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we conduct human evaluation of responses from a few representative systems along various axes of attribution and factuality, by bringing domain experts in the loop. Specifically, we collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. In addition, we ask experts to improve upon responses from language models. The output of our analysis is ExpertQA, a high-quality long-form QA dataset with 2177 questions spanning 32 fields, along with verified answers and attributions for claims in the answers.																																	2024-04-18	PPRN:85017493		
J	Du, Zhengxiao; Zeng, Aohan; Dong, Yuxiao; Tang, Jie										Understanding Emergent Abilities of Language Models from the Loss Perspective								Arxiv											3	3;2025-01-15;https://www.arxiv.org/abs/2403.15796v3| 2;2024-03-30;https://www.arxiv.org/abs/2403.15796v2| 1;2024-03-23;https://www.arxiv.org/abs/2403.15796v1	arXiv:2403.15796			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 23 2024	2024	Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses.																																	2025-08-07	PPRN:88275031		
J	Rocamonde, Juan; Montesinos, Victoriano; Nava, Elvis; Perez, Ethan; Lindner, David				Rocamonde, Juan/LTY-8353-2024						Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2310.12921v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.12921v1	arXiv:2310.12921			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second "baseline" prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.																																	2024-04-11	PPRN:85720052		
J	Zheng, Hongkai; Nie, Weili; Vahdat, Arash; Anandkumar, Anima				Zheng, Hongkai/OEO-1318-2025						Fast Training of Diffusion Models with Masked Transformers								Arxiv											2	2;2024-03-05;https://www.arxiv.org/abs/2306.09305v2| 1;2023-06-15;https://www.arxiv.org/abs/2306.09305v1	arXiv:2306.09305			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 05 2024	2024	We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance.																																	2024-04-03	PPRN:73359512		
J	Li, Qintong; Cui, Leyang; Zhao, Xueliang; Kong, Lingpeng; Bi, Wei				Li, Qintong/HGA-9822-2022; kong, lingpeng/NHQ-3170-2025						GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19255v1	arXiv:2402.19255			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result. Code and data are available at url{https://github.com/qtli/GSM-Plus}.																																	2024-03-28	PPRN:87986113		
J	Li, Xingxuan; Li, Yutong; Qiu, Lin; Joty, Shafiq; Bing, Lidong				Li, Yupeng/A-7718-2012						Evaluating Psychological Safety of Large Language Models								Arxiv											2	2;2024-02-29;https://www.arxiv.org/abs/2212.10529v3| 1;2022-12-20;https://www.arxiv.org/abs/2212.10529v2	arXiv:2212.10529			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 29 2024	2024	In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.																																	2024-03-28	PPRN:68553851		
J	Tuo, Yuxiang; Xiang, Wangmeng; He, Jun-Yan; Geng, Yifeng; Xie, Xuansong				He, Junyan/HTQ-4319-2023						ANYTEXT: MULTILINGUAL VISUAL TEXT GENERATION AND EDITING								Arxiv											4	4;2024-02-21;https://www.arxiv.org/abs/2311.03054v5| 3;2023-12-15;https://www.arxiv.org/abs/2311.03054v4| 2;2023-11-30;https://www.arxiv.org/abs/2311.03054v3| 1;2023-11-07;https://www.arxiv.org/abs/2311.03054v2	arXiv:2311.03054			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https://github.com/tyxsspa/AnyText to improve and promote the development of text generation technology.																																	2024-03-20	PPRN:86074931		
J	Ye, Wenqian; Zheng, Guangtao; Cao, Xu; Ma, Yunsheng; Hu, Xia; Zhang, Aidong				Zheng, Guangtao/NES-5198-2025; Ma, Yunsheng/HDO-6786-2022; Cao, Xu/K-8388-2019						Spurious Correlations in Machine Learning: A Survey								Arxiv											1	1;2024-02-20;https://www.arxiv.org/abs/2402.12715v1	arXiv:2402.12715			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 20 2024	2024	Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as "spurious" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.																																	2024-03-19	PPRN:87776893		
J	Cai, Han; Li, Junyan; Muyan, Hu; Gan, Chuang; Han, Song				Cai, Han/NKP-2131-2025; Han, Song/AAR-9464-2020						EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction								Arxiv											3	3;2024-02-06;https://www.arxiv.org/abs/2205.14756v6| 2;2023-09-27;https://www.arxiv.org/abs/2205.14756v5| 1;2022-05-29;https://www.arxiv.org/abs/2205.14756v2	arXiv:2205.14756			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 06 2024	2024	High -resolution dense prediction enables many appealing real -world applications, such as computational photography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art highresolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of highresolution vision models with novel multi -scale linear attention. Unlike prior high -resolution dense prediction models that rely on heavy softmax attention, hardware -inefficient large -kernel convolution, or complicated topology structure to obtain good performances, our multi -scale linear attention achieves the global receptive field and multi -scale learning (two desirable features for high -resolution dense prediction) with only lightweight and hardware -efficient operations. As such, EfficientViT delivers remarkable performance gains over previous state-of-the-art models with significant speedup on diverse hardware platforms, including mobile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 13.9× and 6.2× GPU latency reduction over SegFormer and SegNeXt, respectively. For super -resolution, EfficientViT delivers up to 6.4× speedup over Restormer while providing 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers 48.9× higher throughput on A100 GPU while achieving slightly better zero -shot instance segmentation performance on COCO.																																	2024-05-25	PPRN:13744832		
J	Majumdar, Arjun; Yadav, Karmesh; Arnaud, Sergio; Ma, Yecheng Jason; Chen, Claire; Silwal, Sneha; Jain, Aryan; Berges, Vincent-Pierre; Wu, Tingfan; Vakil, Jay; Abbeel, Pieter; Malik, Jitendra; Batra, Dhruv; Lin, Yixin; Maksymets, Oleksandr; Rajeswaran, Aravind; Meier, Franziska				Lin, Yixin/HOC-9030-2023; wu, tingfan/AEK-6979-2022						Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?								Arxiv											2	2;2024-02-01;https://www.arxiv.org/abs/2303.18240v2| 1;2023-03-31;https://www.arxiv.org/abs/2303.18240v1	arXiv:2303.18240			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Feb 01 2024	2024	We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual ‘foundation models’ for Embodied AI. First, we curate CORTEXBENCH, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train differentsized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CORTEXBENCH. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.																																	2024-05-25	PPRN:53132544		
J	Huang, Yuhao; Yang, Xin; Liu, Lian; Zhou, Han; Chang, Ao; Zhou, Xinrui; Chen, Rusi; Yu, Junxuan; Chen, Jiongquan; Chen, Chaoyu; Liu, Sijing; Chi, Haozhe; Hu, Xindi; Yue, Kejuan; Li, Lei; Grau, Vicente; Fan, Deng-Ping; Dong, Fajin; Ni, Dong				Zhai, Wei-dong/B-8426-2012; li, lei/C-9824-2011; Fan, Deng-Ping/ABD-4052-2020; huang, yuhao/AAY-8179-2020						Segment Anything Model for Medical Images?								Arxiv											4	4;2024-01-17;https://www.arxiv.org/abs/2304.14660v7| 3;2023-12-25;https://www.arxiv.org/abs/2304.14660v6| 2;2023-12-12;https://www.arxiv.org/abs/2304.14660v5| 1;2023-04-28;https://www.arxiv.org/abs/2304.14660v2	arXiv:2304.14660			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 17 2024	2024	The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging because of the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. To fully validate SAM's performance on medical data, we collected and sorted 53 open-source datasets and built a large medical segmentation dataset with 18 modalities, 84 objects, 125 object-modality paired targets, 1050K 2D images, and 6033K masks. We comprehensively analyzed different models and strategies on the so-called COSMOS 1050K dataset. Our findings mainly include the following: 1) SAM showed remarkable performance in some specific objects but was unstable, imperfect, or even totally failed in other situations. 2) SAM with the large ViT-H showed better overall performance than that with the small ViT-B. 3) SAM performed better with manual hints, especially box, than the Everything mode. 4) SAM could help human annotation with high labeling quality and less time. 5) SAM was sensitive to the randomness in the center point and tight box prompts, and may suffer from a serious performance drop. 6) SAM performed better than interactive methods with one or a few points, but will be outpaced as the number of points increases. 7) SAM's performance correlated to different factors, including boundary complexity, intensity differences, etc. 8) Finetuning the SAM on specific medical tasks could improve its average DICE performance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that this comprehensive report can help researchers explore the potential of SAM applications in MIS, and guide how to appropriately use and develop SAM.																																	2024-02-02	PPRN:66558489		
J	Holmes, Connor; Tanaka, Masahiro; Wyatt, Michael; Awan, Ammar Ahmad; Rasley, Jeff; Rajbhandari, Samyam; Aminabadi, Reza Yazdani; Qin, Heyang; Bakhtiari, Arash; Kurilenko, Lev; He, Yuxiong										DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference								Arxiv											1	1;2024-01-09;https://www.arxiv.org/abs/2401.08671v1	arXiv:2401.08671			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 09 2024	2024	The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.																																	2024-05-25	PPRN:87210590		
J	Xiao, Guangxuan; Tang, Jiaming; Zuo, Jingwei; Guo, Junxian; Yang, Shang; Tang, Haotian; Fu, Yao; Han, Song										DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads								Arxiv											1	1;2024-10-14;https://www.arxiv.org/abs/2410.10819v1	arXiv:2410.10819			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 14 2024	2024	Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks-referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. 																																	2024-10-27	PPRN:112579292		
J	Zhang, Tao; Yuan, Haobo; Qi, Lu; Zhang, Jiangning; Zhou, Qianyu; Ji, Shunping; Yan, Shuicheng; Li, Xiangtai				Yan, Shuicheng/HCI-1431-2022; Yuan, Haobo/JNF-0317-2023; yan, shuicheng/HCH-9860-2022						Point Cloud Mamba: Point Cloud Learning via State Space Model								Arxiv											4	4;2024-10-11;https://www.arxiv.org/abs/2403.00762v4| 3;2024-05-30;https://www.arxiv.org/abs/2403.00762v3| 2;2024-03-29;https://www.arxiv.org/abs/2403.00762v2| 1;2024-03-01;https://www.arxiv.org/abs/2403.00762v1	arXiv:2403.00762			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 11 2024	2024	Recently, state space models have exhibited strong global modeling capabilities and linear computational complexity in contrast to transformers. This research focuses on applying such architecture to more efficiently and effectively model point cloud data globally with linear computational complexity. In particular, for the first time, we demonstrate that Mamba-based point cloud methods can outperform previous methods based on transformer or multi-layer perceptrons (MLPs). To enable Mamba to process 3-D point cloud data more effectively, we propose a novel Consistent Traverse Serialization method to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of textit{x}, textit{y}, and textit{z} coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences more effectively. Point Cloud Mamba surpasses the state-of-the-art (SOTA) point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, ShapeNetPart, and S3DIS datasets. It is worth mentioning that when using a more powerful local feature extraction module, our PCM achieves 79.6 mIoU on S3DIS, significantly surpassing the previous SOTA models, DeLA and PTv3, by 5.5 mIoU and 4.9 mIoU, respectively.																																	2024-11-04	PPRN:87999038		
J	Zhao, Siyun; Yang, Yuqing; Wang, Zilong; He, Zhiyuan; Qiu, Luna K.; Qiu, Lili				Yuqing, Yang/ADJ-2720-2022; He, Zhiyuan/GSO-0009-2022						Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely								Arxiv											1	1;2024-09-23;https://www.arxiv.org/abs/2409.14924v1	arXiv:2409.14924			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 23 2024	2024	Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.																																	2024-10-07	PPRN:96234960		
J	Chen, Junying; Wang, Xidong; Ji, Ke; Gao, Anningzhe; Jiang, Feng; Chen, Shunian; Zhang, Hongbo; Song, Dingjie; Xie, Wenya; Kong, Chuyi; Li, Jianquan; Wan, Xiang; Li, Haizhou; Wang, Benyou				Jiang, Feng/KRP-8568-2024; Haizhou, LI/ITT-8410-2023; JI, Ke/LHA-0772-2024; Wang, Benyou/Y-5146-2019; Wang, Xidong/IZD-5718-2023						HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs								Arxiv											2	2;2024-09-15;https://www.arxiv.org/abs/2311.09774v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09774v1	arXiv:2311.09774			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 15 2024	2024	Adapting a language model (LM) into a specific domain, a.k.a ‘domain adaption’, is a common practice when specialized knowledge, e.g. medicine, is not encapsulated in a general language model like Llama2. This typically involves a two-stage process including continued pre-training and supervised fine-tuning. Implementing a pipeline solution with these two stages not only introduces complexities (necessitating dual meticulous tuning) but also leads to two occurrences of data distribution shifts, exacerbating catastrophic forgetting. To mitigate these, we propose a one-stage domain adaption protocol where heterogeneous data from both the traditional pre-training and supervised stages are unified into a simple instruction- output pair format to achieve efficient knowledge injection. Subsequently, a data priority sampling strategy is introduced to adaptively adjust data mixture during training. Following this protocol, we train HuatuoGPT-II, a specialized LLM for the medical domain in Chinese. HuatuoGPT-II achieve competitive performance with GPT4 across multiple benchmarks, which especially shows the state-of-the-art (SOTA) performance in multiple Chinese medical benchmarks and the newest pharmacist licensure examinations. Furthermore, we explore the phenomenon of one- stage protocols, and the experiments reflect that the simplicity of the proposed protocol improves training stability and domain generalization. 																																	2024-12-24	PPRN:86177343		
J	Guo, Shu-Yuan; Khlopov, Maxim; Liu, Xuewen; Wu, Lei; Wu, Yongcheng; Zhu, Bin				Khlopov, Maxim/T-5735-2017; Wu, Yik/C-1869-2009						Footprints of Axion-Like Particle in Pulsar Timing Array Data and James Webb Space Telescope Observations								Arxiv											2	2;2024-09-09;https://www.arxiv.org/abs/2306.17022v2| 1;2023-06-29;https://www.arxiv.org/abs/2306.17022v1	arXiv:2306.17022			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 09 2024	2024	Several Pulsar Timing Array (PTA) collaborations have recently reported the evidence for a stochastic gravitational-wave background (SGWB), which can unveil the formation of primordial seeds of inhomogeneities in the early universe. With the SGWB parameters inferred from PTAs data, we can make a prediction of the seeds for early galaxy formation from the domain walls in the axion-like particles (ALPs) field distribution. This also naturally provides a solution to the observation of high redshifts by the James Webb Space Telescope. The predicted photon coupling of the ALP is within the reach of future experimental searches.																																	2024-09-23	PPRN:73637688		
J	Smart, Brandon; Zheng, Chuanxia; Laina, Iro; Prisacariu, Victor Adrian										Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs								Arxiv											2	2;2024-08-27;https://www.arxiv.org/abs/2408.13912v2| 1;2024-08-25;https://www.arxiv.org/abs/2408.13912v1	arXiv:2408.13912			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 27 2024	2024	In this paper, we introduce Splatt3R, a pose-free, feed-forward method for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without requiring any camera parameters or depth information. For generalizability, we build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R, by extending it to deal with both 3D structure and appearance. Specifically, unlike the original MASt3R which reconstructs only 3D point clouds, we predict the additional Gaussian attributes required to construct a Gaussian primitive for each point. Hence, unlike other novel view synthesis methods, Splatt3R is first trained by optimizing the 3D point cloud's geometry loss, and then a novel view synthesis objective. By doing this, we avoid the local minima present in training 3D Gaussian Splats from stereo views. We also propose a novel loss masking strategy that we empirically find is critical for strong performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and the resultant splats can be rendered in real-time.																																	2024-09-19	PPRN:91551543		
J	Jin, Ming; Koh, Huan Yee; Wen, Qingsong; Zambon, Daniele; Alippi, Cesare; Webb, Geoffrey I.; King, Irwin; Pan, Shirui				Webb, Geoff/R-9967-2017; Zambon, Daniele/AAE-7787-2019; Wen, Qingsong/LTF-7625-2024; King, Irwin/C-9681-2015						A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection								Arxiv											2	2;2024-08-09;https://www.arxiv.org/abs/2307.03759v3| 1;2023-07-07;https://www.arxiv.org/abs/2307.03759v1	arXiv:2307.03759			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 09 2024	2024	Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.																																	2024-08-21	PPRN:73866048		
J	Wei, Kang; Li, Jun; Ma, Chuan; Ding, Ming; Wei, Sha; Wu, Fan; Chen, Guihai				Wei, Kang/PBU-6149-2025; Ding, Ming/AAW-4395-2021; Ma, Chuanyang/MGB-6346-2025; Li, Jun/HTO-0407-2023; Sha, Wei/G-4955-2010; Wu, Fan/ABC-1665-2021						Vertical Federated Learning: Challenges, Methodologies and Experiments								Arxiv											1	1;2024-08-05;https://www.arxiv.org/abs/2202.04309v2	arXiv:2202.04309			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 05 2024	2024	Recently, federated learning (FL) has emerged as a promising distributed machine learning (ML) technology, owing to the advancing computational and sensing capacities of end-user devices, however with the increasing concerns on users' privacy. As a special architecture in FL, vertical FL (VFL) is capable of constructing a hyper ML model by embracing sub-models from different clients. These sub-models are trained locally by vertically partitioned data with distinct attributes. Therefore, the design of VFL is fundamentally different from that of conventional FL, raising new and unique research issues. In this paper, we aim to discuss key challenges in VFL with effective solutions, and conduct experiments on real-life datasets to shed light on these issues. Specifically, we first propose a general framework on VFL, and highlight the key differences between VFL and conventional FL. Then, we discuss research challenges rooted in VFL systems under four aspects, i.e., security and privacy risks, expensive computation and communication costs, possible structural damage caused by model splitting, and system heterogeneity. Afterwards, we develop solutions to addressing the aforementioned challenges, and conduct extensive experiments to showcase the effectiveness of our proposed solutions.																																	2024-08-11	PPRN:91247227		
J											Apple Intelligence Foundation Language Models								Arxiv											1	1;2024-07-29;https://www.arxiv.org/abs/2407.21075v1	arXiv:2407.21075			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jul 29 2024	2024	We present foundation language models developed to power Apple Intelligence features, including a-3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.																																	2024-08-08	PPRN:91180491		
J	Lin, Han; Zala, Abhay; Cho, Jaemin; Bansal, Mohit				Bansal, Mohit/Q-9105-2016; Cho, Jae Min/HTO-8759-2023						VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning								Arxiv											2	2;2024-07-12;https://www.arxiv.org/abs/2309.15091v2| 1;2023-09-26;https://www.arxiv.org/abs/2309.15091v1	arXiv:2309.15091			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	Recent text-to-video (T2V) generation methods have seen significant advancements. However, the majority of these works focus on producing short video clips of a single event (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules. This prompts an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which includes the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities. Next, guided by this video plan, our video generator, named Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities across multiple scenes, while being trained only with image-level annotations. Our experiments demonstrate that our proposed VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with consistency, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation. Detailed ablation studies, including dynamic adjustment of layout control strength with an LLM and video generation with user-provided images, confirm the effectiveness of each component of our framework and its future potential.																																	2024-07-23	PPRN:85229380		
J	Cheng, Jiale; Liu, Xiao; Zheng, Kehan; Ke, Pei; Wang, Hongning; Dong, Yuxiao; Tang, Jie; Huang, Minlie				tang, jie/KIE-8633-2024; Wang, Hongning/GPK-7527-2022						Black-Box Prompt Optimization: Aligning Large Language Models without Model Training								Arxiv											2	2;2024-06-21;https://www.arxiv.org/abs/2311.04155v3| 1;2023-11-08;https://www.arxiv.org/abs/2311.04155v2	arXiv:2311.04155			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 21 2024	2024	Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective - Black-Box Prompt Optimization (BPO) - to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO.																																	2024-07-11	PPRN:86098117		
J	Cai, Wenxiao; Ponomarenko, Yaroslav; Yuan, Jianhao; Li, Xiaoqi; Yang, Wankou; Dong, Hao; Zhao, Bo				Cai, Wenxiao/JSL-0496-2023; Xing, Junliang/HGE-9630-2022; Zhao, Bo/AGZ-0290-2022						SpatialBot: Precise Spatial Understanding with Vision Language Models								Arxiv											7	7;2025-03-19;https://www.arxiv.org/abs/2406.13642v7| 6;2024-09-17;https://www.arxiv.org/abs/2406.13642v6| 5;2024-08-01;https://www.arxiv.org/abs/2406.13642v5| 4;2024-07-30;https://www.arxiv.org/abs/2406.13642v4| 3;2024-07-16;https://www.arxiv.org/abs/2406.13642v3| 2;2024-06-27;https://www.arxiv.org/abs/2406.13642v2| 1;2024-06-19;https://www.arxiv.org/abs/2406.13642v1	arXiv:2406.13642			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 19 2024	2024	Vision Language Models (VLMs) have achieved impressive performance in 2D image understanding, however they are still struggling with spatial understanding which is the foundation of Embodied AI. In this paper, we propose SpatialBot for better spatial understanding by feeding both RGB and depth images. Additionally, we have constructed the SpatialQA dataset, which involves multi-level depth-related questions to train VLMs for depth understanding. Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities in spatial understanding at different levels. Extensive experiments on our spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks, demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.																																	2025-08-07	PPRN:89375678		
J	Sferrazza, Carmelo; Huang, Dun-Ming; Lin, Xingyu; Lee, Youngwoon; Abbeel, Pieter				林, 星宇/GSD-2548-2022						HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation								Arxiv											2	2;2024-06-18;https://www.arxiv.org/abs/2403.10506v2| 1;2024-03-15;https://www.arxiv.org/abs/2403.10506v1	arXiv:2403.10506			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning approach achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas. 																																	2024-07-06	PPRN:88170498		
J	Chen, Wentong; Cui, Junbo; Hu, Jinyi; Qin, Yujia; Fang, Junjie; Zhao, Yue; Wang, Chongyi; Liu, Jun; Chen, Guirong; Huo, Yupeng; Yao, Yuan; Lin, Yankai; Liu, Zhiyuan; Sun, Maosong				Hu, Jinyi/GXF-7296-2022; Liu, Zhiyuan/I-2233-2014						GUICourse: From General Vision Language Models to Versatile GUI Agents								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.11317v1	arXiv:2406.11317			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse , a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse .																																	2024-07-04	PPRN:89347569		
J	Zhuo, Le; Du, Ruoyi; Xiao, Han; Li, Yangguang; Liu, Dongyang; Huang, Rongjie; Liu, Wenze; Zhao, Lirui; Wang, Fu-Yun; Ma, Zhanyu; Luo, Xu; Wang, Zehan; Zhang, Kaipeng; Zhu, Xiangyang; Liu, Si; Yue, Xiangyu; Liu, Dingning; Ouyang, Wanli; Liu, Ziwei; Qiao, Yu; Li, Hongsheng; Gao, Peng				liu, dingning/OQK-5738-2025; Liu, Ziwei/AAG-6939-2021; Qiao, Yu/ABD-5787-2021; yang, xiao/JLL-7721-2023; Li, Hongsheng/AES-5328-2022; Gao, Peng/B-4675-2012; Ouyang, Wanli/I-7135-2018; Du, Ruoyi/GXF-3987-2022						Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT								Arxiv											1	1;2024-06-05;https://www.arxiv.org/abs/2406.18583v1	arXiv:2406.18583			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 05 2024	2024	Lumina-T2X is a nascent family of Flow-based Large Diffusion Transformers that establishes a unified framework for transforming noise into various modalities, such as images and videos, conditioned on text instructions. Despite its promising capabilities, Lumina-T2X still encounters challenges including training instability, slow inference, and extrapolation artifacts. In this paper, we present Lumina-Next, an improved version of Lumina-T2X, showcasing stronger generation performance with increased training and inference efficiency. We begin with a comprehensive analysis of the Flag-DiT architecture and identify several suboptimal components, which we address by introducing the Next-DiT architecture with 3D RoPE and sandwich normalizations. To enable better resolution extrapolation, we thoroughly compare different context extrapolation methods applied to text-to-image generation with 3D RoPE, and propose Frequency- and Time-Aware Scaled RoPE tailored for diffusion transformers. Additionally, we introduced a sigmoid time discretization schedule to reduce sampling steps in solving the Flow ODE and the Context Drop method to merge redundant visual tokens for faster network evaluation, effectively boosting the overall sampling speed. Thanks to these improvements, Lumina-Next not only improves the quality and efficiency of basic text-to-image generation but also demonstrates superior resolution extrapolation capabilities and multilingual generation using decoder-based LLMs as the text encoder, all in a zero-shot manner. To further validate Lumina-Next as a versatile generative framework, we instantiate it on diverse tasks including visual recognition, multi-view, audio, music, and point cloud generation, showcasing strong performance across these domains. 																																	2024-07-17	PPRN:90145103		
J	Yao, Linli; Li, Lei; Ren, Shuhuai; Wang, Lean; Liu, Yuanxin; Sun, Xu; Hou, Lu				Liu, Yuanxin/OXB-7414-2025; Li, Lei/LMN-0940-2024; Ren, Shuhuai/KDO-1734-2024						DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models								Arxiv											1	1;2024-05-31;https://www.arxiv.org/abs/2405.20985v1	arXiv:2405.20985			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 31 2024	2024	The visual projector, which bridges the vision and language modalities and facilitates cross-modal alignment, serves as a crucial component in MLLMs. However, measuring the effectiveness of projectors in vision-language alignment remains under-explored, which currently can only be inferred from the performance of MLLMs on downstream tasks. Motivated by the problem, this study examines the projector module by interpreting the vision-language semantic flow within MLLMs. Specifically, we trace back the semantic relevance flow from generated language tokens to raw visual encoder patches and the intermediate outputs produced by projectors. Our findings reveal that compressive projectors (e.g., QFormer), abstract visual patches into a limited set of semantic concepts, such as objects or attributes, resulting in a 'double abstraction' phenomenon. This involves a first visual semantic abstraction by the projector referring to pre-defined query tokens, and a second extraction by the LLM based on text instructions. The double abstraction is inefficient in training and will result in cumulative vision semantics deficiency. To mitigate this issue, we propose the key insight of 'Decouple Compression from Abstraction (DeCo), that is compressing the visual token number at the patch level by projectors and allowing the LLM to handle visual semantic abstraction entirely. Consequently, we adopt a simple compressor, i.e., 2D Adaptive Pooling, to downsample visual patches in a parameter-free manner. Empirical evaluation demonstrates that DeCo surpasses traditional compressive projectors regarding both performance and efficiency. It achieves performance gains of 0.9%, 7.1%, and 2.9% across the MLLM Benchmarks, Visual Localization, and Open-ended VQA tasks with fewer trainable parameters and faster convergence speed. Furthermore, DeCo preserves vision spatial locality and exhibits robustness across various MLLM configurations, including different vision backbones, image resolutions, and LLMs.																																	2024-11-10	PPRN:89125316		
J	Alizadeh, Meysam; Kubli, Mael; Samei, Zeynab; Dehghani, Shirin; Zahedivafa, Mohammadmasiha; Bermeo, Juan Diego; Korobeynikova, Maria; Gilardi, Fabrizio				Kubli, Mael/NLO-4817-2025; Gilardi, Fabrizio/AGF-1855-2022; Alizadeh, Meysam/OMK-7029-2025						Open-Source LLMs for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2307.02179v2| 1;2023-07-05;https://www.arxiv.org/abs/2307.02179v1	arXiv:2307.02179			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 29 2024	2024	This paper studies the performance of open-source Large Language Models (LLMs) in text classification tasks typical for political science research. By examining tasks like stance, topic, and relevance classification, we aim to guide scholars in making informed decisions about their use of LLMs for text analysis. Specifically, we conduct an assessment of both zero-shot and fine-tuned LLMs across a range of text annotation tasks using news articles and tweets datasets. Our analysis shows that fine-tuning improves the performance of open-source LLMs, allowing them to match or even surpass zero-shot GPT-3.5 and GPT-4, though still lagging behind fine-tuned GPT-3.5. We further establish that fine-tuning is preferable to few-shot training with a relatively modest quantity of annotated text. Our findings show that fine-tuned open-source LLMs can be effectively deployed in a broad spectrum of text annotation applications. We provide a Python notebook facilitating the application of LLMs in text annotation for other researchers.																																	2024-08-25	PPRN:73793798		
J	Nguyen, Quynh T.; Schatzki, Louis; Braccia, Paolo; Ragone, Michael; Coles, Patrick J.; Sauvage, Frederic; Larocca, Martin; Cerezo, M.				Cerezo, Marco/ABD-9254-2020						Theory for Equivariant Quantum Neural Networks								Arxiv											1	1;2024-05-11;https://www.arxiv.org/abs/2210.08566v2	arXiv:2210.08566			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 11 2024	2024	Quantum neural network architectures that have little-to-no inductive biases are known to face trainability and generalization issues. Inspired by a similar problem, recent breakthroughs in machine learning address this challenge by creating models encoding the symmetries of the learning task. This is materialized through the usage of equivariant neural networks whose action commutes with that of the symmetry. In this work, we import these ideas to the quantum realm by presenting a comprehensive theoretical framework to design equivariant quantum neural networks (EQNN) for essentially any relevant symmetry group. We develop multiple methods to construct equivariant layers for EQNNs and analyze their advantages and drawbacks. Our methods can find unitary or general equivariant quantum channels efficiently even when the symmetry group is exponentially large or continuous. As a special implementation, we show how standard quantum convolutional neural networks (QCNN) can be generalized to group-equivariant QCNNs where both the convolution and pooling layers are equivariant to the symmetry group. We then numerically demonstrate the effectiveness of a SU(2)-equivariant QCNN over symmetry-agnostic QCNN on a classification task of phases of matter in the bond-alternating Heisenberg model. Our framework can be readily applied to virtually all areas of quantum machine learning. Lastly, we discuss about how symmetry-informed models such as EQNNs provide hopes to alleviate central challenges such as barren plateaus, poor local minima, and sample complexity.																																	2024-05-29	PPRN:88869124		
J	Yao, Weiran; Heinecke, Shelby; Niebles, Juan Carlos; Liu, Zhiwei; Feng, Yihao; Xue, Le; Murthy, Rithesh; Chen, Zeyuan; Zhang, Jianguo; Arpit, Devansh; Xu, Ran; Mui, Phil; Wang, Huan; Xiong, Caiming; Savarese, Silvio				Niebles, Juan/AAT-5882-2021; LE, XUE/IYT-3045-2023; Feng, Yihao/LVR-7524-2024; zhang, jian-guo/P-4114-2015						Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization								Arxiv											3	3;2024-05-05;https://www.arxiv.org/abs/2308.02151v3| 2;2024-04-30;https://www.arxiv.org/abs/2308.02151v2| 1;2023-08-04;https://www.arxiv.org/abs/2308.02151v1	arXiv:2308.02151			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.																																	2024-05-28	PPRN:74277849		
J	Zhang, Kai; Bi, Sai; Tan, Hao; Xiangli, Yuanbo; Zhao, Nanxuan; Sunkavalli, Kalyan; Xu, Zexiang										GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting								Arxiv											1	1;2024-04-30;https://www.arxiv.org/abs/2404.19702v1	arXiv:2404.19702			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	We propose GS-LRM, , a scalable large reconstruction model that can predict high -quality 3D Gaussian primitives from 2-4 posed sparse images in ∼0.23 seconds on single A100 GPU. Our model features a very simple transformer -based architecture; we patchify input posed images, pass the concatenated multi -view image tokens through a sequence of transformer blocks, and decode final per -pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per -pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/. .																																	2024-05-18	PPRN:88700479		
J	Chang, Yapei; Lo, Kyle; Goyal, Tanya; Iyyer, Mohit										BooookScore: A systematic exploration of book-length summarization in the era of LLMs								Arxiv											4	4;2024-04-13;https://www.arxiv.org/abs/2310.00785v4| 3;2024-03-19;https://www.arxiv.org/abs/2310.00785v3| 2;2023-10-05;https://www.arxiv.org/abs/2310.00785v2| 1;2023-10-01;https://www.arxiv.org/abs/2310.00785v1	arXiv:2310.00785			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 13 2024	2024	Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators.																																	2024-04-25	PPRN:85350301		
J	Peng, Puyuan; Huang, Po-Yao; Li, Daniel; Mohamed, Abdelrahman; Harwath, David				Huang, Poyao/IAQ-3889-2023						VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild								Arxiv											3	3;2024-06-14;https://www.arxiv.org/abs/2403.16973v3| 2;2024-04-19;https://www.arxiv.org/abs/2403.16973v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16973v1	arXiv:2403.16973			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 25 2024	2024	We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.																																	2025-08-07	PPRN:88278194		
J	Zhang, Jiazhao; Wang, Kunyu; Xu, Rongtao; Zhou, Gengze; Hong, Yicong; Fang, Xiaomeng; Wu, Qi; Zhang, Zhizheng; Wang, He				Zhang, Zhizheng/AGZ-8479-2022; Wu, Qi/ABD-6304-2021; Zhou, Gengze/MIK-6445-2025; xu, rongtao/IQS-1976-2023						NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation								Arxiv											4	4;2024-06-30;https://www.arxiv.org/abs/2402.15852v7| 3;2024-05-28;https://www.arxiv.org/abs/2402.15852v6| 2;2024-05-27;https://www.arxiv.org/abs/2402.15852v5| 1;2024-03-23;https://www.arxiv.org/abs/2402.15852v4	arXiv:2402.15852			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 23 2024	2024	Vision-and-Language Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavour to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometer and depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision-making and instruction following. We train NaVid with 550k navigation samples collected from VLN-CE trajectories, including action-planning and instruction-reasoning samples, along with 665k large-scale web data. Extensive experiments show that NaVid achieves SOTA performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.																																	2025-08-07	PPRN:88278668		
J	Sun, Zhiqing; Yu, Longhui; Shen, Yikang; Liu, Weiyang; Yang, Yiming; Welleck, Sean; Gan, Chuang										Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision								Arxiv											1	1;2024-03-14;https://www.arxiv.org/abs/2403.09472v1	arXiv:2403.09472			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Current AI alignment methodologies rely on human -provided demonstrations or judgments, and the learned capabilities of AI systems would be upper -bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as easy -to -hard generalization. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy -to -hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process -supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such easy -to -hard generalization from evaluators can enable easy -to -hard generalizations in generators either through re -ranking or reinforcement learning (RL). Notably, our process -supervised 7b RL model achieves an accuracy of 34.0% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.																																	2024-04-11	PPRN:88140950		
J	Liu, Xiaoran; Yan, Hang; Zhang, Shuo; An, Chenxin; Qiu, Xipeng; Lin, Dahua				Lin, Dahua/W-6576-2019; Liu, Xiaoran/ABE-3986-2021						Scaling Laws of RoPE-based Extrapolation								Arxiv											2	2;2024-03-13;https://www.arxiv.org/abs/2310.05209v2| 1;2023-10-08;https://www.arxiv.org/abs/2310.05209v1	arXiv:2310.05209			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 13 2024	2024	The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding (Su et al., 2021) is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of θn = 10000−2n/d in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre -training context length could significantly enhance its extrapolation performance. After that, we propose Scaling Laws of RoPE-based Extrapolation, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by critical dimension for extrapolation. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B (Touvron et al., 2023b).																																	2024-04-11	PPRN:85583199		
J	Lei, Shanglin; Wang, Xiaoping; Dong, Guanting; Wang, Keheng; Wang, Sirui				xiaoping, wang/GRX-3807-2022; dong, guanting/JGL-9364-2023						InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework								Arxiv											4	4;2024-03-12;https://www.arxiv.org/abs/2309.11911v4| 3;2023-11-24;https://www.arxiv.org/abs/2309.11911v3| 2;2023-09-22;https://www.arxiv.org/abs/2309.11911v2| 1;2023-09-21;https://www.arxiv.org/abs/2309.11911v1	arXiv:2309.11911			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 12 2024	2024	The field of emotion recognition of conversation (ERC) has been focusing on separating sentence feature encoding and context modeling, lacking exploration in generative paradigms based on unified designs. In this study, we propose a novel approach, Instruct ERC, to re-formulate the ERC task from a discriminative frame work to a generative framework based on Large Language Models (LLMs). Instruct ERC makes three significant contributions: (1) it introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue super-vision information. (2) We introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships nd future emotional tendencies in conversations. (3) Pioneeringly, we unify emotion labels across benchmarks through the feeling wheel to fit real application scenarios. Instruct ERC still perform impressively on this unified dataset. Our LLM-based plugin framework significantly out performs all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provide sempirical guidance for applying it in practical scenarios. Our code and aligned unified dataset(UIME) can be found in the Github link.1																																	2024-04-08	PPRN:85081267		
J	Chen, Tsai-Shien; Siarohin, Aliaksandr; Menapace, Willi; Deyneka, Ekaterina; Chao, Hsiang-wei; Jeon, Byung Eun; Fang, Yuwei; Lee, Hsin-Ying; Ren, Jian; Yang, Ming-Hsuan; Tulyakov, Sergey				Lee, HsinYing/B-9716-2009; Menapace, Willi/JVY-9690-2024; Yang, Ming-Hsuan/T-9533-2019; Fang, Yuwei/AAX-5314-2020; Ren, Jian/AAP-2636-2021						Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19479v1	arXiv:2402.19479			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 29 2024	2024	The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.																																	2024-11-09	PPRN:87987407		
J	Kasai, Jungo; Sakaguchi, Keisuke; Takahashi, Yoichi; Le Bras, Ronan; Asai, Akari; Yu, Xinyan Velocity; Radev, Dragomir; Smith, Noah A.; Choi, Yejin; Inui, Kentaro				Radev, Dragomir/E-9641-2012						REALTIME QA: What’s the Answer Right Now?								Arxiv											2	2;2024-02-28;https://www.arxiv.org/abs/2207.13332v2| 1;2022-07-27;https://www.arxiv.org/abs/2207.13332v1	arXiv:2207.13332			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 28 2024	2024	We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open -domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly -retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open -domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that REALTIME QA will spur progress in instantaneous applications of question answering and beyond.																																	2024-11-09	PPRN:11542795		
J	Luo, Tongxu; Lei, Jiahe; Lei, Fangyu; Liu, Weihao; He, Shizhu; Zhao, Jun; Liu, Kang				Liu, Weihao/JDD-1835-2023						MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models								Arxiv											1	1;2024-02-20;https://www.arxiv.org/abs/2402.12851v1	arXiv:2402.12851			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 20 2024	2024	Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to learn distinct features. We conducted experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks. With the same number of parameters, our approach outperforms LoRA significantly. In math reasoning, MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on several benchmarks.																																	2024-03-19	PPRN:87776441		
J	Yi, Liping; Yu, Han; Wang, Gang; Liu, Xiaoguang; Li, Xiaoxiao				Yu, Han/R-3297-2017						pFedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning								Arxiv											2	2;2024-02-11;https://www.arxiv.org/abs/2310.13283v2| 1;2023-10-20;https://www.arxiv.org/abs/2310.13283v1	arXiv:2310.13283			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 11 2024	2024	Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (clients) collaboratively to train on decentralized data. In practice, FL often faces statistical, system, and model heterogeneities, which inspires the field of Model-Heterogeneous Personalized Federated Learning (MHPFL). With the increased interest in adopting large language models (LLMs) in FL, the existing MHPFL methods cannot achieve acceptable computational and communication costs, while maintaining satisfactory model performance. To bridge this gap, we propose a novel and efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (pFedLoRA). Inspired by the popular LoRA method for fine-tuning pre-trained LLMs with a low-rank model (a.k.a., an adapter), we design a homogeneous small adapter to facilitate federated client's heterogeneous local model training with our proposed iterative training for global-local knowledge exchange. The homogeneous small local adapters are aggregated on the FL server to generate a global adapter. We theoretically prove the convergence of pFedLoRA. Extensive experiments on two benchmark datasets demonstrate that pFedLoRA outperforms six state-of-the-art baselines, beating the best method by 1.35% in test accuracy, 11.81 times computation overhead reduction and 7.41 times communication cost saving.																																	2024-05-25	PPRN:85741727		
J	Delbracio, Mauricio; Milanfar, Peyman										Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration								Arxiv											3	3;2024-02-02;https://www.arxiv.org/abs/2303.11435v5| 2;2023-10-16;https://www.arxiv.org/abs/2303.11435v4| 1;2023-03-20;https://www.arxiv.org/abs/2303.11435v1	arXiv:2303.11435			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called “regression to the mean” effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.   Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.   While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising.																																	2024-05-25	PPRN:46957808		
J	Akyuerek, Ekin; Wang, Bailin; Kim, Yoon; Andreas, Jacob										In-Context Language Learning: Architectures and Algorithms								Arxiv											2	2;2024-01-30;https://www.arxiv.org/abs/2401.12973v2| 1;2024-01-23;https://www.arxiv.org/abs/2401.12973v1	arXiv:2401.12973			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 30 2024	2024	Large-scale neural language models (LMs) exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the “real” ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. ICLL is designed to be simple enough to study in small-scale LMs, but complex enough to capture the key features of ICL in large-scale LMs. Here we focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including several RNNs, Transformers, and state-space model variants) on regular ICLL tasks, aiming to answer three questions: (1) Which model classes are empirically capable of ICLL? (2) What algorithmic solutions do successful models implement to perform ICLL? (3) What architectural changes can improve ICLL in less performant models? We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks. Next, we provide evidence that their ability to do so relies on specialized “n -gram heads” (higher-order variants of previously-described “induction heads”) that compute input-conditional next-token distributions. Finally, we show that hard-wiring these heads into Transformer, recurrent and convolutional models improves performance not just on synthetic ICLL, but natural language modeling—reducing the perplexity of 340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset. Our results highlight the usefulness of in-context formal language learning as a tool for understanding ICL in models of natural text.																																	2024-05-25	PPRN:87291821		
J	Wang, Lirui; Ling, Yiyang; Yuan, Zhecheng; Shridhar, Mohit; Bao, Chen; Qin, Yuzhe; Wang, Bailin; Xu, Huazhe; Wang, Xiaolong				Qin, Yuzhe/IXN-5900-2023; Bao, Chen/LQK-7168-2024						GenSim: Generating Robotic Simulation Tasks via Large Language Models								Arxiv											2	2;2024-01-21;https://www.arxiv.org/abs/2310.01361v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01361v1	arXiv:2310.01361			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 21 2024	2024	Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models’ (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1																																	2024-05-25	PPRN:85355930		
J	Cao, He; Liu, Zijing; Lu, Xingyu; Yao, Yuan; Li, Yu				Lu, Xingyu/LIF-9692-2024; Li, yu/HHZ-5236-2022						InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery								Arxiv											2	2;2024-12-19;https://www.arxiv.org/abs/2311.16208v2| 1;2023-11-27;https://www.arxiv.org/abs/2311.16208v1	arXiv:2311.16208			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 19 2024	2024	The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol1, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two- stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialists, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.																																	2025-01-26	PPRN:86311390		
J	Zhang, Michael; Sohoni, Nimit S.; Zhang, Hongyang R.; Finn, Chelsea; Re, Christopher										Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations								Arxiv											1	1;2024-12-11;https://www.arxiv.org/abs/2203.01517v2	arXiv:2203.01517			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 11 2024	2024	Spurious correlations pose a major challenge for robust machine learning. Models trained with empirical risk minimization (ERM) may learn to rely on correlations between class labels and spurious attributes, leading to poor performance on data groups without these correlations. This is particularly challenging to address when spurious attribute labels are unavailable. To improve worst-group performance on spuriously correlated data without training attribute labels, we propose Correct-N-Contrast (CNC), a contrastive approach to directly learn representations robust to spurious correlations. As ERM models can be good spurious attribute predictors, CNC works by (1) using a trained ERM model's outputs to identify samples with the same class but dissimilar spurious features, and (2) training a robust model with contrastive learning to learn similar representations for same-class samples. To support CNC, we introduce new connections between worst-group error and a representation alignment loss that CNC aims to minimize. We empirically observe that worst-group error closely tracks with alignment loss, and prove that the alignment loss over a class helps upper-bound the class's worst-group vs. average error gap. On popular benchmarks, CNC reduces alignment loss drastically, and achieves state-of-the-art worst-group accuracy by 3.6% average absolute lift. CNC is also competitive with oracle methods that require group labels.																																	2025-01-19	PPRN:119845058		
J	Xia, Chunqiu Steven; Paltenghi, Matteo; Le Tian, Jia; Pradel, Michael; Zhang, Lingming				Paltenghi, Matteo/JCD-7289-2023						Fuzz4All: Universal Fuzzing with Large Language Models								Arxiv											3	3;2024-12-09;https://www.arxiv.org/abs/2308.04748v3| 2;2024-01-15;https://www.arxiv.org/abs/2308.04748v2| 1;2023-08-09;https://www.arxiv.org/abs/2308.04748v1	arXiv:2308.04748			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 09 2024	2024	Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4ALL, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4ALL is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are well- suited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4ALL on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java, and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4ALL has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.																																	2025-01-18	PPRN:74924039		
J	Guo, Xin; Xia, Haotian; Liu, Zhaowei; Cao, Hanyang; Yang, Zhi; Liu, Zhiqiang; Wang, Sizhe; Niu, Jinyi; Wang, Chuqi; Wang, Yanhui; Liang, Xiaolong; Huang, Xiaoming; Zhu, Bing; Wei, Zhongyu; Chen, Yun; Shen, Weining; Zhang, Liwen				Cao, Hanyang/MZS-4200-2025; dai, wei/GRR-8868-2022; Liu, Zhiqiang/GZK-8794-2022; Wu, Anbo/JQI-4421-2023; wang, chuqi/GZM-4412-2022; Wei, Zhongyu/KSL-9373-2024; Li, Yifei/AAB-4210-2019; 梁, 小龙/JDD-1142-2023						FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models								Arxiv											2	2;2024-12-08;https://www.arxiv.org/abs/2308.09975v2| 1;2023-08-19;https://www.arxiv.org/abs/2308.09975v1	arXiv:2308.09975			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 08 2024	2024	Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown. This paper presents FinEval, a benchmark designed to evaluate LLMs’ financial domain knowledge and practical abilities. The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent. Financial Academic Knowledge comprises 4,661 multiple-choice questions spanning 34 subjects such as finance and economics. Financial Industry Knowledge contains 1,434 questions covering practical scenarios like investment research. Financial Security Knowledge assesses models through 1,640 questions on topics like application security and cryptography. Financial Agent evaluates tool usage and complex reasoning with 616 questions. FinEval has multiple evaluation settings, including zero-shot, five-shot with chain- of-thought, and assesses model performance using objective and subjective criteria. Our results show that Claude 3.5-Sonnet achieves the highest weighted average score of 72.9 across all financial domain categories under zero-shot setting. Our work provides a comprehensive benchmark closely aligned with Chinese financial domain. 																																	2025-01-17	PPRN:81977231		
J	Liu, Zhihan; Lu, Miao; Zhang, Shenao; Liu, Boyi; Guo, Hongyi; Yang, Yingxiang; Blanchet, Jose; Wang, Zhaoran				Zhang, Shenao/HLW-0562-2023; Liu, Boyi/C-9181-2012; Liu, Zhihan/NGS-3762-2025						Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer								Arxiv											3	3;2024-12-04;https://www.arxiv.org/abs/2405.16436v3| 2;2024-11-04;https://www.arxiv.org/abs/2405.16436v2| 1;2024-05-26;https://www.arxiv.org/abs/2405.16436v1	arXiv:2405.16436			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 04 2024	2024	Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output undesired responses. We investigate this problem in a principled manner by identifying the source of the misalignment as a form of distributional shift and uncertainty in learning human preferences. To mitigate overoptimization, we first propose a theoretical algorithm that chooses the best policy for an adversarially chosen reward model; one that simultaneously minimizes the maximum likelihood estimation of the loss and a reward penalty term. Here, the reward penalty term is introduced to prevent the policy from choosing actions with spurious high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation. Using the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines: (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss that explicitly imitates the policy with a (suitable) baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fine-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO). Experiments of aligning LLMs demonstrate the improved performance of RPO compared with DPO baselines. Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.																																	2025-01-15	PPRN:89063689		
J	Liu, Fangfu; Sun, Wenqiang; Wang, Hanyang; Wang, Yikai; Sun, Haowen; Ye, Junliang; Zhang, Jun; Duan, Yueqi				wang, yikai/HLW-7052-2023; Wang, Hanyang/JFB-3017-2023; Liu, Fangfu/KPQ-4616-2024; Zhang, Jun/M-8009-2013						ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model								Arxiv											2	2;2024-11-30;https://www.arxiv.org/abs/2408.16767v2| 1;2024-08-29;https://www.arxiv.org/abs/2408.16767v1	arXiv:2408.16767			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 30 2024	2024	Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability.																																	2025-01-11	PPRN:91783478		
J	Park, Chan-Gyung; Perez, Javier de Cruz; Ratra, Bharat				Ratra, Bharat/I-4979-2012						Using non-DESI data to confirm and strengthen the DESI 2024 spatially-flat<italic> w</italic>0wa CDM cosmological parameterization result								Arxiv											3	3;2024-11-28;https://www.arxiv.org/abs/2405.00502v3| 2;2024-10-04;https://www.arxiv.org/abs/2405.00502v2| 1;2024-05-01;https://www.arxiv.org/abs/2405.00502v1	arXiv:2405.00502			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 28 2024	2024	We use a combination of Planck cosmic microwave background (CMB) anisotropy data and nonCMB data that include Pantheon+ type Ia supernovae (SNIa), Hubble parameter [H(z)], growth factor (f a-8) measurements, and a collection of baryon acoustic oscillation (BAO) data, but not recent DESI 2024 BAO measurements, to confirm the DESI 2024 (DESI+CMB+PantheonPlus) data compilation support for dynamical dark energy with an evolving equation of state parameter w(z)=w0+waz/(1 + z). From our joint compilation of CMB and non-CMB data, in a spatially- flat cosmological model, we obtain w0=−0.850 ± 0.059 and wa=−0.59−0.22+0.26 and find that this dynamical dark energy is favored over a cosmological constant by ∼2σ. Our data constraints on the flat w0waCDM parameterization are slightly more restrictive than the DESI 2024 constraints, with the DESI 2024 and our values of w0 and wa differing by −0.27σ and 0.44σ, respectively. Our data compilation slightly more strongly favors the flat w0waCDM model over the flat Λ CDM model than does the DESI 2024 data compilation. We note that our CMB and non-CMB data w0waCDM parameterization cosmological constraints are discrepant at 2.7σ, a little larger than the 1.9σ discrepancy between DESI DR1 BAO and CMB data flat Λ CDM model cosmological constraints. We also show that if we remove the Pantheon+ SNIa contribution from the non-CMB data, for the w0waCDM parameterization we still find tension between P18 and non-CMB data (2.5σ) and P18+lensing and non-CMB data (2.4σ). Even after the exclusion of Pantheon+ SNIa data the Λ CDM model is still disfavoured at ∼2σc.l.																																	2025-01-11	PPRN:88837207		
J	Fang, Xinyu; Mao, Kangrui; Duan, Haodong; Zhao, Xiangyu; Li, Yining; Lin, Dahua; Chen, Kai				Fang, Xinyu/IYT-2547-2023; Duan, Haodong/ITV-1505-2023; Lin, Dahua/W-6576-2019; Zhao, Xiangyu/AAO-2203-2020						MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding								Arxiv											2	2;2024-10-23;https://www.arxiv.org/abs/2406.14515v2| 1;2024-06-20;https://www.arxiv.org/abs/2406.14515v1	arXiv:2406.14515			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 23 2024	2024	The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models' temporal comprehension. To address these limitations, we introduce MMBench-Video, a quantitative benchmark designed to rigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. 																																	2024-11-26	PPRN:89375740		
J	Tan, Sijun; Zhuang, Siyuan; Montgomery, Kyle; Tang, William Y.; Cuadron, Alejandro; Wang, Chenguang; Popa, Raluca Ada; Stoica, Ion				Zhuang, Siyuan/JLK-9571-2023						JudgeBench: A Benchmark for Evaluating LLM-based Judges								Arxiv											1	1;2024-10-16;https://www.arxiv.org/abs/2410.12784v1	arXiv:2410.12784			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 16 2024	2024	LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. 																																	2024-11-11	PPRN:114181192		
J	Fei, Jiajun; Li, Dian; Deng, Zhidong; Wang, Zekun; Liu, Gang; Wang, Hui										Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos								Arxiv											1	1;2024-08-26;https://www.arxiv.org/abs/2408.14023v1	arXiv:2408.14023			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 26 2024	2024	Multi-modal large language models (MLLMs) have demonstrated considerable potential across various downstream tasks that require cross-domain knowledge. MLLMs capable of processing videos, known as Video-MLLMs, have attracted broad interest in video-language understanding. However, videos, especially long videos, contain more visual tokens than images, making them difficult for LLMs to process. Existing works either downsample visual features or extend the LLM context size, risking the loss of high-resolution information or slowing down inference speed. To address these limitations, we apply cross-attention layers in the intermediate projector between the visual encoder and the large language model (LLM). As the naive cross-attention mechanism is insensitive to temporal order, we further introduce causal cross-attention masks (CCAMs) within the cross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a straightforward two-stage fashion: feature alignment and visual instruction tuning. We develop several Video-CCAM models based on LLMs of different sizes (4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows outstanding performance from short videos to long ones. Among standard video benchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding performances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA, MSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos, Video-CCAM models can be directly adapted to long video understanding and still achieve exceptional scores despite being trained solely with images and 16-frame videos. Using 96 frames (6 × the training number of frames), Video-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among all open-source Video-MLLMs, respectively. We provide a theoretical analysis of its temporal consistency and emphasize several key factors in its architecture through experiments. We hope that Video-CCAM can serve as a straightforward yet robust baseline for future Video-MLLM development. 																																	2024-09-04	PPRN:91548313		
J	Pan, Zhuoshi; Wu, Qianhui; Jiang, Huiqiang; Xia, Menglin; Luo, Xufang; Zhang, Jue; Lin, Qingwei; Ruhle, Victor; Yang, Yuqing; Lin, Chin-Yew; Zhao, H. Vicky; Qiu, Lili; Zhang, Dongmei				Jiang, Huiqiang/KHX-2210-2024; Lin, Qingwei/AAZ-3604-2021; Yuqing, Yang/ADJ-2720-2022						LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression								Arxiv											2	2;2024-08-12;https://www.arxiv.org/abs/2403.12968v2| 1;2024-03-19;https://www.arxiv.org/abs/2403.12968v1	arXiv:2403.12968			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 12 2024	2024	This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective. To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT. We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.1																																	2024-08-21	PPRN:88240907		
J	Lin, Zheng; Hu, Xuanjie; Zhang, Yuxin; Chen, Zhe; Fang, Zihan; Chen, Xianhao; Li, Ang; Vepakomma, Praneeth; Gao, Yue				Li, Ang/GWD-0502-2022; Chen, Zhe/JNR-1848-2023; CHEN, XIANHAO/AAX-6311-2021; Gao, Yue/AAJ-9469-2020						SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large Language Models								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.00952v1	arXiv:2407.00952			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	The scalability of large language models (LLMs) in handling high-complexity models and large-scale datasets has led to tremendous successes in pivotal domains. While there is an urgent need to acquire more training data for LLMs, a concerning reality is the depletion of high-quality public datasets within a few years. In view of this, the federated learning (FL) LLM fine-tuning paradigm recently has been proposed to facilitate collaborative LLM fine-tuning on distributed private data, where multiple data owners collaboratively fine-tune a shared LLM without sharing raw data. However, the staggering model size of LLMs imposes heavy computing and communication burdens on clients, posing significant barriers to the democratization of the FL LLM fine-tuning paradigm. To address this issue, split learning (SL) has emerged as a promising solution by offloading the primary training workload to a server via model partitioning while exchanging activation/activation's gradients with smaller data sizes rather than the entire LLM. Unfortunately, research on the SL LLM fine-tuning paradigm is still in its nascent stage. To fill this gap, in this paper, we propose the first SL LLM fine-tuning framework, named SplitLoRA. SplitLoRA is built on the split federated learning (SFL) framework, amalgamating the advantages of parallel training from FL and model splitting from SL and thus greatly enhancing the training efficiency. It is worth noting that SplitLoRA is the inaugural open-source benchmark for SL LLM fine-tuning, providing a foundation for research efforts dedicated to advancing SL LLM fine-tuning. Extensive simulations validate that SplitLoRA achieves target accuracy in significantly less time than state-of-the-art LLM fine-tuning frameworks, demonstrating the superior training performance of SplitLoRA. The project page is available at https://fduinc.github.io/splitlora/.																																	2024-07-18	PPRN:90657328		
J	Xiao, Yijia; Sun, Edward; Liu, Tianyu; Wang, Wei				Liu, Tianyu/JXN-8107-2024; XIAO, YIJIA/OLR-2697-2025						LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts								Arxiv											1	1;2024-07-01;	arXiv:2407.04973			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	We propose LogicVista, an evaluation benchmark that assesses the integrated logical reasoning capabilities of multimodal large language models (MLLMs) in Visual contexts. Recent advancements in MLLMs have demonstrated various fascinating abilities, from crafting poetry based on an image to performing mathematical reasoning. However, there is still a lack of systematic evaluation of MLLMs' proficiency in logical reasoning tasks, which are essential for activities like navigation and puzzle-solving. Thus we evaluate general logical cognition abilities across 5 logical reasoning tasks encompassing 9 different capabilities, using a sample of 448 multiple-choice questions. Each question is annotated with the correct answer and the human-written reasoning behind the selection, enabling both open-ended and multiple-choice evaluation. A total of 8 MLLMs are comprehensively evaluated using LogicVista. 																																	2024-11-17	PPRN:118689461		
J	Ke, Pei; Wen, Bosi; Feng, Zhuoer; Liu, Xiao; Lei, Xuanyu; Cheng, Jiale; Wang, Shengyuan; Zeng, Aohan; Dong, Yuxiao; Wang, Hongning; Tang, Jie; Huang, Minlie				Wang, Hongning/GPK-7527-2022						CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation								Arxiv											2	2;2024-06-26;https://www.arxiv.org/abs/2311.18702v2| 1;2023-11-30;https://www.arxiv.org/abs/2311.18702v1	arXiv:2311.18702			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 26 2024	2024	Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT1.																																	2024-07-15	PPRN:86341521		
J	Havrilla, Alex; Raparthy, Sharath Chandra; Nalmpantis, Christoforus; Dwivedi-Yu, Jane; Zhuravinskyi, Maksym; Hambro, Eric; Raileanu, Roberta										GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements								Arxiv											2	2;2024-06-25;https://www.arxiv.org/abs/2402.10963v2| 1;2024-02-13;https://www.arxiv.org/abs/2402.10963v1	arXiv:2402.10963			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 25 2024	2024	State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify when and where to refine without access to external feedback. Outcome-based Reward Models ( ORMs ), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution. However, when used to indicate where to refine, we find that ORMs tend to be overly-pessimistic when used to assess intermediate reasoning steps, resulting in excessive refinement of valid solutions. Process Based Reward Models ( PRMs ), trained to predict correctness of intermediate steps indicating where to refine, have been used to improve LLM reasoning ability via rejection sampling or reinforcement learning (RL) fine-tuning. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs ( SORMs ) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or V ⋆ . More specifically, SORMs are trained to predict the correctness of the final answer when sampling the current policy many times (rather than only once as in the case of ORMs). Our experiments show that SORMs can more accurately detect incorrect reasoning steps compared to ORMs, thus improving downstream accuracy when doing refinements. We then train global refinement models, which take only the question and a draft solution as input and predict a corrected solution, and local refinement models which also take as input a critique indicating the location of the first reasoning error. We generate training data for both models synthetically by reusing data used to train the SORM. We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually, as well as a best of three sample baseline. With this strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned with RL) on GSM8K from 53% to 65% when greedily sampled.																																	2024-07-15	PPRN:87798435		
J	Tyen, Gladys; Mansoor, Hassan; Carbune, Victor; Chen, Peter; Mak, Tony										LLMs cannot <italic>find </italic>reasoning errors, but can <italic>correct</italic> them given the error location								Arxiv											3	3;2024-06-04;https://www.arxiv.org/abs/2311.08516v3| 2;2024-01-09;https://www.arxiv.org/abs/2311.08516v2| 1;2023-11-14;https://www.arxiv.org/abs/2311.08516v1	arXiv:2311.08516			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 04 2024	2024	While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023b; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we show that poor self-correction performance stems from LLMs' inability to find logical mistakes, rather than their ability to correct a known mistake. Firstly, we benchmark several state-of-the-art LLMs on their mistake-finding ability and demonstrate that they generally struggle with the task, even in highly objective, unambiguous cases. Secondly, we test the correction abilities of LLMs -- separately from mistake finding -- using a backtracking setup that feeds ground truth mistake location information to the model. We show that this boosts downstream task performance across our 5 reasoning tasks, indicating that LLMs' correction abilities are robust. Finally, we show that it is possible to obtain mistake location information without ground truth labels or in-domain training data. We train a small classifier with out-of-domain data, which exhibits stronger mistake-finding performance than prompting a large model. We release our dataset of LLM-generated logical mistakes, BIG-Bench Mistake, to enable further research into locating LLM reasoning mistakes.																																	2024-07-11	PPRN:86163494		
J	Wu, Shuang; Lin, Youtian; Zhang, Feihu; Zeng, Yifei; Xu, Jingxi; Torr, Philip; Cao, Xun; Yao, Yao				Zeng, Yifei/IVV-4419-2023; Cao, Xun/AAM-2895-2021						Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer								Arxiv											2	2;2024-06-01;https://www.arxiv.org/abs/2405.14832v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14832v1	arXiv:2405.14832			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	Generating high-quality 3D assets from text and images has long been challenging, primarily due to the absence of scalable 3D representations capable of capturing intricate geometry distributions. In this work, we introduce Direct3D, a native 3D generative model scalable to in-the-wild input images, without requiring a multiview diffusion model or SDS optimization. Our approach comprises two primary components: a Direct 3D Variational Auto-Encoder (D3D-VAE) and a Direct 3D Diffusion Transformer (D3D-DiT). D3D-VAE efficiently encodes high-resolution 3D shapes into a compact and continuous latent triplane space. Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods relying on rendered images as supervision signals. D3D-DiT models the distribution of encoded 3D latents and is specifically designed to fuse positional information from the three feature maps of the triplane latent, enabling a native 3D generative model scalable to large-scale 3D datasets. Additionally, we introduce an innovative image-to-3D generation pipeline incorporating semantic and pixel-level image conditions, allowing the model to produce 3D shapes consistent with the provided conditional image input. Extensive experiments demonstrate the superiority of our large-scale pre-trained Direct3D over previous image-to-3D approaches, achieving significantly better generation quality and generalization ability, thus establishing a new state-of-the-art for 3D content creation. 																																	2024-06-22	PPRN:88990505		
J	Sun, Wenchao; Lin, Xuewu; Shi, Yining; Zhang, Chuang; Wu, Haoran; Zheng, Sifa				Zheng, Sifa/LMM-8646-2024; 石, 屹宁/KHV-8736-2024						SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation								Arxiv											2	2;2024-05-31;https://www.arxiv.org/abs/2405.19620v2| 1;2024-05-30;https://www.arxiv.org/abs/2405.19620v1	arXiv:2405.19620			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 31 2024	2024	The well-established modular autonomous driving system is decoupled into different standalone tasks, e.g. perception, prediction and planning, suffering from information loss and error accumulation across modules. In contrast, end-to-end paradigms unify multi-tasks into a fully differentiable framework, allowing for optimization in a planning-oriented spirit. Despite the great potential of end-toend paradigms, both the performance and efficiency of existing methods are not satisfactory, particularly in terms of planning safety. We attribute this to the computationally expensive BEV (bird’s eye view) features and the straightforward design for prediction and planning. To this end, we explore the sparse representation and review the task design for end-to-end autonomous driving, proposing a new paradigm named SparseDrive. Concretely, SparseDrive consists of a symmetric sparse perception module and a parallel motion planner. The sparse perception module unifies detection, tracking and online mapping with a symmetric model architecture, learning a fully sparse representation of the driving scene. For motion prediction and planning, we review the great similarity between these two tasks, leading to a parallel design for motion planner. Based on this parallel design, which models planning as a multi-modal problem, we propose a hierarchical planning selection strategy , which incorporates a collision-aware rescore module, to select a rational and safe trajectory as the final planning output. With such effective designs, SparseDrive surpasses previous state-of-the-arts by a large margin in performance of all tasks, while achieving much higher training and inference efficiency. Code will be avaliable at https://github.com/swc-17/SparseDrive for facilitating future research.																																	2024-06-19	PPRN:89113626		
J	Wu, Chengyue; Gan, Yukang; Ge, Yixiao; Lu, Zeyu; Wang, Jiahao; Feng, Ye; Shan, Ying; Luo, Ping				Luo, Ping/HGE-7623-2022; Wang, Jiahao/AAN-1140-2020; Lu, Zeyu/AAB-8116-2022; FENG Ye, Echo/JYP-7979-2024; gan, yukang/NMK-7097-2025						LLaMA Pro: Progressive LLaMA with Block Expansion								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2401.02415v2| 1;2024-01-04;https://www.arxiv.org/abs/2401.02415v1	arXiv:2401.02415			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., , from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model’s knowledge while mitigating forgetting. In this paper, we experiment on the corpus of code and math, yielding LLAMA A MA P RO -8.3B, , a versatile foundation model initialized from LLaMA27B, excelling in general tasks, programming, and mathematics. LLAMA A MA P RO and its instruction -following counterpart (LLAMA A MA P RO - I NSTRUCT ) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.																																	2024-06-16	PPRN:86970988		
J	Meyerson, Elliot; Nelson, Mark J.; Bradley, Herbie; Gaier, Adam; Moradi, Arash; Hoover, Amy K.; Lehman, Joel				Nelson, Mark/D-7036-2014; Galer, Adam/GLR-5448-2022; Moradi Karkaj, Arash/JUV-2238-2023; Lehman, Joel/AAH-9977-2019; Hoover, Amy K/GOK-0692-2022						Language Model Crossover: Variation through Few-Shot Prompting								Arxiv											3	3;2024-05-13;https://www.arxiv.org/abs/2302.12170v3| 2;2023-10-07;https://www.arxiv.org/abs/2302.12170v2| 1;2023-02-23;https://www.arxiv.org/abs/2302.12170v1	arXiv:2302.12170			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 13 2024	2024	This paper pursues the insight that language models naturally enable an intelligent variation operator similar in spirit to evolutionary crossover. In particular, language models of sufficient scale demonstrate in-context learning, i.e. they can learn from associations between a small number of input patterns to generate outputs incorporating such associations (also called few-shot prompting). This ability can be leveraged to form a simple but powerful variation operator, i.e. to prompt a language model with a few text-based genotypes (such as code, plain-text sentences, or equations), and to parse its corresponding output as those genotypes' offspring. The promise of such language model crossover (which is simple to implement and can leverage many different open-source language models) is that it enables a simple mechanism to evolve semantically-rich text representations (with few domain-specific tweaks), and naturally benefits from current progress in language models. Experiments in this paper highlight the versatility of language-model crossover, through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code. The conclusion is that language model crossover is a promising method for evolving genomes representable as text.																																	2024-05-30	PPRN:44120904		
J	Sun, Yutao; Dong, Li; Zhu, Yi; Huang, Shaohan; Wang, Wenhui; Ma, Shuming; Zhang, Quanlu; Wang, Jianyong; Wei, Furu				yuan, Yuan/ISA-0923-2023; Huang, Shaohan/LDF-3300-2024; Wang, Wenhui/AAH-9204-2019; Yuan, Ningyi/HDO-3272-2022						You Only Cache Once: Decoder-Decoder Architectures for Language Models								Arxiv											1	1;2024-05-09;https://www.arxiv.org/abs/2405.05254v2	arXiv:2405.05254			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 09 2024	2024	We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes. Code is available at https://aka.ms/YOCO.																																	2024-05-27	PPRN:88822761		
J	Sierant, Piotr; Lewenstein, Maciej; Scardicchio, Antonello; Vidmar, Lev; Zakrzewski, Jakub				Lewenstein, Maciej/I-1337-2014; Zakrzewski, Jakub/B-4487-2011; Vidmar, Lev/J-2464-2014; Sierant, Piotr/AAA-9930-2020						Many-Body Localization in the Age of Classical Computing								Arxiv											2	2;2024-04-26;https://www.arxiv.org/abs/2403.07111v2| 1;2024-03-11;https://www.arxiv.org/abs/2403.07111v1	arXiv:2403.07111			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	Statistical mechanics provides a framework for describing the physics of large, complex many -body systems using only a few macroscopic parameters to determine the state of the system. For isolated quantum many -body systems, such a description is achieved via the eigenstate thermalization hypothesis (ETH), which links thermalization, ergodicity and quantum chaotic behavior. However, tendency towards thermalization is not observed at finite system sizes and evolution times in a robust many -body localization (MBL) regime found numerically and experimentally in the dynamics of interacting many -body systems at strong disorder. Although the phenomenology of the MBL regime is well -established, the central question remains unanswered: under what conditions does the MBL regime give rise to an MBL phase, , in which the thermalization does not occur even in the asymptotic limit of infinite system size and evolution time? This review focuses on recent numerical investigations aiming to clarify the status of the MBL phase, and it establishes the critical open questions about the dynamics of disordered many -body systems. The last decades of research have brought an unprecedented new variety of tools and indicators to study the breakdown of ergodicity, ranging from spectral and wave function measures, observable matrix elements and unitary quantum dynamics, to transport and quantum information measures. We give a comprehensive overview of these approaches and attempt for providing a unified understanding of their main features. We emphasize general trends towards ergodicity with increasing length and time scales, which exclude naive single -parameter scaling hypothesis, necessitate the use of more refined scaling procedures, and which prevent unambiguous extrapolations of numerical results to the asymptotic limit. Providing a concise description of numerical methods for studying ETH and MBL, we explore various approaches to tackle the question of the MBL phase. Persistent finite size drifts towards ergodicity consistently emerge in quantities derived from eigenvalues and eigenvectors of disordered many -body systems. The drifts are related to continuous inching towards ergodicity and non -vanishing transport observed in the dynamics of many -body systems, even at strong disorder. These phenomena impede understanding of microscopic processes at the ETHMBL crossover. Nevertheless, the abrupt slowdown of dynamics with increasing disorder strength provides premises suggesting the proximity of the MBL phase. This review concludes that the questions about thermalization and its failure in disordered many -body systems remain a captivating area open for further explorations.																																	2024-06-22	PPRN:88112878		
J	Cordova, Clay; Hsin, Po-Shen; Zhang, Carolyn										Anomalies of Non-Invertible Symmetries in (3+1)d								Arxiv											2	2;2024-04-22;https://www.arxiv.org/abs/2308.11706v2| 1;2023-08-22;https://www.arxiv.org/abs/2308.11706v1	arXiv:2308.11706			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 22 2024	2024	Anomalies of global symmetries are important tools for understanding the dynamics of quantum systems. We investigate anomalies of non-invertible symmetries in 3+1d using 4+1d bulk topological quantum field theories given by Abelian two-form gauge theories, with a 0-form permutation symmetry. Gauging the 0-form symmetry gives the 4+1d “inflow” symmetry topological field theory for the non-invertible symmetry. We find a two levels of anomalies: (1) the bulk may fail to have an appropriate set of loop excitations which can condense to trivialize the boundary dynamics, and (2) the “Frobenius-Schur indicator” of the non-invertible symmetry (generalizing the FrobeniusSchur indicator of 1+1d fusion categories) may be incompatible with trivial boundary dynamics. As a consequence we derive conditions for non-invertible symmetries in 3+1d to be compatible with symmetric gapped phases, and invertible gapped phases. Along the way, we see that the defects characterizing Z4 ordinary symmetry host worldvolume theories with time-reversal symmetry T obeying the algebra T2 = C or T2 = (−1)FC, with C a unitary charge conjugation symmetry. We classify the anomalies of this symmetry algebra in 2+1d and further use these ideas to construct 2+1d topological orders with non-invertible time-reversal symmetry that permutes anyons. As a concrete realization of our general discussion, we construct new lattice Hamiltonian models in 3+1d with non-invertible symmetry, and constrain their dynamics.																																	2024-05-01	PPRN:83002799		
J	Fang, Richard; Bindu, Rohan; Gupta, Akul; Kang, Daniel										LLM Agents can Autonomously Exploit One-day Vulnerabilities								Arxiv											2	2;2024-04-17;https://www.arxiv.org/abs/2404.08144v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.08144v1	arXiv:2404.08144			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 17 2024	2024	LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities. In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.																																	2024-04-27	PPRN:88550789		
J	Zhang, Quanjun; Zhang, Tongke; Zhai, Juan; Fang, Chunrong; Yu, Bowen; Sun, Weisong; Chen, Zhenyu				Sun, Weisong/AAU-9503-2020; Bowen, Yu/MFH-7462-2025; Fang, Chunrong/GPW-9872-2022; ZHANG, QUANJUN/LPP-9143-2024						A Critical Review of Large Language Model on Software Engineering: An Example from ChatGPT and Automated Program Repair								Arxiv											2	2;2024-04-17;https://www.arxiv.org/abs/2310.08879v2| 1;2023-10-13;https://www.arxiv.org/abs/2310.08879v1	arXiv:2310.08879			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 17 2024	2024	Large Language Models (LLMs) have been gaining increasing attention and demonstrated promising performance across a variety of Software Engineering (SE) tasks, such as Automated Program Repair (APR), code summarization, and code completion. For example, ChatGPT, the latest black-box LLM, has been investigated by numerous recent research studies and has shown impressive performance in various tasks. However, there exists a potential risk of data leakage since these LLMs are usually close-sourced with unknown specific training details, e.g., pre-training datasets. In this paper, we seek to review the bug-fixing capabilities of ChatGPT on a clean APR benchmark with different research objectives. We first introduce {benchmark}, a new benchmark with buggy and the corresponding fixed programs from competitive programming problems starting from 2023, after the training cutoff point of ChatGPT. The results on {benchmark} show that ChatGPT is able to fix 109 out of 151 buggy programs using the basic prompt within 35 independent rounds, outperforming state-of-the-art LLMs CodeT5 and PLBART by 27.5% and 62.4% prediction accuracy. We also investigate the impact of three types of prompts, i.e., problem description, error feedback, and bug localization, leading to additional 34 fixed bugs. Besides, we provide additional discussion from the interactive nature of ChatGPT to illustrate the capacity of a dialog-based repair workflow with 9 additional fixed bugs. Inspired by the findings, we further pinpoint various challenges and opportunities for advanced SE study equipped with such LLMs (e.g.,~ChatGPT) in the near future. More importantly, our work calls for more research on the reevaluation of the achievements obtained by existing black-box LLMs across various SE tasks, not limited to ChatGPT on APR.																																	2024-04-27	PPRN:85616581		
J	Zhao, Yilong; Lin, Chien-Yu; Zhu, Kan; Ye, Zihao; Chen, Lequn; Zheng, Size; Ceze, Luis; Krishnamurthy, Arvind; Chen, Tianqi; Kasikci, Baris				Chen, Tianqi/AAT-2978-2020; Ye, Zihao/HKM-8264-2023; Zheng, Size/HDO-7288-2022						Atom: Low-bit Quantization for Efficient and Accurate LLM Serving								Arxiv											3	3;2024-04-16;https://www.arxiv.org/abs/2310.19102v3| 2;2023-11-07;https://www.arxiv.org/abs/2310.19102v2| 1;2023-10-29;https://www.arxiv.org/abs/2310.19102v1	arXiv:2310.19102			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8 -bit weight -activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4 -bit integer operators, resulting in sub -optimal performance. To maximize LLMs’ serving throughput, we introduce Atom, a low -bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low -bit operators and considerably reduces memory consumption via low -bit quantization. It attains high accuracy by applying a novel mixed -precision and fine-grained quantization process. We evaluate Atom on 4 -bit weightactivation quantization in the serving context. Atom improves end -to -end throughput (token/s) by up to 7.73× compared to the FP16 and by 2.53× compared to INT8 quantization, while maintaining the same latency target.																																	2024-04-26	PPRN:85883330		
J	Chen, Minshuo; Mei, Song; Fan, Jianqing; Wang, Mengdi				Mei, Song/AFQ-2667-2022; Chen, Minshuo/NQE-7177-2025; Fan, Jianqing/B-2115-2008						An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization								Arxiv											1	1;2024-04-11;https://www.arxiv.org/abs/2404.07771v1	arXiv:2404.07771			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 11 2024	2024	Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.																																	2024-04-25	PPRN:88502908		
J	Chen, Runjin; Zhao, Tong; Jaiswal, Ajay; Shah, Neil; Wang, Zhangyang				Chen, Runjin/LZH-7447-2025; Zhao, Tong/GQB-5245-2022; Zhihua, Wang/AFO-5263-2022						LLaGA: Large Language and Graph Assistant								Arxiv											3	3;2024-04-11;https://www.arxiv.org/abs/2402.08170v3| 2;2024-02-17;https://www.arxiv.org/abs/2402.08170v2| 1;2024-02-13;https://www.arxiv.org/abs/2402.08170v1	arXiv:2402.08170			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 11 2024	2024	Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the Large Language and Graph Assistant (LLaGA), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios. 																																	2024-05-22	PPRN:87668350		
J	Wang, Tan; Li, Linjie; Lin, Kevin; Zhai, Yuanhao; Lin, Chung-Ching; Yang, Zhengyuan; Zhang, Hanwang; Liu, Zicheng; Wang, Lijuan				李, 李林洁/JAD-1884-2023; Yang, Zhengyuan/AGQ-1232-2022; Zhai, Yuanhao/ABG-7095-2020						DisCo: Disentangled Control for Realistic Human Dance Generation								Arxiv											2	2;2024-04-04;https://www.arxiv.org/abs/2307.00040v3| 1;2023-10-11;https://www.arxiv.org/abs/2307.00040v2	arXiv:2307.00040			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	Generative AI has made significant strides in computer vision, particularly in text-driven image/video synthesis (T2I/T2V). Despite the notable advancements, it remains challenging in human-centric content synthesis such as realistic dance generation. Current methodologies, primarily tailored for human motion transfer, encounter difficulties when confronted with real-world dance scenarios (e.g., social media dance), which require to generalize across a wide spectrum of poses and intricate human details. In this paper, we depart from the traditional paradigm of human motion transfer and emphasize two additional critical attributes for the synthesis of human dance content in social media contexts: (i) Generalizability: the model should be able to generalize beyond generic human viewpoints as well as unseen human subjects, backgrounds, and poses; (ii) Compositionality: it should allow for the seamless composition of seen/unseen subjects, backgrounds, and poses from different sources. To address these challenges, we introduce DISCO, which includes a novel model architecture with disentangled control to improve the compositionality of dance synthesis, and an effective human attribute pre-training for better generalizability to unseen humans. Extensive qualitative and quantitative results demonstrate that DisCc can generate high-quality human dance images and videos with diverse appearances and flexible motions. 																																	2024-04-20	PPRN:85540848		
J	Fan, Ruihua; Bao, Yimu; Altman, Ehud; Vishwanath, Ashvin				Vishwanath, Ashvin/AAO-6878-2020; Altman, Ehud/C-1514-2010						Diagnostics of mixed-state topological order and breakdown of quantum memory								Arxiv											1	1;2024-03-12;https://www.arxiv.org/abs/2301.05689v2	arXiv:2301.05689			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 12 2024	2024	Topological quantum memory can protect information against local errors up to finite error thresholds. Such thresholds are usually determined based on the success of decoding algorithms rather than the intrinsic properties of the mixed states describing corrupted memories. Here we provide an intrinsic characterization of the breakdown of topological quantum memory, which both gives a bound on the performance of decoding algorithms and provides examples of topologically distinct mixed states. We employ three information-theoretical quantities that can be regarded as generalizations of the diagnostics of ground-state topological order, and serve as a definition for topological order in error-corrupted mixed states. We consider the topological contribution to entanglement negativity and two other metrics based on quantum relative entropy and coherent information. In the concrete example of the 2D Toric code with local bit-flip and phase errors, we map three quantities to observables in 2D classical spin models and analytically show they all undergo a transition at the same error threshold. This threshold is an upper bound on that achieved in any decoding algorithm and is indeed saturated by that in the optimal decoding algorithm for the Toric code.																																	2024-04-08	PPRN:88120202		
J	Luo, Man; Xu, Xin; Liu, Yue; Pasupat, Panupong; Kazemi, Mehran										In-context Learning with Retrieved Demonstrations for Language Models: A Survey								Arxiv											4	4;2024-03-12;https://www.arxiv.org/abs/2401.11624v4| 3;2024-02-13;https://www.arxiv.org/abs/2401.11624v3| 2;2024-01-23;https://www.arxiv.org/abs/2401.11624v2| 1;2024-03-01;	arXiv:2401.11624			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 01 2024	2024	Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.																																	2025-11-07	PPRN:87291973		
J	Chen, Angelica; Scheurer, Jeremy; Korbak, Tomasz; Campos, Jon Ander; Chan, Jun Shern; Bowman, Samuel R.; Cho, Kyunghyun; Perez, Ethan				Campos, Juan/AFU-0131-2022						Improving Code Generation by Training with Natural Language Feedback								Arxiv											2	2;2024-02-22;https://www.arxiv.org/abs/2303.16749v2| 1;2023-03-28;https://www.arxiv.org/abs/2303.16749v1	arXiv:2303.16749			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	The potential for pre -trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human -written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample -efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof -of -concept on a neural program synthesis task. We use ILF to improve a CODEGEN-MONO 6.1B model’s pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from human -written natural language feedback is both more effective and sample -efficient than training exclusively on demonstrations for improving an LLM’s performance on code generation tasks.																																	2024-03-23	PPRN:56460925		
J	Li, Haoran; Dong, Qingxiu; Tang, Zhengyang; Wang, Chaojun; Zhang, Xingxing; Huang, Haoyang; Huang, Shaohan; Huang, Xiaolong; Huang, Zeqiang; Zhang, Dongdong; Gu, Yuxian; Cheng, Xin; Wang, Xun; Chen, Si-Qing; Dong, Li; Lu, Wei; Sui, Zhifang; Wang, Benyou; Lam, Wai; Wei, Furu				Li, Haoran/HHZ-3847-2022; Lu, Wei/AHA-5606-2022; Huang, HAOYANG/LBB-1093-2024; Huang, Shaohan/LDF-3300-2024; Wang, Benyou/Y-5146-2019; Lam, Wai/GNW-3026-2022						Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models								Arxiv											1	1;2024-02-20;https://www.arxiv.org/abs/2402.13064v1	arXiv:2402.13064			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 20 2024	2024	We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.																																	2024-03-20	PPRN:87778131		
J	Singh, Shivalika; Vargus, Freddie; D'souza, Daniel; Karlsson, Borje F.; Mahendiran, Abinaya; Ko, Wei-Yin; Shandilya, Herumb; Patel, Jay; Mataciunas, Deividas; O'Mahony, Laura; Zhang, Mike; Hettiarachchi, Ramith; Wilson, Joseph; Machado, Marina; Moura, Luisa Souza; Krzeminski, Dominik; Fadaei, Hakimeh; Ergun, Irem; Okoh, Ifeoma; Alaagib, Aisha; Mudannayake, Oshan; Alyafeai, Zaid; Chien, Vu Minh; Ruder, Sebastian; Guthikonda, Surya; Alghamdi, Emad A.; Gehrmann, Sebastian; Muennighoff, Niklas; Bartolo, Max; Kreutzer, Julia; Ustun, Ahmet; Fadaee, Marzieh; Hooker, Sara				da Costa, Marina/JVP-5404-2024; Karlsson, Börje/B-4046-2010						Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning								Arxiv											1	1;2024-02-09;https://www.arxiv.org/abs/2402.06619v1	arXiv:2402.06619			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 09 2024	2024	Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as a valuable framework for future research collaborations that aim to bridge gaps in resources.																																	2024-02-26	PPRN:87608625		
J	Qu, Huilin; Li, Congqiao; Qian, Sitian										Particle Transformer for Jet Tagging								Arxiv											2	2;2024-01-29;https://www.arxiv.org/abs/2202.03772v3| 1;2022-06-18;https://www.arxiv.org/abs/2202.03772v2	arXiv:2202.03772			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 29 2024	2024	Jet tagging is a critical yet challenging classification task in particle physics. While deep learning has transformed jet tagging and significantly improved performance, the lack of a large-scale public dataset impedes further enhancement. In this work, we present JetClass, a new comprehensive dataset for jet tagging. The JetClass dataset consists of 100 M jets, about two orders of magnitude larger than existing public datasets. A total of 10 types of jets are simulated, including several types unexplored for tagging so far. Based on the large dataset, we propose a new Transformer-based architecture for jet tagging, called Particle Transformer (ParT). By incorporating pairwise particle interactions in the attention mechanism, ParT achieves higher tagging performance than a plain Transformer and surpasses the previous state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models, once fine-tuned, also substantially enhance the performance on two widely adopted jet tagging benchmarks. 																																	2024-05-25	PPRN:12236411		
J	Rasp, Stephan; Hoyer, Stephan; Merose, Alexander; Langmore, Ian; Battaglia, Peter; Russel, Tyler; Sanchez-Gonzalez, Alvaro; Yang, Vivian; Carver, Rob; Agrawal, Shreya; Chantry, Matthew; Bouallegue, Zied Ben; Dueben, Peter; Bromberg, Carla; Sisk, Jared; Barrington, Luke; Bell, Aaron; Sha, Fei				Rasp, Stephan/AAE-5257-2020; Sha, Fei/HNS-5968-2023						WeatherBench 2: A benchmark for the next generation of data-driven global weather models								Arxiv											2	2;2024-01-26;https://www.arxiv.org/abs/2308.15560v2| 1;2023-08-29;https://www.arxiv.org/abs/2308.15560v1	arXiv:2308.15560			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 26 2024	2024	WeatherBench 2 is an update to the global, medium-range (1-14 day) weather forecasting benchmark proposed by Rasp et al. (2020), designed with the aim to accelerate progress in data-driven weather modeling. WeatherBench 2 consists of an open-source evaluation framework, publicly available training, ground truth and baseline data as well as a continuously updated website with the latest metrics and state-of-the-art models: https://sites.research.google/weatherbench. This paper describes the design principles of the evaluation framework and presents results for current state-of-the-art physical and data-driven weather models. The metrics are based on established practices for evaluating weather forecasts at leading operational weather centers. We define a set of headline scores to provide an overview of model performance. In addition, we also discuss caveats in the current evaluation setup and challenges for the future of data-driven weather forecasting.																																	2024-05-25	PPRN:84581727		
J	Yenamandra, Sriram; Ramachandran, Arun; Yadav, Karmesh; Wang, Austin; Khanna, Mukul; Gervet, Theophile; Yang, Tsung-Yen; Jain, Vidhi; Clegg, Alexander William; Turner, John; Kira, Zsolt; Savva, Manolis; Chang, Angel; Chaplot, Devendra Singh; Batra, Dhruv; Mottaghi, Roozbeh; Bisk, Yonatan; Paxton, Chris				Yang, Tsung-Yen/GLT-6237-2022						HomeRobot: Open-Vocabulary Mobile Manipulation								Arxiv											2	2;2024-01-10;https://www.arxiv.org/abs/2306.11565v2| 1;2023-06-20;https://www.arxiv.org/abs/2306.11565v1	arXiv:2306.11565			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 10 2024	2024	HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks.   Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. 																																	2024-05-25	PPRN:73435434		
J	Kirsch, Louis; Harrison, James; Sohl-Dickstein, Jascha; Metz, Luke										General-Purpose In-Context Learning by Meta-Learning Transformers								Arxiv											1	1;2024-01-09;https://www.arxiv.org/abs/2212.04458v2	arXiv:2212.04458			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 09 2024	2024	Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-optimization. We further show that the capabilities of meta-trained algorithms are bottlenecked by the accessible state size (memory) determining the next prediction, unlike standard models which are thought to be bottlenecked by parameter count. Finally, we propose practical interventions such as biasing the training distribution that improve the meta-training and meta-generalization of general-purpose in-context learning algorithms.																																	2024-01-25	PPRN:87078598		
J	Liu, Di; Chen, Meng; Lu, Baotong; Jiang, Huiqiang; Han, Zhenhua; Zhang, Qianxi; Chen, Qi; Zhang, Chengruidong; Ding, Bailu; Zhang, Kai; Chen, Chen; Yang, Fan; Yang, Yuqing; Qiu, Lili				Jiang, Huiqiang/KHX-2210-2024; Chen, Qi/MCX-7031-2025; Ding, Bailu/OKS-0022-2025; Yang, Yuqing/LSJ-1291-2024; Lu, Baotong/OEN-2277-2025						<italic>RetrievalAttention</italic>: Accelerating Long-Context LLM Inference via Vector Retrieval								Arxiv											3	3;2024-12-31;https://www.arxiv.org/abs/2409.10516v3| 2;2024-09-18;https://www.arxiv.org/abs/2409.10516v2| 1;2024-09-16;https://www.arxiv.org/abs/2409.10516v1	arXiv:2409.10516			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 31 2024	2024	Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.																																	2025-02-22	PPRN:92183290		
J	Xue, Zhenliang; Song, Yixin; Mi, Zeyu; Zheng, Xinrui; Xia, Yubin; Chen, Haibo				Chen, Haibo/HCI-6124-2022; Mi, Zeyu/NBY-3261-2025; Song, Yixin/IWV-3440-2023						PowerInfer-2: Fast Large Language Model Inference on a Smartphone								Arxiv											3	3;2024-12-12;https://www.arxiv.org/abs/2406.06282v3| 2;2024-06-12;https://www.arxiv.org/abs/2406.06282v2| 1;2024-06-10;https://www.arxiv.org/abs/2406.06282v1	arXiv:2406.06282			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 12 2024	2024	Large language models (LLMs) on smartphones enable real-time AI assistance and privacy-preserving, offline operation. However, resource constraints of smartphones limit current deployments to small language models (SLMs), significantly compromising their capabilities. This paper introduces PowerInfer-2, a smartphone-based framework that enables fast inference for LLMs exceeding the memory capacity. The key insight is decomposing matrix operations into neuron clusters as the basic processing unit, which enables flexible scheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages this neuron-cluster-based design in both computation and storage. For computation, neuron clusters with dense activations are processed on NPU, while sparse clusters use CPU. The storage engine provides a fine-grained pipeline mechanism that coordinates cluster-level computation and I/O operations, enhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2 achieves up to a 27.8x speed increase compared to state-of-the-art frameworks. PowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving 11.68 tokens/s. Notably, these performance improvements preserve model quality with negligible accuracy degradation.																																	2025-01-20	PPRN:89268925		
J	Zhang, Yuxiang; Wu, Shangxi; Yang, Yuqi; Shu, Jiangming; Xiao, Jinlin; Kong, Chao; Sang, Jitao				Yang, YuQi/LIF-1806-2024; Shu, jiangming/NDT-3077-2025; Wu, Shangxi/HPE-9791-2023						o1-Coder: an o1 Replication for Coding								Arxiv											2	2;2024-12-10;https://www.arxiv.org/abs/2412.00154v2| 1;2024-11-29;https://www.arxiv.org/abs/2412.00154v1	arXiv:2412.00154			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 10 2024	2024	The technical report introduces O1-CODER, an attempt to replicate OpenAI’s o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model’s System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode and then generate the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for world model construction. Updated model progress and experimental results will be reported in subsequent versions. 																																	2025-01-19	PPRN:119646338		
J	Zheng, Zibin; Ning, Kaiwen; Zhong, Qingyuan; Chen, Jiachi; Chen, Wenqing; Guo, Lianghong; Wang, Weicheng; Wang, Yanlin				Zhong, Qing-Yuan/AAT-8719-2021; Chen, Jiachi/HOC-4256-2023; Wang, Weicheng/JZD-2432-2024						Towards an Understanding of Large Language Models in Software Engineering Tasks								Arxiv											3	3;2024-12-10;https://www.arxiv.org/abs/2308.11396v3| 2;2024-09-29;https://www.arxiv.org/abs/2308.11396v2| 1;2023-08-22;https://www.arxiv.org/abs/2308.11396v1	arXiv:2308.11396			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 10 2024	2024	Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.																																	2025-01-19	PPRN:82499040		
J	Li, Xinyu; Zhou, Ruiyang; Lipton, Zachary C.; Leqi, Liu				Zhou, Ruiyang/JFJ-6445-2023						Personalized Language Modeling from Personalized Human Feedback								Arxiv											3	3;2024-12-09;https://www.arxiv.org/abs/2402.05133v3| 2;2024-02-06;https://www.arxiv.org/abs/2402.05133v1| 1;2024-07-01;	arXiv:2402.05133			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 09 2024	2024	Personalized large language models (LLMs) are designed to tailor responses to individual user preferences. While Reinforcement Learning from Human Feedback (RLHF) is a commonly used framework for aligning LLMs with human preferences, vanilla RLHF assumes that all human preferences share the same distribution, preventing fine-tuned LLMs from generating personalized content when user preferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF), an efficient framework that utilizes a lightweight user model to capture individual user preferences and jointly learns the user model and the personalized LLM from human feedback. P-RLHF exhibits the following three characteristics: (1) It enables an LLM to generate personalized content and scale efficiently with growing number of users. (2) It handles both explicit user preferences described as textual input and implicit user preferences encoded in the feedback data. (3) It eliminates the need for users to fully articulate their preferences, which are normally needed for prompting LLMs to generate personalized content yet are often impractical to obtain in real-world scenarios. Our experimental results show that personalized LLMs trained using P-RLHF generate responses that are more closely aligned with individual user preferences, outperforming vanilla, non-personalized RLHF and prompting-based personalization approaches across different tasks. 																																	2025-01-17	PPRN:87572507		
J	Huang, Tiansheng; Hu, Sihao; Ilhan, Fatih; Tekin, Selim Furkan; Liu, Ling				İlhan, Fatih/AAC-4591-2021						Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey								Arxiv											4	4;2024-12-03;https://www.arxiv.org/abs/2409.18169v5| 3;2024-10-29;https://www.arxiv.org/abs/2409.18169v4| 2;2024-10-21;https://www.arxiv.org/abs/2409.18169v3| 1;2024-09-30;https://www.arxiv.org/abs/2409.18169v2	arXiv:2409.18169			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 03 2024	2024	Recent research demonstrates that the nascent fine-tuning-as-a-service business model exposes serious safety concerns -- fine-tuning over a few harmful data uploaded by the users can compromise the safety alignment of the model. The attack, known as harmful fine-tuning attack, has raised a broad research interest among the community. However, as the attack is still new, textbf{we observe that there are general misunderstandings within the research community.} To clear up concern, this paper provide a comprehensive overview to three aspects of harmful fine-tuning: attacks setting, defense design and evaluation methodology. Specifically, we first present the threat model of the problem, and introduce the harmful fine-tuning attack and its variants. Then we systematically survey the existing literature on attacks/defenses/mechanical analysis of the problem. Finally, we introduce the evaluation methodology and outline future research directions that might contribute to the development of the field. Additionally, we present a list of questions of interest, which might be useful to refer to when reviewers in the peer review process question the realism of the experiment/attack/defense setting.																																	2025-01-15	PPRN:100734278		
J	Welleck, Sean; Bertsch, Amanda; Finlayson, Matthew; Schoelkopf, Hailey; Xie, Alex; Neubig, Graham; Kulikov, Ilia; Harchaoui, Zaid										From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models								Arxiv											2	2;2024-11-20;https://www.arxiv.org/abs/2406.16838v2| 1;2024-06-24;https://www.arxiv.org/abs/2406.16838v1	arXiv:2406.16838			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 20 2024	2024	One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.																																	2024-12-31	PPRN:89415226		
J	Bai, Ge; Liu, Jie; Bu, Xingyuan; He, Yancheng; Liu, Jiaheng; Zhou, Zhanhui; Lin, Zhuoran; Su, Wenbo; Ge, Tiezheng; Zheng, Bo; Ouyang, Wanli				Bu, Xingyuan/HKE-2520-2023; Zheng, Bo/JDW-6453-2023						MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues								Arxiv											2	2;2024-11-05;https://www.arxiv.org/abs/2402.14762v3| 1;2024-06-25;https://www.arxiv.org/abs/2402.14762v2	arXiv:2402.14762			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 05 2024	2024	The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. 																																	2024-12-10	PPRN:89498205		
J	Yu, Qihang; He, Ju; Deng, Xueqing; Shen, Xiaohui; Chen, Liang-Chieh				Yu, Qihang/NXC-6226-2025						Randomized Autoregressive Visual Generation								Arxiv											1	1;2024-11-01;https://www.arxiv.org/abs/2411.00776v1	arXiv:2411.00776			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. 																																	2024-12-09	PPRN:119030853		
J	Koeksal, Abdullatif; Schick, Timo; Korhonen, Anna; Schuetze, Hinrich										LongForm: Effective Instruction Tuning with Reverse Instructions								Arxiv											3	3;2024-10-03;https://www.arxiv.org/abs/2304.08460v3| 2;2024-02-14;https://www.arxiv.org/abs/2304.08460v2| 1;2023-04-17;https://www.arxiv.org/abs/2304.08460v1	arXiv:2304.08460			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover, LongForm models outperform prior instruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and improve language understanding capabilities further. 																																	2024-10-21	PPRN:63649653		
J	Panagopoulou, Artemis; Xue, Le; Yu, Ning; Li, Junnan; Li, Dongxu; Joty, Shafiq; Xu, Ran; Savarese, Silvio; Xiong, Caiming; Niebles, Juan Carlos				Li, Dongxu/AHA-2144-2022; Niebles, Juan/AAT-5882-2021; LE, XUE/IYT-3045-2023						X-Instruct BLIP: A Framework for Aligning Image,3D, Audio, Video to LLMs and its Emergent Cross-modal Reasoning<br>								Arxiv											2	2;2024-09-09;https://www.arxiv.org/abs/2311.18799v2| 1;2023-11-30;https://www.arxiv.org/abs/2311.18799v1	arXiv:2311.18799			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Sep 09 2024	2024	Recent research has achieved significant advancements in visual reasoning tasks through learning image-to-language projections and leveraging the impressive reasoning abilities of Large Language Models (LLMs). This paper introduces an efficient and effective framework that integrates multiple modalities (images, 3D, audio and video) to a frozen LLM and demonstrates an emergent ability for cross-modal reasoning (2+ modality inputs). Our approach explores two distinct projection mechanisms: Q-Formers and Linear Projections (LPs). Through extensive experimentation across all four modalities on 16 benchmarks, we explore both methods and assess their adaptability in integrated and separate cross-modal reasoning. The Q-Former projection demonstrates superior performance in single modality scenarios and adaptability in joint versus discriminative reasoning involving two or more modalities. However, it exhibits lower generalization capabilities than linear projection in contexts where task-modality data are limited. To enable this framework, we devise a scalable pipeline that automatically generates high-quality, instruction-tuning datasets from readily available captioning data across different modalities, and contribute 24K QA data for audio and 250K QA data for 3D. To facilitate further research in cross-modal reasoning, we introduce the DisCRn (Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA samples and 28K image-3D QA samples that require the model to reason discriminatively across disparate input modalities.																																	2024-09-24	PPRN:86341345		
J	Nagarajan, Vaishnavh; Andreassen, Anders; Neyshabur, Behnam										Understanding the Failure Modes of Out-of-Distribution Generalization								Arxiv											2	2;2024-09-06;https://www.arxiv.org/abs/2010.15775v3| 1;2020-10-29;https://www.arxiv.org/abs/2010.15775v2	arXiv:2010.15775			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 06 2024	2024	Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.																																	2024-09-26	PPRN:11722289		
J	Wei, Haoran; Liu, Chenglong; Chen, Jinyue; Wang, Jia; Kong, Lingyu; Xu, Yanming; Ge, Zheng; Zhao, Liang; Sun, Jianjian; Peng, Yuang; Han, Chunrui; Zhang, Xiangyu				Chen, Jinyue/LIH-3814-2024						General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model								Arxiv											1	1;2024-09-03;https://www.arxiv.org/abs/2409.01704v1	arXiv:2409.01704			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Sep 03 2024	2024	Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as "characters" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above "characters" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.																																	2024-09-12	PPRN:91720667		
J	Gupte, Nihar; Ramos-Buades, Antoni; Buonanno, Alessandra; Gair, Jonathan; Miller, M. Coleman; Dax, Maximilian; Green, Stephen R.; Puerrer, Michael; Wildberger, Jonas; Macke, Jakob; Romero-Shaw, Isobel M.; Schoelkopf, Bernhard				Schölkopf, Bernhard/A-7570-2013; Buades, Antoni/R-4149-2019; Green, Stephen/AAX-8116-2020; Macke, Jakob/IYJ-3747-2023						Evidence for eccentricity in the population of binary black holes observed by LIGO-Virgo-KAGRA								Arxiv											2	2;2024-08-27;https://www.arxiv.org/abs/2404.14286v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.14286v1	arXiv:2404.14286			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 27 2024	2024	Binary black holes (BBHs) in eccentric orbits produce distinct modulations in the emitted gravitational waves (GWs). The measurement of orbital eccentricity can provide robust evidence for dynamical binary formation channels. We analyze 57 GW events from the first, second and third observing runs of the LIGO-VirgoKAGRA (LVK) Collaboration using a multipolar aligned-spin inspiral-merger-ringdown waveform model with two eccentric parameters: eccentricity and relativistic anomaly. This is made computationally feasible with the machine-learning code DINGO, , which accelerates inference by 2-3 orders of magnitude compared to traditional inference techniques. First, when using a uniform prior on the eccentricity, we find eccentric aligned-spin against quasi-circular aligned-spin log10 10 Bayes factors of 1.84 to 4.75 (depending on the glitch mitigation) for GW200129, 3.0 for GW190701 and 1.77 for GW200208 22. We measure e gw, 10Hzto to be 0 . 27 − 0 . 12 + 0 . 10 to 0.17+0.14 . 17− 0 . 13  + 0 . 14for GW200129, 0 . 35− 0 . 11 + 0 . 32 for GW190701 and 0 . 35− 0 . 21 + 0 . 18  for GW200208 22. Second, we find log10 10 Bayes factors between the eccentric aligned-spin versus quasi-circular precessing-spin hypothesis between 1.43 and 4.92 for GW200129, 2.61 for GW190701 and 1.23 for GW200208 22. Third, our analysis does not show evidence for eccentricity in GW190521, which has an eccentric aligned-spin against quasi-circular aligned-spin log10 10 Bayes factor of 0.04. Fourth, we estimate that if we neglect the spin-precession and use an astrophysically-motivated prior on the rate of eccentric BBHs, the probability of one out of the 57 events being eccentric is greater than 99.5% or (100 − 8.4 . 4 × 10−4)%  (depending on the glitch mitigation). Fifth, we study the impact on parameter estimation when neglecting either eccentricity in quasi-circular models or higher modes in eccentric models for GW events. These results underscore the importance of including eccentric parameters in the characterization of BBHs for the upcoming observing runs of the LVK Collaboration and for future detectors on the ground and in space, which will probe a more diverse BBH population.																																	2024-09-13	PPRN:88607226		
J	Yang, Zhenjie; Jia, Xiaosong; Li, Hongyang; Yan, Junchi				Li, Hongyang/ABD-7455-2020; Jia, Xiaosong/IWE-2802-2023; Yan, Junchi/ADK-0658-2022						LLM4Drive: A Survey of Large Language Models for Autonomous Driving								Arxiv											3	3;2024-08-12;https://www.arxiv.org/abs/2311.01043v4| 2;2023-11-27;https://www.arxiv.org/abs/2311.01043v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01043v1	arXiv:2311.01043			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 12 2024	2024	Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their "black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about textit{Large Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. 																																	2024-08-22	PPRN:85976267		
J	Liu, Yuan; Singh, Shraddha; Smith, Kevin C.; Crane, Eleanor; Martyn, John M.; Eickbusch, Alec; Schuckert, Alexander; Li, Richard D.; Sinanan-Singh, Jasmine; Soley, Micheline B.; Tsunoda, Takahiro; Chuang, Isaac L.; Wiebe, Nathan; Girvin, Steven M.				Girvin, Steven/C-1471-2012; Liu, Yuan/AAM-8501-2020						Hybrid Oscillator-Qubit Quantum Processors: Instruction Set Architectures, Abstract Machine Models, and Applications								Arxiv											2	2;2024-08-05;https://www.arxiv.org/abs/2407.10381v2| 1;2024-07-15;https://www.arxiv.org/abs/2407.10381v1	arXiv:2407.10381			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 05 2024	2024	Quantum computing with discrete variable (DV, qubit) hardware is approaching the large scales necessary for computations beyond the reach of classical computers. However, important use cases such as quantum simulations of physical models containing bosonic modes, and quantum error correction are challenging for DV-only systems. Separately, hardware containing native continuous-variable (CV, oscillator) systems has received attention as an alternative approach, yet the universal control of such systems is non-trivial. In this work, we show that hybrid CV-DV hardware offers a great advantage in meeting these challenges, offering a powerful computational paradigm that inherits the strengths of both DV and CV processors. We provide a pedagogical introduction to CV-DV systems and the multiple abstraction layers needed to produce a full software stack connecting applications to hardware. We present a variety of new hybrid CV-DV compilation techniques, algorithms, and applications, including the extension of quantum signal processing concepts to CV-DV systems and strategies to simulate systems of interacting spins, fermions, and bosons. To facilitate the development of hybrid CV-DV processor systems, we introduce formal Abstract Machine Models and Instruction Set Architectures -- essential abstractions that enable developers to formulate applications, compile algorithms, and explore the potential of current and future hardware for realizing fault-tolerant circuits, modules, and processors. Hybrid CV-DV quantum computations are beginning to be performed in superconducting, trapped ion, and neutral atom platforms, and large-scale experiments are set to be demonstrated in the near future. We present a timely and comprehensive guide to this relatively unexplored yet promising approach to quantum computation and providing an architectural backbone to guide future development.																																	2024-08-14	PPRN:90820844		
J	Ye, Tian; Xu, Zicheng; Li, Yuanzhi; Allen-Zhu, Zeyuan				Ye, Tianyong/MSX-9881-2025						Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process								Arxiv											1	1;2024-07-29;https://www.arxiv.org/abs/2407.20311v1	arXiv:2407.20311			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 29 2024	2024	Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model’s hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions? Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.																																	2024-08-06	PPRN:91156564		
J	Liu, Daizong; Yang, Mingyu; Qu, Xiaoye; Zhou, Pan; Cheng, Yu; Hu, Wei				Yang, Mingyu/JXM-0926-2024						A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends								Arxiv											1	1;2024-07-12;https://www.arxiv.org/abs/2407.07403v2	arXiv:2407.07403			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	With the significant development of large models in recent years, Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding and reasoning tasks. Compared to traditional Large Language Models (LLMs), LVLMs present great potential and challenges due to its closer proximity to the multi-resource real-world applications and the complexity of multi-modal processing. However, the vulnerability of LVLMs is relatively underexplored, posing potential security risks in daily usage. In this paper, we provide a comprehensive review of the various forms of existing LVLM attacks. Specifically, we first introduce the background of attacks targeting LVLMs, including the attack preliminary, attack challenges, and attack resources. Then, we systematically review the development of LVLM attack methods, such as adversarial attacks that manipulate model outputs, jailbreak attacks that exploit model vulnerabilities for unauthorized actions, prompt injection attacks that engineer the prompt type and pattern, and data poisoning that affects model training. Finally, we discuss promising research directions in the future. We believe that our survey provides insights into the current landscape of LVLM vulnerabilities, inspiring more researchers to explore and mitigate potential safety issues in LVLM developments. 																																	2024-07-23	PPRN:90793876		
J	Chai, Yuxiang; Huang, Siyuan; Niu, Yazhe; Xiao, Han; Liu, Liang; Zhang, Dingyu; Gao, Peng; Ren, Shuai; Li, Hongsheng				Li, Hongsheng/AES-5328-2022						AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents								Arxiv											1	1;2024-07-03;https://www.arxiv.org/abs/2407.17490v1	arXiv:2407.17490			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 03 2024	2024	AI agents have drawn increasing attention mostly on their ability to perceive environments, understand tasks, and autonomously achieve goals. To advance research on AI agents in mobile scenarios, we introduce the Android Multi-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for generalist mobile GUI-control agents. Their capabilities of completing complex tasks by directly interacting with the graphical user interface (GUI) on mobile devices are trained and evaluated with the proposed dataset. AMEX comprises over 104K high-resolution screenshots from 110 popular mobile applications, which are annotated at multiple levels. Unlike existing mobile device-control datasets, e.g., MoTIF, AitW, etc., AMEX includes three levels of annotations: GUI interactive element grounding, GUI screen and element functionality descriptions, and complex natural language instructions, each averaging 13 steps with stepwise GUI-action chains. We develop this dataset from a more instructive and detailed perspective, complementing the general settings of existing datasets. Additionally, we develop a baseline model SPHINX Agent and compare its performance across state-of-the-art agents trained on other datasets. To facilitate further research, we open-source our dataset, models, and relevant evaluation tools. 																																	2024-08-21	PPRN:91340912		
J	Kim, Bo-Kyeong; Kim, Geonmin; Kim, Tae-Ho; Castells, Thibault; Choi, Shinkook; Shin, Junho; Song, Hyoung-Kyu				Kim, Bo/ABD-2089-2020						Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods								Arxiv											2	2;2024-06-23;https://www.arxiv.org/abs/2402.02834v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.02834v1	arXiv:2402.02834			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 23 2024	2024	Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that simple depth pruning can effectively compress LLMs while achieving comparable or superior performance to recent width pruning studies. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. In retraining pruned models for quality recovery, continued pretraining on a large corpus markedly outperforms LoRA-based tuning, particularly at severe pruning ratios. We hope this work can help build compact yet capable LLMs.																																	2024-07-12	PPRN:87523304		
J	Kossen, Jannik; Han, Jiatong; Razzak, Muhammed; Schut, Lisa; Malik, Shreshth; Gal, Yarin				Han, Jiatong/IQS-3236-2023; Razzak, Muhammed/MGU-9383-2025						Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs								Arxiv											1	1;2024-06-22;https://www.arxiv.org/abs/2406.15927v1	arXiv:2406.15927			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 22 2024	2024	We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. [21] proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.																																	2024-07-15	PPRN:89417002		
J	Li, Zhenxin; Li, Kailin; Wang, Shihao; Lan, Shiyi; Yu, Zhiding; Ji, Yishen; Li, Zhiqi; Zhu, Ziyue; Kautz, Jan; Wu, Zuxuan; Jiang, Yu-Gang; Alvarez, Jose M.				WANG, SHIHAO/KHC-8263-2024						Hydra-MDP: End-to-end Multimodal Planning with Multi-target Hydra-Distillation								Arxiv											1	1;2024-06-19;https://www.arxiv.org/abs/2406.06978v3	arXiv:2406.06978			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 19 2024	2024	We propose Hydra-MDP, a novel paradigm employing multiple teachers in a teacher-student model. This approach uses knowledge distillation from both human and rule-based teachers to train the student model, which features a multi-head decoder to learn diverse trajectory candidates tailored to various evaluation metrics. With the knowledge of rule-based teachers, Hydra-MDP learns how the environment influences the planning in an end-to-end manner instead of resorting to non-differentiable post-processing. This method achieves the $1^{st}$ place in the Navsim challenge, demonstrating significant improvements in generalization across diverse driving environments and conditions. Code will be available at url{https://github.com/NVlabs/Hydra-MDP}.																																	2025-08-07	PPRN:123165165		
J	Yuan, Wentao; Duan, Jiafei; Blukis, Valts; Pumacay, Wilbert; Krishna, Ranjay; Murali, Adithyavairavan; Mousavian, Arsalan; Fox, Dieter				Blukis, Valts/HGA-0955-2022						RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics								Arxiv											1	1;2024-06-15;https://www.arxiv.org/abs/2406.10721v1	arXiv:2406.10721			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 15 2024	2024	From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train R OB O P OINT , a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real -world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, R OBO P OINT is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that R OB O P OINT outperforms state -of -the -art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Project website: robo-point.github.io.																																	2024-07-04	PPRN:89346978		
J	Chen, Pengtao; Shen, Mingzhu; Ye, Peng; Cao, Jianjian; Tu, Chongjun; Bouganis, Christos-Savvas; Zhao, Yiren; Chen, Tao				Bouganis, Christos/AAL-8145-2020; Chen, Pengtao/HGB-8074-2022						Δ-DiT: A Training-Free Acceleration Method Tailored for Diffusion Transformers								Arxiv											1	1;2024-06-03;https://www.arxiv.org/abs/2406.01125v1	arXiv:2406.01125			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	Diffusion models are widely recognized for generating high-quality and diverse images, but their poor real-time performance has led to numerous acceleration works, primarily focusing on UNet-based structures. With the more successful results achieved by diffusion transformers (DiT), there is still a lack of exploration regarding the impact of DiT structure on generation, as well as the absence of an acceleration framework tailored to the DiT architecture. To tackle these challenges, we conduct an investigation into the correlation between DiT blocks and image generation. Our findings reveal that the front blocks of DiT are associated with the outline of the generated images, while the rear blocks are linked to the details. Based on this insight, we propose an overall training-free inference acceleration framework Δ-DiT: using a designed cache mechanism to accelerate the rear DiT blocks in the early sampling stages and the front DiT blocks in the later stages. Specifically, a DiT-specific cache mechanism called Δ-Cache is proposed, which considers the inputs of the previous sampling image and reduces the bias in the inference. Extensive experiments on PIXART-α and DiT-XL demonstrate that the Δ-DiT can achieve a 1.6× speedup on the 20-step generation and even improves performance in most cases. In the scenario of 4-step consistent model generation and the more challenging 1.12× acceleration, our method significantly outperforms existing methods. Our code will be publicly available.																																	2024-06-22	PPRN:89163629		
J	Ku, Max; Jiang, Dongfu; Wei, Cong; Yue, Xiang; Chen, Wenhu				Jiang, Dongfu/JTV-4943-2023; Yue, Xiang/AAG-6582-2019						VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation								Arxiv											2	2;2024-06-03;https://www.arxiv.org/abs/2312.14867v2| 1;2023-12-22;https://www.arxiv.org/abs/2312.14867v1	arXiv:2312.14867			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIESCORE, CORE , a Visual Instruction -guided Explainable metric for evaluating any conditional image generation tasks. VIESCORE CORE leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIESCORE CORE on seven prominent tasks in conditional image tasks and found: (1) VIESCORE CORE (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human -to -human correlation is 0.45. (2) VIES CORE (with open -source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) VIESCORE CORE achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIESCORE CORE shows its great potential to replace human judges in evaluating image synthesis tasks.																																	2024-06-19	PPRN:86784292		
J	Quach, Victor; Fisch, Adam; Schuster, Tal; Yala, Adam; Jae Ho, Sohn; Jaakkola, Tommi S.; Barzilay, Regina										Conformal Language Modeling								Arxiv											2	2;2024-06-01;https://www.arxiv.org/abs/2306.10193v2| 1;2023-06-16;https://www.arxiv.org/abs/2306.10193v1	arXiv:2306.10193			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 01 2024	2024	We propose a novel approach to conformal prediction for generative language models (LMs). Standard conformal prediction produces prediction sets - in place of single predictions - that have rigorous, statistical performance guarantees. LM responses are typically sampled from the model's predicted distribution over the large, combinatorial output space of natural language. Translating this process to conformal prediction, we calibrate a stopping rule for sampling different outputs from the LM that get added to a growing set of candidates until we are confident that the output set is sufficient. Since some samples may be low-quality, we also simultaneously calibrate and apply a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we prove that the sampled set returned by our procedure contains at least one acceptable answer with high probability, while still being empirically precise (i.e., small) on average. Furthermore, within this set of candidate responses, we show that we can also accurately identify subsets of individual components - such as phrases or sentences - that are each independently correct (e.g., that are not "hallucinations"), again with statistical guarantees. We demonstrate the promise of our approach on multiple tasks in open-domain question answering, text summarization, and radiology report generation using different LM variants.																																	2024-06-22	PPRN:73435473		
J	Xu, Jiaqi; Zou, Xinyi; Huang, Kunzhe; Chen, Yunkuo; Liu, Bo; Cheng, Mengli; Shi, Xing; Huang, Jun										EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture								Arxiv											2	2;2024-07-05;https://www.arxiv.org/abs/2405.18991v2| 1;2024-05-29;https://www.arxiv.org/abs/2405.18991v1	arXiv:2405.18991			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	This paper presents EasyAnimate, an advanced method for video generation that leverages the power of transformer architecture for high-performance outcomes. We have expanded the DiT framework originally designed for 2D image synthesis to accommodate the complexities of 3D video generation by incorporating a motion module block. It is used to capture temporal dynamics, thereby ensuring the production of consistent frames and seamless motion transitions. The motion module can be adapted to various DiT baseline methods to generate video with different styles. It can also generate videos with different frame rates and resolutions during both training and inference phases, suitable for both images and videos. Moreover, we introduce slice VAE, a novel approach to condense the temporal axis, facilitating the generation of long duration videos. Currently, EasyAnimate exhibits the proficiency to generate videos with 144 frames. We provide a holistic ecosystem for video production based on DiT, encompassing aspects such as data pre-processing, VAE training, DiT models training (both the baseline model and LoRA model), and end-to-end video inference. 																																	2024-08-25	PPRN:90727158		
J	Chen, Yinghao; Hu, Zehao; Zhi, Chen; Han, Junxiao; Deng, Shuiguang; Yin, Jianwei				Zhi, Chen/JRX-3166-2023; Dustdar, Schahram/G-9877-2015						ChatUniTest: A Framework for LLM-Based Test Generation								Arxiv											2	2;2024-05-07;https://www.arxiv.org/abs/2305.04764v2| 1;2023-05-08;https://www.arxiv.org/abs/2305.04764v1	arXiv:2305.04764			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 07 2024	2024	Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain.																																	2024-05-25	PPRN:68514593		
J	Rando, Javier; Tramer, Florian										Universal Jailbreak Backdoors from Poisoned Human Feedback								Arxiv											4	4;2024-04-29;https://www.arxiv.org/abs/2311.14455v4| 3;2024-02-12;https://www.arxiv.org/abs/2311.14455v3| 2;2024-02-07;https://www.arxiv.org/abs/2311.14455v2| 1;2023-11-24;https://www.arxiv.org/abs/2311.14455v1	arXiv:2311.14455			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 29 2024	2024	Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.																																	2024-06-04	PPRN:86279636		
J	Carloni, Youri; Luongo, Orlando; Muccino, Marco				Luongo, Orlando/J-5328-2016; Muccino, Marco/JZD-9643-2024						Does dark energy really revive using DESI 2024 data?								Arxiv											1	1;2024-04-18;https://www.arxiv.org/abs/2404.12068v1	arXiv:2404.12068			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 18 2024	2024	We investigate the impact of the Dark Energy Spectroscopic Instrument (DESI) 2024 data on dark energy scenarios. We thus analyze three typologies of models, the first in which the cosmic speed up is related to thermodynamics, the second associated with Taylor expansions of the barotropic factor, whereas the third based on ad hoc dark energy parameterizations. In this respect, we perform Monte Carlo Markov chain analyses, adopting the Metropolis -Hastings algorithm, of 12 models. To do so, we first work at the background, inferring a posteriori kinematic quantities associated with each model. Afterwards, we obtain early time predictions, computing departures on the growth evolution with respect to the model that better fits DESI data. We find that the best model to fit data is not the Chevallier-Polarski-Linder (CPL) parametrization, but rather a more complicated log -corrected dark energy contribution. To check the goodness of our findings, we further directly fit the product, rdh0, concluding that rdh0 is anticorrelated with the mass. This treatment is worked out by removing a precise data point placed at z = 0.51. Surprisingly, in this case the results again align with the ΛCDM model, indicating that the possible tension between the concordance paradigm and the CPL model can be severely alleviated. We conclude that future data points will be essential to clarify whether dynamical dark energy is really in tension with the ΛCDM model.																																	2024-04-28	PPRN:88565768		
J	Huang, Yizheng; Huang, Jimmy X.				Huang, Yizheng/IUN-2110-2023						A Survey on Retrieval-Augmented Text Generation for Large Language Models								Arxiv											1	1;2024-04-17;https://www.arxiv.org/abs/2404.10981v1	arXiv:2404.10981			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 17 2024	2024	Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.																																	2024-04-27	PPRN:88557792		
J	Huberman-Spiegelglas, Inbar; Kulikov, Vladimir; Michaeli, Tomer										An Edit Friendly DDPM Noise Space: Inversion and Manipulations								Arxiv											2	2;2024-04-09;https://www.arxiv.org/abs/2304.06140v3| 1;2023-04-12;https://www.arxiv.org/abs/2304.06140v1	arXiv:2304.06140			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 09 2024	2024	Denoising diffusion probabilistic models (DDPMs) employ a sequence of white Gaussian noise samples to generate an image. In analogy with GANs, those noise maps could be considered as the latent code associated with the generated image. However, this native noise space does not possess a convenient structure, and is thus challenging to work with in editing tasks. Here, we propose an alternative latent noise space for DDPM that enables a wide range of editing operations via simple means, and present an inversion method for extracting these edit-friendly noise maps for any given image (real or synthetically generated). As opposed to the native DDPM noise space, the edit-friendly noise maps do not have a standard normal distribution and are not statistically independent across timesteps. However, they allow perfect reconstruction of any desired image, and simple transformations on them translate into meaningful manipulations of the output image (e.g. shifting, color edits). Moreover, in text-conditional models, fixing those noise maps while changing the text prompt, modifies semantics while retaining structure. We illustrate how this property enables text-based editing of real images via the diverse DDPM sampling scheme (in contrast to the popular non-diverse DDIM inversion). We also show how it can be used within existing diffusion-based editing methods to improve their quality and diversity. 																																	2024-04-24	PPRN:59927925		
J	Zhuang, Honglei; Qin, Zhen; Hui, Kai; Wu, Junru; Yan, Le; Wang, Xuanhui; Bendersky, Michael				Yan, Le/G-1370-2019; Hui, Kai/AAM-2600-2020						Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels								Arxiv											3	3;2024-04-02;https://www.arxiv.org/abs/2310.14122v3| 2;2023-11-06;https://www.arxiv.org/abs/2310.14122v2| 1;2023-10-21;https://www.arxiv.org/abs/2310.14122v1	arXiv:2310.14122			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like "Yes" and "No". However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.																																	2024-04-18	PPRN:85756503		
J	Hines, Keegan; Lopez, Gary; Hall, Matthew; Zarfati, Federico; Zunger, Yonatan; Kiciman, Emre				Hall, Matthew/AAB-8228-2019						Defending Against Indirect Prompt Injection Attacks With Spotlighting								Arxiv											1	1;2024-03-20;https://www.arxiv.org/abs/2403.14720v1	arXiv:2403.14720			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 20 2024	2024	Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defense against indirect prompt injection attacks, and find that it is a robust defense that has minimal detrimental impact to underlying NLP tasks. Using GPT-family models, we find that spotlighting reduces the attack success rate from greater than {50}% to below {2}% in our experiments with minimal impact on task efficacy.																																	2024-04-13	PPRN:88268092		
J	Hu, Vincent Tao; Baumann, Stefan Andreas; Gui, Ming; Grebenkova, Olga; Ma, Pingchuan; Fischer, Johannes; Ommer, Bjorn				Ma, Pingchuan/AFR-0634-2022						ZigMa: Zigzag Mamba Diffusion Model								Arxiv											1	1;2024-03-20;https://www.arxiv.org/abs/2403.13802v1	arXiv:2403.13802			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 20 2024	2024	The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ 1024 × 1024 and UCF101, MultiModal-CelebA-HQ, and MS COCO 256 × 256. 																																	2024-04-12	PPRN:88249304		
J	Zhang, Mingxin; Pei, Cuiying; Du, Xian; Hu, Weixiong; Cao, Yantao; Wang, Qi; Wu, Juefei; Li, Yidian; Liu, Huanyu; Wen, Chenhaoping; Zhao, Yi; Li, Changhua; Cao, Weizheng; Zhu, Shihao; Zhang, Qing; Yu, Na; Cheng, Peihong; Zhang, Lili; Li, Zhiwei; Zhao, Jinkui; Chen, Yulin; Guo, Hanjie; Wu, Congjun; Yang, Fan; Yan, Shichao; Yang, Lexian; Qi, Yanpeng				Wang, Zhiwei/B-5981-2016; Yan, Shichao/D-2126-2012; Zhang, Xiaohong/AAG-4134-2019; Wang, Yao/L-8653-2018; Guo, Hanjie/AAB-9695-2019						Superconductivity in trilayer nickelate La4Ni3O10 under pressure								Arxiv											2	2;2024-03-12;https://www.arxiv.org/abs/2311.07423v2| 1;2023-11-13;https://www.arxiv.org/abs/2311.07423v1	arXiv:2311.07423			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 12 2024	2024	Nickelate superconductors have attracted a great deal of attention over the past few decades due to their similar crystal and electronic structures with high -temperature cuprate superconductors. Here, we report the superconductivity in a pressurized Ruddlesden-Popper phase single crystal, La4Ni3O10 (n = 3), and its interplay with the density wave order in the phase diagram. With increasing pressure, the density wave order as indicated by the anomaly in the resistivity is progressively suppressed, followed by the emergence of the superconductivity around 25 K. Our angleresolved photoemission spectroscopy measurements reveal that the electronic structure of La4Ni3O10 is very similar to that of La3Ni2O7, suggesting unified electronic properties of nickelates in Ruddlesden-Popper phases. Moreover, theoretical analysis unveils that antiferromagnetic (AFM) super -exchange interactions can serve as the effective pairing interaction for the emergence of superconductivity (SC) in pressurized La4Ni3O10. Our research provides a new platform for the investigation of the unconventional superconductivity mechanism in Ruddlesden–Popper trilayer perovskite nickelates.																																	2024-04-08	PPRN:86135966		
J	Kochkov, Dmitrii; Yuval, Janni; Langmore, Ian; Norgaard, Peter; Smith, Jamie; Mooers, Griffin; Kloewer, Milan; Lottes, James; Rasp, Stephan; Dueben, Peter; Hatfield, Sam; Battaglia, Peter; Sanchez-Gonzalez, Alvaro; Willson, Matthew; Brenner, Michael P.; Hoyer, Stephan				Yuval, Janni/KCK-2723-2024; Rasp, Stephan/AAE-5257-2020						Neural General Circulation Models for Weather and Climate								Arxiv											3	3;2024-03-08;https://www.arxiv.org/abs/2311.07222v3| 2;2023-11-28;https://www.arxiv.org/abs/2311.07222v2| 1;2023-11-13;https://www.arxiv.org/abs/2311.07222v1	arXiv:2311.07222			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 08 2024	2024	General circulation models (GCMs) are the foundation of weather and climate prediction. GCMs are physics-based simulators which combine a numerical solver for large-scale dynamics with tuned representations for small-scale processes such as cloud formation. Recently, machine learning (ML) models trained on reanalysis data achieved comparable or better skill than GCMs for deterministic weather forecasting. However, these models have not demonstrated improved ensemble forecasts, or shown sufficient stability for long-term weather and climate simulations. Here we present the first GCM that combines a differentiable solver for atmospheric dynamics with ML components, and show that it can generate forecasts of deterministic weather, ensemble weather and climate on par with the best ML and physics-based methods. NeuralGCM is competitive with ML models for 1-10 day forecasts, and with the European Centre for Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts. With prescribed sea surface temperature, NeuralGCM can accurately track climate metrics such as global mean temperature for multiple decades, and climate forecasts with 140 km resolution exhibit emergent phenomena such as realistic frequency and trajectories of tropical cyclones. For both weather and climate, our approach offers orders of magnitude computational savings over conventional GCMs. Our results show that end-to-end deep learning is compatible with tasks performed by conventional GCMs, and can enhance the large-scale physical simulations that are essential for understanding and predicting the Earth system.																																	2024-04-07	PPRN:86138190		
J	Wang, Haoxiang; Lin, Yong; Xiong, Wei; Yang, Rui; Diao, Shizhe; Qiu, Shuang; Zhao, Han; Zhang, Tong				Yang, Rui/LZG-6631-2025; Diao, Shizhe/JXY-7398-2024; 邱, 爽/IAN-9423-2023; Zhang, tong/IAP-2587-2023						Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards								Arxiv											1	1;2024-03-06;https://www.arxiv.org/abs/2402.18571v3	arXiv:2402.18571			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 06 2024	2024	Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).																																	2024-04-03	PPRN:88048533		
J	Chowdhury, Arijit Ghosh; Islam, Md Mofijul; Kumar, Vaibhav; Shezan, Faysal Hossain; Jain, Vinija; Chadha, Aman				Islam, Md Mofijul/ABE-1743-2021; Shezan, FaysalHossain/MEO-1849-2025; Chadha, Dr. Aman/GNM-9565-2022						Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models								Arxiv											1	1;2024-03-03;https://www.arxiv.org/abs/2403.04786v1	arXiv:2403.04786			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 03 2024	2024	Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human -like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insights into the current landscape of LLM vulnerabilities and defense mechanisms. Our objective is to offer a nuanced understanding of LLM attacks, foster awareness within the AI community, and inspire robust solutions to mitigate these risks in future developments.																																	2024-04-07	PPRN:88082031		
J	Xu, Jiacen; Stokes, Jack W.; Mcdonald, Geoff; Bai, Xuesong; Marshall, David; Wang, Siyue; Swaminathan, Adith; Li, Zhou				Bai, Xuesong/AAO-4028-2020; Xu, Jiacen/KHX-4416-2024						AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks								Arxiv											1	1;2024-03-02;https://www.arxiv.org/abs/2403.01038v1	arXiv:2403.01038			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 02 2024	2024	Large language models (LLMs) have demonstrated impressive results on natural language tasks, and security researchers are beginning to employ them in both offensive and defensive systems. In cyber-security, there have been multiple research efforts that utilize LLMs focusing on the pre-breach stage of attacks like phishing and malware generation. However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or "hands-on-keyboard" attacks, under various attack techniques and environments. As LLMs inevitably advance, they may be able to automate both the pre- and post-breach attack stages. This shift may transform organizational attacks from rare, expert-led events to frequent, automated operations requiring no expertise and executed at automation speed and scale. This risks fundamentally changing global computer security and correspondingly causing substantial economic impacts, and a goal of this work is to better understand these risks now so we can better prepare for these inevitable ever-more-capable LLMs on the horizon. On the immediate impact side, this research serves three purposes. First, an automated LLM-based, post-breach exploitation framework can help analysts quickly test and continually improve their organization's network security posture against previously unseen attacks. Second, an LLM-based penetration test system can extend the effectiveness of red teams with a limited number of human analysts. Finally, this research can help defensive systems and teams learn to detect novel attack behaviors preemptively before their use in the wild. To this end, we propose an LLM-guided system, AUTOAT- TACKER, to automate “hands-on-keyboard” attacks on a simulated organizational network with varied attack tasks, end-point configurations (Windows and Linux systems), and leverage Metasploit as the post-breach attack framework along with other capabilities like command-line/shell interaction. To best utilize the LLM’s capabilities to obtain precise attack commands, AUTOATTACKER contains a summarizer to summarize the previous interactions and the execution environment, a planner to establish the attack planning, and a navigator to select the optimal action. The executed tasks are stored in a Retrieval Augmented Generation (RAG) inspired experience manager to build the complex attacks from the basic or executed attack tasks. We carefully design prompt templates for these components to harness the responses from the LLM. We conduct extensive tests and show that while GPT-3.5, Llama2-7B-chat and Llama2-70B-chat do not work well for automated penetration testings, GPT-4 demonstrates remarkable capabilities in automatically conducting post-breach attacks requiring limited or no human involvement.																																	2024-03-30	PPRN:88021354		
J	Luo, Haoyan; Specia, Lucia										From Understanding to Utilization: A Survey on Explainability for Large Language Models								Arxiv											2	2;2024-02-22;https://www.arxiv.org/abs/2401.12874v2| 1;2024-01-23;https://www.arxiv.org/abs/2401.12874v1	arXiv:2401.12874			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 22 2024	2024	Explainability for Large Language Models (LLMs) is a critical yet challenging aspect of natural language processing. As LLMs are increasingly integral to diverse applications, their “black-box” nature sparks significant concerns regarding transparency and ethical use. This survey underscores the imperative for increased explainability in LLMs, delving into both the research on explainability and the various methodologies and tasks that utilize an understanding of these models. Our focus is primarily on pre-trained Transformer-based LLMs, such as LLaMA (Touvron et al., 2023), which pose distinctive interpretability challenges due to their scale and complexity. In terms of existing methods, we classify them into local and global analyses, based on their explanatory objectives. When considering the utilization of explainability, we explore several compelling methods that concentrate on model editing, control generation, and model enhancement. Additionally, we examine representative evaluation metrics and datasets, elucidating their advantages and limitations. Our goal is to reconcile theoretical and empirical understanding with practical implementation, proposing exciting avenues for explanatory techniques and their applications in the LLMs era.																																	2024-03-21	PPRN:87291773		
J	Bravyi, Sergey; Cross, Andrew W.; Gambetta, Jay M.; Maslov, Dmitri; Rall, Patrick; Yoder, Theodore J.				Gambetta, Jay/D-2794-2009						High-threshold and low-overhead fault-tolerant quantum memory								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2308.07915v2| 1;2023-08-15;https://www.arxiv.org/abs/2308.07915v1	arXiv:2308.07915			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	Quantum error correction becomes a practical possibility only if the physical error rate is below a threshold value that depends on a particular quantum code, syndrome measurement circuit, and decoding algorithm. Here we present an end -to -end quantum error correction protocol that implements fault -tolerant memory based on a family of LDPC codes with a high encoding rate that achieves an error threshold of 0.8% for the standard circuitbased noise model. This is on par with the surface code which has remained an uncontested leader in terms of its high error threshold for nearly 20 years. The full syndrome measurement cycle for a length -n code in our family requires n ancillary qubits and a depth -7 circuit composed of nearest -neighbor CNOT gates. The required qubit connectivity is a degree -6 graph that consists of two edge -disjoint planar subgraphs. As a concrete example, we show that 12 logical qubits can be preserved for nearly one million syndrome cycles using 288 physical qubits in total, assuming the physical error rate of 0.1%. We argue that achieving the same level of error suppression on 12 logical qubits with the surface code would require nearly 3000 physical qubits. Our findings bring demonstrations of a low -overhead fault -tolerant quantum memory within the reach of near -term quantum processors.																																	2024-03-20	PPRN:77857343		
J	Agarwal, Chirag; Tanneru, Sree Harsha; Lakkaraju, Himabindu										Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models								Arxiv											1	1;2024-02-08;https://www.arxiv.org/abs/2402.04614v2	arXiv:2402.04614			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 08 2024	2024	Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we urge the community to identify the faithfulness requirements of real-world applications and ensure explanations meet those needs. Finally, we propose some directions for future work, emphasizing the need for novel methodologies and frameworks that can enhance the faithfulness of self-explanations without compromising their plausibility, essential for the transparent deployment of LLMs in diverse high-stakes domains.																																	2024-05-25	PPRN:87568987		
J	Muhlgay, Dor; Ram, Ori; Magar, Inbal; Levine, Yoav; Ratner, Nir; Belinkov, Yonatan; Abend, Omri; Leyton-Brown, Kevin; Shashua, Amnon; Shoham, Yoav										Generating Benchmarks for Factuality Evaluation of Language Models								Arxiv											2	2;2024-02-04;https://www.arxiv.org/abs/2307.06908v2| 1;2023-07-13;https://www.arxiv.org/abs/2307.06908v1	arXiv:2307.06908			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 04 2024	2024	Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM’s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert -FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators. We make our data and code publicly available1.																																	2024-05-25	PPRN:73908239		
J	Yang, Linyao; Chen, Hongyang; Li, Zhao; Ding, Xiao; Wu, Xindong				Xiang, Xiao/IQT-2849-2023; Wu, Xindong/AAB-6713-2022; Li, Zhao/LJL-9012-2024; Chen, Hongyang/F-7634-2015						Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling								Arxiv											2	2;2024-01-30;https://www.arxiv.org/abs/2306.11489v2| 1;2023-06-20;https://www.arxiv.org/abs/2306.11489v1	arXiv:2306.11489			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 30 2024	2024	Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research.																																	2024-05-25	PPRN:73441706		
J	Chen, Xinlei; Liu, Zhuang; Xie, Saining; He, Kaiming										Deconstructing Denoising Diffusion Models for Self-Supervised Learning								Arxiv											1	1;2024-01-25;https://www.arxiv.org/abs/2401.14404v1	arXiv:2401.14404			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 25 2024	2024	In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.																																	2024-05-25	PPRN:87336253		
J	Hong, Ke; Dai, Guohao; Xu, Jiaming; Mao, Qiuli; Li, Xiuhong; Liu, Jun; Chen, Kangdi; Dong, Yuhan; Wang, Yu				WANG, Yu/B-7985-2011; Li, Xiuhong/ABB-8030-2020; Hong, Ke/KIB-3489-2024; Dong, Hanyu/AAS-4827-2021						FlashDecoding++: Faster Large Language Model Inference on GPUs								Arxiv											4	4;2024-01-05;https://www.arxiv.org/abs/2311.01282v4| 3;2023-11-10;https://www.arxiv.org/abs/2311.01282v3| 2;2023-11-03;https://www.arxiv.org/abs/2311.01282v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01282v1	arXiv:2311.01282			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 05 2024	2024	As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference. We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1) Asynchronized softmax with unified max value. FlashDecoding++ introduces a unified max value technique for different partial softmax computations to avoid synchronization. (2) Flat GEMM optimization with double buffering. FlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource considering input dynamics. Due to the versatility of optimizations in FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37x compared to state-of-the-art LLM inference engines on mainstream LLMs.																																	2024-05-25	PPRN:85983471		
J	Qu, Liao; Zhang, Huichao; Liu, Yiheng; Wang, Xu; Jiang, Yi; Gao, Yiming; Ye, Hu; Du, Daniel K.; Yuan, Zehuan; Wu, Xinglong				wu, xinglong/MTA-1232-2025; Zhang, Haiqiang/H-8186-2018						TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation								Arxiv											1	1;2024-12-04;https://www.arxiv.org/abs/2412.03069v1	arXiv:2412.03069			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 04 2024	2024	We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow’s superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384×384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256×256 resolution, achieving comparable results to SDXL.																																	2025-01-15	PPRN:119698062		
J	Zimmermann, Eric; Vorontsov, Eugene; Viret, Julian; Casson, Adam; Zelechowski, Michal; Shaikovski, George; Tenenholtz, Neil; Hall, James; Klimstra, David; Yousfi, Razik; Fuchs, Thomas; Fusi, Nicolo; Liu, Siqi; Severson, Kristen										Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology								Arxiv											3	3;2024-11-06;https://www.arxiv.org/abs/2408.00738v3| 2;2024-08-14;https://www.arxiv.org/abs/2408.00738v2| 1;2024-08-01;https://www.arxiv.org/abs/2408.00738v1	arXiv:2408.00738			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 06 2024	2024	Foundation models are rapidly being developed for computational pathology applications. However, it remains an open question which factors are most important for downstream performance with data scale and diversity, model size, and training algorithm all playing a role. In this work, we propose algorithmic modifications, tailored for pathology, and we present the result of scaling both data and model size, surpassing previous studies in both dimensions. We introduce three new models: Virchow2, a 632 million parameter vision transformer, Virchow2G, a 1.9 billion parameter vision transformer, and Virchow2G Mini, a 22 million parameter distillation of Virchow2G, each trained with 3.1 million histopathology whole slide images, with diverse tissues, originating institutions, and stains. We achieve state of the art performance on 12 tile-level tasks, as compared to the top performing competing models. Our results suggest that data diversity and domain-specific methods can outperform models that only scale in the number of parameters, but, on average, performance benefits from the combination of domain-specific methods, data scale, and model scale.																																	2024-12-16	PPRN:91197923		
J	Huang, Lianghua; Wang, Wei; Wu, Zhi-Fan; Shi, Yupeng; Dou, Huanzhang; Liang, Chen; Feng, Yutong; Liu, Yu; Zhou, Jingren				Zhou, Mingyuan/AAE-8717-2021						In-Context LoRA for Diffusion Transformers								Arxiv											1	1;2024-11-01;https://www.arxiv.org/abs/2410.23775v2	arXiv:2410.23775			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 01 2024	2024	Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., $20sim 100$ samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. 																																	2024-12-06	PPRN:119012186		
J	Yang, Rui; Pan, Xiaoman; Luo, Feng; Qiu, Shuang; Zhong, Han; Yu, Dong; Chen, Jianshu				Yang, Rui/LZG-6631-2025						Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment								Arxiv											5	5;2024-10-16;https://www.arxiv.org/abs/2402.10207v6| 4;2024-05-24;https://www.arxiv.org/abs/2402.10207v4| 3;2024-02-25;https://www.arxiv.org/abs/2402.10207v3| 2;2024-02-19;https://www.arxiv.org/abs/2402.10207v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.10207v1	arXiv:2402.10207			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 16 2024	2024	We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.																																	2024-11-07	PPRN:87704021		
J	Chaudhari, Harsh; Severi, Giorgio; Abascal, John; Jagielski, Matthew; Choquette-Choo, Christopher A.; Nasr, Milad; Nita-Rotaru, Cristina; Oprea, Alina				Severi, Giorgio/AAD-5058-2022						Phantom: General Trigger Attacks on Retrieval Augmented Language Generation								Arxiv											2	2;2024-10-15;https://www.arxiv.org/abs/2405.20485v2| 1;2024-05-30;https://www.arxiv.org/abs/2405.20485v1	arXiv:2405.20485			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 15 2024	2024	Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs), by anchoring, adapting, and personalizing their responses to the most relevant knowledge sources. It is particularly useful in chatbot applications, allowing developers to customize LLM output without expensive retraining. Despite their significant utility in various applications, RAG systems present new security risks. In this work, we propose new attack vectors that allow an adversary to inject a single malicious document into a RAG system’s knowledge base, and mount a backdoor poisoning attack. We design Phantom, a general two-stage optimization framework against RAG systems, that crafts a malicious poisoned document leading to an integrity violation in the model’s output. First, the document is constructed to be retrieved only when a specific trigger sequence of tokens appears in the victim’s queries. Second, the document is further optimized with crafted adversarial text that induces various adversarial objectives on the LLM output, including refusal to answer, reputation damage, privacy violations, and harmful behaviors. We demonstrate our attacks on multiple LLM architectures, including Gemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and GPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA’s black-box production RAG system, "Chat with RTX".																																	2024-11-11	PPRN:89128325		
J	Pourreza, Mohammadreza; Li, Hailong; Sun, Ruoxi; Chung, Yeounoh; Talaei, Shayan; Kakkar, Gaurav Tarlok; Gan, Yu; Saberi, Amin; Ozcan, Fatma; Arik, Sercan O.				Ozcan, Fatma/AAC-7384-2021; Saberi, Amin/ACW-0953-2022						CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL								Arxiv											1	1;2024-10-02;https://www.arxiv.org/abs/2410.01943v1	arXiv:2410.01943			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 02 2024	2024	In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs' intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions.To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0% and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).																																	2024-10-21	PPRN:102618781		
J	Skarlinski, Michael D.; Cox, Sam; Laurent, Jon M.; Braza, James D.; Hinks, Michaela; Hammerling, Michael J.; Ponnapati, Manvitha; Rodriques, Samuel G.; White, Andrew D.				Cox, Sam/LHA-1044-2024						Language agents achieve superhuman synthesis of scientific knowledge								Arxiv											2	2;2024-09-26;https://www.arxiv.org/abs/2409.13740v2| 1;2024-09-10;https://www.arxiv.org/abs/2409.13740v1	arXiv:2409.13740			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Sep 26 2024	2024	Language models are known to “hallucinate” incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipediastyle summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2 .34 + 1 .99 (mean + SD, N = 93 papers) contradictions per paper in a random subset of biology papers, of which 70% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.																																	2024-10-08	PPRN:96000729		
J	Qu, Yuxiao; Zhang, Tianjun; Garg, Naman; Kumar, Aviral										Recursive Introspection: Teaching Language Model Agents How to <italic>Self-Improve</italic>								Arxiv											1	1;2024-07-26;https://www.arxiv.org/abs/2407.18219v2	arXiv:2407.18219			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 26 2024	2024	A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.																																	2024-08-02	PPRN:91119772		
J	Muralidharan, Saurav; Sreenivas, Sharath Turuvekere; Joshi, Raviraj; Chochowski, Marcin; Patwary, Mostofa; Shoeybi, Mohammad; Catanzaro, Bryan; Kautz, Jan; Molchanov, Pavlo										Compact Language Models via Pruning and Knowledge Distillation								Arxiv											1	1;2024-07-19;https://www.arxiv.org/abs/2407.14679v1	arXiv:2407.14679			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 19 2024	2024	Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4×, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40× × fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8× for training the full model family (15B, 8B, and 4B). M INITRON models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced M INITRON model weights on Huggingface 2 , with corresponding supplementary material including example code available on GitHub 3 .																																	2024-07-28	PPRN:91025892		
J	Yadkori, Yasin Abbasi; Kuzborskij, Ilja; Gyorgy, Andras; Szepesvari, Csaba										To Believe or Not to Believe Your LLM								Arxiv											1	1;2024-07-17;https://www.arxiv.org/abs/2406.02543v2	arXiv:2406.02543			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 17 2024	2024	We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.																																	2024-07-26	PPRN:90868675		
J	Wendler, Chris; Veselovsky, Veniamin; Monea, Giovanni; West, Robert				West, Robert/B-5414-2009						Do Llamas Work in English? On the Latent Language of Multilingual Transformers								Arxiv											3	3;2024-06-08;https://www.arxiv.org/abs/2402.10588v4| 2;2024-06-05;https://www.arxiv.org/abs/2402.10588v3| 1;2024-02-16;https://www.arxiv.org/abs/2402.10588v1	arXiv:2402.10588			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 08 2024	2024	We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in "input space", "concept space", and "output space", respectively. Crucially, our evidence suggests that the abstract "concept space" lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.																																	2024-07-04	PPRN:87798218		
J	Zhang, Hanning; Diao, Shizhe; Lin, Yong; Fung, Yi R.; Lian, Qing; Wang, Xingyao; Chen, Yangyi; Ji, Heng; Zhang, Tong				Chen, Yangyi/LSL-4051-2024; Wang, Xingyao/HNC-0090-2023; Diao, Shizhe/JXY-7398-2024; Zhang, Tong/HGC-1090-2022						R-Tuning: Instructing Large Language Models to Say 'I Don't Know'								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2311.09677v3| 2;2024-05-05;https://www.arxiv.org/abs/2311.09677v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09677v1	arXiv:2311.09677			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing.																																	2024-07-04	PPRN:86177728		
J	Jia, Xiaojun; Pang, Tianyu; Du, Chao; Huang, Yihao; Gu, Jindong; Liu, Yang; Cao, Xiaochun; Lin, Min				Liu, Yang/D-2306-2013; Tianyu, Pang/AAW-2653-2020; Jia, Xiaojun/IUM-2172-2023						Improved Techniques for Optimization-Based Jailbreaking on Large Language Models								Arxiv											2	2;2024-06-05;https://www.arxiv.org/abs/2405.21018v2| 1;2024-05-31;https://www.arxiv.org/abs/2405.21018v1	arXiv:2405.21018			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 05 2024	2024	Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of "Sure" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate. 																																	2024-06-22	PPRN:89123500		
J	Gu, Xiangming; Zheng, Xiaosen; Pang, Tianyu; Du, Chao; Liu, Qian; Wang, Ye; Jiang, Jing; Lin, Min				Wang, Jingjing/B-7476-2016; Zheng, Xiaosen/KFA-4269-2024; Gu, Xiangming/KDM-6482-2024; Tianyu, Pang/AAW-2653-2020						Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast								Arxiv											2	2;2024-06-03;https://www.arxiv.org/abs/2402.08567v2| 1;2024-02-13;https://www.arxiv.org/abs/2402.08567v1	arXiv:2402.08567			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. 																																	2024-06-22	PPRN:87673352		
J	Dong, Yihong; Jiang, Xue; Liu, Huanyu; Jin, Zhi; Gu, Bin; Yang, Mengfei; Li, Ge				Dong, Yihong/LCE-6194-2024; Jin, Zhi/AAB-2440-2022						Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models								Arxiv											2	2;2024-05-31;https://www.arxiv.org/abs/2402.15938v3| 1;2024-05-16;https://www.arxiv.org/abs/2402.15938v2	arXiv:2402.15938			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 31 2024	2024	Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs’ training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for C ontamination D etection via output D istribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM’s output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: T rustworthy E valuation via output D istribution, based on the correction of LLM’s output distribution. To facilitate this study, we introduce two benchmarks, i.e., DET- ET- C ON and C OMI E VAL , for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8%-30.2% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9% attributed to data contamination across various contamination setups. In realworld applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.1 1																																	2024-06-19	PPRN:88918217		
J	Sun, Chunyi; Han, Junlin; Deng, Weijian; Wang, Xinlong; Qin, Zishan; Gould, Stephen				DENG, WEIJIAN/AAT-2754-2020; Han, Junlin/AEQ-7629-2022; Wang, Xinlong/AFI-8800-2022						3D-GPT: Procedural 3D Modeling with Large Language Models								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2310.12945v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.12945v1	arXiv:2310.12945			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	In the pursuit of efficient automated content creation, procedural generation, leveraging modifiable parameters and rule-based systems, emerges as a promising approach. Nonetheless, it could be a demanding endeavor, given its intricate nature necessitating a deep understanding of rules, algorithms, and parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing large language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT positions LLMs as proficient problem solvers, dissecting the procedural 3D modeling tasks into accessible segments and appointing the apt agent for each task. 3D-GPT integrates three core agents: the task dispatch agent, the conceptualization agent, and the modeling agent. They collaboratively achieve two objectives. First, it enhances concise initial scene descriptions, evolving them into detailed forms while dynamically adapting the text based on subsequent instructions. Second, it integrates procedural generation, extracting parameter values from enriched text to effortlessly interface with 3D software for asset creation. Our empirical investigations confirm that 3D-GPT not only interprets and executes instructions, delivering reliable results but also collaborates effectively with human designers. Furthermore, it seamlessly integrates with Blender, unlocking expanded manipulation possibilities. Our work highlights the potential of LLMs in 3D modeling, offering a basic framework for future advancements in scene generation and animation.																																	2024-06-16	PPRN:85721342		
J	Wang, Chen; Liao, Minpeng; Huang, Zhongqiang; Lu, Jinliang; Wu, Junhong; Liu, Yuchen; Zong, Chengqing; Zhang, Jiajun										BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing								Arxiv											2	2;2024-05-28;https://www.arxiv.org/abs/2309.00916v2| 1;2023-09-02;https://www.arxiv.org/abs/2309.00916v1	arXiv:2309.00916			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.																																	2024-06-12	PPRN:84764221		
J	Eisner, Ben; Zhang, Harry; Held, David										FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated Objects								Arxiv											2	2;2024-05-02;https://www.arxiv.org/abs/2205.04382v6| 1;2022-07-11;https://www.arxiv.org/abs/2205.04382v2	arXiv:2205.04382			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 02 2024	2024	We explore a novel method to perceive and manipulate 3D articulated objects that generalizes to enable a robot to articulate unseen classes of objects. We propose a vision-based system that learns to predict the potential motions of the parts of a variety of articulated objects to guide downstream motion planning of the system to articulate the objects. To predict the object motions, we train a neural network to output a dense vector field representing the point-wise motion direction of the points in the point cloud under articulation. We then deploy an analytical motion planner based on this vector field to achieve a policy that yields maximum articulation. We train the vision system entirely in simulation, and we demonstrate the capability of our system to generalize to unseen object instances and novel categories in both simulation and the real world, deploying our policy on a Sawyer robot with no finetuning. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments.																																	2024-05-20	PPRN:10389169		
J	Xue, Le; Yu, Ning; Zhang, Shu; Panagopoulou, Artemis; Li, Junnan; Martin-Martin, Roberto; Wu, Jiajun; Xiong, Caiming; Xu, Ran; Niebles, Juan Carlos; Savarese, Silvio				Wu, Jiajun/AAM-6936-2021; YU, NING/Y-7269-2018; Niebles, Juan/AAT-5882-2021; Xue, Le/JFJ-7470-2023; Martin, Roberto/Y-6009-2019						ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding								Arxiv											3	3;2024-04-26;https://www.arxiv.org/abs/2305.08275v4| 2;2024-04-24;https://www.arxiv.org/abs/2305.08275v3| 1;2023-05-14;https://www.arxiv.org/abs/2305.08275v1	arXiv:2305.08275			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 26 2024	2024	Recent advancements in multimodal pre -training have shown promising efficacy in 3D representation learning by aligning multimodal features across 3D shapes, their 2D counterparts, and language descriptions. However, the methods used by existing frameworks to curate such multimodal data, in particular language descriptions for 3D shapes, are not scalable, and the collected language descriptions are not diverse. To address this, we introduce ULIP-2, a simple yet effective tri -modal pre -training framework that leverages large multimodal models to automatically generate holistic language descriptions for 3D shapes. It only needs 3D data as input, eliminating the need for any manual 3D annotations, and is therefore scalable to large datasets. ULIP-2 is also equipped with scaled-up backbones for better multimodal representation learning. We conduct experiments on two large-scale 3D datasets, Objaverse and ShapeNet, and augment them with tri -modal datasets of 3D point clouds, images, and language for training ULIP-2. Experiments show that ULIP-2 demonstrates substantial benefits in three downstream tasks: zero-shot 3D classification, standard 3D classification with fine-tuning, and 3D captioning (3D-tolanguage generation). It achieves a new SOTA of 50.6% (top1) on Objaverse-LVIS and 84.7% (top-1) on ModelNet40 in zero-shot classification. In the ScanObjectNN benchmark for standard fine-tuning, ULIP-2 reaches an overall accuracy of 91.5% with a compact model of only 1.4 million parameters. ULIP-2 sheds light on a new paradigm for scalable multimodal 3D representation learning without human annotations and shows significant improvements over existing baselines. The code and datasets are released																																	2024-06-05	PPRN:69650387		
J	Cohen, Jeremy M.; Ghorbani, Behrooz; Krishnan, Shankar; Agarwal, Naman; Medapati, Sourabh; Badura, Michal; Suo, Daniel; Cardoze, David; Nado, Zachary; Dahl, George E.; Gilmer, Justin										Adaptive Gradient Methods at the Edge of Stability								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2207.14484v2| 1;2022-07-29;https://www.arxiv.org/abs/2207.14484v1	arXiv:2207.14484			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	Very little is known about the training dynamics of adaptive gradient methods like Adam in deep learning. In this paper, we shed light on the behavior of these algorithms in the full -batch and sufficiently large batch settings. Specifically, we empirically demonstrate that during full -batch training, the maximum eigenvalue of the preconditioned Hessian typically equilibrates at a certain numerical value — the stability threshold of a gradient descent algorithm. For Adam with step size η and β1 = 0.9, this stability threshold is 38/η. Similar effects occur during minibatch training, especially as the batch size grows. Yet, even though adaptive methods train at the “Adaptive Edge of Stability” (AEoS), their behavior in this regime differs in a significant way from that of non -adaptive methods at the EoS. Whereas nonadaptive algorithms at the EoS are blocked from entering high -curvature regions of the loss landscape, adaptive gradient methods at the AEoS keep advancing into high -curvature regions, while adapting the preconditioner to compensate. Our findings can serve as a foundation for the community’s future understanding of adaptive gradient methods in deep learning.																																	2024-04-26	PPRN:12303878		
J	Fang, Zihan; Lin, Zheng; Chen, Zhe; Chen, Xianhao; Gao, Yue; Fang, Yuguang				Chen, Zhe/JNR-1848-2023; Fang, Yuguang/JJF-2146-2023; CHEN, XIANHAO/AAX-6311-2021; Gao, Yue/AAJ-9469-2020						Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models								Arxiv											1	1;2024-04-09;https://www.arxiv.org/abs/2404.06448v1	arXiv:2404.06448			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 09 2024	2024	Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs). However, for many downstream tasks, it is necessary to fine-tune LLMs using private data. While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning. To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM. Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks.																																	2024-04-22	PPRN:88467947		
J	Cheng, Huajie; Chiu, Wen Han; Fang, Yaquan; Gao, Yu; Gu, Jiayin; Li, Gang; Li, Lingfeng; Li, Tianjun; Liang, Zhijun; Liu, Bo; Liu, Jia; Liu, Zhen; Ruan, Manqi; Shu, Jing; Wang, Kechen; Wang, Lian-Tao; Xie, Ke-Pan; Yang, Shuo; Yuan, Jiarong; Zhang, Kaili; Zhang, Mengchao; Zhang, Yang; Zhuang, Xuai				Xie, Ke-Pan/AAQ-4456-2020; Liu, Zhen/AHB-3855-2022; Liu, jiayue/GXV-1130-2022; 张, 阳/GSD-8644-2022						The Physics potential of the CEPC. Prepared for the US Snowmass Community Planning Exercise (Snowmass 2021)								Arxiv											1	1;2024-04-08;https://www.arxiv.org/abs/2205.08553v2	arXiv:2205.08553			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 08 2024	2024	The Circular Electron Positron Collider (CEPC) is a large-scale collider facility that can serve as a factory of the Higgs, Z, and W bosons and is upgradable to run at the ttbar threshold. This document describes the latest CEPC nominal operation scenario and particle yields and updates the corresponding physics potential. A new detector concept is also briefly described. This submission is for consideration by the Snowmass process.																																	2024-04-27	PPRN:88444706		
J	Yan, Chi; Qu, Delin; Xu, Dan; Zhao, Bin; Wang, Zhigang; Wang, Dong; Li, Xuelong				Qu, delin/IUP-8464-2023; Xu, Dan/OML-8012-2025; Li, Xuelong/Z-3785-2019						GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting								Arxiv											2	2;2024-04-07;https://www.arxiv.org/abs/2311.11700v4| 1;2024-01-05;https://www.arxiv.org/abs/2311.11700v3	arXiv:2311.11700			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 07 2024	2024	In this paper, we introduce GS -SLAM that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussians in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse -to -fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUMRGBD datasets. Project page: https://gs-slam.github.io/.																																	2024-04-21	PPRN:86744313		
J	Wang, Mengru; Zhang, Ningyu; Xu, Ziwen; Xi, Zekun; Deng, Shumin; Yao, Yunzhi; Zhang, Qishen; Yang, Linyi; Wang, Jindong; Chen, Huajun				wang, jindong/ACD-8485-2022; Deng, Shumin/AAP-7003-2021; Huajun, Chen/B-6340-2013						Detoxifying Large Language Models via Knowledge Editing								Arxiv											4	4;2024-05-28;https://www.arxiv.org/abs/2403.14472v5| 3;2024-04-13;https://www.arxiv.org/abs/2403.14472v3| 2;2024-03-28;https://www.arxiv.org/abs/2403.14472v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14472v1	arXiv:2403.14472			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.																																	2025-08-07	PPRN:88257795		
J	Yasunaga, Michihiro; Chen, Xinyun; Li, Yujia; Pasupat, Panupong; Leskovec, Jure; Liang, Percy; Chi, Ed H.; Zhou, Denny				Chen, Xinyun/ABZ-9877-2022; Yasunaga, Michihiro/GPW-9499-2022						Large Language Models as Analogical Reasoners								Arxiv											3	3;2024-03-09;https://www.arxiv.org/abs/2310.01714v3| 2;2023-10-07;https://www.arxiv.org/abs/2310.01714v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.01714v1	arXiv:2310.01714			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 09 2024	2024	Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.																																	2024-04-07	PPRN:85375541		
J	Li, Gen; Wei, Yuting; Chen, Yuxin; Chi, Yuejie				Chi, Yuejie/AAG-5084-2019; Chen, Yuxin/HOH-1234-2023						Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models								Arxiv											3	3;2024-03-07;https://www.arxiv.org/abs/2306.09251v3| 2;2023-10-01;https://www.arxiv.org/abs/2306.09251v2| 1;2023-06-15;https://www.arxiv.org/abs/2306.09251v1	arXiv:2306.09251			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 07 2024	2024	Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non -asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to l2 -accurate estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to 1/T (with T the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model), we derive a convergence rate proportional to 1/ √T, matching the state-of-the-art theory. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results characterize how l2 score estimation errors affect the quality of the data generation processes. In contrast to prior works, our theory is developed based on an elementary yet versatile non -asymptotic approach without resorting to toolboxes for SDEs and ODEs. Further, we design two accelerated variants, improving the convergence to 1/T2 for the ODE -based sampler and 1/T for the DDPM-type sampler, which might be of independent theoretical and empirical interest.																																	2024-04-05	PPRN:73359075		
J	Adilazuarda, Muhammad Farid; Mukherjee, Sagnik; Lavania, Pradhyumna; Singh, Siddhant; Dwivedi, Ashutosh; Aji, Alham Fikri; O'Neill, Jacki; Modi, Ashutosh; Choudhury, Monojit										Towards Measuring and Modeling "Culture" in LLMs: A Survey								Arxiv											4	4;2024-09-04;https://www.arxiv.org/abs/2403.15412v5| 3;2024-04-27;https://www.arxiv.org/abs/2403.15412v3| 2;2024-04-12;https://www.arxiv.org/abs/2403.15412v2| 1;2024-03-05;https://www.arxiv.org/abs/2403.15412v1	arXiv:2403.15412			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 05 2024	2024	We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations, we provide several recommendations for a holistic and practically useful research agenda for furthering cultural inclusion in LLMs and LLM-based applications.																																	2025-08-07	PPRN:88280156		
J	Platonov, Oleg; Kuznedelev, Denis; Diskin, Michael; Babenko, Artem; Prokhorenkova, Liudmila				Prokhorenkova, Liudmila/M-5135-2016; Diskin, Michael/AAB-6411-2022; Babenko, Artem/M-3540-2016; Platonov, Oleg/KSM-4105-2024						A critical look at the evaluation of GNNs under heterophily: Are we really making progress?								Arxiv											2	2;2024-03-02;https://www.arxiv.org/abs/2302.11640v2| 1;2023-02-22;https://www.arxiv.org/abs/2302.11640v1	arXiv:2302.11640			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 02 2024	2024	Node classification is a classical graph machine learning task on which Graph Neural Networks (GNNs) have recently achieved strong results. However, it is often believed that standard GNNs only work well for homophilous graphs, i.e., graphs where edges tend to connect nodes of the same class. Graphs without this property are called heterophilous, and it is typically assumed that specialized methods are required to achieve strong performance on such graphs. In this work, we challenge this assumption. First, we show that the standard datasets used for evaluating heterophily-specific models have serious drawbacks, making results obtained by using them unreliable. The most significant of these drawbacks is the presence of a large number of duplicate nodes in the datasets Squirrel and Chameleon, which leads to train-test data leakage. We show that removing duplicate nodes strongly affects GNN performance on these datasets. Then, we propose a set of heterophilous graphs of varying properties that we believe can serve as a better benchmark for evaluating the performance of GNNs under heterophily. We show that standard GNNs achieve strong results on these heterophilous graphs, almost always outperforming specialized models. 																																	2024-04-02	PPRN:44002677		
J	Srivastava, Saurabh; Menon, Shashank; Ajay, Sukumar; Philipose, Alan; Prince, Stevin; Thomas, Sooraj										Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19450v1	arXiv:2402.19450			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. 																																	2024-03-28	PPRN:88011004		
J	Zhu, Lanyun; Ji, Deyi; Chen, Tianrun; Xu, Peng; Ye, Jieping; Liu, Jun				Ji, Deyi/MYR-4920-2025; Chen, Tianrun/KYP-2807-2024; Zhu, Lanyun/LNQ-2089-2024						IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18476v1	arXiv:2402.18476			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.																																	2024-11-10	PPRN:87989917		
J	Choudhury, Sayantan										Single field inflation in the light of Pulsar Timing Array Data: Quintessential interpretation of blue tilted tensor spectrum through Non-Bunch Davies initial condition								Arxiv											2	2;2024-02-26;https://www.arxiv.org/abs/2307.03249v3| 1;2023-07-06;https://www.arxiv.org/abs/2307.03249v1	arXiv:2307.03249			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	In this work, we present a quintessential interpretation of having a blue -tilted tensor power spectrum for canonical single -field slow -roll inflation to explain the recently observed Pulsar Timing Array (NANOGrav 15 -year and EPTA) signal of Gravitational Waves (GW). We formulate the complete semi -classical description of cosmological perturbation theory in terms of scalar and tensor modes using the Non -Bunch Davies initial condition. We found that the existence of the blue tilt (nt) within the favoured range 1.2 < nt < 2.5 can be explained in terms of a newly derived consistency relation. Further, we compute a new field excursion formula using the Non -Bunch Davies initial condition, that validates the requirement of Effective Field Theory in the sub-Planckian regime, | ∆ϕ|  ≪ Mpl for the allowed value of the tensor -to -scalar ratio, r < 0.06 from CMB observations. In our study, we refer to this result as Anti Lyth bound as it violates the well-known Lyth bound originally derived for Bunch Davies initial condition. Further, we study the behaviour of the spectral density of GW and the associated abundance with the frequency, which shows that within the frequency domain 10−9Hz < f < 10−7Hz the outcome obtained from our analysis is completely consistent with the Pulsar Timing Array (NANOGrav 15 -year and EPTA) signal. Also, we found that the behaviour of GW spectra satisfies the CMB constraints at the low frequency, f∗ 7.7 x 10−17Hz corresponding to the pivots scale wave number, k∗ 0.05Mpc−1. Finally, the sharp falling behaviour of the GW spectra within the frequency domain 10−7Hz < f < 1Hz validates our theory in the comparatively high -frequency regime as well.																																	2024-04-11	PPRN:73837711		
J	Huang, James Y.; Sengupta, Sailik; Bonadiman, Daniele; Lai, Yi-an; Gupta, Arshit; Pappas, Nikolaos; Mansour, Saab; Kirchhoff, Katrin; Roth, Dan				Lai, Yi-An/JQW-5482-2023						DeAL: Decoding-time Alignment for Large Language Models								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2402.06147v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.06147v1	arXiv:2402.06147			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constraints such as keyword and length constraints (studied widely in the pre-LLM era) and abstract objectives such as harmlessness and helpfulness (proposed in the post-LLM era) show that we can DeAL with fine-grained trade-offs, improve adherence to alignment objectives, and address residual gaps in LLMs. Lastly, while DeAL can be effectively paired with RLHF and prompting techniques, its generality makes decoding slower, an optimization we leave for future work.																																	2024-03-20	PPRN:87616165		
J	Zhu, Dawei; Yang, Nan; Wang, Liang; Song, Yifan; Wu, Wenhao; Wei, Furu; Li, Sujian				zhu, dawei/HIK-0564-2022; Wu, Wenhao/JCE-1538-2023						PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training								Arxiv											3	3;2024-02-21;https://www.arxiv.org/abs/2309.10400v3| 2;2023-10-10;https://www.arxiv.org/abs/2309.10400v2| 1;2023-09-19;https://www.arxiv.org/abs/2309.10400v1	arXiv:2309.10400			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 21 2024	2024	Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Full-length fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that PoSE greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens using a 2k training context window. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies. Notably, our method can potentially support infinite length, limited only by memory usage in inference. With ongoing progress for efficient inference, we believe PoSE can further scale the context window beyond 128k.																																	2024-03-20	PPRN:85071695		
J	Lee, Joo Chan; Rho, Daniel; Sun, Xiangyu; Ko, Jong Hwan; Park, Eunbyung				Park, Eunbyung/JUV-0796-2023; Lee, Joo Chan/AAY-6280-2021; Sun, Xiangyu/IWN-7833-2023						Compact 3D Gaussian Representation for Radiance Field								Arxiv											2	2;2024-02-15;https://www.arxiv.org/abs/2311.13681v2| 1;2023-11-22;https://www.arxiv.org/abs/2311.13681v1	arXiv:2311.13681			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 15 2024	2024	Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in capturing complex 3D scenes with high fidelity. However, one persistent challenge that hinders the widespread adoption of NeRFs is the computational bottleneck due to the volumetric rendering. On the other hand, 3D Gaussian splatting (3DGS) has recently emerged as an alternative representation that leverages a 3D Gaussisan-based representation and adopts the rasterization pipeline to render the images rather than volumetric rendering, achieving very fast rendering speed and promising image quality. However, a significant drawback arises as 3DGS entails a substantial number of 3D Gaussians to maintain the high fidelity of the rendered images, which requires a large amount of memory and storage. To address this critical issue, we place a specific emphasis on two key objectives: reducing the number of Gaussian points without sacrificing performance and compressing the Gaussian attributes, such as view-dependent color and covariance. To this end, we propose a learnable mask strategy that significantly reduces the number of Gaussians while preserving high performance. In addition, we propose a compact but effective representation of view-dependent color by employing a grid-based neural field rather than relying on spherical harmonics. Finally, we learn codebooks to compactly represent the geometric attributes of Gaussian by vector quantization. With model compression techniques such as quantization and entropy coding, we consistently show over 25$times$ reduced storage and enhanced rendering speed, while maintaining the quality of the scene representation, compared to 3DGS. Our work provides a comprehensive framework for 3D scene representation, achieving high performance, fast training, compactness, and real-time rendering. 																																	2024-03-13	PPRN:86279483		
J	Zhou, Pei; Pujara, Jay; Ren, Xiang; Chen, Xinyun; Cheng, Heng-Tze; Le, Quoc V.; Chi, Ed H.; Zhou, Denny; Mishra, Swaroop; Zheng, Huaixiu Steven				Chen, Xinyun/ABZ-9877-2022; Pujara, Jay/IVH-3182-2023; ren, xiang/HLQ-5068-2023						Self-Discover: Large Language Models Self-Compose Reasoning Structures								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2402.03620v1	arXiv:2402.03620			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 06 2024	2024	We introduce SELF -DISCOVER, a general framework for LLMs to self -discover the task -intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a selfdiscovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF -DISCOVER substantially improves GPT-4 and PaLM 2’s performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF - DISCOVER outperforms inference -intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self -discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.																																	2024-05-25	PPRN:87529608		
J	Cui, Tianyu; Wang, Yanling; Fu, Chuanpu; Xiao, Yong; Li, Sijia; Deng, Xinhao; Liu, Yunpeng; Zhang, Qinglin; Qiu, Ziyi; Li, Peiyang; Tan, Zhixing; Xiong, Junwu; Kong, Xinyu; Wen, Zujie; Xu, Ke; Li, Qi				Liu, Yunpeng/LPP-9697-2024; Qiu, Ziyi/IQU-4826-2023; Li, Sijia/ABG-1700-2020; Zhang, Qinglin/D-9258-2013; Li, Peiyang/GQB-2447-2022; Deng, Xinhao/MTA-7151-2025; Li, Qixian/ITV-3609-2023						Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems								Arxiv											1	1;2024-01-11;https://www.arxiv.org/abs/2401.05778v1	arXiv:2401.05778			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 11 2024	2024	Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.																																	2024-01-26	PPRN:87128094		
J	Jiang, Minhao; Liu, Ken Ziyu; Zhong, Ming; Schaeffer, Rylan; Ouyang, Siru; Han, Jiawei; Koyejo, Sanmi				Ouyang, Siru/OKS-7297-2025						Investigating Data Contamination for Pre-training Language Models								Arxiv											1	1;2024-01-11;https://www.arxiv.org/abs/2401.06059v1	arXiv:2401.06059			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre training corpus — a phenomenon known as data contamination — in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs’ performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models from scratch. We highlight the effect of both text contamination (i.e. input text of the evaluation samples) and ground-truth contamination (i.e. the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination’s effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.																																	2024-01-26	PPRN:87124704		
J	D'Arcy, Mike; Hope, Tom; Birnbaum, Larry; Downey, Doug				D'Arcy, Michel/D-4848-2016						MARG: Multi-Agent Review Generation for Scientific Papers								Arxiv											1	1;2024-01-08;https://www.arxiv.org/abs/2401.04259v1	arXiv:2401.04259			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 08 2024	2024	We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).																																	2024-05-25	PPRN:87081725		
J	Liu, Yiheng; He, Hao; Han, Tianle; Zhang, Xu; Liu, Mengyuan; Tian, Jiaming; Zhang, Yutong; Wang, Jiaqi; Gao, Xiaohui; Zhong, Tianyang; Pan, Yi; Xu, Shaochen; Wu, Zihao; Liu, Zhengliang; Zhang, Xin; Zhang, Shu; Hu, Xintao; Zhang, Tuo; Qiang, Ning; Liu, Tianming; Ge, Bao				wang, jiaqi/HHS-0123-2022; wu, zihao/R-8745-2019; zhang, yutong/GXV-2287-2022; Zhang, Tuo/NHP-8722-2025; Liu, Tianming/GLS-1211-2022; Gao, Xiaohui/J-2948-2016; Liu, Mengyuan/IUO-6172-2023						Understanding LLMs: A Comprehensive Overview from Training to Inference								Arxiv											2	2;2024-01-06;https://www.arxiv.org/abs/2401.02038v2| 1;2024-01-04;https://www.arxiv.org/abs/2401.02038v1	arXiv:2401.02038			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 06 2024	2024	The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.																																	2024-01-25	PPRN:86964813		
J	Giare, William; Sabogal, Miguel A.; Nunes, Rafael C.; Di Valentino, Eleonora				Nunes, Rafael/AAT-8629-2021; Sabogal, Miguel/NES-3844-2025						Interacting Dark Energy after DESI Baryon Acoustic Oscillation measurements								Arxiv											2	2;2024-12-19;https://www.arxiv.org/abs/2404.15232v2| 1;2024-04-23;https://www.arxiv.org/abs/2404.15232v1	arXiv:2404.15232			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 19 2024	2024	We investigate the implications of the Baryon Acoustic Oscillations measurement released by the Dark Energy Spectroscopic Instrument (DESI) for Interacting Dark Energy (IDE) models characterized by an energy-momentum flow from Dark Matter to Dark Energy. By combining Planck-2018 and DESI data, we observe a preference for interactions, leading to a non-vanishing interaction rate ξ = −0.32−0.14+0.18 , which results in a present-day expansion rate H0 = 70.8−1.7+1.4 km/s/Mpc, reducing the tension with the value provided by the SH0ES collaboration to less than ∼ 1.3σ. The preference for interactions remains robust when including measurements of the expansion rate H(z) obtained from the relative ages of massive, early-time, and passively-evolving galaxies, as well as when considering distance moduli measurements from Type-Ia Supernovae sourced from the Pantheon-plus catalog using the SH0ES Cepheid host distances as calibrators. Overall, the IDE framework provides an equally good, or better, explanation of both high- and low-redshift background observations compared to ΛCDM, while also yielding higher H0 values that align more closely with the local distance ladder estimates. However, a limitation of the IDE model is that it predicts lower Ωm and higher σ8 values, which may not be fully consistent with large-scale structure data at the perturbation level.																																	2025-01-27	PPRN:88622878		
J	Huang, Keli; Shi, Botian; Li, Xiang; Li, Xin; Huang, Siyuan; Li, Yikang				Shi, Botian/HTT-0363-2023						Multi-modal Sensor Fusion for Auto Driving Perception: A Survey								Arxiv											2	2;2024-12-16;https://www.arxiv.org/abs/2202.02703v3| 1;2022-02-27;https://www.arxiv.org/abs/2202.02703v2	arXiv:2202.02703			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 16 2024	2024	Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.																																	2025-01-25	PPRN:12095210		
J	Chen, Boyuan; Monso, Diego Marti; Du, Yilun; Simchowitz, Max; Tedrake, Russ; Sitzmann, Vincent										Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion								Arxiv											3	3;2024-12-10;https://www.arxiv.org/abs/2407.01392v4| 2;2024-07-04;https://www.arxiv.org/abs/2407.01392v3| 1;2024-07-02;https://www.arxiv.org/abs/2407.01392v2	arXiv:2407.01392			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 10 2024	2024	This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. 																																	2025-01-18	PPRN:90674338		
J	Lei, Jiahui; Weng, Yijia; Harley, Adam W.; Guibas, Leonidas; Daniilidis, Kostas										MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds								Arxiv											2	2;2024-11-29;https://www.arxiv.org/abs/2405.17421v2| 1;2024-05-27;https://www.arxiv.org/abs/2405.17421v1	arXiv:2405.17421			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 29 2024	2024	We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models and lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions/deformations. The scene geometry and appearance are then disentangled from the deformation field and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera focal length and poses can be solved using bundle adjustment without the need of any other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks and its effectiveness on real videos. 																																	2025-01-11	PPRN:89063092		
J	Lin, Kevin Qinghong; Li, Linjie; Gao, Difei; Yang, Zhengyuan; Wu, Shiwei; Bai, Zechen; Lei, Weixian; Wang, Lijuan; Shou, Mike Zheng				Wu, shiwei/HGE-3099-2022; Yang, Zhengyuan/AGQ-1232-2022; Shou, Mike Zheng/LXW-9197-2024; 李, 李林洁/JAD-1884-2023						ShowUI: One Vision-Language-Action Model for GUI Visual Agent								Arxiv											1	1;2024-11-26;https://www.arxiv.org/abs/2411.17465v1	arXiv:2411.17465			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 26 2024	2024	Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. 																																	2025-01-08	PPRN:119437624		
J	Huang, Ziqi; Zhang, Fan; Xu, Xiaojie; He, Yinan; Yu, Jiashuo; Dong, Ziyue; Ma, Qianli; Chanpaisit, Nattapol; Si, Chenyang; Jiang, Yuming; Wang, Yaohui; Chen, Xinyuan; Chen, Ying-Cong; Wang, Limin; Lin, Dahua; Qiao, Yu; Liu, Ziwei				Ma, Qianli/GSO-3147-2022; Lin, Dahua/W-6576-2019; Xu, xiaojie/AAG-2989-2019; Liu, Ziwei/AAG-6939-2021; Wang, Limin/AAE-3419-2019; Si, ChenYang/HTS-0409-2023						VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models								Arxiv											1	1;2024-11-20;https://www.arxiv.org/abs/2411.13503v1	arXiv:2411.13503			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 20 2024	2024	Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects “video generation quality” into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has several appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation ( e.g. , subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc ). The evaluation metrics with fine-grained levels reveal individual models’ strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks’ alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models’ ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. 4) Versatile Benchmarking: VBench++ is designed to evaluate a wide range of video generation tasks, including text-to-video and image-to-video. We introduce a high-quality Image Suite with an adaptive aspect ratio to enable fair evaluations across different image-to-video generation settings. Beyond assessing technical quality, VBench++ evaluates the trustworthiness of video generative models, providing a more holistic view of model performance. 																																	2024-12-31	PPRN:119300264		
J	Kuratov, Yuri; Bulatov, Aydar; Anokhin, Petr; Rodkin, Ivan; Sorokin, Dmitry; Sorokin, Artyom; Burtsev, Mikhail				Burtsev, Mikhail/G-6293-2010; Kuratov, Yuri/LTY-5852-2024						BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack								Arxiv											2	2;2024-11-06;https://www.arxiv.org/abs/2406.10149v2| 1;2024-06-14;https://www.arxiv.org/abs/2406.10149v1	arXiv:2406.10149			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 06 2024	2024	In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.																																	2024-12-16	PPRN:89327887		
J	Xie, Zhifei; Wu, Changqiao				XIE, Zhifei/KIG-2667-2024						Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities								Arxiv											2	2;2024-11-05;https://www.arxiv.org/abs/2410.11190v3| 1;2024-10-16;https://www.arxiv.org/abs/2410.11190v2	arXiv:2410.11190			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 05 2024	2024	GPT-4o, an all-encompassing model, represents a milestone in the development of large multi-modal language models. It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction. Models from the open-source community often achieve some functionalities of GPT-4o, such as visual understanding and voice chat. Nevertheless, training a unified model that incorporates all modalities is challenging due to the complexities of multimodal data, intricate model architectures, and training processes. In this paper, we introduce Mini-Omni2, a visual-audio assistant capable of providing real-time, end-to-end voice responses to visoin and audio queries. By integrating pretrained visual and auditory encoders, Mini-Omni2 maintains performance in individual modalities. We propose a three-stage training process to align modalities, allowing the language model to handle multi-modal inputs and outputs after training on a limited dataset. For interaction, we introduce a command-based interruption mechanism, enabling more flexible interaction with users. To the best of our knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have similar form of functionality, and we hope it can offer valuable insights for subsequent research.																																	2024-12-09	PPRN:114118487		
J	Valmeekam, Karthik; Stechly, Kaya; Kambhampati, Subbarao										LLMS S STILL C AN’T P LAN ; CAN LRMS?&nbsp;A PRELIMINARY EVALUATION OF OPEN AI’S O1 ON P LAN BENCH								Arxiv											1	1;2024-09-20;https://www.arxiv.org/abs/2409.13373v1	arXiv:2409.13373			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 20 2024	2024	The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities. PlanBench [1], an extensible benchmark we developed in 2022, soon after the release of GPT3, has remained an important tool for evaluating the planning abilities of LLMs. Despite the slew of new private and open source LLMs since GPT3, progress on this benchmark has been surprisingly slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs–making it a new kind of model: a Large Reasoning Model (LRM). Using this development as a catalyst, this paper takes a comprehensive look at how well current LLMs and new LRMs do on PlanBench. As we shall see, while o1’s performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it. This improvement also brings to the fore questions about accuracy, efficiency, and guarantees which must be considered before deploying such systems.																																	2024-10-03	PPRN:94108927		
J	Choudhury, Sayantan; Karde, Ahaskar; Panda, Sudhakar; Sami, M.										Scalar induced gravity waves from ultra slow-roll Galileon inflation								Arxiv											4	4;2024-09-05;https://www.arxiv.org/abs/2308.09273v4| 3;2024-04-03;https://www.arxiv.org/abs/2308.09273v3| 2;2023-10-19;https://www.arxiv.org/abs/2308.09273v2| 1;2023-08-18;https://www.arxiv.org/abs/2308.09273v1	arXiv:2308.09273			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 05 2024	2024	We consider the production of secondary gravity waves in Galileon inflation with an ultra-slowroll (USR) phase and show that the spectrum of scalar-induced gravitational waves (SIGWs) in this case is consistent with the recent NANO Grav 15-year data and with sensitivities of other ground and space-based missions, LISA, BBO, DECIGO, CE, ET, HLVK (consists of aLIGO, aVirgo, and KAGRA), and HLV(03). Thanks to the non-renormalization property of Galileon theory, the amplitude of the large fluctuation is controllable at the sharp transitions between SR and USRregions. We show that the behaviour of the GW spectrum, when one-loop effects are included in the scalar power spectrum, is preserved under a shift of the sharp transition scale with peak amplitude ΩGWh2∼O(10−6), and hence it can cover a wide range of frequencies within O(10−9Hz−107Hz).An analysis of the allowed mass range for primordial black holes (PBHs) is also performed, where we find that mass values ranging from O(1M⊙−10−18M⊙)can be generated over the corresponding allowed range of low and high frequencies																																	2024-09-15	PPRN:85724902		
J	Dong, Junkai; Wang, Taige; Wang, Tianle; Soejima, Tomohiro; Zaletel, Michael P.; Vishwanath, Ashvin; Parker, Daniel E.				Vishwanath, Ashvin/AAO-6878-2020; Dong, Junkai/JKH-8381-2023						Anomalous Hall Crystals in Rhombohedral Multilayer Graphene I: Interaction-Driven Chern Bands and Fractional Quantum Hall States at Zero Magnetic Field								Arxiv											2	2;2024-08-18;https://www.arxiv.org/abs/2311.05568v2| 1;2023-11-09;https://www.arxiv.org/abs/2311.05568v1	arXiv:2311.05568			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 18 2024	2024	Recent experiments on rhombohedral pentalayer graphene with a substrate-induced moire potential have identified both Chern insulators and fractional Quantum Hall states at zero magnetic field. Surprisingly, these states are observed in strong displacement fields where the effects of the moire lattice are weak, and seem to be readily accessed without fine-tuning. To address these experimental puzzles, we study a model of interacting electrons in this geometry. Within self-consistent HartreeFock (SCHF), we find an isolated Chern band with small bandwidth and good quantum geometry. Exact diagonalization and density-matrix renormalization group calculations both confirm the band hosts fractional quantum Hall states without a magnetic field. Remarkably, the Chern band is stable at a wide range of angles, at four through six rhombohedral layers, at varying rhombohedral hopping parameters, and — most strikingly — survives in SCHF when the moire potential vanishes. In this limit, the state spontaneously breaks time-reversal and translation symmetry simultaneously, giving a topological crystalline state that we term the “anomalous Hall crystal” (AHC). We argue this is a general mechanism to create stable Chern bands in rhombohedral multilayer graphene, opening the door to studying the interplay between electronic topology, fractionalization, and spontaneous translation symmetry breaking.																																	2024-08-30	PPRN:86114589		
J	Liu, Shengchao; Li, Yanjing; Li, Zhuoxinran; Gitter, Anthony; Zhu, Yutao; Lu, Jiarui; Xu, Zhao; Nie, Weili; Ramanathan, Arvind; Xiao, Chaowei; Tang, Jian; Guo, Hongyu; Anandkumar, Anima				Xiao, Chaowei/AAT-8772-2021; Ramanathan, Arvind/E-5388-2010						A Text-guided Protein Design Framework								Arxiv											3	3;2024-08-12;https://www.arxiv.org/abs/2302.04611v3| 2;2023-12-03;https://www.arxiv.org/abs/2302.04611v2| 1;2023-02-09;https://www.arxiv.org/abs/2302.04611v1	arXiv:2302.04611			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 12 2024	2024	Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins' high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 12 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.																																	2024-08-21	PPRN:36867188		
J	Cheng, Daixuan; Huang, Shaohan; Wei, Furu				cheng, daixuan/LZE-0736-2025; Huang, Shaohan/LDF-3300-2024						Adapting Large Language Models to Domains via Reading Comprehension								Arxiv											4	4;2024-07-25;https://www.arxiv.org/abs/2309.09530v4| 3;2024-07-14;https://www.arxiv.org/abs/2309.09530v3| 2;2024-02-21;https://www.arxiv.org/abs/2309.09530v2| 1;2023-09-18;https://www.arxiv.org/abs/2309.09530v1	arXiv:2309.09530			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 25 2024	2024	We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension-practice after reading improves the ability to answer questions based on the learned knowledge-we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model's performance even on general benchmarks, showing the potential to develop a general model across even more domains. 																																	2024-08-03	PPRN:85026458		
J	Wu, Philipp; Shentu, Yide; Yi, Zhongke; Lin, Xingyu; Abbeel, Pieter				林, 星宇/GSD-2548-2022						GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators								Arxiv											2	2;2024-07-18;https://www.arxiv.org/abs/2309.13037v2| 1;2023-09-22;https://www.arxiv.org/abs/2309.13037v1	arXiv:2309.13037			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 18 2024	2024	Humans can teleoperate robots to accomplish complex manipulation tasks. Imitation learning has emerged as a powerful framework that leverages human teleoperated demonstrations to teach robots new skills. However, the performance of the learned policies is bottlenecked by the quality, scale, and variety of the demonstration data. In this paper, we aim to lower the barrier to collecting large and high-quality human demonstration data by proposing a GEneraL framework for building LOw-cost and intuitive teleoperation systems for robotic manipulation (GELLO). Given a target robot arm, we build a GELLO controller device that has the same kinematic structure as the target arm, leveraging 3D-printed parts and economical off-the-shelf motors. GELLO is easy to build and intuitive to use. Through an extensive user study, we show that GELLO enables more reliable and efficient demonstration collection compared to other cost efficient teleoperation devices commonly used in the imitation learning literature such as virtual reality controllers and 3D spacemouses. We further demonstrate the capabilities of GELLO for performing complex bi-manual and contact-rich manipulation tasks. To make GELLO accessible to everyone, we have designed and built GELLO systems for 3 commonly used robotic arms: Franka, UR5, and xArm. 																																	2024-07-26	PPRN:85177985		
J	Scheurer, Jeremy; Balesni, Mikita; Hobbhahn, Marius										Large Language Models can Strategically Deceive their Users when Put Under Pressure								Arxiv											3	3;2024-07-15;https://www.arxiv.org/abs/2311.07590v4| 2;2024-05-09;https://www.arxiv.org/abs/2311.07590v3| 1;2023-11-27;https://www.arxiv.org/abs/2311.07590v2	arXiv:2311.07590			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 15 2024	2024	We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.																																	2024-07-23	PPRN:86296714		
J	Pan, Yichen; Kong, Dehan; Zhou, Sida; Cui, Cheng; Leng, Yifei; Jiang, Bing; Liu, Hangyu; Shang, Yanyi; Zhou, Shuyan; Wu, Tongshuang; Wu, Zhengyang				Pan, Yichen/KCK-7738-2024						WebCanvas: Benchmarking Web Agents in Online Environments								Arxiv											1	1;2024-06-27;https://www.arxiv.org/abs/2406.12373v2	arXiv:2406.12373			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 27 2024	2024	For web agents to be practically useful, they must adapt to the continuously evolving web environment characterized by frequent updates to user interfaces and content. However, most existing benchmarks only capture the static aspects of the web. To bridge this gap, we introduce WebCanvas, an innovative online evaluation framework for web agents that effectively addresses the dynamic nature of web interactions. WebCanvas contains three main components to facilitate realistic assessments: (1) A novel evaluation metric which reliably capture critical intermediate actions or states necessary for task completions while disregarding noise caused by insignificant events or changed web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version of original Mind2Web static dataset containing 542 tasks with 2439 intermediate evaluation states; (3) Lightweight and generalizable annotation tools and testing pipelines that enables the community to collect and maintain the high-quality, up-to-date dataset. Building on WebCanvas, we open-source an agent framework with extensible modules for reasoning, providing a foundation for the community to conduct online inference and evaluations. Our best-performing agent achieves a task success rate of 23.1% and a task completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we analyze the performance discrepancies across various websites, domains, and experimental environments. We encourage the community to contribute further insights on online agent evaluation, thereby advancing this field of research.																																	2025-08-13	PPRN:123421610		
J	Jin, Haibo; Hu, Leyang; Li, Xinuo; Zhang, Peiyan; Chen, Chonghan; Zhuang, Jun; Wang, Haohan				Wang, Haohan/MEO-3902-2025; Zhuang, Jun/HGE-5882-2022; Jin, Haibo/JSK-2892-2023; ZHANG, Peiyan/JVZ-9661-2024						JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models								Arxiv											1	1;2024-06-26;https://www.arxiv.org/abs/2407.01599v1	arXiv:2407.01599			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 26 2024	2024	The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. 																																	2024-12-06	PPRN:90669397		
J	Tedeschi, Simone; Friedrich, Felix; Schramowski, Patrick; Kersting, Kristian; Navigli, Roberto; Nguyen, Huu; Li, Bo				Friedrich, Felix/LKM-6617-2024						ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming								Arxiv											3	3;2024-06-24;https://www.arxiv.org/abs/2404.08676v3| 2;2024-06-20;https://www.arxiv.org/abs/2404.08676v2| 1;2024-04-06;https://www.arxiv.org/abs/2404.08676v1	arXiv:2404.08676			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 24 2024	2024	When building Large Language Models (LLMs), it is paramount to bear safety in mind and protect them with guardrails. Indeed, LLMs should never generate content promoting or normalizing harmful, illegal, or unethical behavior that may contribute to harm to individuals or society. This principle applies to both normal and adversarial use. In response, we introduce ALERT, a large-scale benchmark to assess safety based on a novel fine-grained risk taxonomy. It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy. By subjecting LLMs to adversarial testing scenarios, ALERT aims to identify vulnerabilities, inform improvements, and enhance the overall safety of the language models. Furthermore, the fine-grained taxonomy enables researchers to perform an in-depth evaluation that also helps one to assess the alignment with various policies. In our experiments, we extensively evaluate 10 popular open- and closed-source LLMs and demonstrate that many of them still struggle to attain reasonable levels of safety.																																	2024-07-12	PPRN:88529392		
J	Gottschling, Nina M.; Antun, Vegard; Hansen, Anders C.; Adcock, Ben										The troublesome kernel - On hallucinations, no free lunches and the accuracy-stability trade-off in inverse problems								Arxiv											4	4;2024-06-18;https://www.arxiv.org/abs/2001.01258v4| 3;2023-12-14;https://www.arxiv.org/abs/2001.01258v3| 2;2020-01-05;https://www.arxiv.org/abs/2001.01258v1| 1;2020-01-05;https://www.arxiv.org/abs/2001.01258v1	arXiv:2001.01258			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Methods inspired by Artificial Intelligence (AI) are starting to fundamentally change computational science and engineering through breakthrough performances on challenging problems. However, reliability and trustworthiness of such techniques is a major concern. In inverse problems in imaging, the focus of this paper, there is increasing empirical evidence that methods may suffer from hallucinations, i.e., false, but realistic-looking artifacts; instability, i.e., sensitivity to perturbations in the data; and unpredictable generalization, i.e., excellent performance on some images, but significant deterioration on others. This paper provides a theoretical foundation for these phenomena. We give mathematical explanations for how and when such effects arise in arbitrary reconstruction methods, with several of our results taking the form of  'no free lunch' theorems. Specifically, we show that (i) methods that overperform on a single image can wrongly transfer details from one image to another, creating a hallucination, (ii) methods that overperform on two or more images can hallucinate or be unstable, (iii) optimizing the accuracy-stability trade-off is generally difficult, (iv) hallucinations and instabilities, if they occur, are not rare events, and may be encouraged by standard training, (v) it may be impossible to construct optimal reconstruction maps for certain problems. Our results trace these effects to the kernel of the forward operator whenever it is nontrivial, but also apply to the case when the forward operator is ill-conditioned. Based on these insights, our work aims to spur research into new ways to develop robust and reliable AI-based methods for inverse problems in imaging.																																	2024-07-10	PPRN:22593511		
J	Chen, Dong; Lin, Shaoxin; Zeng, Muhan; Zan, Daoguang; Wang, Jian-Gang; Cheshkov, Anton; Sun, Jun; Yu, Hao; Dong, Guoliang; Aliev, Artem; Wang, Jie; Cheng, Xiao; Liang, Guangtai; Ma, Yuchi; Bian, Pan; Xie, Tao; Wang, Qianxiang				Zan, Daoguang/KLY-4874-2024; Ma, Yuchi/C-8792-2018; Cheng, Si/JDW-6131-2023						CodeR: Issue Resolving with Multi-Agent and Task Graphs								Arxiv											3	3;2024-06-11;https://www.arxiv.org/abs/2406.01304v3| 2;2024-06-07;https://www.arxiv.org/abs/2406.01304v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01304v1	arXiv:2406.01304			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	GitHub issue resolving recently has attracted significant attention from academia and industry. SWE-bench is proposed to measure the performance in resolving issues. In this paper, we propose CodeR, which adopts a multi-agent framework and pre-defined task graphs to Repair & Resolve reported bugs and add new features within code Repository. On SWE-bench lite, CodeR is able to solve 28.33% of issues, when submitting only once for each issue. We examine the performance impact of each design of CodeR and offer insights to advance this research direction.																																	2024-07-11	PPRN:89132042		
J	Low, Guang Hao; Kliuchnikov, Vadym; Schaeffer, Luke				Schaeffer, Luke/LMN-4989-2024; Low, Guang Hao/HGE-8352-2022						Trading T gates for dirty qubits in state preparation and unitary synthesis								Arxiv											2	2;2024-06-11;https://www.arxiv.org/abs/1812.00954v2| 1;2018-12-03;https://www.arxiv.org/abs/1812.00954v1	arXiv:1812.00954			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	Efficient synthesis of arbitrary quantum states and unitaries from a universal fault-tolerant gate-set e.g. Clifford+T is a key subroutine in quantum computation. As large quantum algorithms feature many qubits that encode coherent quantum information but remain idle for parts of the computation, these should be used if it minimizes overall gate counts, especially that of the expensive T-gates. We present a quantum algorithm for preparing any dimension-N N pure quantum state specified by a list of N classical numbers, that realizes a trade-off between space and T-gates. Our scheme uses O(log (N/ϵ)) N/ϵ )) clean qubits and a tunable number of ∼ (λ λ log ( log N/ ϵ )) dirty qubits, to reduce the T-gate cost to O( N/ λ + λ log N/ϵ log log N/ϵ ). This trade-off is optimal up to logarithmic factors, proven through an unconditional gate counting lower bound, and is, in the best case, a quadratic improvement in T-count over prior ancillary-free approaches. We prove similar statements for unitary synthesis by reduction to state preparation. Underlying our constructions is a T–efficient circuit implementation of a quantum oracle for arbitrary classical data.																																	2024-07-04	PPRN:13508636		
J	Yang, Yiyuan; Jin, Ming; Wen, Haomin; Zhang, Chaoli; Liang, Yuxuan; Ma, Lintao; Wang, Yi; Liu, Chenghao; Yang, Bin; Xu, Zenglin; Bian, Jiang; Pan, Shirui; Wen, Qingsong				Liu, Chenghao/M-1202-2014; Wen, Qingsong/LTF-7625-2024; Liang, Yuxuan/KXR-3882-2024; Wang, Yi/D-5346-2018; Yang, Yiyuan/HJG-7778-2022; wen, Haomin/KLE-1789-2024; Xu, Zenglin/A-1639-2013; Jin, Ming/AAK-3665-2021						A Survey on Diffusion Models for Time Series and Spatio-Temporal Data								Arxiv											3	3;2024-06-11;https://www.arxiv.org/abs/2404.18886v3| 2;2024-05-31;https://www.arxiv.org/abs/2404.18886v2| 1;2024-04-29;https://www.arxiv.org/abs/2404.18886v1	arXiv:2404.18886			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	—The study of time series is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.																																	2024-07-02	PPRN:88696067		
J	Zhao, Qinlin; Wang, Jindong; Zhang, Yixuan; Jin, Yiqiao; Zhu, Kaijie; Chen, Hao; Xie, Xing				Jin, Yiqiao/ABD-3071-2022; Zhu, Kaijie/KHX-8423-2024; wang, jindong/ACD-8485-2022						CompeteAI: Understanding the Competition Dynamics in Large Language Model-based Agents								Arxiv											2	2;2024-06-07;https://www.arxiv.org/abs/2310.17512v2| 1;2023-10-26;https://www.arxiv.org/abs/2310.17512v1	arXiv:2310.17512			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most of the work has focused on cooperation and collaboration between agents, little work explores competition, , another important mechanism that promotes the development of society and economy. In this paper, we seek to examine the competition dynamics in LLM-based agents. We first propose a general framework for studying the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, the restaurant agents compete with each other to attract more customers, where competition encourages them to transform, such as cultivating new operating strategies. Simulation experiments reveal several interesting findings at the micro and macro levels, which align well with existing market and sociological theories. We hope that the framework and environment can be a promising testbed to study the competition that fosters understanding of society. Code is available at: https: //github.com/microsoft/competeai. .																																	2024-06-22	PPRN:85822214		
J	Ghandeharioun, Asma; Caciularu, Avi; Pearce, Adam; Dixon, Lucas; Geva, Mor				Dixon, Lucas/AFL-2608-2022; Caciularu, Avi/V-1760-2019; Geva, Mor/JJF-9095-2023						Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models								Arxiv											4	4;2024-06-06;https://www.arxiv.org/abs/2401.06102v4| 3;2024-05-30;https://www.arxiv.org/abs/2401.06102v3| 2;2024-01-12;https://www.arxiv.org/abs/2401.06102v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.06102v1	arXiv:2401.06102			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 06 2024	2024	Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.																																	2024-06-22	PPRN:87128230		
J	Heek, Jonathan; Hoogeboom, Emiel; Salimans, Tim										Multistep Consistency Models								Arxiv											2	2;2024-06-03;https://www.arxiv.org/abs/2403.06807v2| 1;2024-03-11;https://www.arxiv.org/abs/2403.06807v1	arXiv:2403.06807			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step. In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1 -step consistency model is a conventional consistency model whereas a ∞ -step consistency model is a diffusion model. Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 steps with consistency distillation, using simple losses without adversarial training. We also show that our method scales to a text -to -image diffusion model, generating samples that are close to the quality of the original model.																																	2024-06-22	PPRN:88104887		
J	Wang, Xiang; Zhang, Shiwei; Gao, Changxin; Wang, Jiayu; Zhou, Xiaoqiang; Zhang, Yingya; Yan, Luxin; Sang, Nong				Luxin, Yan/T-5852-2019						UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation								Arxiv											1	1;2024-06-03;https://www.arxiv.org/abs/2406.01188v1	arXiv:2406.01188			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	Recent diffusion -based human image animation techniques have demonstrated impressive success in synthesizing videos that faithfully follow a given reference identity and a sequence of desired movement poses. Despite this, there are still two limitations: i) an extra reference model is required to align the identity image with the main video branch, which significantly increases the optimization burden and model parameters; ii) the generated video is usually short in time (e.g., e.g ., 24 frames), hampering practical applications. To address these shortcomings, we present a UniAnimate framework to enable efficient and long-term human video generation. First, to reduce the optimization difficulty and ensure temporal coherence, we map the reference image along with the posture guidance and noise video into a common feature space by incorporating a unified video diffusion model. Second, we propose a unified noise input that supports random noised input as well as first frame conditioned input, which enhances the ability to generate long-term video. Finally, to further efficiently handle long sequences, we explore an alternative temporal modeling architecture based on state space model to replace the original computation -consuming temporal Transformer. Extensive experimental results indicate that UniAnimate achieves superior synthesis results over existing state-of-the-art counterparts in both quantitative and qualitative evaluations. Notably, UniAnimate can even generate highly consistent one -minute videos by iteratively employing the first frame conditioning strategy. Code and models will be publicly available. Project page: https://unianimate.github.io/.																																	2024-11-20	PPRN:89163380		
J	Sel, Bilgehan; Al-Tawaha, Ahmad; Khattar, Vanshaj; Jia, Ruoxi; Jin, Ming				Jin, Ming/AAK-3665-2021						Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models								Arxiv											3	3;2024-06-02;https://www.arxiv.org/abs/2308.10379v3| 2;2023-09-28;https://www.arxiv.org/abs/2308.10379v2| 1;2023-08-20;https://www.arxiv.org/abs/2308.10379v1	arXiv:2308.10379			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 02 2024	2024	Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. 																																	2024-06-22	PPRN:82027346		
J	Xiang, Chong; Wu, Tong; Zhong, Zexuan; Wagner, David; Chen, Danqi; Mittal, Prateek				Xiang, Chong/ADZ-8911-2022						Certifiably Robust RAG against Retrieval Corruption								Arxiv											1	1;2024-05-24;https://www.arxiv.org/abs/2405.15556v1	arXiv:2405.15556			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 24 2024	2024	Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. In this paper, we propose RobustRAG as the first defense framework against retrieval corruption attacks. The key insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM responses from each passage in isolation and then securely aggregate these isolated responses. To instantiate RobustRAG, we design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. Notably, RobustRAG can achieve certifiable robustness: we can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when the attacker has full knowledge of our defense and can arbitrarily inject a small number of malicious passages. We evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability across various tasks and datasets.																																	2024-06-08	PPRN:89010214		
J	Gao, Dawei; Li, Zitao; Pan, Xuchen; Kuang, Weirui; Ma, Zhijian; Qian, Bingchen; Wei, Fei; Zhang, Wenhao; Xie, Yuexiang; Chen, Daoyuan; Yao, Liuyi; Peng, Hongyi; Zhang, Zeyu; Zhu, Lin; Cheng, Chen; Shi, Hongzhu; Li, Yaliang; Ding, Bolin; Zhou, Jingren				Gao, Dawei/HBE-2722-2022; Zhou, Mingyuan/AAE-8717-2021; Li, Zitao/AAT-3918-2021						AgentScope: A Flexible yet Robust Multi-Agent Platform								Arxiv											2	2;2024-05-20;https://www.arxiv.org/abs/2402.14034v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.14034v1	arXiv:2402.14034			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 20 2024	2024	With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications. However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. The abundant syntactic tools, built-in agents and service functions, user-friendly interfaces for application demonstration and utility monitor, zero-code programming workstation, and automatic prompt tuning mechanism significantly lower the barriers to both development and deployment. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms. At the same time, it is also armed with system-level support for managing and utilizing multi-modal data, tools, and external knowledge. Additionally, we design an actor-based distribution framework, enabling easy conversion between local and distributed deployments and automatic parallel optimization without extra effort. With these features, AgentScope empowers developers to build applications that fully realize the potential of intelligent agents. 																																	2024-06-01	PPRN:87799168		
J	Yang, Greg; Simon, James B.; Bernstein, Jeremy										A Spectral Condition for Feature Learning								Arxiv											2	2;2024-05-14;https://www.arxiv.org/abs/2310.17813v2| 1;2023-10-26;https://www.arxiv.org/abs/2310.17813v1	arXiv:2310.17813			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 14 2024	2024	The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network’s internal representations evolve nontrivially at all widths, a process known as feature learning . Here, we show that feature lear ning is achieved by scaling the spectral norm of weight matrices and their updates like √fan-out / fan-in , in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of maximal update parametrization . We develop this spectral perspective and structure the text with the intent of providing the reader with a solid conceptual understanding of the scaling behavior of feature learning in neural networks.																																	2024-05-30	PPRN:85859049		
J	Yin, Lu; Wu, You; Zhang, Zhenyu; Hsieh, Cheng-Yu; Wang, Yaqing; Jia, Yiling; Li, Gen; Jaiswal, Ajay; Pechenizkiy, Mykola; Liang, Yi; Bendersky, Michael; Wang, Zhangyang; Liu, Shiwei				Jia, Yiling/GQI-1112-2022; Zhang, Zhenyu/AHD-3937-2022; Zhihua, Wang/AFO-5263-2022						Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity								Arxiv											3	3;2024-05-06;https://www.arxiv.org/abs/2310.05175v3| 2;2024-02-16;https://www.arxiv.org/abs/2310.05175v2| 1;2023-10-08;https://www.arxiv.org/abs/2310.05175v1	arXiv:2310.05175			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 06 2024	2024	Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. 																																	2024-05-28	PPRN:85583415		
J	Hyland, Stephanie L.; Bannur, Shruthi; Bouzid, Kenza; Castro, Daniel C.; Ranjit, Mercy; Schwaighofer, Anton; Perez-Garcia, Fernando; Salvatelli, Valentina; Srivastav, Shaury; Thieme, Anja; Codella, Noel; Lungren, Matthew P.; Wetscherek, Maria Teodora; Oktay, Ozan; Alvarez-Valle, Javier				Pérez-García, Fernando/ACR-8536-2022						MAIRA-1: A specialised large multimodal model for radiology report generation								Arxiv											3	3;2024-04-26;https://www.arxiv.org/abs/2311.13668v3| 2;2024-02-09;https://www.arxiv.org/abs/2311.13668v2| 1;2023-11-22;https://www.arxiv.org/abs/2311.13668v1	arXiv:2311.13668			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website: https://aka.ms/maira.																																	2024-05-05	PPRN:86274479		
J	Wang, Deng										The Self-Consistency of DESI Analysis and Comment on “Does DESI 2024 Confirm ΛCDM?”								Arxiv											1	1;2024-04-22;https://www.arxiv.org/abs/2404.13833v1	arXiv:2404.13833			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 22 2024	2024	We demonstrate that the constraints on the evolution of dark energy implemented by the DESI collaboration may be insufficient or incomplete using their own BAO data. Using large enough prior ranges for the present-day equation of state of dark energy ω0 and amplitude of dark energy evolution ωa , we obtain the complete 1 σ and 2 σ constraints ω0 = 1.04+0.91+2.00−1.00−1.90 and ωa = −7.4+3.8+6.8 −3.2−7.3 indicating a beyond 2 σ preference of quintessence-like dark energy today and an evidence of evolving dark energy at beyond 2 σ CL, respectively. Our results are different from ω0 = −0.55+0.39 −0.21 and the 2 σ upper limit ωa < −1.32 reported by the DESI collaboration [1]. Employing a data combination of cosmic microwave background, DESI BAO and type Ia supernova, we obtain the 1 σ , 2 σ and 3σ constraints ω0 = −0.707+0.089+0.18+0.24 −0.089−0.17−0.22 and ωa= −1.09+0.38+0.67+0.82 −0.31−0.72−1.00 , which reveals a ∼ 4 σ evidence of dynamical dark energy when the redshift z ≲ 0.1. We verify that the BAO data point from luminous red galaxies at the effective redshift zeff= 0.51 hardly affects the joint constraint from the data combination of cosmic microwave background, DESI BAO and type Ia supernova. We also point out the shortcomings and advantages of the binning method widely used in cosmological analyses.																																	2024-05-01	PPRN:88606324		
J	Bian, Ning; Han, Xianpei; Sun, Le; Lin, Hongyu; Lu, Yaojie; He, Ben; Jiang, Shanshan; Dong, Bin				Han, Xianpei/MTC-8266-2025; Lin, Hongyu/LXA-3658-2024; JIANG, SHAN/LNQ-9056-2024						ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models								Arxiv											3	3;2024-04-19;https://www.arxiv.org/abs/2303.16421v3| 2;2024-03-12;https://www.arxiv.org/abs/2303.16421v2| 1;2023-03-29;https://www.arxiv.org/abs/2303.16421v1	arXiv:2303.16421			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 19 2024	2024	Large language models (LLMs) have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point. In this paper, we specifically focus on ChatGPT, a widely used and easily accessible LLM, and ask the following questions: (1) Can ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question? (3) Is ChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage commonsense for answering questions? We conduct a series of experiments on 11 datasets to evaluate ChatGPT’s commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again. Experimental results show that: (1) ChatGPT can achieve good QA accuracies in commonsense tasks, while still struggling with certain domains of datasets. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question. These findings raise the need to explore improved mechanisms for effectively incorporating commonsense into LLMs like ChatGPT, such as better instruction following and commonsense guidance.																																	2024-04-29	PPRN:56471217		
J	Zhang, Haotian; You, Haoxuan; Dufter, Philipp; Zhang, Bowen; Chen, Chen; Fu, Tsu-Jui; Chen, Hong-You; Wang, William Yang; Chang, Shih-Fu; Gan, Zhe; Yang, Yinfei				Zhang, Haotian/CAH-0725-2022; Zhang, Bowen/CAG-9533-2022						Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models								Arxiv											1	1;2024-04-11;https://www.arxiv.org/abs/2404.07973v1	arXiv:2404.07973			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 11 2024	2024	While Ferret seamlessly integrates regional understanding into the Large Language Model (LLM) to facilitate its referring and grounding capability, it poses certain limitations: constrained by the pre-trained fixed visual encoder and failed to perform well on broader tasks. In this work, we unveil Ferret-v2, a significant upgrade to Ferret, with three key designs. (1) Any resolution grounding and referring: A flexible approach that effortlessly handles higher image resolution, improving the model's ability to process and understand images in greater detail. (2) Multi-granularity visual encoding: By integrating the additional DINOv2 encoder, the model learns better and diverse underlying contexts for global and fine-grained visual information. (3) A three-stage training paradigm: Besides image-caption alignment, an additional stage is proposed for high-resolution dense alignment before the final instruction tuning. Experiments show that Ferret-v2 provides substantial improvements over Ferret and other state-of-the-art methods, thanks to its high-resolution scaling and fine-grained visual processing.																																	2024-04-24	PPRN:88501226		
J	Zhang, Tianshu; Yue, Xiang; Li, Yifei; Sun, Huan										TableLlama: Towards Open Large Generalist Models for Tables								Arxiv											3	3;2024-04-04;https://www.arxiv.org/abs/2311.09206v3| 2;2024-03-21;https://www.arxiv.org/abs/2311.09206v2| 1;2023-11-15;https://www.arxiv.org/abs/2311.09206v1	arXiv:2311.09206			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of tablebased tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has taskspecific design. On 6 out-of-domain datasets, it achieves 5-44 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model’s generalizability. We open source our dataset and trained model to boost future work on developing open generalist models for tables.1																																	2024-04-19	PPRN:86174170		
J	Mueller, Luis; Galkin, Mikhail; Rampasek, Ladislav; Morris, Christopher										Attending to Graph Transformers								Arxiv											1	1;2024-03-28;https://www.arxiv.org/abs/2302.04181v3	arXiv:2302.04181			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 28 2024	2024	Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future work. 																																	2024-04-17	PPRN:86744373		
J	Wang, Xiaohan; Zhang, Yuhui; Zohar, Orr; Yeung-Levy, Serena				Zhang, Yuhui/IST-0722-2023						<italic>VideoAgent</italic>: Long-form Video Understanding with Large Language Model as Agent								Arxiv											1	1;2024-03-15;https://www.arxiv.org/abs/2403.10517v1	arXiv:2403.10517			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 15 2024	2024	Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.																																	2024-04-11	PPRN:88168434		
J	Cho, Jaemin; Hu, Yushi; Garg, Roopal; Anderson, Peter; Krishna, Ranjay; Baldridge, Jason; Bansal, Mohit; Pont-Tuset, Jordi; Wang, Su				Cho, Jae Min/HTO-8759-2023; Bansal, Mohit/Q-9105-2016; Hu, Yushi/MGV-6188-2025						DAVIDSONIAN SCENE GRAPH: IMPROVING RELIABILITY IN FINE-GRAINED EVALUA- TION FOR TEXT-TO-IMAGE GENERATION								Arxiv											3	3;2024-03-13;https://www.arxiv.org/abs/2310.18235v4| 2;2024-02-09;https://www.arxiv.org/abs/2310.18235v3| 1;2023-10-30;https://www.arxiv.org/abs/2310.18235v2	arXiv:2310.18235			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 13 2024	2024	Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and VQA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics, which is adaptable to any QG/A frameworks. DSG produces atomic and unique questions organized in dependency graphs, which (i) ensure appropriate semantic coverage and (ii) sidestep inconsistent answers. With extensive experimentation and human evaluation on a range of model configurations (LLM, VQA, and T2I), we empirically demonstrate that DSG addresses the challenges noted above. Finally, we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide range of fine-grained semantic categories with a balanced distribution. We release the DSG-1k prompts and the corresponding DSG questions.																																	2024-04-11	PPRN:85858610		
J	Fontana, Enrico; Herman, Dylan; Chakrabarti, Shouvanik; Kumar, Niraj; Yalovetzky, Romina; Heredge, Jamie; Sureshbabu, Shree Hari; Pistoia, Marco										The Adjoint Is All You Need: Characterizing Barren Plateaus in Quantum Ansätze								Arxiv											4	4;2024-03-06;https://www.arxiv.org/abs/2309.07902v4| 3;2023-09-21;https://www.arxiv.org/abs/2309.07902v3| 2;2023-09-15;https://www.arxiv.org/abs/2309.07902v2| 1;2023-09-14;https://www.arxiv.org/abs/2309.07902v1	arXiv:2309.07902			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 06 2024	2024	Using tools from the representation theory of compact Lie groups, we formulate a theory of Barren Plateaus (BPs) for parameterized quantum circuits whose observables lie in their dynamical Lie algebra (DLA), a setting that we term Lie algebra Supported Ansatz (LASA). A large variety of commonly used ansatze such as the Hamiltonian Variational Ansatz, Quantum Alternating Operator Ansatz, and many equivariant quantum neural networks are LASAs. In particular, our theory provides, for the first time, the ability to compute the variance of the gradient of the cost function of the quantum compound ansatz. We rigorously prove that, for LASA, the variance of the gradient of the cost function, for a 2-design of the dynamical Lie group, scales inversely with the dimension of the DLA, which agrees with existing numerical observations. In addition, to motivate the applicability of our results for 2-designs to practical settings, we show that rapid mixing occurs for LASAs with polynomial DLA. Lastly, we include potential extensions for handling cases when the observable lies outside of the DLA and the implications of our results.																																	2024-04-03	PPRN:85017432		
J	Wang, Junjie; Huang, Yuchao; Chen, Chunyang; Liu, Zhe; Wang, Song; Wang, Qing										Software Testing with Large Language Models: Survey, Landscape, and Vision								Arxiv											3	3;2024-03-04;https://www.arxiv.org/abs/2307.07221v3| 2;2024-01-05;https://www.arxiv.org/abs/2307.07221v2| 1;2023-07-14;https://www.arxiv.org/abs/2307.07221v1	arXiv:2307.07221			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 04 2024	2024	Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.																																	2024-03-30	PPRN:73930008		
J	Kramar, Janos; Lieberum, Tom; Shah, Rohin; Nanda, Neel										AtP*: An efficient and scalable method for localizing LLM behaviour to components								Arxiv											1	1;2024-03-01;https://www.arxiv.org/abs/2403.00745v1	arXiv:2403.00745			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 01 2024	2024	Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.																																	2024-03-28	PPRN:88004252		
J	Yang, June Yong; Kim, Byeongwook; Bae, Jeongin; Kwon, Beomseok; Park, Gunho; Yang, Eunho; Kwon, Se Jung; Lee, Dongsoo										No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18096v1	arXiv:2402.18096			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.																																	2024-03-28	PPRN:87989318		
J	Geiping, Jonas; Stein, Alex; Shu, Manli; Saifullah, Khalid; Wen, Yuxin; Goldstein, Tom				Wen, Yuxin/AAA-4882-2019						COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING								Arxiv											1	1;2024-02-21;https://www.arxiv.org/abs/2402.14020v1	arXiv:2402.14020			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.																																	2024-03-20	PPRN:87793951		
J	Xu, Yudong; Li, Wenhao; Vaezipoor, Pashootan; Sanner, Scott; Khalil, Elias B.				Xu, Yudong/S-4321-2019; Li, Wenhao/IQW-3235-2023						LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations								Arxiv											2	2;2024-02-14;https://www.arxiv.org/abs/2305.18354v2| 1;2023-05-26;https://www.arxiv.org/abs/2305.18354v1	arXiv:2305.18354			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 14 2024	2024	Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to "reason" perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability. 																																	2024-03-09	PPRN:72763441		
J	Gruver, Nate; Sriram, Anuroop; Madotto, Andrea; Wilson, Andrew Gordon; Zitnick, C.Lawrence; Ulissi, Zachary				Sriram, Anuroop/AAF-1926-2021						Fine-Tuned Language Models Generate Stable Inorganic Materials as Text								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2402.04379v1	arXiv:2402.04379			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text -encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold -standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting’s inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text -conditional generation. Finally, we show that language models’ ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well -suited for atomistic data.																																	2024-05-25	PPRN:87561823		
J	Balaguer, Angels; Benara, Vinamra; Cunha, Renato; Estevao, Roberto; Hendry, Todd; Holstein, Daniel; Marsman, Jennifer; Mecklenburg, Nick; Malvar, Sara; Nunes, Leonardo O.; Padilha, Rafael; Sharp, Morris; Silva, Bruno; Sharma, Swati; Aski, Vijay; Chandra, Ranveer										RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture								Arxiv											3	3;2024-01-30;https://www.arxiv.org/abs/2401.08406v3| 2;2024-01-17;https://www.arxiv.org/abs/2401.08406v2| 1;2024-01-16;https://www.arxiv.org/abs/2401.08406v1	arXiv:2401.08406			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jan 30 2024	2024	There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.																																	2024-05-25	PPRN:87190992		
J	Maini, Pratyush; Seto, Skyler; Bai, He; Grangier, David; Zhang, Yizhe; Jaitly, Navdeep										Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling								Arxiv											1	1;2024-01-29;https://www.arxiv.org/abs/2401.16380v1	arXiv:2401.16380			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training (WRAP) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as “like Wikipedia” or in “question-answer format” to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by ∼ 3×. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher ‘quality’ than web-scraped data.																																	2024-02-15	PPRN:87393190		
J	Malladi, Sadhika; Gao, Tianyu; Nichani, Eshaan; Damian, Alex; Lee, Jason D.; Chen, Danqi; Arora, Sanjeev										Fine-Tuning Language Models with Just Forward Passes								Arxiv											3	3;2024-01-11;https://www.arxiv.org/abs/2305.17333v3| 2;2023-10-31;https://www.arxiv.org/abs/2305.17333v2| 1;2023-05-27;https://www.arxiv.org/abs/2305.17333v1	arXiv:2305.17333			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.1																																	2024-01-26	PPRN:72755281		
J	Yang, Yuhao; Wang, Yue; Li, Dongxu; Luo, Ziyang; Chen, Bei; Huang, Chao; Li, Junnan										Aria-UI: Visual Grounding for GUI Instructions								Arxiv											1	1;2024-12-20;https://www.arxiv.org/abs/2412.16256v1	arXiv:2412.16256			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 20 2024	2024	Digital agents for automating tasks across different platforms by directly manipulating the GUIs are increasingly important. For these agents, grounding from language instructions to target elements remains a significant challenge due to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI, a large multimodal model specifically designed for GUI grounding. Aria-UI adopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt to heterogeneous planning instructions, we propose a scalable data pipeline that synthesizes diverse and high-quality instruction samples for grounding. To handle dynamic contexts in task performing, Aria-UI incorporates textual and text-image interleaved action histories, enabling robust context-aware reasoning for grounding. Aria-UI sets new state-of-the-art results across offline and online agent benchmarks, outperforming both vision-only and AXTree-reliant baselines. 																																	2025-01-24	PPRN:120130057		
J	Cheng, Jeffrey; Van Durme, Benjamin										Compressed Chain of Thought: Efficient Reasoning Through Dense Representations								Arxiv											1	1;2024-12-17;https://www.arxiv.org/abs/2412.13171v1	arXiv:2412.13171			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 17 2024	2024	Chain-of-thought (CoT) decoding enables language models to improve reasoning performance at the cost of high generation latency in decoding. Recent proposals have explored variants of contemplation tokens, a term we introduce that refers to special tokens used during inference to allow for extra computation. Prior work has considered fixed-length sequences drawn from a discrete set of embeddings as contemplation tokens. Here we propose Compressed Chain-of-Thought (CCoT), a framework to generate contentful and continuous contemplation tokens of variable sequence length. The generated contemplation tokens are compressed representations of explicit reasoning chains, and our method can be applied to off-the- shelf decoder language models. Through experiments, we illustrate how CCoT enables additional reasoning over dense contentful representations to achieve corresponding improvements in accuracy. Moreover, the reasoning improvements can be adaptively modified on demand by controlling the number of contemplation tokens generated.																																	2025-01-24	PPRN:120007811		
J	Lessa, Leonardo A.; Ma, Ruochen; Zhang, Jian-Hao; Bi, Zhen; Cheng, Meng; Wang, Chong				Bi, Zhen/IAQ-8089-2023						Strong-to-Weak Spontaneous Symmetry Breaking in Mixed Quantum States								Arxiv											3	3;2024-11-18;https://www.arxiv.org/abs/2405.03639v3| 2;2024-07-03;https://www.arxiv.org/abs/2405.03639v2| 1;2024-05-06;https://www.arxiv.org/abs/2405.03639v1	arXiv:2405.03639			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 18 2024	2024	Symmetry in mixed quantum states can manifest in two distinct forms: strong symmetry, where each individual pure state in the quantum ensemble is symmetric with the same charge, and weak symmetry, which applies only to the entire ensemble. This paper explores a novel type of spontaneous symmetry breaking (SSB) where a strong symmetry is broken to a weak one. While the SSB of a weak symmetry is measured by the long-ranged two-point correlation function, the strong-to- weak SSB (SW-SSB) is measured by the fidelity correlator. We prove that SW-SSB is a universal property of mixed-state quantum phases, in the sense that the phenomenon of SW-SSB is robust against symmetric low-depth local quantum channels. We also show that the symmetry breaking is “spontaneous” in the sense that the effect of a local symmetry-breaking measurement cannot be recovered locally. We argue that a thermal state at a nonzero temperature in the canonical ensemble (with fixed symmetry charge) should have spontaneously broken strong symmetry. Additionally, we study non-thermal scenarios where decoherence induces SW-SSB, leading to phase transitions described by classical statistical models with bond randomness. In particular, the SW-SSB transition of a decohered Ising model can be viewed as the “ungauged” version of the celebrated toric code decodability transition. We confirm that, in the decohered Ising model, the SW-SSB transition defined by the fidelity correlator is the only physical transition in terms of channel recoverability. We also comment on other (inequivalent) definitions of SW-SSB, through correlation functions with higher Renyi indices.																																	2024-12-28	PPRN:88790811		
J	Torne, Marcel; Simeonov, Anthony; Li, Zechu; Chan, April; Chen, Tao; Gupta, Abhishek; Agrawal, Pulkit				chen, tao/LZH-0357-2025						Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation								Arxiv											1	1;2024-10-29;https://www.arxiv.org/abs/2403.03949v2	arXiv:2403.03949			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 29 2024	2024	Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection.																																	2024-11-30	PPRN:118902963		
J	Wang, Fei; Wan, Xingchen; Sun, Ruoxi; Chen, Jiefeng; Arik, Sercan O				Chen, Jiefeng/AAU-7768-2020; Wan, Xingchen/AAE-2146-2022						Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models								Arxiv											1	1;2024-10-09;https://www.arxiv.org/abs/2410.07176v1	arXiv:2410.07176			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 09 2024	2024	Retrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address the limitations of large language models (LLMs), can be undermined by imperfect retrieval, which may introduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies have rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval attribute and propagate, and how potential conflicts arise between the LLMs' internal knowledge and external sources. We find that imperfect retrieval augmentation might be inevitable and quite harmful, through controlled analysis under realistic conditions. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach that adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments using Gemini and Claude demonstrate that Astute RAG significantly outperforms previous robustness-enhanced RAG methods. Notably, Astute RAG is the only approach that matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further analysis reveals that Astute RAG effectively resolves knowledge conflicts, improving the reliability and trustworthiness of RAG systems.																																	2024-10-28	PPRN:105753586		
J	Karazija, Laurynas; Laina, Iro; Vedaldi, Andrea; Rupprecht, Christian				Rupprecht, Christian/ABF-7744-2021						Diffusion Models for Open-Vocabulary Segmentation								Arxiv											2	2;2024-09-30;https://www.arxiv.org/abs/2306.09316v2| 1;2023-06-15;https://www.arxiv.org/abs/2306.09316v1	arXiv:2306.09316			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 30 2024	2024	Open-vocabulary segmentation is the task of segmenting anything that can be named in an image. Recently, large-scale vision-language modelling has led to significant advances in open-vocabulary segmentation, but at the cost of gargantuan and increasing training and annotation efforts. Hence, we ask if it is possible to use existing foundation models to synthesise on-demand efficient segmentation algorithms for specific class sets, making them applicable in an open-vocabulary setting without the need to collect further data, annotations or perform training. To that end, we present OVDiff, a novel method that leverages generative text-to-image diffusion models for unsupervised open-vocabulary segmentation. OVDiff synthesises support image sets for arbitrary textual categories, creating for each a set of prototypes representative of both the category and its surrounding context (background). It relies solely on pre-trained components and outputs the synthesised segmenter directly, without training. Our approach shows strong performance on a range of benchmarks, obtaining a lead of more than 5% over prior work on PASCAL VOC.																																	2024-10-10	PPRN:73355785		
J	Bahmani, Sherwin; Skorokhodov, Ivan; Siarohin, Aliaksandr; Menapace, Willi; Qian, Guocheng; Vasilkovsky, Michael; Lee, Hsin-Ying; Wang, Chaoyang; Zou, Jiaxu; Tagliasacchi, Andrea; Lindell, David B.; Tulyakov, Sergey				Qian, Guocheng/AAW-8713-2021; zhaoyang, wang/AAI-1593-2019; Lindell, David/JFT-0746-2023; Войнов, Олег/KUF-2109-2024; Lee, HsinYing/B-9716-2009; Menapace, Willi/JVY-9690-2024						VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control								Arxiv											2	2;2024-07-20;https://www.arxiv.org/abs/2407.12781v2| 1;2024-07-17;https://www.arxiv.org/abs/2407.12781v1	arXiv:2407.12781			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 20 2024	2024	Modern text-to-video synthesis models demonstrate coherent, photorealistic generation of complex videos from a text description. However, most existing models lack fine-grained control over camera movement, which is critical for downstream applications related to content creation, visual effects, and 3D vision. Recently, new methods demonstrate the ability to generate videos with controllable camera poses these techniques leverage pre-trained U-Net-based diffusion models that explicitly disentangle spatial and temporal generation. Still, no existing approach enables camera control for new, transformer-based video diffusion models that process spatial and temporal information jointly. Here, we propose to tame video transformers for 3D camera control using a ControlNet-like conditioning mechanism that incorporates spatiotemporal camera embeddings based on Plucker coordinates. The approach demonstrates state-of-the-art performance for controllable video generation after fine-tuning on the RealEstate10K dataset. To the best of our knowledge, our work is the first to enable camera control for transformer-based video diffusion models.																																	2024-07-28	PPRN:90868538		
J	Jiang, Ting; Song, Minghui; Zhang, Zihan; Huang, Haizhen; Deng, Weiwei; Sun, Feng; Zhang, Qi; Wang, Deqing; Zhuang, Fuzhen				Deng, Weiwei/I-4266-2012; Zhang, Zihan/Y-7713-2018; Wang, Deqing/E-4845-2013						E5-V: Universal Embeddings with Multimodal Large Language Models								Arxiv											1	1;2024-07-17;https://www.arxiv.org/abs/2407.12580v1	arXiv:2407.12580			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 17 2024	2024	Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored. In this work, we introduce a new framework, E5-V, designed to adapt MLLMs for achieving universal multimodal embeddings. Our findings highlight the significant potential of MLLMs in representing multimodal inputs compared to previous approaches. By leveraging MLLMs with prompts, E5-V effectively bridges the modality gap between different types of inputs, demonstrating strong performance in multimodal embeddings even without fine-tuning. We propose a single modality training approach for E5-V, where the model is trained exclusively on text pairs. This method demonstrates significant improvements over traditional multimodal training on image-text pairs, while reducing training costs by approximately 95%. Additionally, this approach eliminates the need for costly multimodal training data collection. Extensive experiments across four types of tasks demonstrate the effectiveness of E5-V. As a universal multimodal model, E5-V not only achieves but often surpasses state-of-the-art performance in each task, despite being trained on a single modality. [GRAPHICS].																																	2024-07-26	PPRN:90871905		
J	Loschnauer, C.M.; Mosca Toba, J.; Hughes, A.C.; King, S.A.; Weber, M.A.; Srinivas, R.; Matt, R.; Nourshargh, R.; Allcock, D.T.C.; Ballance, C.J.; Matthiesen, C.; Malinowski, M.; Harty, T.P.										Scalable, high-fidelity all-electronic control of trapped-ion qubits								Arxiv											1	1;2024-07-10;https://www.arxiv.org/abs/2407.07694v1	arXiv:2407.07694			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 10 2024	2024	The central challenge of quantum computing is implementing high-fidelity quantum gates at scale. However, many existing approaches to qubit control suffer from a scale-performance trade-off, impeding progress towards the creation of useful devices. Here, we present a vision for an electronically controlled trapped-ion quantum computer that alleviates this bottleneck. Our architecture utilizes shared current-carrying traces and local tuning electrodes in a microfabricated chip to perform quantum gates with low noise and crosstalk regardless of device size. To verify our approach, we experimentally demonstrate low-noise site-selective single- and two-qubit gates in a seven-zone ion trap that can control up to 10 qubits. We implement electronic single-qubit gates with 99.99916(7)% fidelity, and demonstrate consistent performance with low crosstalk across the device. We also electronically generate two-qubit maximally entangled states with 99.97(1)% fidelity and long-term stable performance over continuous system operation. These state-of-the-art results validate the path to directly scaling these techniques to large-scale quantum computers based on electronically controlled trapped-ion qubits.																																	2024-07-21	PPRN:90760165		
J	Mireshghallah, Niloofar; Kim, Hyunwoo; Zhou, Xuhui; Tsvetkov, Yulia; Sap, Maarten; Shokri, Reza; Choi, Yejin										Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory								Arxiv											2	2;2024-06-28;https://www.arxiv.org/abs/2310.17884v2| 1;2023-10-27;https://www.arxiv.org/abs/2310.17884v1	arXiv:2310.17884			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 28 2024	2024	Existing efforts on quantifying privacy implications for large language models (LLMs) solely focus on measuring leakage of training data. In this work, we shed light on the often -overlooked interactive settings where an LLM receives information from multiple sources at inference time and generates an output to be shared with other entities, creating the potential of exposing sensitive input data in inappropriate contexts. In these scenarios, humans naturally uphold privacy by choosing whether or not to disclose information depending on the context. We ask the question “ Can LLMs demonstrate an equivalent discernment and reasoning capability when considering privacy in context?” ” We propose C ONF AI DE , a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction -tuned LLMs. C ONF AI DE consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory of mind capabilities. Our experiments show that even commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively, highlighting the urgent need for a new direction of privacy -preserving approaches as we demonstrate a larger underlying problem stemmed in the models’ lack of reasoning capabilities.																																	2024-07-18	PPRN:85860154		
J	Wang, Xin; Chen, Hong; Tang, Si'ao; Wu, Zihao; Zhu, Wenwu				Zhu, Wenwu/C-5025-2018						Disentangled Representation Learning								Arxiv											3	3;2024-06-26;https://www.arxiv.org/abs/2211.11695v4| 2;2024-05-02;https://www.arxiv.org/abs/2211.11695v3| 1;2023-08-16;https://www.arxiv.org/abs/2211.11695v2	arXiv:2211.11695			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 26 2024	2024	Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, and data mining. In this article, we comprehensively investigate DRL from various aspects including motivations, definitions, methodologies, evaluations, applications, and model designs. We first present two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition for disentangled representation learning. We further categorize the methodologies for DRL into four groups from the following perspectives, the model type, representation structure, supervision signal, and independence assumption. We also analyze principles to design different DRL models that may benefit different tasks in practical applications. Finally, we point out challenges in DRL as well as potential research directions deserving future investigations. We believe this work may provide insights for promoting the DRL research in the community.																																	2024-07-17	PPRN:78329466		
J	Wang, Zirui; Xia, Mengzhou; He, Luxi; Chen, Howard; Liu, Yitao; Zhu, Richard; Liang, Kaiqu; Wu, Xindi; Liu, Haotian; Malladi, Sadhika; Chevalier, Alexis; Arora, Sanjeev; Chen, Danqi				Wang, Zirui/HDO-7058-2022; LIU, Haotian/T-8010-2019; He, Luxi/KBB-8984-2024						CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs								Arxiv											1	1;2024-06-26;https://www.arxiv.org/abs/2406.18521v1	arXiv:2406.18521			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 26 2024	2024	Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress.																																	2024-07-15	PPRN:89902419		
J	Mbeng, G. B.; Russomanno, A.; Santoro, G. E.				Russomanno, Angelo/LTZ-1027-2024; Santoro, Giuseppe Ernesto/H-2306-2012						The quantum Ising chain for beginners								Arxiv											2	2;2024-06-19;https://www.arxiv.org/abs/2009.09208v2| 1;2020-09-19;https://www.arxiv.org/abs/2009.09208v1	arXiv:2009.09208			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 19 2024	2024	We present here various techniques to work with clean and disordered quantum Ising chains, for the benefit of students and non -experts. Starting from the Jordan-Wigner transformation, which maps spin -1/2 systems into fermionic ones, we review some of the basic approaches to deal with the superconducting correlations that naturally emerge in this context. In particular, we analyze the form of the ground state and excitations of the model, relating them to the symmetry -breaking physics, and illustrate aspects connected to calculating dynamical quantities, thermal averages, correlation functions, and entanglement entropy. A few problems provide simple applications of the techniques.																																	2024-07-06	PPRN:13153260		
J	Li, Xianhang; Tu, Haoqin; Hui, Mude; Wang, Zeyu; Zhao, Bingchen; Xiao, Junfei; Ren, Sucheng; Mei, Jieru; Liu, Qing; Zheng, Huangjie; Zhou, Yuyin; Xie, Cihang				Zheng, Huangjie/AAD-9157-2022						What If We Recaption Billions of Web Images with LLaMA-3?								Arxiv											1	1;2024-06-18;https://www.arxiv.org/abs/2406.08478v2	arXiv:2406.08478			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and textit{open-sourced} LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. Our project page is https://www.haqtu.me/Recap-Datacomp-1B/																																	2025-08-07	PPRN:123164552		
J	Manole, Tudor; Balakrishnan, Sivaraman; Niles-Weed, Jonathan; Wasserman, Larry										Plugin Estimation of Smooth Optimal Transport Maps								Arxiv											1	1;2024-06-16;https://www.arxiv.org/abs/2107.12364v3	arXiv:2107.12364			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 16 2024	2024	We analyze a number of natural estimators for the optimal transport map between two distributions and show that they are minimax optimal. We adopt the plugin approach: our estimators are simply optimal couplings between measures derived from our observations, appropriately extended so that they define functions on Rd. When the underlying map is assumed to be Lipschitz, we show that computing the optimal coupling between the empirical measures, and extending it using linear smoothers, already gives a minimax optimal estimator. When the underlying map enjoys higher regularity, we show that the optimal coupling between appropriate nonparametric density estimates yields faster rates. Our work also provides new bounds on the risk of corresponding plugin estimators for the quadratic Wasserstein distance, and we show how this problem relates to that of estimating optimal transport maps using stability arguments for smooth and strongly convex Brenier potentials. As an application of our results, we derive central limit theorems for plugin estimators of the squared Wasserstein distance, which are centered at their population counterpart when the underlying distributions have sufficiently smooth densities. In contrast to known central limit theorems for empirical estimators, this result easily lends itself to statistical inference for the quadratic Wasserstein distance.																																	2024-07-04	PPRN:89352221		
J	van der Weij, Teun; Hofstaetter, Felix; Jaffe, Ollie; Brown, Samuel F.; Ward, Francis Rhys										AI Sandbagging: Language Models can Strategically Underperform on Evaluations								Arxiv											2	2;2024-06-14;https://www.arxiv.org/abs/2406.07358v3| 1;2024-06-12;https://www.arxiv.org/abs/2406.07358v2	arXiv:2406.07358			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 14 2024	2024	Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI’s actual capability. These conflicting interests lead to the problem of sandbagging – which we define as strategic underperformance on an evaluation. . In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high -quality, held -out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted, or password -locked, to target specific scores on a capability evaluation. Even more, we found that a capable password -locked model (Llama 3 70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.																																	2024-07-04	PPRN:89289413		
J	Li, Shiyao; Ning, Xuefei; Wang, Luning; Liu, Tengxuan; Shi, Xiangsheng; Yan, Shengen; Dai, Guohao; Yang, Huazhong; Wang, Yu				Wang, Luning/B-2491-2012; Li, Shiyao/OYE-4903-2025; WANG, Yu/B-7985-2011; Zheng, Shuangjia/JXR-6276-2024						Evaluating Quantized Large Language Models								Arxiv											2	2;2024-06-06;https://www.arxiv.org/abs/2402.18158v2| 1;2024-02-28;https://www.arxiv.org/abs/2402.18158v1	arXiv:2402.18158			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their applicability. Based on the extensive experiments, we systematically summarize the effect of quantization, provide recommendations to apply quantization techniques, and point out future directions.																																	2024-06-22	PPRN:87991604		
J	Rathore, Pratik; Lei, Weimu; Frangella, Zachary; Lu, Lu; Udell, Madeleine										Challenges in Training PINNs: A Loss Landscape Perspective								Arxiv											2	2;2024-06-03;https://www.arxiv.org/abs/2402.01868v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01868v1	arXiv:2402.01868			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations.																																	2024-06-22	PPRN:87522952		
J	Lee, Kwonjoon; Chang, Huiwen; Jiang, Lu; Zhang, Han; Tu, Zhuowen; Liu, Ce				Chang, Hui-Wen/C-9058-2017						ViTGAN: Training GANs with Vision Transformers								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2107.04589v2| 1;2021-07-09;https://www.arxiv.org/abs/2107.04589v1	arXiv:2107.04589			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to facilitate convergence. Empirically, our approach, named ViTGAN, , achieves comparable performance to the leading CNNbased GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom. Our code is available online1. 1 .																																	2024-06-16	PPRN:11840817		
J	Tsai, Yun-Da; Liu, Mingjie; Ren, Haoxing				Tsai, YunDa/MVU-2214-2025						RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language Models								Arxiv											3	3;2024-05-20;https://www.arxiv.org/abs/2311.16543v3| 2;2024-02-07;https://www.arxiv.org/abs/2311.16543v2| 1;2023-11-28;https://www.arxiv.org/abs/2311.16543v1	arXiv:2311.16543			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 20 2024	2024	This paper presents RTLFixer, , a novel framework enabling automatic syntax errors fixing for Verilog code with Large Language Models (LLMs). Despite LLM’s promising capabilities, our analysis indicates that approximately 55% of errors in LLM-generated Verilog are syntax-related, leading to compilation failures. To tackle this issue, we introduce a novel debugging framework that employs Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act as autonomous agents in interactively debugging the code with feedback. This framework demonstrates exceptional proficiency in resolving syntax errors, successfully correcting about 98.5% of compilation errors in our debugging dataset, comprising 212 erroneous implementations derived from the VerilogEval benchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates in the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively. The source code and benchmark are available at https://github.com/NVlabs/RTLFixer.																																	2024-08-23	PPRN:86309427		
J	Sun, Chuanneng; Huang, Songjun; Pompili, Dario				Sun, Chuanneng/AEM-3240-2022						LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions								Arxiv											1	1;2024-05-17;https://www.arxiv.org/abs/2405.11106v1	arXiv:2405.11106			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 17 2024	2024	In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.																																	2024-08-23	PPRN:91459686		
J	Xie, Yiming; Jampani, Varun; Zhong, Lei; Sun, Deqing; Jiang, Huaizu				Jiang, Huaizu/AAE-5876-2022; Jain, Varun/HHN-1250-2022						OmniControl: Control Any Joint at Any Time for Human Motion Generation								Arxiv											2	2;2024-04-14;https://www.arxiv.org/abs/2310.08580v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08580v1	arXiv:2310.08580			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 14 2024	2024	We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text -conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT -ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints. Project page: https://neu-vi.github.io/omnicontrol/.																																	2024-04-25	PPRN:85604424		
J	Zhang, Jian-Hao; Qi, Yang; Bi, Zhen				Bi, Zhen/IAQ-8089-2023; Qi, Yang/V-1562-2018; Zhang, Jianhao/ABB-8327-2020						Strange Correlation Function for Average Symmetry-Protected Topological Phases								Arxiv											1	1;2024-04-09;https://www.arxiv.org/abs/2210.17485v2	arXiv:2210.17485			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 09 2024	2024	Average symmetry-protected topological (ASPT) phase is a generalization of symmetry-protected topological phases to disordered systems or open quantum systems. We devise a “strange correlator” in one and two dimensions to detect nontrivial ASPT states. We demonstrate that for a nontrivial ASPT phase this strange correlator exhibits long-range or power-law behavior. We explore the connection between the strange correlators and correlation functions in two-dimensional loop models with quantum corrections, leading to the exact scaling exponents of the strange correlators.																																	2024-05-22	PPRN:88476220		
J	Feng, Duanyu; Qin, Bowen; Huang, Chen; Zhang, Zheng; Lei, Wenqiang				Feng, Duanyu/OML-4880-2025						Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective								Arxiv											1	1;2024-04-06;https://www.arxiv.org/abs/2404.04626v1	arXiv:2404.04626			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 06 2024	2024	Direct Preference Optimization (DPO), which derives reward signals directly from pairwise preference data, has shown its effectiveness on aligning Large Language Models (LLMs) with human preferences. Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the SFT's effectiveness and its hindrance to the learning capacity towards human-preferred responses, leading to less satisfactory performance. To overcome those limitations, the theoretical understanding of DPO are indispensable but still lacking. To this end, we take a step towards theoretically analyzing and understanding the limitations of DPO. Specifically, we provide an analytical framework using the field theory to analyze the optimization process of DPO. By analyzing the gradient vector field of the DPO loss function, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This provides theoretical insights for understanding the limitations of DPO discovered in the related research experiments, thereby setting the foundation for its improvement.																																	2024-04-21	PPRN:88441574		
J	Tian, Changyao; Zhu, Xizhou; Xiong, Yuwen; Wang, Weiyun; Chen, Zhe; Wang, Wenhai; Chen, Yuntao; Lu, Lewei; Lu, Tong; Zhou, Jie; Li, Hongsheng; Qiao, Yu; Dai, Jifeng				Qiao, Yu/ABD-5787-2021; Xiong, Yuwen/JFJ-4367-2023; Dai, Jifeng/HGU-8741-2022; Li, Hongsheng/AES-5328-2022; Wang, Wen-Jing/HOH-7164-2023						MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer								Arxiv											2	2;2024-04-02;https://www.arxiv.org/abs/2401.10208v2| 1;2024-01-18;https://www.arxiv.org/abs/2401.10208v1	arXiv:2401.10208			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions.																																	2024-04-18	PPRN:87222846		
J	Gerstgrasser, Matthias; Schaeffer, Rylan; Dey, Apratim; Rafailov, Rafael; Pai, Dhruv; Sleight, Henry; Hughes, John; Korbak, Tomasz; Agrawal, Rajashree; Gromov, Andrey; Roberts, Daniel A.; Yang, Diyi; Donoho, David L.; Koyejo, Sanmi				Gerstgrasser, Matthias/HIR-4233-2022; Dey, Apratim/ODJ-9214-2025						Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2404.01413v1	arXiv:2404.01413			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this result by proving that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations. We next empirically test whether accumulating data similarly prevents model collapse by pretraining sequences of language models on text corpora. We confirm that replacing data does indeed cause model collapse, then demonstrate that accumulating data prevents model collapse; these results hold across a range of model sizes, architectures and hyperparameters. We further show that similar results hold for other deep generative models on real data: diffusion models for molecule generation and variational autoencoders for image generation. Our work provides consistent theoretical and empirical evidence that data accumulation mitigates model collapse.																																	2024-04-18	PPRN:88378054		
J	Yu, Fei; Gao, Anningzhe; Wang, Benyou				Wang, Benyou/Y-5146-2019						OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning								Arxiv											2	2;2024-04-01;https://www.arxiv.org/abs/2311.09724v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09724v1	arXiv:2311.09724			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a stepby-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a value estimation problem in planning. Inspired by the findings that outcome supervision for guided decoding essentially acts as a value model, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM eliminates the need for labor-intensive annotations of step-level correctness, thereby significantly enhancing its scalability. Our experiments on two multi -step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training value models for multi -step reasoning tasks and provide theoretical justification for its advantage in value estimation for guided decoding.																																	2024-04-17	PPRN:86176905		
J	Wu, Yue; Tang, Xuan; Mitchell, Tom M.; Li, Yuanzhi				Wu, Yue/LFR-8488-2024						SmartPlay: A Benchmark for LLMs as Intelligent Agents								Arxiv											4	4;2024-03-17;https://www.arxiv.org/abs/2310.01557v5| 3;2024-03-13;https://www.arxiv.org/abs/2310.01557v4| 2;2023-12-08;https://www.arxiv.org/abs/2310.01557v3| 1;2023-10-04;https://www.arxiv.org/abs/2310.01557v2	arXiv:2310.01557			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 17 2024	2024	Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies.																																	2024-04-11	PPRN:85397870		
J	Zhang, Yue; Cui, Leyang; Bi, Wei; Shi, Shuming				Zhang, Jake/JFK-9874-2023						Alleviating Hallucinations of Large Language Models through Induced Hallucinations								Arxiv											2	2;2024-03-11;https://www.arxiv.org/abs/2312.15710v2| 1;2023-12-25;https://www.arxiv.org/abs/2312.15710v1	arXiv:2312.15710			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 11 2024	2024	Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various model sizes and families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively.																																	2024-04-08	PPRN:86821336		
J	Li, Junyao; Silverman, John D.; Shen, Yue; Volonteri, Marta; Jahnke, Knud; Zhuang, Ming-Yang; Scoggins, Matthew T.; Ding, Xuheng; Harikane, Yuichi; Onoue, Masafusa; Tanaka, Takumi S.				Onoue, Masafusa/KMY-0551-2024; li, junyao/IUM-8591-2023; Harikane, Yuichi/KHY-2680-2024						Tip of the iceberg: overmassive black holes at 4 < z < 7 found by JWST are not inconsistent with the local MBH − M* relation								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2403.00074v1	arXiv:2403.00074			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 29 2024	2024	JWST is revealing a new remarkable population of high-redshift (z ≳ 4), low -luminosity Active Galactic Nuclei (AGNs) in deep surveys and detecting the host galaxy stellar light in the most luminous and massive quasars at z ∼ 6 for the first time. Latest results claim supermassive black holes (SMBHs) in these systems to be significantly more massive than expected from the local BH mass– stellar mass (MBH − M*) relation and that this is not due to sample selection effects. Through detailed statistical modeling, we demonstrate that the coupled effects of selection biases (i.e., finite detection limit and requirements on detecting broad lines) and measurement uncertainties in MBH and M* can in fact largely account for the reported offset and flattening in the observed MBH − M* relation toward the upper envelope of the local relation, even for those at MBH < 108 M⊙. We further investigate the possible evolution of the MBH − M* relation at z≳ 4 with careful treatment of observational biases and consideration of the degeneracy between intrinsic evolution and dispersion in this relation. The bias-corrected intrinsic MBH − M* relation in the low-mass regime suggests that there might be a large population of low-mass BHs (log MBH ≲ 5), possibly originating from lighter seeds, remaining undetected or unidentified even in the deepest JWST surveys. These results have important consequences for JWST studies of BH seeding and the coevolution between SMBHs and their host galaxies at the earliest cosmic times.																																	2024-03-28	PPRN:87997987		
J	Radosavovic, Ilija; Zhang, Bike; Shi, Baifeng; Rajasegaran, Jathushan; Kamat, Sarthak; Darrell, Trevor; Sreenath, Koushil; Malik, Jitendra										Humanoid Locomotion as Next Token Prediction								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19469v1	arXiv:2402.19469			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi -modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real -world control tasks by generative modeling of sensorimotor trajectories.																																	2024-11-10	PPRN:87988695		
J	Zhou, Yucheng; Li, Xiang; Wang, Qianning; Shen, Jianbing				Shen, Jianbing/KPB-2753-2024						Visual In-Context Learning for Large Vision-Language Models								Arxiv											1	1;2024-02-18;https://www.arxiv.org/abs/2402.11574v1	arXiv:2402.11574			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 18 2024	2024	In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ''Retrieval & Rerank'' paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use of in-context unlearning further shows promise in resetting specific model knowledge without retraining.																																	2024-11-09	PPRN:87762303		
J	Sun, Quan; Wang, Jinsheng; Yu, Qiying; Cui, Yufeng; Zhang, Fan; Zhang, Xiaosong; Wang, Xinlong				Wang, Xinlong/AFI-8800-2022; Wang, Jinsheng/AAZ-1470-2021						EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2402.04252v1	arXiv:2402.04252			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 06 2024	2024	Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18 -billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the potential of EVAstyle [30, 29, 63] weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.																																	2024-05-25	PPRN:87529292		
J	Khanov, Maxim; Burapacheep, Jirayu; Li, Yixuan										ARGS: Alignment as Reward-Guided Search								Arxiv											1	1;2024-01-23;https://www.arxiv.org/abs/2402.01694v1	arXiv:2402.01694			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 23 2024	2024	Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing decoding-time alignment, paves the way for more responsive language models in the future.																																	2024-02-21	PPRN:87522238		
J	He, Xinlei; Shen, Xinyue; Chen, Zeyuan; Backes, Michael; Zhang, Yang				Chen, Zeyuan/NSV-1807-2025						MGTBench: Benchmarking Machine-Generated Text Detection								Arxiv											2	2;2024-01-16;https://www.arxiv.org/abs/2303.14822v3| 1;2023-03-26;https://www.arxiv.org/abs/2303.14822v1	arXiv:2303.14822			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 16 2024	2024	Nowadays, powerful large language models (LLMs) such as ChatGPT have demonstrated revolutionary power in a variety of tasks. Consequently, the detection of machine-generated texts (MGTs) is becoming increasingly crucial as LLMs become more advanced and prevalent. These models have the ability to generate human-like language, making it challenging to discern whether a text is authored by a human or a machine. This raises concerns regarding authenticity, accountability, and potential bias. However, existing methods for detecting MGTs are evaluated using different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework that encompasses various methodologies. Furthermore, it remains unclear how existing detection methods would perform against powerful LLMs.   In this paper, we fill this gap by proposing the first benchmark framework for MGT detection against powerful LLMs, named MGTBench. Extensive evaluations on public datasets with curated texts generated by various powerful LLMs such as ChatGPT-turbo and Claude demonstrate the effectiveness of different detection methods. Our ablation study shows that a larger number of words in general leads to better performance and most detection methods can achieve similar performance with much fewer training samples. Moreover, we delve into a more challenging task: text attribution. Our findings indicate that the model-based detection methods still perform well in the text attribution task. To investigate the robustness of different detection methods, we consider three adversarial attacks, namely paraphrasing, random spacing, and adversarial perturbations. We discover that these attacks can significantly diminish detection effectiveness, underscoring the critical need for the development of more robust detection methods.1																																	2024-05-25	PPRN:49720227		
J	Kinniment, Megan; Sato, Lucas Jun Koba; Du, Haoxing; Goodrich, Brian; Hasin, Max; Chan, Lawrence; Miles, Luke Harold; Lin, Tao R.; Wijk, Hjalmar; Burget, Joel; Ho, Aaron; Barnes, Elizabeth; Christiano, Paul										Evaluating Language-Model Agents on Realistic Autonomous Tasks								Arxiv											2	2;2024-01-04;https://www.arxiv.org/abs/2312.11671v2| 1;2023-12-18;https://www.arxiv.org/abs/2312.11671v1	arXiv:2312.11671			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 04 2024	2024	In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult. We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.																																	2024-05-25	PPRN:86726164		
J	Zhu, Yuqi; Wang, Xiaohan; Chen, Jing; Qiao, Shuofei; Ou, Yixin; Yao, Yunzhi; Deng, Shumin; Chen, Huajun; Zhang, Ningyu				Deng, Shumin/AAP-7003-2021; Huajun, Chen/B-6340-2013; Wang, Xiaohan/JKI-4414-2023; Zhang, Ningyu/AAQ-7391-2021						LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities								Arxiv											4	4;2024-12-26;https://www.arxiv.org/abs/2305.13168v4| 3;2024-08-18;https://www.arxiv.org/abs/2305.13168v3| 2;2024-02-22;https://www.arxiv.org/abs/2305.13168v2| 1;2023-05-22;https://www.arxiv.org/abs/2305.13168v1	arXiv:2305.13168			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 26 2024	2024	This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs’ performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs.																																	2025-02-05	PPRN:70783823		
J	Dam, Sumit Kumar; Hong, Choong Seon; Qiao, Yu; Zhang, Chaoning				Hong, Choong Seon/ABF-5527-2020; Qiao, Yu/O-2445-2015; Zhang, Chaoning/ABG-1572-2022						A Complete Survey on LLM-based AI Chatbots								Arxiv											2	2;2024-11-18;https://www.arxiv.org/abs/2406.16937v2| 1;2024-06-17;https://www.arxiv.org/abs/2406.16937v1	arXiv:2406.16937			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 18 2024	2024	The past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.																																	2024-12-28	PPRN:89508406		
J	Leutheusser, Samuel; Liu, Hong										Subregion-subalgebra duality: emergence of space and time in holography								Arxiv											2	2;2024-11-14;https://www.arxiv.org/abs/2212.13266v2| 1;2022-12-26;https://www.arxiv.org/abs/2212.13266v1	arXiv:2212.13266			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 14 2024	2024	In holographic duality, a higher dimensional quantum gravity system emerges from a lower dimensional conformal field theory (CFT) with a large number of degrees of freedom. We propose a formulation of duality for a general causally complete bulk spacetime region, called subregion-subalgebra duality, which provides a framework to describe how geometric notions in the gravity system, such as spacetime subregions, different notions of times, and causal structure, emerge from the dual CFT. Subregion-subalgebra duality generalizes and brings new insights into subregion-subregion duality (or equivalently entanglement wedge reconstruction). It provides a mathematically precise definition of subregion-subregion duality and gives an independent definition of entanglement wedges without using entropy. Geometric properties of entanglement wedges, including those that play a crucial role in interpreting the bulk as a quantum error correcting code, can be understood from the duality as the geometrization of the superadditivity of certain algebras. Using general boundary subalgebras rather than those associated with geometric subregions makes it possible to find duals for general bulk spacetime regions, including those not touching the boundary. Applying subregion-subalgebra duality to a boundary state describing a single-sided black hole also provides a precise way to define mirror operators.																																	2025-01-08	PPRN:35875315		
J	Rafailov, Rafael; Chittepu, Yaswanth; Park, Ryan; Sikchi, Harshit; Hejna, Joey; Knox, W. Bradley; Finn, Chelsea; Niekum, Scott										Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms								Arxiv											2	2;2024-11-05;https://www.arxiv.org/abs/2406.02900v2| 1;2024-06-05;https://www.arxiv.org/abs/2406.02900v1	arXiv:2406.02900			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 05 2024	2024	Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is reward over-optimization or reward hacking, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DAAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.																																	2024-12-10	PPRN:89262179		
J	Yang, Wenkai; Bi, Xiaohan; Lin, Yankai; Chen, Sishuo; Zhou, Jie; Sun, Xu										Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents								Arxiv											2	2;2024-10-29;https://www.arxiv.org/abs/2402.11208v2| 1;2024-02-17;https://www.arxiv.org/abs/2402.11208v1	arXiv:2402.11208			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 29 2024	2024	Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack , to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization . Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. 3 																																	2024-12-09	PPRN:87761756		
J	Sakshi, S; Tyagi, Utkarsh; Kumar, Sonal; Seth, Ashish; Selvakumar, Ramaneswaran; Nieto, Oriol; Duraiswami, Ramani; Ghosh, Sreyan; Manocha, Dinesh				Ghosh, Dr.Shyamasree/ABA-4456-2021; Duraiswami, Ramani/J-6070-2012; Nieto, Oriol/AAT-9148-2021						MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark								Arxiv											1	1;2024-10-24;https://www.arxiv.org/abs/2410.19168v1	arXiv:2410.19168			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 24 2024	2024	The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.																																	2024-11-29	PPRN:118851799		
J	Yan, Xu; Jiang, Yaoting; Liu, Wenyi; Yi, Didi; Wei, Jianjun				Liu, Wenyi/KIA-3624-2024						Transforming Multidimensional Time Series into Interpretable Event Sequences for Advanced Data Mining								Arxiv											2	2;2024-10-08;https://www.arxiv.org/abs/2409.14327v2| 1;2024-09-22;https://www.arxiv.org/abs/2409.14327v1	arXiv:2409.14327			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 08 2024	2024	This paper introduces a novel spatiotemporal feature representation model designed to address the limitations of traditional methods in multidimensional time series (MTS) analysis. The proposed approach converts MTS into one-dimensional sequences of spatially evolving events, preserving the complex coupling relationships between dimensions. By employing a variable-length tuple mining method, key spatiotemporal features are extracted, enhancing the interpretability and accuracy of time series analysis. Unlike conventional models, this unsupervised method does not rely on large training datasets, making it adaptable across different domains. Experimental results from motion sequence classification validate the model's superior performance in capturing intricate patterns within the data. The proposed framework has significant potential for applications across various fields, including backend services for monitoring and optimizing IT infrastructure, medical diagnosis through continuous patient monitoring and health trend analysis, and internet businesses for tracking user behavior and forecasting sales. This work offers a new theoretical foundation and technical support for advancing time series data mining and its practical applications in human behavior recognition and other domains.																																	2024-10-28	PPRN:96184234		
J	Tang, Liyan; Laban, Philippe; Durrett, Greg										MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents								Arxiv											2	2;2024-10-01;https://www.arxiv.org/abs/2404.10774v2| 1;2024-04-16;https://www.arxiv.org/abs/2404.10774v1	arXiv:2404.10774			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 01 2024	2024	Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.																																	2024-11-10	PPRN:88538340		
J	Arkani-Hamed, N.; Frost, H.; Salvatori, G.; Plamondon, P-G.; Thomas, H.										All Loop Scattering As A Counting Problem								Arxiv											2	2;2024-09-27;https://www.arxiv.org/abs/2309.15913v2| 1;2023-09-27;https://www.arxiv.org/abs/2309.15913v1	arXiv:2309.15913			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 27 2024	2024	This is the first in a series of papers presenting a new understanding of scattering amplitudes based on fundamentally combinatorial ideas in the kinematic space of the scattering data. We study the simplest theory of colored scalar particles with cubic interactions, at all loop orders and to all orders in the topological 't Hooft expansion. We find a novel formula for loop-integrated amplitudes, with no trace of the conventional sum over Feynman diagrams, but instead determined by a beautifully simple counting problem attached to any order of the topological expansion. These results represent a significant step forward in the decade-long quest to formulate the fundamental physics of the real world in a radically new language, where the rules of spacetime and quantum mechanics, as reflected in the principles of locality and unitarity, are seen to emerge from deeper mathematical structures.																																	2024-10-11	PPRN:85322695		
J	Girish, Sharath; Gupta, Kamal; Shrivastava, Abhinav				Shrivastava, Abhinav/AAK-2538-2021						EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS								Arxiv											3	3;2024-09-26;https://www.arxiv.org/abs/2312.04564v3| 2;2024-04-24;https://www.arxiv.org/abs/2312.04564v2| 1;2023-12-07;https://www.arxiv.org/abs/2312.04564v1	arXiv:2312.04564			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 26 2024	2024	Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce per-point memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach develops a pruning stage which results in scene representations with fewer Gaussians, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce storage memory by more than an order of magnitude all while preserving the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x lesser memory and faster training/inference speed. 																																	2024-10-09	PPRN:86443694		
J	Zhuang, Ziwen; Yao, Shenzhe; Zhao, Hang										Humanoid Parkour Learning								Arxiv											2	2;2024-09-26;https://www.arxiv.org/abs/2406.10759v2| 1;2024-06-15;https://www.arxiv.org/abs/2406.10759v1	arXiv:2406.10759			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 26 2024	2024	Parkour is a grand challenge for legged locomotion, even for quadruped robots, requiring active perception and various maneuvers to overcome multiple challenging obstacles. Existing methods for humanoid locomotion either optimize a trajectory for a single parkour track or train a reinforcement learning policy only to walk with a significant amount of motion references. In this work, we propose a framework for learning an end-to-end vision-based whole-body-control parkour policy for humanoid robots that overcomes multiple parkour skills without any motion prior. Using the parkour policy, the humanoid robot can jump on a 0.42m platform, leap over hurdles, 0.8m gaps, and much more. It can also run at 1.8m/s in the wild and walk robustly on different terrains. We test our policy in indoor and outdoor environments to demonstrate that it can autonomously select parkour skills while following the rotation command of the joystick. We override the arm actions and show that this framework can easily transfer to humanoid mobile manipulation tasks.  																																	2024-10-09	PPRN:89352573		
J	Xu, Jiajun; Li, Zhiyuan; Chen, Wei; Wang, Qun; Gao, Xin; Cai, Qi; Ling, Ziyuan				Xu, Jiajun/JXL-4630-2024; Gao, Xin/C-7209-2019						On-Device Language Models: A Comprehensive Review								Arxiv											2	2;2024-09-14;https://www.arxiv.org/abs/2409.00088v2| 1;2024-08-26;https://www.arxiv.org/abs/2409.00088v1	arXiv:2409.00088			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 14 2024	2024	The advent of large language models (LLMs) revolutionized natural language processing applications, and running LLMs on edge devices has become increasingly attractive for reasons including reduced latency, data localization, and personalized user experiences. This comprehensive review examines the challenges of deploying computationally expensive LLMs on resource-constrained devices and explores innovative solutions across multiple domains. The paper investigates the development of on-device language models, their efficient architectures, including parameter sharing and modular designs, as well as state-of-the-art compression techniques like quantization, pruning, and knowledge distillation. Hardware acceleration strategies and collaborative edge-cloud deployment approaches are analyzed, highlighting the intricate balance between performance and resource utilization. Case studies of on-device language models from major mobile manufacturers demonstrate real-world applications and potential benefits. The review also addresses critical aspects such as adaptive learning, multi-modal capabilities, and personalization. By identifying key research directions and open challenges, this paper provides a roadmap for future advancements in on-device language models, emphasizing the need for interdisciplinary efforts to realize the full potential of ubiquitous, intelligent computing while ensuring responsible and ethical deployment. For a comprehensive review of research work and educational resources on on-device large language models (LLMs)																																	2024-12-24	PPRN:91719789		
J	Jain, Samyak; Kirk, Robert; Lubana, Ekdeep Singh; Dick, Robert P.; Tanaka, Hidenori; Grefenstette, Edward; Rocktaschel, Tim; Krueger, David				Dick, Robert/B-7137-2009						Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks								Arxiv											2	2;2024-08-21;https://www.arxiv.org/abs/2311.12786v2| 1;2023-11-21;https://www.arxiv.org/abs/2311.12786v1	arXiv:2311.12786			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 21 2024	2024	Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a 'wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such hidden capabilities are relevant leads to sample-efficient 'revival' of the capability, i.e., the model begins reusing these capability after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.																																	2024-08-31	PPRN:86223771		
J	Zhang, Yiming; Carlini, Nicholas; Ippolito, Daphne										Effective Prompt Extraction from Language Models								Arxiv											3	3;2024-08-07;https://www.arxiv.org/abs/2307.06865v3| 2;2024-02-17;https://www.arxiv.org/abs/2307.06865v2| 1;2023-07-13;https://www.arxiv.org/abs/2307.06865v1	arXiv:2307.06865			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 07 2024	2024	The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold on marketplaces.1 However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction from real systems such as Claude 3 and ChatGPT further suggest that system prompts can be revealed by an adversary despite existing defenses in place.2																																	2024-08-17	PPRN:73909338		
J	Kapoor, Raghav; Butala, Yash Parag; Russak, Melisa; Koh, Jing Yu; Kamble, Kiran; Alshikh, Waseem; Salakhutdinov, Ruslan										OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web								Arxiv											2	2;2024-07-21;https://www.arxiv.org/abs/2402.17553v3| 1;2024-02-28;https://www.arxiv.org/abs/2402.17553v2	arXiv:2402.17553			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 21 2024	2024	For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such as "Send an email to John Doe mentioning the time and place to meet". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.																																	2024-07-28	PPRN:87987579		
J	Wettig, Alexander; Gupta, Aatmik; Malik, Saumya; Chen, Danqi				CHEN, DANQI/C-6441-2013						QuRating: Selecting High-Quality Data for Training Language Models								Arxiv											3	3;2024-07-17;https://www.arxiv.org/abs/2402.09739v3| 2;2024-06-13;https://www.arxiv.org/abs/2402.09739v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.09739v1	arXiv:2402.09739			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 17 2024	2024	(2021). Fakhredine, R. M. Sattigeri, G. Cuono, and C. Autieri, Interbetween altermagnetism and nonsymmorphic symmetries generating large anomalous Hall conductivity by semi-Dirac points induced anticrossings, Phys. Rev. B 108 , , 115138 (2023). S. Wadge, M. Ahmad, B. J. Kowalski, D. Jastrze˛bski, ˛ bski, K. Zberecki, K. Dybko, P. Iwanowski, R. Diduszko, N. Olszowska, Rosmus, and A. Wis´niewski, Investigation of Topological Nodal Line Semimetal: ZrAs2 , 2 , ECOSS-36, 36th European Conference on Surface Science, 28 August–1 – 1 September 2023, Łódz´, Poland. Nandi, B. B. Maity, V. Sharma, R. Verma, V. Saini, B. Singh, Aoki, and A. Thamizhavel, Magnetotransport and Fermi surface studies of a purported nodal line semimetal ZrAs2 , 2 , Phys. B 109 , , 075155 (2024). Supplemental Material at http://link.aps.org/supplemental/ 10.1103/PhysRevB.110.035142 for EDX spectrum, slab calculations, nodal line along S-X path, photon energy dependent ARPES spectra, termination dependent calculations, calculated Fermi surface on (001), (010), and (100) plane, and bulk band structure for Zr (d d orbital) and As ( p orbital) contributions.																																	2024-07-26	PPRN:87703902		
J	Castellano, Marco; Napolitano, Lorenzo; Fontana, Adriano; Roberts-Borsani, Guido; Treu, Tommaso; Vanzella, Eros; Zavala, Jorge A.; Haro, Pablo Arrabal; Calabro, Antonello; Llerena, Mario; Mascia, Sara; Merlin, Emiliano; Paris, Diego; Pentericci, Laura; Santini, Paola; Bakx, Tom J.L.C.; Bergamini, Pietro; Cupani, Guido; Dickinson, Mark; Filippenko, Alexei V.; Glazebrook, Karl; Grillo, Claudio; Kelly, Patrick L.; Malkan, Matthew A.; Mason, Charlotte A.; Morishita, Takahiro; Nanayakkara, Themiya; Rosati, Piero; Sani, Eleonora; Wang, Xin; Yoon, Ilsang				Malkan, Matthew/IWM-5356-2023; Calabrò, Antonello/AAX-1028-2020; Glazebrook, Karl/N-3488-2015; Nanayakkara, Themiya/OAJ-0439-2025; Treu, Tommaso/KYP-7127-2024; Mason, Charlotte/IYJ-2820-2023; Llerena, Mario/OON-4774-2025; Cupani, Guido/JXM-2388-2024						JWST NIRSpec Spectroscopy of the Remarkable Bright Galaxy GHZ2/GLASS-z12 at Redshift 12.34								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2403.10238v2| 1;2024-03-15;https://www.arxiv.org/abs/2403.10238v1	arXiv:2403.10238			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	We spectroscopically confirm the MUV=−20.5 mag galaxy GHZ2/GLASS-z12 to be at redshift z=12.34. The source was selected via NIRCam photometry in GLASS-JWST ERS data, providing the first evidence of a surprising abundance of bright galaxies at z ≳ 10. The NIRSpec PRISM spectrum shows detections of N IV, C IV, He II, O III, C III, O II, and Ne III lines, and the first detection at high-redshift of the O III Bowen fluorescence line at 3133 Å rest-frame. The prominent C IV line with rest-frame equivalent width (EW) ≈46 Å puts GHZ2 in the category of extreme C IV emitters. GHZ2 displays UV lines with EWs that are only found in active galactic nuclei (AGNs) or composite objects at low/intermediate redshifts. The UV line-intensity ratios are compatible both with AGNs and star formation in a low-metallicity environment, with the low limit on the [Ne IV]/[N IV] ratio favoring a stellar origin of the ionizing photons. We discuss a possible scenario in which the high ionizing output is due to low metallicity stars forming in a dense environment. We estimate a metallicity ≲0.1Z/Z⊙, a high ionization parameter logU >−2, a N/O abundance 4-5 times the solar value, and a subsolar C/O ratio similar to the recently discovered class of nitrogen-enhanced objects. Considering its abundance patterns and the high stellar mass density (104~M⊙~pc−2), GHZ2 is an ideal formation site for the progenitors of today's globular clusters. The remarkable brightness of GHZ2 makes it a "Rosetta stone'' for understanding the physics of galaxy formation within just 360 Myr after the Big Bang.																																	2024-08-02	PPRN:88169222		
J	Smith, David; Myers, Joseph Samuel; Kaplan, Craig S.; Goodman-Strauss, Chaim										An aperiodic monotile								Arxiv											1	1;2024-07-03;https://www.arxiv.org/abs/2303.10798v3	arXiv:2303.10798			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	A longstanding open problem asks for an aperiodic monotile, also known as an "einstein": a shape that admits tilings of the plane, but never periodic tilings. We answer this problem for topological disk tiles by exhibiting a continuum of combinatorially equivalent aperiodic polygons. We first show that a representative example, the "hat" polykite, can form clusters called "metatiles", for which substitution rules can be defined. Because the metatiles admit tilings of the plane, so too does the hat. We then prove that generic members of our continuum of polygons are aperiodic, through a new kind of geometric incommensurability argument. Separately, we give a combinatorial, computer-assisted proof that the hat must form hierarchical -- and hence aperiodic -- tilings.																																	2024-07-21	PPRN:90749058		
J	Ahn, Michael; Dwibedi, Debidatta; Finn, Chelsea; Arenas, Montse Gonzalez; Gopalakrishnan, Keerthana; Hausman, Karol; Ichter, Brian; Irpan, Alex; Joshi, Nikhil; Julian, Ryan; Kirmani, Sean; Leal, Isabel; Lee, Edward; Levine, Sergey; Lu, Yao; Maddineni, Sharath; Rao, Kanishka; Sadigh, Dorsa; Sanketi, Pannag; Sermanet, Pierre; Vuong, Quan; Welker, Stefan; Xia, Fei; Xiao, Ted; Xu, Peng; Xu, Steve; Xu, Zhuo				Xia, Fei/AAW-8782-2021; Leal, isabel/M-7651-2013						AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents								Arxiv											2	2;2024-07-02;https://www.arxiv.org/abs/2401.12963v2| 1;2024-01-23;https://www.arxiv.org/abs/2401.12963v1	arXiv:2401.12963			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 02 2024	2024	Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such "in-the-wild" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.																																	2024-07-19	PPRN:87299813		
J	Ding, Bosheng; Qin, Chengwei; Zhao, Ruochen; Luo, Tianze; Li, Xinze; Chen, Guizhen; Xia, Wenhan; Hu, Junjie; Luu, Anh Tuan; Joty, Shafiq				qin, chengwei/MBV-9309-2025; Hu, Junjie/HHR-9040-2022; Luu, Anh Tuan/AAG-3582-2021						Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges								Arxiv											4	4;2024-07-02;https://www.arxiv.org/abs/2403.02990v4| 3;2024-06-28;https://www.arxiv.org/abs/2403.02990v3| 2;2024-06-16;https://www.arxiv.org/abs/2403.02990v2| 1;2024-03-05;https://www.arxiv.org/abs/2403.02990v1	arXiv:2403.02990			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 02 2024	2024	In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.																																	2024-07-19	PPRN:88028205		
J	Abdelnabi, Sahar; Gomaa, Amr; Sivaprasad, Sarath; Schonherr, Lea; Fritz, Mario										Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2309.17234v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17234v1	arXiv:2309.17234			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	There is an growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessing complex situations. Yet, we still have a limited understanding of LLMs' communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose multiple metrics to rigorously quantify agents' performance and alignment with the assigned role. We provide procedures to create new games and increase games' difficulty to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents influenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform.																																	2024-07-04	PPRN:85338154		
J	Su, Weihang; Wang, Changyue; Ai, Qingyao; Hu, Yiran; Wu, Zhijing; Zhou, Yujia; Liu, Yiqun				Wang, Changyue/NTR-3899-2025						Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2403.06448v2| 1;2024-03-11;https://www.arxiv.org/abs/2403.06448v1	arXiv:2403.06448			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM’s inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection 1 .																																	2024-07-04	PPRN:88100215		
J	Li, Yunshui; Hui, Binyuan; Xia, Xiaobo; Yang, Jiaxi; Yang, Min; Zhang, Lei; Si, Shuzheng; Chen, Ling-Hao; Liu, Tongliang; Liu, Junhao; Huang, Fei; Li, Yongbin				Liu, Tongliang/AAA-1506-2021; Liu, Junhao/KOD-3158-2024; Lu, Junling/D-7644-2018; Bin/B-6414-2019; Li, Yunshui/JNS-3522-2023; Chen, Ling-Hao/JXX-2220-2024						One-Shot Learning as Instruction Data Prospector for Large Language Models								Arxiv											3	3;2024-06-03;https://www.arxiv.org/abs/2312.10302v4| 2;2024-01-04;https://www.arxiv.org/abs/2312.10302v3| 1;2023-12-19;https://www.arxiv.org/abs/2312.10302v2	arXiv:2312.10302			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce N UGGETS , a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. N UGGETS assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. N UGGETS utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through comprehensive evaluations on two benchmarks, including MT-Bench and Alpaca-Eval, we show that instruction tuning with the top 1% of examples curated by N UGGETS substantially outperforms conventional methods employing the entire dataset.																																	2024-06-22	PPRN:86725791		
J	Dong, Yi; Mu, Ronghui; Jin, Gaojie; Qi, Yi; Hu, Jinwei; Zhao, Xingyu; Meng, Jie; Ruan, Wenjie; Huang, Xiaowei				Xiaowei, Huang/JKI-6599-2023; Zhao, Xingyu/HZI-1451-2023; Dong, Yi/HOF-1502-2023; Ruan, Wenjie/W-3957-2019						Building Guardrails for Large Language Models								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2402.01822v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01822v1	arXiv:2402.01822			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and testing to ensure the utmost quality of the final product.																																	2024-08-24	PPRN:87523961		
J	Chen, Qiguang; Qin, Libo; Zhang, Jin; Chen, Zhi; Xu, Xiao; Che, Wanxiang				Qin, Libo/IUQ-5109-2023; Chen, Qiguang/IVH-6127-2023; XIN, WANG/KGK-5385-2024						M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought								Arxiv											1	1;2024-05-26;https://www.arxiv.org/abs/2405.16473v1	arXiv:2405.16473			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 26 2024	2024	Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-bystep reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning , (2) single-step visual modal reasoning , and (3) Domain missing , thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M3CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M3CoT and there remains a large gap between existing VLLMs and human performance in M3CoT , despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M3CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.																																	2024-06-08	PPRN:89052617		
J	Choe, Sang Keun; Ahn, Hwijeen; Bae, Juhan; Zhao, Kewen; Kang, Minsoo; Chung, Youngseog; Pratapa, Adithya; Neiswanger, Willie; Strubell, Emma; Mitamura, Teruko; Schneider, Jeff; Hovy, Eduard; Grosse, Roger; Xing, Eric										What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions								Arxiv											1	1;2024-05-22;https://www.arxiv.org/abs/2405.13954v1	arXiv:2405.13954			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 22 2024	2024	Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited. In response to this issue, data valuation (or data attribution), which quantifies the contribution or value of each data to the model output, has been discussed as a potential solution. Nevertheless, applying existing data valuation methods to recent LLMs and their vast training datasets has been largely limited by prohibitive compute and memory costs. In this work, we focus on influence functions, a popular gradient-based data valuation method, and significantly improve its scalability with an efficient gradient projection strategy called LoGra that leverages the gradient structure in backpropagation. We then provide a theoretical motivation of gradient projection approaches to influence functions to promote trust in the data valuation process. Lastly, we lower the barrier to implementing data valuation systems by introducing LogIX, a software package that can transform existing training code into data valuation code with minimal effort. In our data valuation experiments, LoGra achieves competitive accuracy against more expensive baselines while showing up to 6,500x improvement in throughput and 5x reduction in GPU memory usage when applied to Llama3-8B-Instruct and the 1B-token dataset.																																	2024-06-05	PPRN:88989660		
J	Ge, Yuying; Zhao, Sijie; Li, Chen; Ge, Yixiao; Shan, Ying										SEED-Data-Edit Technical Report: A Hybrid Dataset for Instructional Image Editing								Arxiv											1	1;2024-05-07;https://www.arxiv.org/abs/2405.04007v1	arXiv:2405.04007			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 07 2024	2024	In this technical report, we introduce SEED-Data-Edit: a unique hybrid dataset for instruction-guided image editing, which aims to facilitate image manipulation using open-form language. SEED-Data-Edit is composed of three distinct types of data: (1) High-quality editing data produced by an automated pipeline, ensuring a substantial volume of diverse image editing pairs. (2) Real-world scenario data collected from the internet, which captures the intricacies of user intentions for promoting the practical application of image editing in the real world. (3) High-precision multi-turn editing data annotated by humans, which involves multiple rounds of edits for simulating iterative editing processes. The combination of these diverse data sources makes SEED-Data-Edit a comprehensive and versatile dataset for training language-guided image editing model. We fine-tune a pretrained Multimodal Large Language Model (MLLM) that unifies comprehension and generation with SEED-Data-Edit. The instruction tuned model demonstrates promising results, indicating the potential and effectiveness of SEED-Data-Edit in advancing the field of instructional image editing. 																																	2024-06-04	PPRN:88977829		
J	Zhu, Zheng; Wang, Xiaofeng; Zhao, Wangbo; Min, Chen; Deng, Nianchen; Dou, Min; Wang, Yuqi; Shi, Botian; Wang, Kai; Zhang, Chi; You, Yang; Zhang, Zhaoxiang; Zhao, Dawei; Xiao, Liang; Zhao, Jian; Lu, Jiwen; Huang, Guan				zhao, jian/HTM-3920-2023; Zhang, Zhao-xiang/R-2819-2018; Deng, Nianchen/GXZ-4749-2022; Zhao, Dawei/K-4073-2019; Xiao, Liang/A-5160-2016; Shi, Botian/HTT-0363-2023; Wang, Xiaofeng/GLR-2215-2022						Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond								Arxiv											1	1;2024-05-06;https://www.arxiv.org/abs/2405.03520v1	arXiv:2405.03520			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. 																																	2024-05-24	PPRN:88790149		
J	An, Shengnan; Ma, Zexiong; Lin, Zeqi; Zheng, Nanning; Lou, Jian-Guang										Make Your LLM Fully Utilize the Context								Arxiv											2	2;2024-04-26;https://www.arxiv.org/abs/2404.16811v2| 1;2024-04-25;https://www.arxiv.org/abs/2404.16811v1	arXiv:2404.16811			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost -in -the -middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long -context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents IN formation- IN tensive (IN2) N 2) training, , a purely data -driven solution to overcome lost -in -the -middle. Specifically, IN2 N 2 training leverages a synthesized long -context question -answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (∼128 ∼ 128 tokens) within a synthesized long context (4K−32K − 32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information -intensive training on Mistral -7B, we present FILM IL M -7B (FILl- FIL l- in-the-Middle). M iddle). To thoroughly assess the ability of FILM IL M -7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured -data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM IL M -7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM IL M -7B significantly improves the performance on real -world long -context tasks (e.g., 23.5→26.9 → 26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short -context tasks (e.g., 59.3→59.2 → 59.2 accuracy on MMLU). 																																	2024-05-05	PPRN:88651277		
J	Mozannar, Hussein; Bansal, Gagan; Fourney, Adam; Horvitz, Eric				Mozannar, Hussein/JCN-8837-2023; Bansal, Gagan/LZG-0954-2025						Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming								Arxiv											2	2;2024-04-22;https://www.arxiv.org/abs/2210.14306v5| 1;2022-10-25;https://www.arxiv.org/abs/2210.14306v1	arXiv:2210.14306			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 22 2024	2024	Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To seek insights about human-AI collaboration with code recommendations systems, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics. [GRAPHICS]																																	2024-05-01	PPRN:22172513		
J	Dong, Zhichen; Zhou, Zhanhui; Yang, Chao; Shao, Jing; Qiao, Yu				Qiao, Yu/ABD-5787-2021						Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey								Arxiv											4	4;2024-03-27;https://www.arxiv.org/abs/2402.09283v3| 3;2024-03-19;https://www.arxiv.org/abs/2402.09283v2| 2;2024-02-14;https://www.arxiv.org/abs/2402.09283v1| 1;2024-02-14;https://www.arxiv.org/abs/2402.09283v1	arXiv:2402.09283			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 27 2024	2024	Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. 																																	2024-04-15	PPRN:87688867		
J	Pan, Rui; Liu, Xiang; Diao, Shizhe; Pi, Renjie; Zhang, Jipeng; Han, Chi; Zhang, Tong				han, chi/LPR-1781-2024; Diao, Shizhe/JXY-7398-2024						LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning								Arxiv											4	4;2024-12-25;https://www.arxiv.org/abs/2403.17919v4| 3;2024-05-25;https://www.arxiv.org/abs/2403.17919v3| 2;2024-03-28;https://www.arxiv.org/abs/2403.17919v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17919v1	arXiv:2403.17919			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 26 2024	2024	The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over $11%$-$37%$ in terms of MT-Bench scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.																																	2025-08-07	PPRN:88291847		
J	Yepes, Antonio Jimeno; You, Yao; Milczek, Jan; Laverde, Sebastian; Li, Leah										Financial Report Chunking for Effective Retrieval Augmented Generation								Arxiv											3	3;2024-03-16;https://www.arxiv.org/abs/2402.05131v3| 2;2024-02-10;https://www.arxiv.org/abs/2402.05131v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.05131v1	arXiv:2402.05131			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 16 2024	2024	Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question & Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the quality of RAG outputs. Findings support that element type based chunking largely improve RAG results on financial reporting. Through this research, we are also able to answer how to uncover highly accurate RAG.																																	2024-04-11	PPRN:87573034		
J	Yuan, Lifan; Chen, Yangyi; Wang, Xingyao; Fung, Yi R.; Peng, Hao; Ji, Heng				Chen, Yangyi/LSL-4051-2024						CRAFT: CUSTOMIZING LLMS BY CREATING AND RETRIEVING FROM SPECIALIZED TOOLSETS								Arxiv											2	2;2024-03-13;https://www.arxiv.org/abs/2309.17428v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17428v1	arXiv:2309.17428			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 13 2024	2024	Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code snippets to enhance reusability, and deduplicated for higher quality. At inference time, the language model retrieves snippets from the toolsets and then executes them or generates the output conditioning on the retrieved snippets. Our method is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning. Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements compared to strong baselines. In addition, our in-depth analysis reveals that: (1) consistent performance improvement can be achieved by scaling up the number of tools and the capability of the backbone models; (2) each component of our approach contributes to the performance gains; (3) the created tools are well-structured and reliable with low complexity and atomicity. 																																	2024-04-11	PPRN:85339327		
J	Meyer, Nico; Ufrecht, Christian; Periyasamy, Maniraman; Scherer, Daniel D.; Plinge, Axel; Mutschler, Christopher				Meyer, Nico/KIC-6850-2024; Scherer, Daniel/A-3589-2016						A Survey on Quantum Reinforcement Learning								Arxiv											2	2;2024-03-08;https://www.arxiv.org/abs/2211.03464v2| 1;2022-11-07;https://www.arxiv.org/abs/2211.03464v1	arXiv:2211.03464			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 08 2024	2024	Quantum reinforcement learning is an emerging field at the intersection of quantum computing and machine learning. While we intend to provide a broad overview of the literature on quantum reinforcement learning - our interpretation of this term will be clarified below - we put particular emphasis on recent developments. With a focus on already available noisy intermediate-scale quantum devices, these include variational quantum circuits acting as function approximators in an otherwise classical reinforcement learning setting. In addition, we survey quantum reinforcement learning algorithms based on future fault-tolerant hardware, some of which come with a provable quantum advantage. We provide both a birds-eye-view of the field, as well as summaries and reviews for selected parts of the literature.																																	2024-04-07	PPRN:70810720		
J	Sakakibara, Hirofumi; Kitamine, Naoya; Ochi, Masayuki; Kuroki, Kazuhiko				Ochi, Masayuki/B-1933-2015						Possible High Tc Superconductivity in La3Ni2O7 under High Pressure through Manifestation of a Nearly-Half-Filled Bilayer Hubbard Model								Arxiv											4	4;2024-03-06;https://www.arxiv.org/abs/2306.06039v5| 3;2024-02-17;https://www.arxiv.org/abs/2306.06039v4| 2;2024-01-24;https://www.arxiv.org/abs/2306.06039v3| 1;2023-06-09;https://www.arxiv.org/abs/2306.06039v2	arXiv:2306.06039			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 06 2024	2024	Inspired by a recent experiment showing that La3Ni2O7 exhibits high Tc superconductivity under high pressure, we theoretically revisit the possibility of superconductivity in this material. We find that superconductivity can take place which is somewhat similar to that of the bilayer Hubbard model consisting of the Ni 3d3z2−r2 orbitals. Although the coupling with the 3dx2−y2 orbitals degrades superconductivity, Tc can still be high enough to understand the experiment thanks to the very high Tc reached in the bilayer Hubbard model.																																	2024-04-03	PPRN:73262025		
J	Elazar, Yanai; Bhagia, Akshita; Magnusson, Ian; Ravichander, Abhilasha; Schwenk, Dustin; Suhr, Alane; Walsh, Pete; Groeneveld, Dirk; Soldaini, Luca; Singh, Sameer; Hajishirzi, Hanna; Smith, Noah A.; Dodge, Jesse										What's In My Big Data?								Arxiv											2	2;2024-03-05;https://www.arxiv.org/abs/2310.20707v2| 1;2023-10-31;https://www.arxiv.org/abs/2310.20707v1	arXiv:2310.20707			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 05 2024	2024	Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT’S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities—count and search—at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low -quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open -source WIMBD’s code and artifacts to provide a standard set of evaluations for new text -based corpora and to encourage more analyses and transparency around them.																																	2024-04-01	PPRN:85909703		
J	Chen, Lingjiao; Davis, Jared Quincy; Hanin, Boris; Bailis, Peter; Stoica, Ion; Zaharia, Matei; Zou, James										Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems								Arxiv											1	1;2024-03-04;https://www.arxiv.org/abs/2403.02419v1	arXiv:2403.02419			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 04 2024	2024	Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LLM calls lead to higher performance on "easy" queries, but lower performance on "hard" queries, and non-monotone behavior emerges when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LLM calls that maximizes system performance, and define a scaling law of Voting Inference Systems. Experiments show that our scaling law can predict the performance of Voting Inference Systems and find the optimal number of LLM calls to make.																																	2024-03-31	PPRN:88028020		
J	Kovachki, Nikola B.; Lanthaler, Samuel; Stuart, Andrew M.										de Frutos Pablo M. Olmos Manuel A. Vázquez Joaquı´n								Arxiv											1	1;2024-02-24;https://www.arxiv.org/abs/2402.15715v1	arXiv:2402.15715			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 24 2024	2024	Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article summarizes recent progress and the current state of our theoretical understanding of neural operators, focusing on an approximation theoretic point of view.																																	2024-03-25	PPRN:87889630		
J	Bruce, Jake; Dennis, Michael; Edwards, Ashley; Parker-Holder, Jack; Shi, Yuge (Jimmy); Hughes, Edward; Lai, Matthew; Mavalankar, Aditi; Steigerwald, Richie; Apps, Chris; Aytar, Yusuf; Bechtle, Sarah; Behbahani, Feryal; Chan, Stephanie; Heess, Nicolas; Gonzalez, Lucy; Osindero, Simon; Ozair, Sherjil; Reed, Scott; Jingwei, Zhang; Zolna, Konrad; Clune, Jeff; de Freitas, Nando; Singh, Satinder; Rocktaschel, Tim										Genie: Generative Interactive Environments								Arxiv											1	1;2024-02-23;https://www.arxiv.org/abs/2402.15391v1	arXiv:2402.15391			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of actioncontrollable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame -by -frame basis despite training without any ground -truth action labels or other domain -specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.																																	2024-03-23	PPRN:87871964		
J	Ding, Ruomeng; Zhang, Chaoyun; Wang, Lu; Xu, Yong; Ma, Minghua; Zhang, Wei; Qin, Si; Rajmohan, Saravan; Lin, Qingwei; Zhang, Dongmei				Qin, Si/GLU-9233-2022; Ma, Minghua/JFA-3786-2023; Zhang, Chi/U-9607-2019; Lin, Qingwei/AAZ-3604-2021						Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation								Arxiv											3	3;2024-02-23;https://www.arxiv.org/abs/2311.04254v3| 2;2023-11-12;https://www.arxiv.org/abs/2311.04254v2| 1;2023-11-07;https://www.arxiv.org/abs/2311.04254v1	arXiv:2311.04254			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as "thoughts". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called "Everything of Thoughts" (XoT) to defy the law of "Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions. We evaluate XoT on several challenging multi-solution problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches. Notably, XoT can yield multiple solutions with just one LLM call, showcasing its remarkable proficiency in addressing complex problems across diverse domains.																																	2024-03-23	PPRN:86097708		
J	Chen, Zhuoming; May, Avner; Svirschevski, Ruslan; Huang, Yuhsun; Ryabinin, Max; Jia, Zhihao; Chen, Beidi										Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding								Arxiv											1	1;2024-02-19;https://www.arxiv.org/abs/2402.12374v1	arXiv:2402.12374			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 19 2024	2024	As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces SEQUOIA, a scalable, robust, and hardware -aware algorithm for speculative decoding. To attain better scalability, SEQUOIA introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, SEQUOIA uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, SEQUOIA introduces a hardware -aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. SEQUOIA improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna -33B on an A100 GPU by up to 4.04×, 3.84×, and 2.37×, and Llama2-70B offloading speed by up to 10.33× on an L40.																																	2024-03-16	PPRN:87760957		
J	Wang, Liang; Yang, Nan; Wei, Furu										Learning to Retrieve In-Context Examples for Large Language Models								Arxiv											2	2;2024-01-26;https://www.arxiv.org/abs/2307.07164v2| 1;2023-07-14;https://www.arxiv.org/abs/2307.07164v1	arXiv:2307.07164			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 26 2024	2024	Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.																																	2024-02-14	PPRN:73977286		
J	Li, Mukai; Li, Lei; Yin, Yuwei; Ahmed, Masood; Liu, Zhenguang; Liu, Qi				LIU, Qi/I-4900-2017; Li, Lei/LMN-0940-2024; Yin, Yuwei/AGC-5865-2022						Red Teaming Visual Language Models								Arxiv											1	1;2024-01-23;https://www.arxiv.org/abs/2401.12915v1	arXiv:2401.12915			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jan 23 2024	2024	VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.																																	2024-05-25	PPRN:87288441		
J	Liu, Yang; Zhu, Muzhi; Li, Hengtao; Chen, Hao; Wang, Xinlong; Shen, Chunhua				Wang, Xinlong/HTS-0660-2023						Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching								Arxiv											2	2;2024-01-19;https://www.arxiv.org/abs/2305.13310v2| 1;2023-05-22;https://www.arxiv.org/abs/2305.13310v1	arXiv:2305.13310			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 19 2024	2024	Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. However, unlike large language mod -els that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks. In this work, we present Matcher, a novel perception paradigm that utilizes off-the-shelf vision foundation models to address various perception tasks. Matcher can segment anything by using an in-context example without training. Additionally, we design three effective components within the Matcher framework to collaborate with these foundation models and unleash their full potential in diverse perception tasks. Matcher demonstrates impressive generalization performance across various segmentation tasks, all without training. For example, it achieves 52.7% mIoU on COCO-20i with one example, surpassing the state-of-the-art specialist model by 1.6%. In addition, Matcher achieves 33.0% mIoU on the proposed LVIS-92i for one-shot semantic segmentation, outperforming the state-of-the-art generalist model by 14.4%. Our visualization results further showcase the open-world gener-ality and flexibility of Matcher when applied to images in the wild. 																																	2024-02-10	PPRN:70962159		
J	El-Nouby, Alaaeldin; Klein, Michal; Zhai, Shuangfei; Bautista, Miguel Angel; Toshev, Alexander; Shankar, Vaishaal; Susskind, Joshua M; Joulin, Armand										Scalable Pre-training of Large Autoregressive Image Models								Arxiv											1	1;2024-01-16;https://www.arxiv.org/abs/2401.08541v1	arXiv:2401.08541			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 16 2024	2024	This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2 billion images, that achieves 84.0% on ImageNet-1k with a frozen trunk. Interestingly, even at this scale, we observe no sign of saturation in performance, suggesting that AIM potentially represents a new frontier for training large-scale vision models. The pre-training of AIM is similar to the pre-training of LLMs, and does not require any image-specific strategy to stabilize the training at scale.																																	2024-05-25	PPRN:87190804		
J	Ling, Huan; Kim, Seung Wook; Torralba, Antonio; Fidler, Sanja; Kreis, Karsten				Torralba, Antonio/K-2145-2012; Kim, Seung/AAV-9567-2021						Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models								Arxiv											2	2;2024-01-03;https://www.arxiv.org/abs/2312.13763v2| 1;2023-12-21;https://www.arxiv.org/abs/2312.13763v1	arXiv:2312.13763			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 03 2024	2024	Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.																																	2024-05-25	PPRN:86832621		
J	Zheng, Ruijie; Liang, Yongyuan; Huang, Shuaiyi; Gao, Jianfeng; Daume III, Hal; Kolobov, Andrey; Huang, Furong; Yang, Jianwei										TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies								Arxiv											2	2;2024-12-25;https://www.arxiv.org/abs/2412.10345v2| 1;2024-12-13;https://www.arxiv.org/abs/2412.10345v1	arXiv:2412.10345			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 25 2024	2024	Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models’ spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuning OpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X- Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency.																																	2025-02-15	PPRN:119933624		
J	Mao, Rui; Chen, Guanyi; Zhang, Xulang; Guerin, Frank; Cambria, Erik				Cambria, Erik/C-2103-2013; Mao, Rui/ABM-7006-2022						GPTEval: A Survey on Assessments of ChatGPT and GPT-4								Arxiv											2	2;2024-12-24;https://www.arxiv.org/abs/2308.12488v2| 1;2023-08-24;https://www.arxiv.org/abs/2308.12488v1	arXiv:2308.12488			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 24 2024	2024	The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research.																																	2025-02-02	PPRN:83899855		
J	Zhang, Qizhe; Cheng, Aosong; Lu, Ming; Zhuo, Zhiyong; Wang, Minqi; Cao, Jiajun; Guo, Shaobo; She, Qi; Zhang, Shanghang				Zhang, Qizhe/LBI-4862-2024; Wang, Minqi/O-7752-2018; Zhang, Qian/LKK-3896-2024; Guo, Shaobo/J-2237-2012; Lu, Ming/OEO-0673-2025						[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster								Arxiv											1	1;2024-12-02;https://www.arxiv.org/abs/2412.01818v1	arXiv:2412.01818			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	Large vision-language models (VLMs) often rely on a substantial number of visual tokens when interacting with large language models (LLMs), which has proven to be inefficient. Recent efforts have aimed to accelerate VLM inference by pruning visual tokens. Most existing methods assess the importance of visual tokens based on the text-visual cross-attentions in LLMs. In this study, we find that the cross- attentions between text and visual tokens in LLMs are inaccurate. Pruning tokens based on these inaccurate attentions leads to significant performance degradation, especially at high reduction ratios. To this end, we introduce FasterVLM, a simple yet effective training-free visual token pruning method that evaluates the importance of visual tokens more accurately by utilizing attentions between the [ CLS ] token and image tokens from the visual encoder. Since FasterVLM eliminates redundant visual tokens immediately after the visual encoder, ensuring they do not interact with LLMs and resulting in faster VLM inference. It is worth noting that, benefiting from the accuracy of [ CLS ] cross-attentions, FasterVLM can prune 95% of visual tokens while maintaining 90% of the performance of LLaVA-1.5-7B. We apply FasterVLM to various VLMs, including LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA, to demonstrate its effectiveness. Experimental results show that our FasterVLM maintains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing text-visual attention-based methods. 																																	2025-01-11	PPRN:119641757		
J	Turkulainen, Matias; Ren, Xuqian; Melekhov, Iaroslav; Seiskari, Otto; Rahtu, Esa; Kannala, Juho										DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing								Arxiv											3	3;2024-11-07;https://www.arxiv.org/abs/2403.17822v3| 2;2024-07-17;https://www.arxiv.org/abs/2403.17822v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17822v1	arXiv:2403.17822			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Nov 07 2024	2024	High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and AR applications. 3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. In this work, we explore the use of readily accessible geometric cues to enhance Gaussian splatting optimization in challenging, ill-posed, and textureless scenes. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use off-the-shelf monocular networks to achieve better alignment with the true scene geometry. We propose an adaptive depth loss based on the gradient of color images, improving depth estimation and novel view synthesis results over various baselines. Our simple yet effective regularization technique enables direct mesh extraction from the Gaussian representation, yielding more physically accurate reconstructions of indoor scenes. Our code will be released in https://github.com/maturk/dn-splatter.																																	2024-12-16	PPRN:88296450		
J	Ren, Yuxi; Xia, Xin; Lu, Yanzuo; Zhang, Jiacheng; Wu, Jie; Xie, Pan; Wang, Xing; Xiao, Xuefeng				Lu, Yanzuo/KIA-9494-2024; Zhang, Jiacheng/GLR-4003-2022						Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis								Arxiv											3	3;2024-11-04;https://www.arxiv.org/abs/2404.13686v3| 2;2024-05-22;https://www.arxiv.org/abs/2404.13686v2| 1;2024-04-21;https://www.arxiv.org/abs/2404.13686v1	arXiv:2404.13686			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	Recently, a series of diffusion-aware distillation algorithms have emerged to alleviate the computational overhead associated with the multi-step inference process of Diffusion Models (DMs). Current distillation techniques often dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii) ODE Trajectory Reformulation. However, these approaches suffer from severe performance degradation or domain shifts. To address these limitations, we propose Hyper-SD, a novel framework that synergistically amalgamates the advantages of ODE Trajectory Preservation and Reformulation, while maintaining near-lossless performance during step compression. Firstly, we introduce Trajectory Segmented Consistency Distillation to progressively perform consistent distillation within pre-defined time-step segments, which facilitates the preservation of the original ODE trajectory from a higher-order perspective. Secondly, we incorporate human feedback learning to boost the performance of the model in a low-step regime and mitigate the performance loss incurred by the distillation process. Thirdly, we integrate score distillation to further improve the low-step generation capability of the model and offer the first attempt to leverage a unified LoRA to support the inference process at all steps. Extensive experiments and user studies demonstrate that Hyper-SD achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5. For example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and +0.51 in Aes Score in the 1-step inference.																																	2024-12-09	PPRN:88604700		
J	Lin, Feng; Kim, Dong Jae										SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents								Arxiv											1	1;2024-10-31;https://www.arxiv.org/abs/2403.15852v2	arXiv:2403.15852			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 31 2024	2024	Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspired by these software engineering practices, we present FlowGen - a code generation framework that emulates software process models based on multiple Large Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall, FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e., requirement engineer, architect, developer, tester, and scrum master) that correspond to everyday development activities and organize their communication patterns. The agents work collaboratively using chain-of-thought and prompt composition with continuous self-refinement to improve the code quality. We use GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion) to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Our findings show that FlowGenScrum excels compared to other process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in statistically significant improvements, achieving the highest Pass@1 scores. Our analysis also reveals that the development activities impacted code smell and exception handling differently, with design and code review adding more exception handling and reducing code smells. Finally, FlowGen models maintain stable Pass@1 scores across GPT3.5 versions and temperature values, highlighting the effectiveness of software process models in enhancing the quality and stability of LLM-generated code.																																	2024-12-06	PPRN:88280662		
J	He, Xuan; Jiang, Dongfu; Zhang, Ge; Ku, Max; Soni, Achint; Siu, Sherman; Chen, Haonan; Chandra, Abhranil; Jiang, Ziyan; Arulraj, Aaran; Wang, Kai; Do, Quy Duc; Ni, Yuansheng; Lyu, Bohan; Narsupalli, Yaswanth; Fan, Rongqi; Lyu, Zhiheng; Lin, Bill Yuchen; Chen, Wenhu				陈, 浩楠/GWZ-1776-2022; Jiang, Dongfu/JTV-4943-2023						VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2406.15252v3| 1;2024-06-24;https://www.arxiv.org/abs/2406.15252v2	arXiv:2406.15252			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of large-scale human-annotated dataset. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videos from 11 existing video generative models. We train VideoScore (initialized from Mantis) based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman correlation between VideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with human judges than other metrics. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.																																	2024-11-06	PPRN:89408109		
J	Ye, Haoran; Wang, Jiarui; Cao, Zhiguang; Berto, Federico; Hua, Chuanbo; Kim, Haeyeon; Park, Jinkyoo; Song, Guojie				Cao, Zhiguang/OVY-2305-2025						ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution								Arxiv											3	3;2024-10-14;https://www.arxiv.org/abs/2402.01145v3| 2;2024-05-20;https://www.arxiv.org/abs/2402.01145v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01145v1	arXiv:2402.01145			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 14 2024	2024	The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design. The long-standing endeavor of design automation has gained new momentum with the rise of large language models (LLMs). This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution ( ReEvo ), a novel integration of evolutionary search for efficiently exploring the heuristic space, and LLM reflections to provide verbal gradients within the space. Across five heterogeneous algorithmic types, six different COPs, and both white-box and black-box views of COPs, ReEvo yields state-of-the-art and competitive meta-heuristics, evolutionary algorithms, heuristics, and neural solvers, while being more sample-efficient than prior LHHs.																																	2024-11-06	PPRN:87507599		
J	Dong, Zhihuan; Patri, Adarsh S.; Senthil, T.				Todadri, Senthil/ABG-1337-2021						Theory of quantum anomalous Hall phases in pentalayer rhombohedral graphene moiré structures								Arxiv											3	3;2024-09-14;https://www.arxiv.org/abs/2311.03445v3| 2;2023-11-23;https://www.arxiv.org/abs/2311.03445v2| 1;2023-11-06;https://www.arxiv.org/abs/2311.03445v1	arXiv:2311.03445			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 14 2024	2024	Remarkable recent experiments on the moire´ structure formed by pentalayer rhombohedral graphene aligned with a hexagonal Boron-Nitride substrate report the discovery of a zero field fractional quantum hall effect. These “(Fractional) Quantum Anomalous Hall” ((F)QAH) phases occur for one sign of a perpendicular displacement field, and correspond, experimentally, to full or partial filling of a valley polarized Chern-1 band. Such a band is absent in the non-interacting band structure. Here we show that electron-electron interactions play a crucial role, and present microscopic theoretical calculations demonstrating the emergence of a nearly flat, isolated, Chern-1 band and FQAH phases in this system. We also study the four and six-layer analogs and identify parameters where a nearly flat isolated Chern-1 band emerges which may be suitable to host FQAH physics.																																	2025-01-17	PPRN:86073829		
J	Xing, Zhen; Feng, Qijun; Chen, Haoran; Dai, Qi; Hu, Han; Xu, Hang; Wu, Zuxuan; Jiang, Yu-Gang				Chen, Haoran/AAN-3251-2020; Xing, Zhen/NKP-8149-2025; Dai, Qi/JXM-6895-2024						A Survey on Video Diffusion Models								Arxiv											2	2;2024-09-14;https://www.arxiv.org/abs/2310.10647v2| 1;2023-10-16;https://www.arxiv.org/abs/2310.10647v1	arXiv:2310.10647			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 14 2024	2024	The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-todate reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.																																	2024-12-24	PPRN:85660433		
J	Ghosh, Shaona; Varshney, Prasoon; Galinkin, Erick; Parisien, Christopher										AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts								Arxiv											2	2;2024-09-11;https://www.arxiv.org/abs/2404.05993v2| 1;2024-04-09;https://www.arxiv.org/abs/2404.05993v1	arXiv:2404.05993			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 11 2024	2024	As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET , a new dataset of approximately 26, 000 humanLLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS ), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose A EGIS , a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment.																																	2024-09-27	PPRN:88469159		
J	Ghafarollahi, Alireza; Buehler, Markus J.										SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning								Arxiv											1	1;2024-09-09;https://www.arxiv.org/abs/2409.05556v1	arXiv:2409.05556			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 09 2024	2024	A key challenge in artificial intelligence is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. In this work, we present SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities. Applied to biologically inspired materials, SciAgents reveals hidden interdisciplinary relationships that were previously considered unrelated, achieving a scale, precision, and exploratory power that surpasses traditional human-driven research methods. The framework autonomously generates and refines research hypotheses, elucidating underlying mechanisms, design principles, and unexpected material properties. By integrating these capabilities in a modular fashion, the intelligent system yields material discoveries, critique and improve existing hypotheses, retrieve up-to-date data about existing research, and highlights their strengths and limitations. Our case studies demonstrate scalable capabilities to combine generative AI, ontological representations, and multi-agent modeling, harnessing a ‘swarm of intelligence’ similar to biological systems. This provides new avenues for materials discovery and accelerates the development of advanced materials by unlocking Nature’s design principles.																																	2024-09-26	PPRN:91812461		
J	Zhang, Quanjun; Fang, Chunrong; Xie, Yang; Zhang, Yaxin; Yang, Yun; Sun, Weisong; Yu, Shengcheng; Chen, Zhenyu				Fang, Chunrong/GPW-9872-2022; Sun, Weisong/AAU-9503-2020; ZHANG, QUANJUN/LPP-9143-2024						A Survey on Large Language Models for Software Engineering								Arxiv											2	2;2024-09-08;https://www.arxiv.org/abs/2312.15223v2| 1;2023-12-23;https://www.arxiv.org/abs/2312.15223v1	arXiv:2312.15223			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 08 2024	2024	Software Engineering (SE) is the systematic design, development, maintenance, and management of software applications underpinning the digital infrastructure of our modern world. Very recently, the SE community has seen a rapidly increasing number of techniques employing Large Language Models (LLMs) to automate a broad range of SE tasks. Nevertheless, existing information of the applications, effects, and possible limitations of LLMs within SE is still not well-studied. In this paper, we provide a systematic survey to summarize the current state-of-the-art research in the LLM-based SE community. We summarize 62 representative LLMs of Code across three model architectures, 15 pre-training objectives across four categories, and 16 downstream tasks across five categories. We then present a detailed summarization of the recent SE studies for which LLMs are commonly utilized, including 947 studies for 112 specific code-related tasks across five crucial phases within the SE workflow. We also discuss several critical aspects during the integration of LLMs into SE, such as empirical evaluation, benchmarking, security and reliability, domain tuning, compressing and distillation. Finally, we highlight several challenges and potential opportunities on applying LLMs for future SE studies, such as exploring domain LLMs and constructing clean evaluation datasets. Overall, our work can help researchers gain a comprehensive understanding about the achievements of the existing LLM-based SE studies and promote the practical application of these techniques. 																																	2024-09-24	PPRN:86826656		
J	Chen, Jiangjie; Yuan, Siyu; Ye, Rong; Majumder, Bodhisattwa Prasad; Richardson, Kyle				Ye, Rong/NOF-3177-2025; Chen, Jiangjie/JCE-5486-2023						Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena								Arxiv											3	3;2024-08-25;https://www.arxiv.org/abs/2310.05746v4| 2;2024-04-03;https://www.arxiv.org/abs/2310.05746v3| 1;2023-10-09;https://www.arxiv.org/abs/2310.05746v1	arXiv:2310.05746			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 25 2024	2024	Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM performance and occasional outperformance by simpler methods indicate opportunities for further advancements in LLM design and the value of our simulation environment for ongoing testing and refinement.2																																	2024-09-06	PPRN:85583212		
J	Zhang, Chubin; Yan, Juncheng; Wei, Yi; Li, Jiaxin; Liu, Li; Tang, Yansong; Duan, Yueqi; Lu, Jiwen										OccNeRF: Advancing 3D Occupancy Prediction in LiDAR-Free Environments								Arxiv											3	3;2024-08-21;https://www.arxiv.org/abs/2312.09243v3| 2;2024-03-30;https://www.arxiv.org/abs/2312.09243v2| 1;2023-12-14;https://www.arxiv.org/abs/2312.09243v1	arXiv:2312.09243			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 21 2024	2024	Occupancy prediction reconstructs 3D structures of surrounding environments. It provides detailed information for autonomous driving planning and navigation. However, most existing methods heavily rely on the LiDAR point clouds to generate occupancy ground truth, which is not available in the vision-based system. In this paper, we propose an OccNeRF method for training occupancy networks without 3D supervision. Different from previous works which consider a bounded scene, we parameterize the reconstructed occupancy fields and reorganize the sampling strategy to align with the cameras' infinite perceptive range. The neural rendering is adopted to convert occupancy fields to multi-camera depth maps, supervised by multi-frame photometric consistency. Moreover, for semantic occupancy prediction, we design several strategies to polish the prompts and filter the outputs of a pretrained open-vocabulary 2D segmentation model. Extensive experiments for both self-supervised depth estimation and 3D occupancy prediction tasks on nuScenes and SemanticKITTI datasets demonstrate the effectiveness of our method.																																	2024-08-31	PPRN:86592840		
J	Shen, Fei; Jiang, Xin; He, Xin; Ye, Hu; Wang, Cong; Du, Xiaoyu; Li, Zechao; Tang, Jinhui				Jiang, Xin/IYJ-7486-2023; 沈, 飞/HKM-4286-2023; Du, Xiaoyu/AAU-5942-2021; tang, jinhui/ISS-9956-2023						IMAGDressing-v1: Customizable Virtual Dressing								Arxiv											2	2;2024-08-06;https://www.arxiv.org/abs/2407.12705v2| 1;2024-07-17;https://www.arxiv.org/abs/2407.12705v1	arXiv:2407.12705			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 06 2024	2024	Latest advances have achieved realistic virtual try-on (VTON) through localized garment inpainting using latent diffusion models, significantly enhancing consumers’ online shopping experience. However, existing VTON technologies neglect the need for merchants to showcase garments comprehensively, including flexible control over garments, optional faces, poses, and scenes. To address this issue, we define a virtual dressing (VD) task focused on generating freely editable human images with fixed garments and optional conditions. Meanwhile, we design a comprehensive affinity metric index (CAMI) to evaluate the consistency between generated images and reference garments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet that captures semantic features from CLIP and texture features from VAE. We present a hybrid attention module, including a frozen self-attention and a trainable cross-attention, to integrate garment features from the garment UNet into a frozen denoising UNet, ensuring users can control different scenes through text. IMAGDressing-v1 can be combined with other extension plugins, such as ControlNet and IP-Adapter, to enhance the diversity and controllability of generated images. Furthermore, to address the lack of data, we release the interactive garment pairing (IGPair) dataset, containing over 300,000 pairs of clothing and dressed images, and establish a standard pipeline for data assembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves state-of-the-art human image synthesis performance under various controlled conditions. 																																	2024-08-13	PPRN:90867212		
J	Stojkovic, Jovan; Zhang, Chaojie; Goiri, Inigo; Torrellas, Josep; Choukse, Esha										DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency								Arxiv											1	1;2024-08-01;https://www.arxiv.org/abs/2408.00741v1	arXiv:2408.00741			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 01 2024	2024	The rapid evolution and widespread adoption of generative large language models (LLMs) have made them a pivotal workload in various applications. Today, LLM inference clusters receive a large number of queries with strict Service Level Objectives (SLOs). To achieve the desired performance, these models execute on power-hungry GPUs causing the inference clusters to consume large amount of energy and, consequently, result in excessive carbon emissions. Fortunately, we find that there is a great opportunity to exploit the heterogeneity in inference compute properties and fluctuations in inference workloads, to significantly improve energy-efficiency. However, such a diverse and dynamic environment creates a large search-space where different system configurations (e.g., number of instances, model parallelism, and GPU frequency) translate into different energy-performance trade-offs. To address these challenges, we propose DynamoLLM, the first energy-management framework for LLM inference environments. DynamoLLM automatically and dynamically reconfigures the inference cluster to optimize for energy and cost of LLM serving under the service's performance SLOs. We show that at a service-level, DynamoLLM conserves 53% energy and 38% operational carbon emissions, and reduces 61% cost to the customer, while meeting the latency SLOs.																																	2024-08-08	PPRN:91201489		
J	Chen, Zhaorun; Xiang, Zhen; Xiao, Chaowei; Song, Dawn; Li, Bo				Xiao, Chaowei/AAT-8772-2021; Chen, Zhaorun/AAT-1611-2021						AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases								Arxiv											1	1;2024-07-17;https://www.arxiv.org/abs/2407.12784v1	arXiv:2407.12784			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 17 2024	2024	LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach A GENT P OISON , the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, A GENT P OISON requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate A GENT P OISON ’s effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of A GENT P OISON . On each agent, A GENT P OISON achieves an average attack success rate of ≥ 80% with minimal impact on benign performance ( ≤ 1%) with a poison rate < 0 .1%. 																																	2024-07-26	PPRN:90872538		
J	Lin, Haitao; Huang, Yufei; Zhang, Odin; Ma, Siqi; Liu, Meng; Wu, Lirong; Li, Xuanjing; Ji, Shuiwang; Hou, Tingjun; Li, Stan Z.				Li, SY/JPK-3839-2023; Liu, Meng/HRC-8764-2023; LI, Stan/OYF-6553-2025; Liu, Haitao/ACC-5889-2022						DiffBP: Generative Diffusion of 3D Molecules for Target Protein Binding								Arxiv											2	2;2024-07-14;https://www.arxiv.org/abs/2211.11214v4| 1;2022-11-21;https://www.arxiv.org/abs/2211.11214v2	arXiv:2211.11214			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 14 2024	2024	Generating molecules that bind to specific proteins is an important but challenging task in drug discovery. Most previous works typically generate atoms autoregressively, with element types and 3D coordinates of atoms generated one by one. However, in real-world molecular systems, interactions among atoms are global, spanning the entire molecule, leading to pair-coupled energy function among atoms. With such energy-based consideration, modeling probability should rely on joint distributions rather than sequential conditional ones. Thus, the unnatural sequential auto-regressive approach to molecule generation is prone to violating physical rules, yielding molecules with unfavorable properties. In this study, we propose DiffBP, a generative diffusion model that generates molecular 3D structures, leveraging target proteins as contextual constraints at the full-atom level in a non-autoregressive way. Given a designated 3D protein binding site, our model learns to denoise both element types and 3D coordinates of an entire molecule using an equivariant network. In experimental evaluations, DiffBP demonstrates competitive performance against existing methods, generating molecules with high protein affinity, appropriate molecule sizes, and favorable drug-like profiles. Additionally, we developed a website server for medicinal chemists interested in exploring the art of molecular generation, 																																	2024-07-23	PPRN:23256470		
J	Lin, Bin; Zhang, Chen; Peng, Tao; Zhao, Hanyu; Xiao, Wencong; Sun, Minmin; Liu, Anmin; Zhang, Zhipeng; Li, Lanbo; Qiu, Xiafei; Li, Shen; Ji, Zhigang; Xie, Tao; Li, Yong; Lin, Wei				Tao, Peng/H-4925-2014; zhang, zhipeng/GRR-8534-2022; Zhang, Jie/HNR-9865-2023; Zhao, Hanyu/AAP-2268-2020						Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache								Arxiv											2	2;2024-07-04;https://www.arxiv.org/abs/2401.02669v2| 1;2024-01-05;https://www.arxiv.org/abs/2401.02669v1	arXiv:2401.02669			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 04 2024	2024	Large Language Models (LLMs) demonstrate substantial potential across a diverse array of domains via request serving. However, as trends continue to push for expanding context sizes, the autoregressive nature of LLMs results in highly dynamic behavior of the attention layers, showcasing significant differences in computational characteristics and memory requirements from the non-attention layers. This presents substantial challenges for resource management and performance optimization in service systems. Existing static model parallelism and resource allocation strategies fall short when dealing with this dynamicity. To address the issue, we propose Infinite-LLM, a novel LLM serving system designed to effectively handle dynamic context lengths. Infinite-LLM disaggregates attention layers from an LLM’s inference process, facilitating flexible and independent resource scheduling that optimizes computational performance and enhances memory utilization jointly. By leveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only significantly boosts system throughput but also supports extensive context lengths. Evaluated on a dataset with context lengths ranging from a few to 2000K tokens across a cluster with 32 A100 GPUs, InfiniteLLM demonstrates throughput improvement of 1.35-3.4x compared to state-of-the-art methods, enabling efficient and elastic LLM deployment.																																	2024-07-20	PPRN:86995783		
J	Ko, Jongwoo; Kim, Sungnyun; Chen, Tianyi; Yun, Se-Young										DistiLLM: Towards Streamlined Distillation for Large Language Models								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2402.03898v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.03898v1	arXiv:2402.03898			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 03 2024	2024	Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for autoregressive sequence models ( e.g. , large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training -inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DISTILLM, a more effective and efficient KD framework for auto -regressive language models. DISTILLM  comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off -policy approach designed to enhance the efficiency in utilizing student -generated outputs. Extensive experiments, including instruction -following tasks, demonstrate the effectiveness of DISTILLM ISTI LLM in building high -performing student models while achieving up to 4.3×  speedup compared to recent KD methods.																																	2024-07-19	PPRN:87534496		
J	Zhang, Yiming; Gu, Yicheng; Zeng, Yanhong; Xing, Zhening; Wang, Yuancheng; Wu, Zhizheng; Chen, Kai				Wang, Yuancheng/GLR-2067-2022; zeng, yanhong/Y-2891-2018						FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.01494v1	arXiv:2407.01494			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	We study Neural Foley, the automatic generation of high-quality sound effects synchronizing with videos, enabling an immersive audio-visual experience. Despite its wide range of applications, existing approaches encounter limitations when it comes to simultaneously synthesizing high-quality and video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To overcome these limitations, we propose FoleyCrafter, a novel framework that leverages a pre-trained text-to-audio model to ensure high-quality audio generation. FoleyCrafter comprises two key components: the semantic adapter for semantic alignment and the temporal controller for precise audio-video synchronization. The semantic adapter utilizes parallel cross-attention layers to condition audio generation on video features, producing realistic sound effects that are semantically relevant to the visual content. Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment. One notable advantage of FoleyCrafter is its compatibility with text prompts, enabling the use of text descriptions to achieve controllable and diverse video-to-audio generation according to user intents. We conduct extensive quantitative and qualitative experiments on standard benchmarks to verify the effectiveness of FoleyCrafter. 																																	2024-07-20	PPRN:90659330		
J	Li, Huao; Chong, Yu Quan; Stepputtis, Simon; Campbell, Joseph; Hughes, Dana; Lewis, Michael; Sycara, Katia				Li, Huao/ABE-3168-2021						Theory of Mind for Multi-Agent Collaboration via Large Language Models								Arxiv											3	3;2024-06-26;https://www.arxiv.org/abs/2310.10701v3| 2;2023-10-22;https://www.arxiv.org/abs/2310.10701v2| 1;2023-10-16;https://www.arxiv.org/abs/2310.10701v1	arXiv:2310.10701			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 26 2024	2024	While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.																																	2024-07-17	PPRN:85680091		
J	Zhou, Wangchunshu; Ou, Yixin; Ding, Shengwei; Li, Long; Wu, Jialong; Wang, Tiannan; Chen, Jiamin; Wang, Shuai; Xu, Xiaohua; Zhang, Ningyu; Chen, Huajun; Jiang, Yuchen Eleanor				Chen, Jiamin/KDM-5096-2024						Symbolic Learning Enables Self-Evolving Agents								Arxiv											1	1;2024-06-26;https://www.arxiv.org/abs/2406.18532v1	arXiv:2406.18532			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 26 2024	2024	The AI community has been exploring a pathway to artificial general intelligence (AGI) by developing "language agents", which are complex large language models (LLMs) pipelines involving both prompting techniques and tool usage methods. While language agents have demonstrated impressive capabilities for many real-world tasks, a fundamental limitation of current language agents research is that they are model-centric, or engineering-centric. That's to say, the progress on prompts, tools, and pipelines of language agents requires substantial manual engineering efforts from human experts rather than automatically learning from data. We believe the transition from model-centric, or engineering-centric, to data-centric, i.e., the ability of language agents to autonomously learn and evolve in environments, is the key for them to possibly achieve AGI. In this work, we introduce agent symbolic learning, a systematic framework that enables language agents to optimize themselves on their own in a data-centric way using symbolic optimizers. Specifically, we consider agents as symbolic networks where learnable weights are defined by prompts, tools, and the way they are stacked together. Agent symbolic learning is designed to optimize the symbolic network within language agents by mimicking two fundamental algorithms in connectionist learning: back-propagation and gradient descent. Instead of dealing with numeric weights, agent symbolic learning works with natural language simulacrums of weights, loss, and gradients. We conduct proof-of-concept experiments on both standard benchmarks and complex real-world tasks and show that agent symbolic learning enables language agents to update themselves after being created and deployed in the wild, resulting in "self-evolving agents".																																	2024-07-15	PPRN:89901947		
J	Yuan, Zhengqing; Li, Zhaoxu; Huang, Weiran; Ye, Yanfang; Sun, Lichao				Li, Zhaoxu/JTS-6041-2023; Yuan, Zhengqing/HTS-6231-2023						TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones								Arxiv											3	3;2024-06-21;https://www.arxiv.org/abs/2312.16862v3| 2;2024-04-04;https://www.arxiv.org/abs/2312.16862v2| 1;2023-12-28;https://www.arxiv.org/abs/2312.16862v1	arXiv:2312.16862			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 21 2024	2024	In recent years, multimodal large language models (MLLMs) such as GPT-4V have demonstrated remarkable advancements, excelling in a variety of vision-language tasks. Despite their prowess, the closed-source nature and computational demands of such models limit their accessibility and applicability. This study introduces TinyGPT-V, a novel open-source MLLM, designed for efficient training and inference across various vision-language tasks, including image captioning (IC) and visual question answering (VQA). Leveraging a compact yet powerful architecture, TinyGPT-V integrates the Phi-2 language model with pre-trained vision encoders, utilizing a unique mapping module for visual and linguistic information fusion. With a training regimen optimized for small backbones and employing a diverse dataset amalgam, TinyGPT-V requires significantly lower computational resources 24GB for training and as little as 8GB for inference without compromising on performance. Our experiments demonstrate that TinyGPT-V, with its language model 2.8 billion parameters, achieves comparable results in VQA and image inference tasks to its larger counterparts while being uniquely suited for deployment on resource-constrained devices through innovative quantization techniques. This work not only paves the way for more accessible and efficient MLLMs but also underscores the potential of smaller, optimized models in bridging the gap between high performance and computational efficiency in real-world applications. Additionally, this paper introduces a new approach to multimodal large language models using smaller backbones. Our code and training weights are available in the supplementary material.																																	2024-07-11	PPRN:86852246		
J	Zhou, Xingcheng; Liu, Mingyu; Yurtsever, Ekim; Zagar, Bare Luka; Zimmer, Walter; Cao, Hu; Knoll, Alois C.				Knoll, Alois/AAN-8417-2021; Cao, Hu/LTY-8027-2024; Zhou, Xingcheng/OGQ-9992-2025; Liu, Mingyu/ABC-4695-2020						Vision Language Models in Autonomous Driving: A Survey and Outlook								Arxiv											2	2;2024-06-20;https://www.arxiv.org/abs/2310.14414v2| 1;2023-10-22;https://www.arxiv.org/abs/2310.14414v1	arXiv:2310.14414			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By incorporating language data, driving systems can gain a better understanding of real-world environments, thereby enhancing driving safety and efficiency. In this work, we present a comprehensive and systematic survey of the advances in vision language models in this domain, encompassing perception and understanding, navigation and planning, decision-making and control, end-to-end autonomous driving, and data generation. We introduce the mainstream VLM tasks in AD and the commonly utilized metrics. Additionally, we review current studies and applications in various areas and summarize the existing language-enhanced autonomous driving datasets thoroughly. Lastly, we discuss the benefits and challenges of VLMs in AD and provide researchers with the current research gaps and future trends.																																	2024-07-15	PPRN:85757802		
J	Ma, Weiyu; Mi, Qirui; Zeng, Yongcheng; Yan, Xue; Wu, Yuqiao; Lin, Runji; Zhang, Haifeng; Wang, Jun				Ma, Weiyu/NJS-4865-2025						Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach								Arxiv											2	2;2024-06-18;https://www.arxiv.org/abs/2312.11865v3| 1;2023-12-19;https://www.arxiv.org/abs/2312.11865v1	arXiv:2312.11865			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	StarCraft II is a challenging benchmark for AI agents due to the necessity of both precise micro level operations and strategic macro awareness. Previous works, such as Alphastar and SCC, achieve impressive performance on tackling StarCraft II , however, still exhibit deficiencies in long term strategic planning and strategy interpretability. Emerging large language model (LLM) agents, such as Voyage and MetaGPT, presents the immense potential in solving intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs on StarCraft II, a highly complex RTS game.To conveniently take full advantage of LLMs` reasoning abilities, we first develop textual StratCraft II environment, called TextStarCraft II, which LLM agent can interact. Secondly, we propose a Chain of Summarization method, including single frame summarization for processing raw observations and multi frame summarization for analyzing game information, providing command recommendations, and generating strategic decisions. Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the relevant knowledge and complex planning abilities needed to address StarCraft II scenarios; 2. Human experts consider the performance of LLM agents to be close to that of an average player who has played StarCraft II for eight years; 3. LLM agents are capable of defeating the built in AI at the Harder(Lv5) difficulty level. We have open sourced the code and released demo videos of LLM agent playing StarCraft II.																																	2024-07-04	PPRN:86726960		
J	Ghosh, Sreyan; Kumar, Sonal; Seth, Ashish; Evuru, Chandra Kiran Reddy; Tyagi, Utkarsh; Sakshi, S; Nieto, Oriol; Duraiswami, Ramani; Manocha, Dinesh				Ghosh, Dr.Shyamasree/ABA-4456-2021; Seth, Ashish/HZJ-2429-2023; Duraiswami, Ramani/J-6070-2012; Nieto, Oriol/AAT-9148-2021						GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.11768v1	arXiv:2406.11768			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning and instruction following capabilities.																																	2024-07-04	PPRN:89343692		
J	Chin, Zhi-Yi; Jiang, Chieh-Ming; Huang, Ching-Chun; Chen, Pin-Yu; Chiu, Wei-Chen				Chen, Pin-Yu/AAA-1059-2020						Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts								Arxiv											1	1;2024-06-08;https://www.arxiv.org/abs/2309.06135v2	arXiv:2309.06135			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 08 2024	2024	Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered "safe" can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.																																	2024-07-04	PPRN:89265034		
J	Tao, Zhengwei; Lin, Ting-En; Chen, Xiancai; Li, Hangyu; Wu, Yuchuan; Li, Yongbin; Jin, Zhi; Huang, Fei; Tao, Dacheng; Zhou, Jingren				wu, yuchuan/NTR-2009-2025; Jin, Zhi/AAB-2440-2022; Li, Yongbin/GWM-7528-2022; Zhou, Mingyuan/AAE-8717-2021; Shen, Li/AEZ-9528-2022						A Survey on Self-Evolution of Large Language Models								Arxiv											2	2;2024-06-03;https://www.arxiv.org/abs/2404.14387v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.14387v1	arXiv:2404.14387			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve selfevolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-SelfEvolution-of-LLM. †																																	2024-06-22	PPRN:88614838		
J	Neel, Seth; Chang, Peter W										Privacy Issues in Large Language Models: A Survey								Arxiv											4	4;2024-05-30;https://www.arxiv.org/abs/2312.06717v4| 3;2024-02-20;https://www.arxiv.org/abs/2312.06717v3| 2;2024-01-23;https://www.arxiv.org/abs/2312.06717v2| 1;2023-12-11;https://www.arxiv.org/abs/2312.06717v1	arXiv:2312.06717			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 30 2024	2024	This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we have missed some of your work please contact us, as we will attempt to keep this survey relatively up to date. 																																	2024-11-09	PPRN:86555230		
J	Stark, Hannes; Jing, Bowen; Wang, Chenyu; Corso, Gabriele; Berger, Bonnie; Barzilay, Regina; Jaakkola, Tommi										Dirichlet Flow Matching with Applications to DNA Sequence Design								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2402.05841v2| 1;2024-02-08;https://www.arxiv.org/abs/2402.05841v1	arXiv:2402.05841			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that naïve linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in O(L) speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets. 																																	2024-11-09	PPRN:87573002		
J	Yao, Jin; Chien, Eli; Du, Minxin; Niu, Xinyao; Wang, Tianhao; Cheng, Zezhou; Yue, Xiang				Wang, Tianhao/AAU-5463-2021						Machine Unlearning of Pre-trained Large Language Models								Arxiv											3	3;2024-05-30;https://www.arxiv.org/abs/2402.15159v3| 2;2024-02-27;https://www.arxiv.org/abs/2402.15159v2| 1;2024-02-23;https://www.arxiv.org/abs/2402.15159v1	arXiv:2402.15159			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 30 2024	2024	This study investigates the concept of the 'right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over 105 times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.1																																	2024-06-16	PPRN:87870577		
J	Guo, Siyuan; Deng, Cheng; Wen, Ying; Chen, Hechang; Chang, Yi; Wang, Jun				Guo, Siyuan/GRF-5525-2022; Chen, Hechang/Y-7950-2018						DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning								Arxiv											5	5;2024-05-28;https://www.arxiv.org/abs/2402.17453v5| 4;2024-05-24;https://www.arxiv.org/abs/2402.17453v4| 3;2024-04-06;https://www.arxiv.org/abs/2402.17453v3| 2;2024-03-13;https://www.arxiv.org/abs/2402.17453v2| 1;2024-02-27;https://www.arxiv.org/abs/2402.17453v1	arXiv:2402.17453			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing $1.60 and $0.13 per run with GPT-4, respectively. 																																	2024-06-12	PPRN:87919312		
J	Brandon, William; Mishra, Mayank; Nrusimha, Aniruddha; Panda, Rameswar; Kelly, Jonathan Ragan				Panda, Rameswar/AAY-9834-2020						Reducing Transformer Key-Value Cache Size with Cross-Layer Attention								Arxiv											1	1;2024-05-21;https://www.arxiv.org/abs/2405.12981v1	arXiv:2405.12981			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 21 2024	2024	Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another 2x while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible																																	2024-08-23	PPRN:91459698		
J	Mishra, Mayank; Stallone, Matt; Zhang, Gaoyuan; Shen, Yikang; Prasad, Aditya; Soria, Adriana Meza; Merler, Michele; Selvam, Parameswaran; Surendran, Saptha; Singh, Shivdeep; Sethi, Manish; Dang, Xuan-Hong; Li, Pengyuan; Wu, Kun-Lung; Zawad, Syed; Coleman, Andrew; White, Matthew; Lewis, Mark; Pavuluri, Raju; Koyfman, Yan; Lublinsky, Boris; de Bayser, Maximilien; Abdelaziz, Ibrahim; Basu, Kinjal; Agarwal, Mayank; Zhou, Yi; Johnson, Chris; Goyal, Aanchal; Patel, Hima; Shah, Yousaf; Zerfos, Petros; Ludwig, Heiko; Munawar, Asim; Crouse, Maxwell; Kapanipathi, Pavan; Salaria, Shweta; Calio, Bob; Wen, Sophia; Seelam, Seetharami; Belgodere, Brian; Fonseca, Carlos; Singhee, Amith; Desai, Nirmit; Cox, David D.; Puri, Ruchir; Panda, Rameswar				Panda, Rameswar/AAY-9834-2020; Cox, David/C-4888-2008; Li, Pengyuan/GNW-6822-2022; Munawar, Asim/KJL-6373-2024; Basu, Kinjal/MFH-7151-2025; Abdelaziz, Ibrahim/AAE-2152-2021						Granite Code Models: A Family of Open Foundation Models for Code Intelligence								Arxiv											1	1;2024-05-07;https://www.arxiv.org/abs/2405.04324v1	arXiv:2405.04324			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 07 2024	2024	Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open -source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile “all around” code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.																																	2024-06-04	PPRN:88975763		
J	Mizrahi, Moran; Kaplan, Guy; Malkin, Dan; Dror, Rotem; Shahaf, Dafna; Stanovsky, Gabriel				Dror, Rotem/JXY-2685-2024; Stanovsky, Gabriel/MSW-2061-2025						State of What Art? A Call for Multi-Prompt LLM Evaluation								Arxiv											3	3;2024-05-06;https://www.arxiv.org/abs/2401.00595v3| 2;2024-01-30;https://www.arxiv.org/abs/2401.00595v2| 1;2023-12-31;https://www.arxiv.org/abs/2401.00595v1	arXiv:2401.00595			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a largescale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single -prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, , specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.																																	2024-05-25	PPRN:86904035		
J	Figueroa, Daniel G.; Pieroni, Mauro; Ricciardone, Angelo; Simakachorn, Peera										Cosmological Background Interpretation of Pulsar Timing Array Data								Arxiv											2	2;2024-04-26;https://www.arxiv.org/abs/2307.02399v3| 1;2023-07-05;https://www.arxiv.org/abs/2307.02399v1	arXiv:2307.02399			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	We discuss the interpretation of the detected signal by Pulsar Timing Array (PTA) observations as a gravitational wave background (GWB) of cosmological origin. We combine NANOGrav 15 -years and EPTA-DR2new data sets and confront them against backgrounds from supermassive black hole binaries (SMBHBs), and cosmological signals from inflation, cosmic (super)strings, first -order phase transitions, Gaussian and non -Gaussian large scalar fluctuations, and audible axions. We find that scalar -induced, and to a lesser extent audible axion and cosmic superstring signals, provide a better fit than SMBHBs. These results depend, however, on modeling assumptions, so further data and analysis are needed to reach robust conclusions. Independently of the signal origin, the data strongly constrain the parameter space of cosmological signals, for example, setting an upper bound on primordial non-Gaussianity at PTA scales as |f nl | ≲ 2.34 .34 at 95% CL.																																	2024-05-05	PPRN:73800567		
J	Liu, Jiacheng; Cohen, Andrew; Pasunuru, Ramakanth; Choi, Yejin; Hajishirzi, Hannaneh; Celikyilmaz, Asli				Liu, Jiacheng/ISS-1763-2023						Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding								Arxiv											3	3;2024-04-02;https://www.arxiv.org/abs/2309.15028v3| 2;2023-10-18;https://www.arxiv.org/abs/2309.15028v2| 1;2023-09-26;https://www.arxiv.org/abs/2309.15028v1	arXiv:2309.15028			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS greatly improves the preferability of generated text compared to the standard practice of using only the PPO policy. Our results demonstrate the promise of search algorithms even on top of the aligned language models from PPO, and the under-explored benefit of the value network. [GRAPHICS]																																	2024-04-18	PPRN:85224474		
J	Li, Jia; Li, Ge; Zhang, Xuanming; Dong, Yihong; Jin, Zhi				Dong, Yihong/LCE-6194-2024; Jin, Zhi/E-1288-2013						EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories								Arxiv											1	1;2024-03-31;https://www.arxiv.org/abs/2404.00599v1	arXiv:2404.00599			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 31 2024	2024	How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.1																																	2024-04-17	PPRN:88360535		
J	Gandelsman, Yossi; Efros, Alexei A.; Steinhardt, Jacob				Gandelsman, Yossi/MFJ-8895-2025						Interpreting CLIP's Image Representation via Text-Based Decomposition								Arxiv											3	3;2024-03-29;https://www.arxiv.org/abs/2310.05916v4| 2;2024-01-22;https://www.arxiv.org/abs/2310.05916v3| 1;2023-10-10;https://www.arxiv.org/abs/2310.05916v2	arXiv:2310.05916			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP’s text representation to interpret the summands. Interpreting the attention heads, we characterize each head’s role by automatically finding text representations that span its output space, which reveals property -specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero -shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.1																																	2024-04-17	PPRN:85524531		
J	Huang, Xinyu; Zhang, Youcai; Ma, Jinyu; Tian, Weiwei; Feng, Rui; Zhang, Yuejie; Li, Yaqian; Guo, Yandong; Zhang, Lei				huang, xinyu/HCH-0792-2022; tian, weiwei/GSN-8325-2022						Tag2Text: Guiding Vision-Language Model via Image Tagging								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2303.05657v3| 2;2023-09-20;https://www.arxiv.org/abs/2303.05657v2| 1;2023-03-10;https://www.arxiv.org/abs/2303.05657v1	arXiv:2303.05657			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	This paper presents Tag2Text, a vision language pre-training (VLP) framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works which utilize object tags either manually labeled or automatically detected with an off-the-shelf detector with limited performance, our approach explicitly learns an image tagger using tags parsed from image-paired text and thus provides a strong semantic guidance to vision-language models. In this way, Tag2Text can utilize large-scale annotation-free image tags in accordance with image-text pairs, and provides more diverse tag categories beyond objects. As a result, Tag2Text demonstrates the ability of a foundational image tagging model, with superior zero-shot performance even comparable to fully supervised models. Moreover, by leveraging the tagging guidance, Tag2Text effectively enhances the performance of vision-language models on both generation-based and alignment-based tasks. Across a wide range of downstream benchmarks, Tag2Text achieves state-of-the-art results with similar model sizes and data scales, demonstrating the efficacy of the proposed tagging guidance. Code, demo and pre-trained models are available at https://github.com/xinyu1205/recognize-anything.																																	2024-04-11	PPRN:46087944		
J	Verresen, Ruben; Borla, Umberto; Vishwanath, Ashvin; Moroz, Sergej; Thorngren, Ryan				Moroz, Sergej/A-6545-2019						Higgs Condensates are Symmetry-Protected Topological Phases: I. Discrete Symmetries								Arxiv											2	2;2024-03-15;https://www.arxiv.org/abs/2211.01376v2| 1;2022-11-02;https://www.arxiv.org/abs/2211.01376v1	arXiv:2211.01376			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 15 2024	2024	Where in the landscape of many-body phases of matter do we place the Higgs condensate of a gauge theory? On the one hand, the Higgs phase is gapped, has no local order parameter, and for fundamental Higgs fields is adiabatically connected to the confined phase. On the other hand, Higgs phases such as superconductors display rich phenomenology. In this work, we propose a minimal description of the Higgs phase as a symmetry-protected topological (SPT) phase, utilizing conventional and higher-form symmetries. In this first part, we focus on 2+1D $mathbb Z_2$ gauge theory and find that the Higgs phase is protected by a higher-form magnetic symmetry and a matter symmetry, whose meaning depends on the physical context. While this proposal captures known properties of Higgs phases, it also predicts that the Higgs phase of the Fradkin-Shenker model has SPT edge modes in the symmetric part of the phase diagram, which we confirm analytically. In addition, we argue that this SPT property is remarkably robust upon explicitly breaking the magnetic symmetry. Although the Higgs and confined phases are then connected without a bulk transition, they are separated by a boundary phase transition, which we confirm with tensor network simulations. More generally, the boundary anomaly of the Higgs SPT phase coincides with the emergent anomaly of symmetry-breaking phases, making precise the relation between Higgs phases and symmetry breaking. The SPT nature of the Higgs phase can also manifest in the bulk, e.g., at transitions between distinct Higgs condensates. Finally, we extract insights which are applicable to general SPT phases, such as a 'bulk-defect correspondence' generalizing discrete gauge group analogs of Superconductor-Insulator-Superconductor (SIS) junctions. The sequel to this work will generalize 'Higgs=SPT' to continuous symmetries, interpreting superconductivity as an SPT property.																																	2024-04-11	PPRN:22461874		
J	Calandriello, Daniele; Guo, Daniel; Munos, Remi; Rowland, Mark; Tang, Yunhao; Pires, Bernardo Avila; Richemond, Pierre Harvey; Lan, Charline Le; Valko, Michal; Liu, Tianqi; Joshi, Rishabh; Zheng, Zeyu; Piot, Bilal				Liu, Tianqi/KIC-6685-2024; zheng, zeyu/A-3867-2013						Human Alignment of Large Language Models through Online Preference Optimisation								Arxiv											1	1;2024-03-13;https://www.arxiv.org/abs/2403.08635v1	arXiv:2403.08635			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 13 2024	2024	Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD. This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model. Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play. Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm. We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task.																																	2024-04-08	PPRN:88127102		
J	Siddiq, Mohammed Latif; Santos, Joanna C.S.; Tanvir, Ridwanul Hasan; Ulfat, Noshin; Al Rifat, Fahmid; Lopes, Vinicius Carvalho				da Silva Santos, Joanna Cecilia/AAF-7544-2021; Siddiq, Mohammed Latif/AFS-5042-2022						Using Large Language Models to Generate JUnit Tests: An Empirical Study								Arxiv											4	4;2024-03-09;https://www.arxiv.org/abs/2305.00418v4| 3;2024-01-22;https://www.arxiv.org/abs/2305.00418v3| 2;2023-10-30;https://www.arxiv.org/abs/2305.00418v2| 1;2023-04-30;https://www.arxiv.org/abs/2305.00418v1	arXiv:2305.00418			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 09 2024	2024	A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.																																	2024-04-07	PPRN:66512660		
J	Ali, Ameen; Zimerman, Itamar; Wolf, Lior										The Hidden Attention of Mamba Models								Arxiv											2	2;2024-03-31;https://www.arxiv.org/abs/2403.01590v2| 1;2024-03-03;https://www.arxiv.org/abs/2403.01590v1	arXiv:2403.01590			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 03 2024	2024	The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains including NLP, long-range sequences processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available .																																	2024-07-04	PPRN:88480001		
J	Liu, Yaofang; Cun, Xiaodong; Liu, Xuebo; Wang, Xintao; Zhang, Yong; Chen, Haoxin; Liu, Yang; Zeng, Tieyong; Chan, Raymond; Shan, Ying				chen, haoxin/IYS-2513-2023; Zeng, Tieyong/B-7147-2009; Liu, Baochang/X-3464-2019; Cun, Xiaodong/AAA-4674-2022						EvalCrafter: Benchmarking and Evaluating Large Video Generation Models								Arxiv											2	2;2023-10-18;https://www.arxiv.org/abs/2310.11440v2| 1;2024-03-01;	arXiv:2310.11440			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 01 2024	2024	The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services have been developed to generate high-quality videos. However, these methods often use a few metrics, e.g., FVD or IS, to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a novel framework and pipeline for exhaustively evaluating the performance of the generated videos. Our approach involves generating a diverse and comprehensive list of 700 prompts for text-to-video generation, which is based on an analysis of real-world user data and generated with the assistance of a large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmark, in terms of visual qualities, content qualities, motion qualities, and text-video alignment with 17 well-selected objective metrics. To obtain the final leaderboard of the models, we further fit a series of coefficients to align the objective metrics to the users' opinions. Based on the proposed human alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.																																	2025-11-07	PPRN:85702962		
J	Balloccu, Simone; Schmidtova, Patricia; Lango, Mateusz; Dusek, Ondrej				Dusek, Ondrej/J-7852-2017; Lango, Mateusz/O-7329-2016						Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs								Arxiv											2	2;2024-02-22;https://www.arxiv.org/abs/2402.03927v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.03927v1	arXiv:2402.03927			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to ∼4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues.																																	2024-03-21	PPRN:87529367		
J	Yang, Shu; Ali, Muhammad Asif; Wang, Cheng-Long; Hu, Lijie; Wang, Di				Hu, Lijie/KTI-4317-2024; Chenglong, Wang/JJF-4560-2023						MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning								Arxiv											1	1;2024-02-17;https://www.arxiv.org/abs/2402.11260v1	arXiv:2402.11260			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 17 2024	2024	Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture - of -Experts augmented Low Rank Adaptation for Lifelong learning. MoRAL combines the multi -tasking abilities of MoE with the finetuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple questionanswer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L -bench) encompassing a newly curated dataset of question -answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open -book and closedbook settings. Experimental evaluation shows (i) LLMs learn fast in open -book settings with up to 30.15% improvement in "RA" for Phi -22.7B compared to closed -book (for models finetuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.																																	2024-03-19	PPRN:87762375		
J	Vidgen, Bertie; Scherrer, Nino; Kirk, Hannah Rose; Qian, Rebecca; Kannappan, Anand; Hale, Scott A.; Roettger, Paul				A, KANNAPPAN/GOJ-9062-2022						SIMPLESAFETYTESTS: A Test Suite for Identifying Critical Safety Risks in Large Language Models								Arxiv											1	1;2024-02-16;https://www.arxiv.org/abs/2311.08370v2	arXiv:2311.08370			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 16 2024	2024	The past year has seen rapid acceleration in the development of large language models (LLMs). However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content. We introduce SIMPLESAFETYTESTS as a new test suite for rapidly and systematically identifying such critical safety risks. The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with. We test 11 open -access and open-source LLMs and 4 closed-source LLMs, and find critical safety weaknesses. While some of the models do not give a single unsafe response, most give unsafe responses to more than 20% of the prompts, with over 50% unsafe responses in the extreme. Prepending a safety -emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happening. Trained annotators labelled every model response to SIMPLESAFETYTESTS (n = 3, 000). We use these annotations to evaluate five AI safety filters (which assess whether a models’ response is unsafe given a prompt) as a way of automatically evaluating models’ performance on SST. The filters’ performance varies, with differences by harm area and whether the responses are unsafe or safe. The widely-used Perspective API has 72% accuracy and a newly-created zero -shot prompt to OpenAI’s GPT-4 performs best with 89% accuracy.																																	2024-03-14	PPRN:86737730		
J	Chen, Lichang; Zhu, Chen; Soselia, Davit; Chen, Jiuhai; Zhou, Tianyi; Goldstein, Tom; Huang, Heng; Shoeybi, Mohammad; Catanzaro, Bryan										ODIN: Disentangled Reward Mitigates Hacking in RLHF								Arxiv											1	1;2024-02-11;https://www.arxiv.org/abs/2402.07319v1	arXiv:2402.07319			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 11 2024	2024	In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the other trained to decorrelate with length and therefore focus more on the actual content. We then discard the length head in RL to prevent reward hacking on length. Experiments demonstrate that our approach almost eliminates the reward correlation with length, and improves the obtained policy by a significant margin.																																	2024-05-25	PPRN:87631726		
J	Moller, Anders Giovanni; Dalsgaard, Jacob Aarup; Pera, Arianna; Aiello, Luca Maria				Aiello, Luca Maria/ABB-2507-2021						The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks								Arxiv											2	2;2024-02-05;https://www.arxiv.org/abs/2304.13861v2| 1;2023-04-26;https://www.arxiv.org/abs/2304.13861v1	arXiv:2304.13861			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 05 2024	2024	In the realm of Computational Social Science(CSS), practitioners often navigate complex,low-resource domains and face the costly andtime-intensive challenges of acquiring and an-notating data. We aim to establish a set ofguidelines to address such challenges, compar-ing the use of human-labeled data with synthet-ically generated data from GPT-4 and Llama-2 in ten distinct CSS classification tasks ofvarying complexity. Additionally, we exam-ine the impact of training data sizes on perfor-mance. Our findings reveal that models trainedon human-labeled data consistently exhibit su-perior or comparable performance comparedto their synthetically augmented counterparts.Nevertheless, synthetic augmentation provesbeneficial, particularly in improving perfor-mance on rare classes within multi-class tasks.Furthermore, we leverage GPT-4 and Llama-2for zero-shot classification and find that, whilethey generally display strong performance, theyoften fall short when compared to specializedclassifiers trained on moderately sized trainingsets																																	2024-02-21	PPRN:65759153		
J	Zhan, Yang; Xiong, Zhitong; Yuan, Yuan				zhan, yang/HTM-0139-2023; Xiong, Zhitong/HPC-7558-2023; Yuan, Yuan/GXW-1549-2022						SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model								Arxiv											1	1;2024-01-18;https://www.arxiv.org/abs/2401.09712v1	arXiv:2401.09712			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 18 2024	2024	Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities. However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding. To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions. After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples. Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules. Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks. In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities. Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT's superiority in image-level and region-level tasks, such as captioning and visual grounding. In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests. The online demo, code, and dataset will be released in https://github.com/ZhanYang-nwpu/SkyEyeGPT.																																	2024-05-25	PPRN:87224754		
J	Kim, Bo-Kyeong; Song, Hyoung-Kyu; Castells, Thibault; Choi, Shinkook				Kim, Bo/ABD-2089-2020						BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion								Arxiv											4	4;2024-12-02;https://www.arxiv.org/abs/2305.15798v4| 3;2023-11-16;https://www.arxiv.org/abs/2305.15798v3| 2;2023-09-30;https://www.arxiv.org/abs/2305.15798v2| 1;2023-05-25;https://www.arxiv.org/abs/2305.15798v1	arXiv:2305.15798			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves high computing demands due to billion-scale parameters. To enhance efficiency, recent studies have reduced sampling steps and applied network quantization while retaining the original architectures. The lack of architectural reduction attempts may stem from worries over expensive retraining for such massive models. In this work, we uncover the surprising potential of block pruning and feature distillation for low-cost general-purpose T2I. By removing several residual and attention blocks from the U-Net of SDMs, we achieve 30%∼50% reduction in model size, MACs, and latency. We show that distillation retraining is effective even under limited resources: using only 13 A100 days and a tiny dataset, our compact models can imitate the original SDMs (v1.4 and v2.1-base with over 6,000 A100 days). Benefiting from the transferred knowledge, our BKSDMs deliver competitive results on zero-shot MS-COCO against larger multi-billion parameter models. We further demonstrate the applicability of our lightweight backbones in personalized generation and image-to- image translation. Deployment of our models on edge devices attains 4-second inference.																																	2025-03-27	PPRN:72716067		
J	Wang, Jiangshan; Pu, Junfu; Qi, Zhongang; Guo, Jiayi; Ma, Yue; Huang, Nisha; Chen, Yuxin; Li, Xiu; Shan, Ying				Pu, Junfu/LKJ-1092-2024						Taming Rectified Flow for Inversion and Editing								Arxiv											2	2;2024-11-28;https://www.arxiv.org/abs/2411.04746v2| 1;2024-11-07;https://www.arxiv.org/abs/2411.04746v1	arXiv:2411.04746			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 28 2024	2024	Rectified-flow-based diffusion transformers like FLUX and OpenSora have demonstrated outstanding performance in the field of image and video generation. Despite their robust generative capabilities, these models often struggle with inversion inaccuracies, which could further limit their effectiveness in downstream tasks such as image and video editing. To address this issue, we propose RF-Solver, a novel training-free sampler that effectively enhances inversion precision by mitigating the errors in the ODE-solving process of rectified flow. Specifically, we derive the exact formulation of the rectified flow ODE and apply the high-order Taylor expansion to estimate its nonlinear components, significantly enhancing the precision of ODE solutions at each timestep. Building upon RF-Solver, we further propose RF-Edit, a general feature-sharing-based framework for image and video editing. By incorporating self-attention features from the inversion process into the editing process, RF-Edit effectively preserves the structural information of the source image or video while achieving high-quality editing results. Our approach is compatible with any pre-trained rectified-flow-based models for image and video tasks, requiring no additional training or optimization. Extensive experiments across generation, inversion, and editing tasks in both image and video modalities demonstrate the superiority and versatility of our method. 																																	2025-01-10	PPRN:119074397		
J	Bainbridge, Matt; Chen, Dawei; Gendron, Quentin; Grushevsky, Samuel; Moeller, Martin				Chen, Dawei/ACX-4433-2022						The moduli space of multi-scale differentials								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/1910.13492v3	arXiv:1910.13492			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	We construct a compactification PΞM(sic)g,n(µ) of the moduli spaces of abelian differentials on Riemann surfaces with prescribed zeroes and poles. This compactification, called the moduli space of multi-scale differentials, is a complex orbifold with normal crossing boundary. Locally, PΞM(sic)g,n(µ) can be described as the normalization of an explicit blowup of the incidence variety compactification, which was defined in [BCGGM18] as the closure of the stratum of abelian differentials in the closure of the Hodge bundle. We also define families of projectivized multi-scale differentials, which gives a proper smooth Deligne-Mumford stack, and ΞM(sic)g,n(µ) is the orbifold corresponding t o it. Moreover, we perform a real oriented blowup of the unprojectivized space PΞM(sic)g,n(µ) such that the GL+2(R)-action in the interior of the moduli space extends continuously to the boundary. A multi-scale differential on a pointed stable curve is the data of an enhanced level structure on the dual graph, prescribing the orders of poles and zeroes at the nodes, together with a collection of meromorphic differentials on the irreducible components satisfying certain conditions. Additionally, the multi-scale differential encodes the data of a prong-matching at the nodes, matching the incoming and outgoing horizontal trajectories in the flat structure. The construction of P∈Mg,n(µ) furthermore requires defining families of multi-scale differentials, where the underlying curve can degenerate, and understanding the notion of equivalence of multi-scale differentials under various rescalings. Our construction of the compactification proceeds via first constructing an augmented Teichmuller space of flat surfaces, and then taking its suitable quotient. Along the way, we give a complete proof of the fact that the conformal and quasiconformal topologies on the (usual) augmented Teichmuller space agree.																																	2025-01-11	PPRN:119585780		
J	Liu, Yong; Qin, Guo; Huang, Xiangdong; Wang, Jianmin; Long, Mingsheng										AutoTimes: Autoregressive Time Series Forecasters via Large Language Models								Arxiv											5	5;2024-10-31;https://www.arxiv.org/abs/2402.02370v4| 4;2024-10-12;https://www.arxiv.org/abs/2402.02370v3| 3;2024-05-23;https://www.arxiv.org/abs/2402.02370v2| 2;2024-02-04;https://www.arxiv.org/abs/2402.02370v1| 1;2024-02-04;https://www.arxiv.org/abs/2402.02370v1	arXiv:2402.02370			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 31 2024	2024	Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To fully revitalize the general-purpose token transition and multi-step generation capability of large language models, we propose AutoTimes to repurpose LLMs as Auto regressive Time s eries forecasters, which projects time series into the embedding space of language tokens and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the look- back length and scalability with larger LLMs. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By introducing LLM-embedded textual timestamps, Auto- Times can utilize chronological information to align multivariate time series. Empirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and over 5× training/inference speedup compared to advanced LLM-based forecasters. 																																	2024-12-10	PPRN:87522513		
J	Yang, Shuai; Ge, Yuying; Li, Yang; Chen, Yukang; Ge, Yixiao; Shan, Ying; Chen, Yingcong				Chen, Yukang/HKW-0344-2023						SEED-Story: Multimodal Long Story Generation with Large Language Model								Arxiv											2	2;2024-10-11;https://www.arxiv.org/abs/2407.08683v2| 1;2024-07-11;https://www.arxiv.org/abs/2407.08683v1	arXiv:2407.08683			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 11 2024	2024	With the remarkable advancements in image generation and open-form text generation, the creation of interleaved image-text content has become an increasingly intriguing field. Multimodal story generation, characterized by producing narrative texts and vivid images in an interleaved manner, has emerged as a valuable and practical task with broad applications. However, this task poses significant challenges, as it necessitates the comprehension of the complex interplay between texts and images, and the ability to generate long sequences of coherent, contextually relevant texts and visuals. In this work, we propose SEED-Story, a novel method that leverages a Multimodal Large Language Model (MLLM) to generate extended multimodal stories. Our model, built upon the powerful comprehension capability of MLLM, predicts text tokens as well as visual tokens, which are subsequently processed with an adapted visual de-tokenizer to produce images with consistent characters and styles. We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner. Additionally, we present a large-scale and high-resolution dataset named StoryStream for training our model and quantitatively evaluating the task of multimodal story generation in various aspects.																																	2024-11-09	PPRN:90770128		
J	Huang, Jen-tse; Lam, Man Ho; Li, Eric John; Ren, Shujie; Wang, Wenxuan; Jiao, Wenxiang; Tu, Zhaopeng; Lyu, Michael R.				Lam, Man Ho/LRS-9509-2024; Wang, Wenxuan/AAW-9073-2020; Tu, Zhaopeng/AAS-4259-2021; Huang, Jen-Tse/IRZ-7526-2023						EMOTIONALLY NUMB OR EMPATHETIC ? EVALUATING HOW LLMS&nbsp;FEEL USING E MOTION BENCH								Arxiv											6	6;2024-10-04;https://www.arxiv.org/abs/2308.03656v6| 5;2024-08-13;https://www.arxiv.org/abs/2308.03656v5| 4;2024-04-24;https://www.arxiv.org/abs/2308.03656v4| 3;2024-01-04;https://www.arxiv.org/abs/2308.03656v3| 2;2023-11-16;https://www.arxiv.org/abs/2308.03656v2| 1;2023-08-07;https://www.arxiv.org/abs/2308.03656v1	arXiv:2308.03656			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations.																																	2024-10-27	PPRN:74307533		
J	Su, Weihang; Tang, Yichen; Ai, Qingyao; Wu, Zhijing; Liu, Yiqun										DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models								Arxiv											3	3;2024-09-21;https://www.arxiv.org/abs/2403.10081v3| 2;2024-06-05;https://www.arxiv.org/abs/2403.10081v2| 1;2024-03-15;https://www.arxiv.org/abs/2403.10081v1	arXiv:2403.10081			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 21 2024	2024	Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. 																																	2024-10-07	PPRN:88167092		
J	Taylor, Anthony J.; Finkelstein, Steven L.; Kocevski, Dale D.; Jeon, Junehyoung; Bromm, Volker; Amorin, Ricardo O.; Haro, Pablo Arrabal; Backhaus, Bren E.; Bagley, Micaela B.; Banados, Eduardo; Bhatawdekar, Rachana; Brooks, Madisyn; Calabro, Antonello; Chavez Ortiz, Oscar A.; Cheng, Yingjie; Cleri, Nikko J.; Cole, Justin W.; Davis, Kelcey; Dickinson, Mark; Donnan, Callum; Dunlop, James S.; Ellis, Richard S.; Fernandez, Vital; Fontana, Adriano; Fujimoto, Seiji; Giavalisco, Mauro; Grazian, Andrea; Guo, Jingsong; Hathi, Nimish P.; Holwerda, Benne W.; Hirschmann, Michaela; Inayoshi, Kohei; Kartaltepe, Jeyhan S.; Khusanova, Yana; Koekemoer, Anton M.; Kokorev, Vasily; Larson, Rebecca L.; Leung, Gene C.K.; Lucas, Ray A.; Mcleod, Derek J.; Napolitano, Lorenzo; Onoue, Masafusa; Pacucci, Fabio; Papovich, Casey; Perez-Gonzalez, Pablo G.; Pirzkal, Nor; Somerville, Rachel S.; Trump, Jonathan R.; Wilkins, Stephen M.; Aaron Yung, L.Y.; Zhang, Haowen				Hathi, Nimish/N-4156-2019; Yung, L. Y. Aaron/AAT-6862-2021; Kartaltepe, Jeyhan/ABI-1561-2020; Cheng, Yingjie/JPA-5703-2023; Perez-Gonzalez, Pablo G./IVH-0781-2023; Taylor, Anthony/NXC-5403-2025; Leung, Gene/J-8225-2014; Calabrò, Antonello/AAX-1028-2020; Trump, Jonathan/NPI-4613-2025; Ellis, Richard/ABL-1310-2022; Onoue, Masafusa/KMY-0551-2024; Giavalisco, Mauro/AEV-5974-2022; Zhang, Haowen/KRQ-2263-2024; Inayoshi, Kohei/AAW-2098-2020; Kokorev, Vasily/GPK-2541-2022						Broad-Line AGN at 3.5 < z < 6: The Black Hole Mass Function and a Connection with Little Red Dots								Arxiv											1	1;2024-09-10;https://www.arxiv.org/abs/2409.06772v1	arXiv:2409.06772			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 10 2024	2024	We present a sample of 50 Hα detected broad-line active galactic nuclei (BLAGN) at redshifts 3.5 < z < 6.8 using data from the CEERS and RUBIES surveys. We select these sources directly from JWST /NIRSpec G395M/F290LP spectra. We use a multi-step pre-selection and a Bayesian fitting procedure to ensure a high-quality sample of sources with broad Balmer lines and narrow forbidden lines. We compute rest-frame ultraviolet and optical spectral slopes for these objects, and determine that 10 BLAGN in our sample are also little red dots (LRDs). These LRD BLAGN, when examined in aggregate, show broader Hα line profiles and a higher fraction of broad-to-narrow component Hα emission than non-LRD BLAGN. Moreover, we find that ∼ 66% of these objects are intrinsically reddened (βopt > 0), independent of the contributions of emission lines to the broadband photometry. We construct the black hole (BH) mass function at 3.5 < z < 6 after computing robust observational and line detection completeness corrections. This BH mass function shows broad agreement with both recent JWST /NIRSpec and JWST /NIRCam WFSS based BH mass functions, though we extend these earlier results to log (MBH/M⊙) BH < 7. The derived BH mass function is consistent with a variety of theoretical models, indicating that the observed abundance of black holes in the early universe is not discrepant with physically-motivated predictions. The BH mass function shape resembles a largely featureless power-law, suggesting that any signature from black-hole seeding has been lost by redshift z ∼5–6. Finally, we compute the BLAGN UV luminosity function and find good agreement with JWST-detected BLAGN samples from recent works, finding that BLAGN hosts constitute ≲10% of the total observed UV luminosity at all but the brightest luminosities.																																	2025-01-24	PPRN:91834376		
J	He, Luxi; Xia, Mengzhou; Henderson, Peter				He, Luxi/KBB-8984-2024						What is in Your Safe Data? Identifying Benign Data that Breaks Safety								Arxiv											2	2;2024-08-20;https://www.arxiv.org/abs/2404.01099v2| 1;2024-04-01;https://www.arxiv.org/abs/2404.01099v1	arXiv:2404.01099			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 20 2024	2024	Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Additionally, we propose a bi-directional anchoring method that, during the selection process, prioritizes data points that are close to harmful examples and far from benign ones. Our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints surprisingly leads to the fine-tuned model affirmatively responding to >70% of tested harmful requests, compared to <20% after fine-tuning on randomly selected data. We also observe that the selected data frequently appear as lists, bullet points, or math questions, indicating a systematic pattern in fine-tuning data that contributes to jailbreaking.																																	2024-08-30	PPRN:88361694		
J	Qin, Zhen; Yang, Songlin; Sun, Weixuan; Shen, Xuyang; Li, Dong; Sun, Weigao; Zhong, Yiran				yang, songlin/JTS-7738-2023; Yang, Yifan/JTV-1487-2023						HGRN2: Gated Linear RNNs with State Expansion								Arxiv											2	2;2024-08-19;https://www.arxiv.org/abs/2404.07904v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.07904v1	arXiv:2404.07904			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 19 2024	2024	Hierarchically gated linear RNN (HGRN, citealt{HGRN}) has demonstrated competitive training speed and performance in language modeling while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, limiting its expressiveness. To address this issue, we introduce a simple outer product-based state expansion mechanism, which significantly enlarges the recurrent state size without introducing any additional parameters. This enhancement also provides a linear attention interpretation for HGRN2, enabling hardware-efficient training. Our extensive experiments verify the advantage of HGRN2 over HGRN consistently across different settings and competitive with other recurrent models.																																	2024-08-28	PPRN:88502224		
J	Nichani, Eshaan; Damian, Alex; Lee, Jason D.										How Transformers Learn Causal Structure with Gradient Descent								Arxiv											1	1;2024-08-13;https://www.arxiv.org/abs/2402.14735v2	arXiv:2402.14735			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 13 2024	2024	The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.																																	2024-08-22	PPRN:91368893		
J	Zhou, Da-Wei; Cai, Zi-Wen; Ye, Han-Jia; Zhan, De-Chuan; Liu, Ziwei				Zhou, Da-Wei/ABB-6259-2021; Liu, Ziwei/AAG-6939-2021						Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need								Arxiv											2	2;2024-08-05;https://www.arxiv.org/abs/2303.07338v2| 1;2023-03-13;https://www.arxiv.org/abs/2303.07338v1	arXiv:2303.07338			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 05 2024	2024	Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. 1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. 2) Due to the distribution gap between pre-trained and downstream datasets, PTM can be further cultivated with adaptivity via model adaptation. We propose AdaPt and mERge (APER), which aggregates the embeddings of PTM and adapted models for classifier construction. APER is a general framework that can be orthogonally combined with any parameter-efficient tuning method, which holds the advantages of PTM's generalizability and adapted model's adaptivity. 3) Additionally, considering previous ImageNet-based benchmarks are unsuitable in the era of PTM due to data overlapping, we propose four new benchmarks for assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the effectiveness of APER with a unified and concise framework. 																																	2024-08-11	PPRN:46490801		
J	Choi, Yisol; Kwak, Sangkyung; Lee, Kyungmin; Choi, Hyungwon; Shin, Jinwoo				Shin, Jinwoo/LUZ-4295-2024						Improving Diffusion Models for Authentic Virtual Try-on in the Wild								Arxiv											2	2;2024-07-29;https://www.arxiv.org/abs/2403.05139v3| 1;2024-03-08;https://www.arxiv.org/abs/2403.05139v1	arXiv:2403.05139			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 29 2024	2024	This paper considers image-based virtual try-on, which renders an image of a person wearing a curated garment, given a pair of images depicting the person and the garment, respectively. Previous works adapt existing exemplar-based inpainting diffusion models for virtual try-on to improve the naturalness of the generated visuals compared to other methods (e.g., GAN-based), but they fail to preserve the identity of the garments. To overcome this limitation, we propose a novel diffusion model that improves garment fidelity and generates authentic virtual try-on images. Our method, coined IDM-VTON, uses two different modules to encode the semantics of garment image; given the base UNet of the diffusion model, 1) the high-level semantics extracted from a visual encoder are fused to the cross-attention layer, and then 2) the low-level features extracted from parallel UNet are fused to the self-attention layer. In addition, we provide detailed textual prompts for both garment and person images to enhance the authenticity of the generated visuals. Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity. Our experimental results show that our method outperforms previous approaches (both diffusion-based and GAN-based) in preserving garment details and generating authentic virtual try-on images, both qualitatively and quantitatively. Furthermore, the proposed customization method demonstrates its effectiveness in a real-world scenario. 																																	2024-11-30	PPRN:88087489		
J	Chewi, Sinho; Niles-Weed, Jonathan; Rigollet, Philippe										Statistical optimal transport								Arxiv											1	1;2024-07-25;https://www.arxiv.org/abs/2407.18163v1	arXiv:2407.18163			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 25 2024	2024	We present an introduction to the field of statistical optimal transport, based on lectures given at École d'Été de Probabilités de Saint-Flour XLIX.																																	2024-08-08	PPRN:91100332		
J	Chern, Ethan; Su, Jiadi; Ma, Yan; Liu, Pengfei				Liu, Pengfei/JUV-0307-2023						ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation								Arxiv											1	1;2024-07-08;https://www.arxiv.org/abs/2407.06135v1	arXiv:2407.06135			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 08 2024	2024	Previous open-source large multimodal models (LMMs) have faced several limitations: (1) they often lack native integration, requiring adapters to align visual representations with pre-trained large language models (LLMs); (2) many are restricted to single-modal generation; (3) while some support multimodal generation, they rely on separate diffusion models for visual modeling and generation. To mitigate these limitations, we present Anole, an open, autoregressive, native large multimodal model for interleaved image-text generation. We build Anole from Meta AI's Chameleon, adopting an innovative fine-tuning strategy that is both data-efficient and parameter-efficient. Anole demonstrates high-quality, coherent multimodal generation capabilities. We have open-sourced our model, training framework, and instruction tuning data.																																	2024-07-21	PPRN:90741345		
J	Franciolini, Gabriele; Iovino, Antonio Junior; Taoso, Marco; Urbano, Alfredo										One loop to rule them all: Perturbativity in the presence of ultra slow-roll dynamics								Arxiv											2	2;2024-06-28;https://www.arxiv.org/abs/2305.03491v2| 1;2023-05-05;https://www.arxiv.org/abs/2305.03491v1	arXiv:2305.03491			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jun 28 2024	2024	We discuss the issue of perturbativity in single-field inflationary models with a phase of ultra slow-roll (USR) tailor suited to generate an order-one abundance of primordial black holes (PBHs). More in detail, we impose the condition that loop corrections made up of short-wavelength modes enhanced by the USR dynamics do not alter the tree-level power spectrum of curvature perturbations. In our analysis, the USR phase is preceded and followed by two stages of ordinary slow-roll (SR), and we model the resulting SR/USR/SR dynamics using both instantaneous and smooth transitions. Focusing on scales relevant for CMB observations, we find that it is not possible, with these arguments, to rule out the scenario of PBH formation via USR, not even in the limit of instantaneous transition. However, we also find that loop corrections of short modes on the power spectrum of long modes, even though not large enough to violate perturbativity requirements, remain appreciable and, most importantly, are not tamed in realistic realisations of smooth SR/USR/SR transitions. This makes perturbativity a powerful theoretical tool to constrain USR dynamics. We extend the analysis at any scale beyond those relevant for CMB observations. We find that loop corrections of short modes remain within the few percent if compared to the tree-level power spectrum. However, we also find one notable exception of phenomenological relevance: we show that the so-called dip in the power spectrum of curvature perturbation is an artifact of the tree-level computation.																																	2024-07-15	PPRN:68138943		
J	Jia, Jinghan; Zhang, Yihua; Zhang, Yimeng; Liu, Jiancheng; Runwal, Bharat; Diffenderfer, James; Kailkhura, Bhavya; Liu, Sijia				Liu, Sijia/HOC-2459-2023; Liu, JC/LPI-0149-2024						SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning								Arxiv											3	3;2024-06-24;https://www.arxiv.org/abs/2404.18239v4| 2;2024-06-03;https://www.arxiv.org/abs/2404.18239v3| 1;2024-04-28;https://www.arxiv.org/abs/2404.18239v1	arXiv:2404.18239			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 24 2024	2024	Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed SecondOrder UnLearning ( SOUL ), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.																																	2024-07-15	PPRN:88696359		
J	Gasteiger, Johannes; Becker, Florian; Guennemann, Stephan				Becker, Florian/KPA-8932-2024						GemNet: Universal Directional Graph Neural Networks for Molecules								Arxiv											1	1;2024-06-22;https://www.arxiv.org/abs/2106.08903v10	arXiv:2106.08903			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 22 2024	2024	Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.																																	2024-07-12	PPRN:89407090		
J	Decross, M; Haghshenas, R; Liu, M; Rinaldi, E; Gray, J; Alexeev, Y; Baldwin, C H; Bartolotta, J P; Bohn, M; Chertkov, E; Cline, J; Colina, J; Delvento, D; Dreiling, J M; Foltz, C; Gaebler, J P; Gatterman, T M; Gilbreth, C N; Giles, J; Gresh, D; Hall, A; Hankin, A; Hansen, A; Hewitt, N; Hoffman, I; Holliman, C; Hutson, R B; Jacobs, T; Johansen, J; Lee, P J; Lehman, E; Lucchetti, D; Lykov, D; Madjarov, I S; Mathewson, B; Mayer, K; Mills, M; Niroula, P; Pino, J M; Roman, C; Schecter, M; Siegfried, P E; Tiemann, B G; Volin, C; Walker, J; Shaydulin, R; Pistoia, M; Moses, S A; Hayes, D; Neyenhuis, B; Stutz, R P; Foss-Feig, M				Rinaldi, Enrico/J-8753-2018; Rey-Pino, Juan/JAN-6923-2023; Lee, Pyoung/A-9495-2010; Bartolotta, John/AAY-6784-2020; Liu, Minzhao/HLQ-2648-2023; Lykov, Danylo/JAN-5070-2023						The computational power of random quantum circuits in arbitrary geometries								Arxiv											3	3;2024-06-21;https://www.arxiv.org/abs/2406.02501v3| 2;2024-06-10;https://www.arxiv.org/abs/2406.02501v2| 1;2024-06-04;https://www.arxiv.org/abs/2406.02501v1	arXiv:2406.02501			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 21 2024	2024	Empirical evidence for a gap between the computational powers of classical and quantum computers has been provided by experiments that sample the output distributions of two-dimensional quantum circuits. Many attempts to close this gap have utilized classical simulations based on tensor network techniques, and their limitations shed light on the improvements to quantum hardware required to frustrate classical simulability. In particular, quantum computers having in excess of ∼ 50 qubits are primarily vulnerable to classical simulation due to restrictions on their gate fidelity and their connectivity, the latter determining how many gates are required (and therefore how much infidelity is suffered) in generating highly-entangled states. Here, we describe recent hardware upgrades to Quantinuum’s H2 quantum computer enabling it to operate on up to 56 qubits with arbitrary connectivity and 99 .843(5)% two-qubit gate fidelity. Utilizing the flexible connectivity of H2, we present data from random circuit sampling in highly connected geometries, doing so at unprecedented fidelities and a scale that appears to be beyond the capabilities of state-of-the-art classical algorithms. The considerable difficulty of classically simulating H2 is likely limited only by qubit number, demonstrating the promise and scalability of the QCCD architecture as continued progress is made towards building larger machines.																																	2024-08-04	PPRN:89262935		
J	Zhu, Zehao; Fan, Zhiwen; Jiang, Yifan; Wang, Zhangyang				Jiang, Yifan/ABB-4400-2021						FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting								Arxiv											2	2;2024-06-16;https://www.arxiv.org/abs/2312.00451v2| 1;2023-12-01;https://www.arxiv.org/abs/2312.00451v1	arXiv:2312.00451			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 16 2024	2024	Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender.																																	2024-07-04	PPRN:86358115		
J	Lanthaler, Samuel; Li, Zongyi; Stuart, Andrew M.				Li, Zongyi/AAY-3602-2020						Nonlocality and Nonlinearity Implies Universality in Operator Learning								Arxiv											2	2;2024-06-15;https://www.arxiv.org/abs/2304.13221v2| 1;2023-04-26;https://www.arxiv.org/abs/2304.13221v1	arXiv:2304.13221			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 15 2024	2024	Neural operator architectures approximate operators between infinite-dimensional Banach spaces of functions. They are gaining increased attention in computational science and engineering, due to their potential both to accelerate traditional numerical methods and to enable data-driven discovery. As the field is in its infancy basic questions about minimal requirements for universal approximation remain open. It is clear that any general approximation of operators between spaces of functions must be both nonlocal and nonlinear. In this paper we describe how these two attributes may be combined in a simple way to deduce universal approximation. In so doing we unify the analysis of a wide range of neural operator architectures and open up consideration of new ones. A popular variant of neural operators is the Fourier neural operator (FNO). Previous analysis proving universal operator approximation theorems for FNOs resorts to use of an unbounded number of Fourier modes, relying on intuition from traditional analysis of spectral methods. The present work challenges this point of view: (i) the work reduces FNO to its core essence, resulting in a minimal architecture termed the ``averaging neural operator'' (ANO); and (ii) analysis of the ANO shows that even this minimal ANO architecture benefits from universal approximation. This result is obtained based on only a spatial average as its only nonlocal ingredient (corresponding to retaining only a emph{single} Fourier mode in the special case of the FNO). The analysis paves the way for a more systematic exploration of nonlocality, both through the development of new operator learning architectures and the analysis of existing and new architectures. Numerical results are presented which give insight into complexity issues related to the roles of channel width (embedding dimension) and number of Fourier modes.																																	2024-07-04	PPRN:65568500		
J	Holleis, Ludwig; Patterson, Caitlin L.; Zhang, Yiran; Vituri, Yaar; Yoo, Heun Mo; Zhou, Haoxin; Taniguchi, Takashi; Watanabe, Kenji; Berg, Erez; Nadj-Perge, Stevan; Young, Andrea F.				TANIGUCHI, Takashi/H-2718-2011; Young, Andrea/E-2892-2015; Young, Andrea/H-9697-2012; Zhou, Haoxin/GQB-0513-2022						Nematicity and Orbital Depairing in Superconducting Bernal Bilayer Graphene with Strong Spin Orbit Coupling								Arxiv											2	2;2024-06-12;https://www.arxiv.org/abs/2303.00742v2| 1;2023-03-01;https://www.arxiv.org/abs/2303.00742v1	arXiv:2303.00742			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 12 2024	2024	Superconductivity (SC) is a ubiquitous feature of graphite allotropes, having been observed in Bernal bilayers[1], rhombohedral trilayers[2], and a wide variety of angle-misaligned multilayers[3–6]. Despite significant differences in the electronic structure across these systems, supporting the graphite layer on a WSe2 substrate has been consistently observed to expand the range of SC in carrier density and temperature[7–10]. Here, we report the observation of two distinct superconducting states (denoted SC1 and SC2) in Bernal bilayer graphene with strong proximity-induced Ising spin-orbit coupling. Quantum oscillations show that while the normal state of SC1 is consistent with the single-particle band structure, SC2 emerges from a nematic normal state with broken rotational symmetry. Both superconductors are robust to in-plane magnetic fields, violating the paramagnetic limit; however, neither reach fields expected for spinvalley locked Ising superconductors. We use our knowledge of the Fermi surface geometry of SC1 to argue that superconductivity is limited by orbital depairing arising from the imperfect layer polarization of the electron wavefunctions. Finally, a comparative analysis of transport and thermodynamic compressibility measurements in SC2 shows that the proximity to the observed isospin phase boundaries, observed in other rhombohedral graphene allotropes, is likely coincidental, constraining theories of unconventional superconducting pairing mechanisms in theses systems.																																	2024-07-02	PPRN:41247250		
J	Li, Zhen; Xu, Xiaohan; Shen, Tao; Xu, Can; Gu, Jia-Chen; Lai, Yuxuan; Tao, Chongyang; Ma, Shuai				ma, shuai/NAX-5505-2025; Tao, Chongyang/MBG-8179-2025						Leveraging Large Language Models for NLG Evaluation: Advances and Challenges								Arxiv											2	2;2024-06-12;https://www.arxiv.org/abs/2401.07103v2| 1;2024-01-13;https://www.arxiv.org/abs/2401.07103v1	arXiv:2401.07103			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 12 2024	2024	In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.																																	2024-07-02	PPRN:87186359		
J	Myrzakhan, Aidar; Bsharat, Sondos Mahmoud; Shen, Zhiqiang										Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena								Arxiv											1	1;2024-06-11;https://www.arxiv.org/abs/2406.07545v1	arXiv:2406.07545			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs. Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. Another problem of MCQ is the lottery ticket choice by ''random guessing''. The LLM does not learn particular knowledge, but the option is guessed correctly. This situation is especially serious for those small-scale LLMs. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions. Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc. 																																	2024-07-02	PPRN:89286566		
J	Chen, Siyu; Sheen, Heejune; Wang, Tianhao; Yang, Zhuoran				Wang, Tianhao/KLY-4214-2024; Yang, Zhuoran/IYJ-4459-2023						Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2402.19442v2| 1;2024-02-29;https://www.arxiv.org/abs/2402.19442v1	arXiv:2402.19442			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting “task allocation” phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases — a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor. Our analysis also delineates a strict separation in terms of the prediction accuracy of ICL between single-head and multi-head attention models. The key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation. To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model.																																	2024-07-11	PPRN:87985359		
J	Wang, Xintao; Xiao, Yunze; Huang, Jen-tse; Yuan, Siyu; Xu, Rui; Guo, Haoran; Tu, Quan; Fei, Yaying; Leng, Ziang; Wang, Wei; Chen, Jiangjie; Li, Cheng; Xiao, Yanghua				Xu, Rui/LRV-2470-2024; Huang, Jen-Tse/IRZ-7526-2023; Zhuang, Wei/KCJ-6093-2024; Chen, Jiangjie/JCE-5486-2023						InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2310.17976v4| 2;2024-02-17;https://www.arxiv.org/abs/2310.17976v3| 1;2023-10-30;https://www.arxiv.org/abs/2310.17976v2	arXiv:2310.17976			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 07 2024	2024	Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to 80.7%.																																	2024-06-22	PPRN:85860675		
J	Xiong, Zhitong; Wang, Yi; Zhang, Fahong; Stewart, Adam J.; Hanna, Joelle; Borth, Damian; Papoutsis, Ioannis; Le Saux, Bertrand; Camps-Valls, Gustau; Zhu, Xiao Xiang				Papoutsis, Ioannis/L-5072-2013; Camps-Valls, Gustau/A-2532-2011; Xiong, Zhitong/HPC-7558-2023; Zhu, Xiao Xiang/ABE-7138-2020						Neural Plasticity-Inspired Multimodal Foundation Model for Earth Observation								Arxiv											2	2;2024-06-07;https://www.arxiv.org/abs/2403.15356v2| 1;2024-03-22;https://www.arxiv.org/abs/2403.15356v1	arXiv:2403.15356			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	The development of foundation models has revolutionized our ability to interpret the Earth's surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyperspectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile Transformer jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA's innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of multimodal Earth observation data.																																	2024-07-04	PPRN:88268137		
J	Yang, Dongjie; Han, Xiaodong; Gao, Yan; Hu, Yao; Zhang, Shilin; Zhao, Hai				Zhang, Xiaohong/ACF-0457-2022						PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference								Arxiv											2	2;2024-06-05;https://www.arxiv.org/abs/2405.12532v2| 1;2024-05-21;https://www.arxiv.org/abs/2405.12532v1	arXiv:2405.12532			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 05 2024	2024	Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.																																	2024-06-22	PPRN:89098650		
J	Cheng, Pengzhou; Ding, Yidong; Ju, Tianjie; Wu, Zongru; Du, Wei; Yi, Ping; Zhang, Zhuosheng; Liu, Gongshen				Cheng, Pengzhou/GQI-0879-2022; Ju, Tianjie/LTC-8143-2024; Zhang, Zhuosheng/AAF-4919-2020						TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models								Arxiv											3	3;2024-05-31;https://www.arxiv.org/abs/2405.13401v3| 2;2024-05-24;https://www.arxiv.org/abs/2405.13401v2| 1;2024-05-22;https://www.arxiv.org/abs/2405.13401v1	arXiv:2405.13401			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 31 2024	2024	Large language models (LLMs) have raised concerns about potential security threats despite performing significantly in Natural Language Processing (NLP). Backdoor attacks initially verified that LLM is doing substantial harm at all stages, but the cost and robustness have been criticized. Attacking LLMs is inherently risky in security review, while prohibitively expensive. Besides, the continuous iteration of LLMs will degrade the robustness of backdoors. In this paper, we propose TrojanRAG, which employs a joint backdoor attack in the Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack scenarios. Specifically, the adversary constructs elaborate target contexts and trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized by contrastive learning, thus constraining the triggering conditions to a parameter subspace to improve the matching. To improve the recall of the RAG for the target contexts, we introduce a knowledge graph to construct structured data to achieve hard matching at a fine-grained level. Moreover, we normalize the backdoor scenarios in LLMs to analyze the real harm caused by backdoors from both attackers' and users' perspectives and further verify whether the context is a favorable tool for jailbreaking models. Extensive experimental results on truthfulness, language understanding, and harmfulness show that TrojanRAG exhibits versatility threats while maintaining retrieval capabilities on normal queries3.																																	2024-11-10	PPRN:88988947		
J	Wu, Xingyu; Wu, Sheng-hao; Wu, Jibin; Feng, Liang; Tan, Kay Chen				wu, xingyu/KXR-5147-2024; Tan, Kay Chen/AAS-7461-2020						Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap								Arxiv											3	3;2024-05-29;https://www.arxiv.org/abs/2401.10034v3| 2;2024-02-07;https://www.arxiv.org/abs/2401.10034v2| 1;2024-01-18;https://www.arxiv.org/abs/2401.10034v1	arXiv:2401.10034			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM’s further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.																																	2024-08-24	PPRN:87223225		
J	Glorioso, Paolo; Anthony, Quentin; Tokpanov, Yury; Whittington, James; Pilault, Jonathan; Ibrahim, Adam; Millidge, Beren										Zamba: A Compact 7B SSM Hybrid Model								Arxiv											1	1;2024-05-26;https://www.arxiv.org/abs/2405.16712v1	arXiv:2405.16712			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 26 2024	2024	In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. We open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.																																	2024-06-08	PPRN:89047473		
J	Masoud, Reem I.; Liu, Ziquan; Ferianc, Martin; Treleaven, Philip; Rodrigues, Miguel				Ferianc, Martin/ACC-1433-2022						Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions								Arxiv											2	2;2024-05-08;https://www.arxiv.org/abs/2309.12342v2| 1;2023-08-25;https://www.arxiv.org/abs/2309.12342v1	arXiv:2309.12342			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 08 2024	2024	The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals and societies with diverse cultural backgrounds. While the discourse has focused mainly on political and social biases, our research proposes a Cultural Alignment Test (Hoftede’s CAT) to quantify cultural alignment using Hofstede’s cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to quantitatively evaluate LLMs—namely Llama 2, GPT-3.5, and GPT-4—against the cultural dimensions of regions like the United States, China, and Arab countries, using different prompting styles and exploring the effects of language -specific fine-tuning on the models’ behavioural tendencies and cultural values. Our results quantify the cultural alignment of LLMs and reveal the difference between LLMs in explanatory cultural dimensions. Our study demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability to adapt to cultural nuances, particularly in Chinese settings. However, it faces challenges with American and Arab cultures. The research also highlights that fine-tuning LLama 2 models with different languages changes their responses to cultural questions, emphasizing the need for culturally diverse development in AI for worldwide acceptance and ethical use. For more details or to contribute to this research, visit our GitHub page https://github.com/reemim/Hofstedes CAT.																																	2024-05-26	PPRN:85177039		
J	Gupta, Anuradha; Arun, K.G.; Barausse, Enrico; Bernard, Laura; Berti, Emanuele; Bhat, Sajad A.; Buonanno, Alessandra; Cardoso, Vitor; Cheung, Shun Yin; Clarke, Teagan A.; Datta, Sayantani; Dhani, Arnab; Ezquiaga, Jose Maria; Gupta, Ish; Guttman, Nir; Hinderer, Tanja; Hu, Qian; Janquart, Justin; Johnson-McDaniel, Nathan K.; Kashyap, Rahul; Krishnendu, N.V.; Lasky, Paul D.; Lundgren, Andrew; Maggio, Elisa; Mahapatra, Parthapratim; Maselli, Andrea; Narayan, Purnima; Nielsen, Alex B.; Nuttall, Laura K.; Pani, Paolo; Passenger, Lachlan; Payne, Ethan; Pompili, Lorenzo; Reali, Luca; Saini, Pankaj; Samajdar, Anuradha; Tiwari, Shubhanshu; Tong, Hui; Broeck, Chris Van Den; Yagi, Kent; Yang, Huan; Yunes, Nicolas; Sathyaprakash, B.S.				Berti, Emanuele/AAI-1513-2019; Gupta, Ish/JGM-1363-2023; Yunes, Nicolas/AAG-3146-2019; Pani, Paolo/AAG-3902-2021; Kashyap, Rahul/AEM-3686-2022; Lasky, Paul/A-2660-2009; barausse, enrico/M-3755-2017; Bhat, Sajad/JVD-8193-2023; Reali, Luca/ABG-4169-2020; Maggio, Elisa/AAO-6422-2020; Cardoso, Vitor/HPE-7296-2023						Possible Causes of False General Relativity Violations in Gravitational Wave Observations								Arxiv											1	1;2024-05-03;https://www.arxiv.org/abs/2405.02197v1	arXiv:2405.02197			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 03 2024	2024	General relativity (GR) has proven to be a highly successful theory of gravity since its inception. The theory has thrivingly passed numerous experimental tests, predominantly in weak gravity, low relative speeds, and linear regimes, but also in the strong-field and very low-speed regimes with binary pulsars. Observable gravitational waves (GWs) originate from regions of spacetime where gravity is extremely strong, making them a unique tool for testing GR, in previously inaccessible regions of large curvature, relativistic speeds, and strong gravity. Since their first detection, GWs have been extensively used to test GR, but no deviations have been found so far. Given GR's tremendous success in explaining current astronomical observations and laboratory experiments, accepting any deviation from it requires a very high level of statistical confidence and consistency of the deviation across GW sources. In this paper, we compile a comprehensive list of potential causes that can lead to a false identification of a GR violation in standard tests of GR on data from current and future ground-based GW detectors. These causes include detector noise, signal overlaps, gaps in the data, detector calibration, source model inaccuracy, missing physics in the source and in the underlying environment model, source misidentification, and mismodeling of the astrophysical population. We also provide a rough estimate of when each of these causes will become important for tests of GR for different detector sensitivities. We argue that each of these causes should be thoroughly investigated, quantified, and ruled out before claiming a GR violation in GW observations.																																	2024-08-06	PPRN:88760690		
J	Morgenstern, Wieland; Barthel, Florian; Hilsmann, Anna; Eisert, Peter										Compact 3D Scene Representation via Self-Organizing Gaussian Grids								Arxiv											2	2;2024-05-02;https://www.arxiv.org/abs/2312.13299v2| 1;2023-12-19;https://www.arxiv.org/abs/2312.13299v1	arXiv:2312.13299			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	3D Gaussian Splatting has recently emerged as a highly promising technique for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it utilizes efficient rasterization allowing for very fast rendering at high-quality. However, the storage size is significantly higher, which hinders practical deployment, e.g. on resource constrained devices. In this paper, we introduce a compact scene representation organizing the parameters of 3D Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a drastic reduction in storage requirements without compromising visual quality during rendering. Central to our idea is the explicit exploitation of perceptual redundancies present in natural scenes. In essence, the inherent nature of a scene allows for numerous permutations of Gaussian parameters to equivalently represent it. To this end, we propose a novel highly parallel algorithm that regularly arranges the high-dimensional Gaussian parameters into a 2D grid while preserving their neighborhood structure. During training, we further enforce local smoothness between the sorted parameters in the grid. The uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless integration with established renderers. Our method achieves a reduction factor of 17x to 42x in size for complex scenes with no increase in training time, marking a substantial leap forward in the domain of 3D scene distribution and consumption. 																																	2024-05-20	PPRN:86832743		
J	Wu, Xun; Huang, Shaohan; Wei, Furu				Huang, Shaohan/LDF-3300-2024						Mixture of LoRA Experts								Arxiv											1	1;2024-04-21;https://www.arxiv.org/abs/2404.13628v1	arXiv:2404.13628			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 21 2024	2024	Low -Rank Adaptation (LoRA) (Hu et al., 2021) has emerged as a pivotal technique for fine-tuning large pre -trained models, renowned for its efficacy across a wide array of tasks. The modular architecture of LoRA has catalyzed further research into the synergistic composition of multiple trained LoRAs, aiming to amplify performance across various tasks. However, the effective composition of these trained LoRAs presents a formidable challenge: (1) Linear arithmetic composition can lead to the diminution of the generative capabilities inherent in the original pre -trained models or the distinctive attributes of the individually trained LoRAs, potentially resulting in suboptimal outcomes. (2) Reference tuning -based composition exhibits limitations in adaptability and incurs significant computational costs due to the requirements to retrain a large model. In response to these challenges, we propose Mixture of LoRA Experts (MOLE). MOLE treats each layer of trained LoRAs as a distinct expert and implements hierarchical weight control by integrating a learnable gating function within each layer to learn optimal composition weights tailored specifically to the objectives of a given domain. MOLE not only demonstrates enhanced performance in LoRA composition but also preserves the essential flexibility necessary for effective composition of trained LoRAs with minimal computational overhead. Extensive experiments conducted in both Natural Language Processing (NLP) and Vision & Language (V&L) domains validate the effects of MOLE. Our code are available at https://github.com/yushuiwx/MoLE.git.																																	2024-04-30	PPRN:88601041		
J	Wang, Deng										Constraining Cosmological Physics with DESI BAO Observations								Arxiv											3	3;2024-04-17;https://www.arxiv.org/abs/2404.06796v3| 2;2024-04-12;https://www.arxiv.org/abs/2404.06796v2| 1;2024-04-10;https://www.arxiv.org/abs/2404.06796v1	arXiv:2404.06796			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 17 2024	2024	The DESI year one observations can help probe new physics on cosmological scales. In light of the latest DESI BAO measurements, we constrain five popular cosmological scenarios including inflation, modified gravity, annihilating dark matter and interacting dark energy. Using a data combination of BICEP/Keck array, cosmic microwave background and DESI, we obtain the 1σ and 2σ constraints on the tensor -to -scalar ratio r0.05 = 0.0176−0.0130+0.0070 and r0.05 = 0.018 −0.017+0.020 indicating a beyond 2σ evidence of primordial gravitational waves. Using the combination of cosmic microwave background and DESI, we find a 2.4σ evidence for gravitational theories beyond the general relativity, shrink the dark matter annihilation cross-section by 12% relative to cosmic microwave background, obtain a 1.3σ hint of the positive interaction between dark matter and dark energy implying that energy may be transferred from dark matter to dark energy in the dark sector of the universe, and give a clue of massive sterile neutrinos via the 2σ constraint on the effective number of relativistic degrees of freedom Neff = 3.16−0.11+0.26 and the effective mass mν,sterile eff < 0.52 eV. Future DESI observations could go a step further to explore the nature of inflation, dark matter, dark energy and neutrinos, and test the validity of general relativity on cosmological scales.																																	2024-04-27	PPRN:88479586		
J	Wang, Xiao; Wang, Shiao; Ding, Yuhe; Li, Yuehang; Wu, Wentao; Rong, Yao; Kong, Weizhe; Huang, Ju; Li, Shihao; Yang, Haoxiang; Wang, Ziwen; Jiang, Bo; Li, Chenglong; Wang, Yaowei; Tian, Yonghong; Tang, Jin				Li, Yuehang/LDF-4062-2024; Wu, Wentao/JAX-2475-2023; TIAN, Yonghong/M-4937-2013; Li, Chenglong/AAU-4968-2021						State Space Model for New-Generation Network Alternative to Transformers: A Survey								Arxiv											1	1;2024-04-15;https://www.arxiv.org/abs/2404.09516v1	arXiv:2404.09516			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 15 2024	2024	In the post-deep learning era, the Transformer architecture has demonstrated its powerful performance across pre-trained big models and various downstream tasks. However, the enormous computational demands of this architecture have deterred many researchers. To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods. Among them, the State Space Model (SSM), as a possible replacement for the self-attention based Transformer model, has drawn more and more attention in recent years. In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM. Specifically, we first give a detailed description of principles to help the readers quickly capture the key ideas of SSM. After that, we dive into the reviews of existing SSMs and their various applications, including natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains. In addition, we give statistical comparisons and analysis of these models and hope it helps the readers to understand the effectiveness of different structures on various tasks. Then, we propose possible research points in this direction to better promote the development of the theoretical model and application of SSM.																																	2024-04-25	PPRN:88530125		
J	Liu, Jiacheng; Min, Sewon; Zettlemoyer, Luke; Choi, Yejin; Hajishirzi, Hannaneh										Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens								Arxiv											2	2;2024-04-04;https://www.arxiv.org/abs/2401.17377v3| 1;2024-01-30;https://www.arxiv.org/abs/2401.17377v1	arXiv:2401.17377			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 04 2024	2024	Are n -gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we showcase their values in both text analysis and improving neural LLMs. This was done by modernizing n -gram LMs in two aspects. First, we train them at the same data scale as neural LLMs—5 trillion tokens. This is the largest n -gram LM ever built. Second, existing n -gram LMs use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new ∞-gram LM with backoff. Instead of pre -computing n -gram count tables (which would be very expensive), we develop an engine named infini-gram—powered by suffix arrays—that can compute ∞-gram (as well as n -gram with arbitrary n) probabilities with millisecond -level latency. The ∞-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human -written and machine -generated text: we find that the ∞-gram LM has fairly high accuracy for next -token prediction (47%), and can complement neural LLMs to greatly reduce their perplexity. When analyzing machine -generated text, we also observe irregularities in the machine–∞-gram agreement level with respect to the suffix length, which indicates deficiencies in neural LLM pretraining and the positional embeddings of Transformers. [GRAPHICS]																																	2024-04-19	PPRN:87436480		
J	Shi, Yuan; Xia, Bin; Jin, Xiaoyu; Wang, Xing; Zhao, Tianyu; Xia, Xin; Xiao, Xuefeng; Yang, Wenming				jin, xiaoyu/LZE-9028-2025						VmambaIR: Visual State Space Model for Image Restoration								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.11423v1	arXiv:2403.11423			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Image restoration is a critical task in low-level computer vision, aiming to restore high-quality images from degraded inputs. Various models, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models (DMs), have been employed to address this problem with significant impact. However, CNNs have limitations in capturing long-range dependencies. DMs require large prior models and computationally intensive denoising steps. Transformers have powerful modeling capabilities but face challenges due to quadratic complexity with input image size. To address these challenges, we propose VmambaIR, which introduces State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks. We utilize a Unet architecture to stack our proposed Omni Selective Scan (OSS) blocks, consisting of an OSS module and an Efficient Feed-Forward Network (EFFN). Our proposed omni selective scan mechanism overcomes the unidirectional modeling limitation of SSMs by efficiently modeling image information flows in all six directions. Furthermore, we conducted a comprehensive evaluation of our VmambaIR across multiple image restoration tasks, including image deraining, single image super-resolution, and real-world image super-resolution. Extensive experimental results demonstrate that our proposed VmambaIR achieves state-of-the-art (SOTA) performance with much fewer computational resources and parameters. Our research highlights the potential of state space models as promising alternatives to the transformer and CNN architectures in serving as foundational frameworks for next-generation low-level visual tasks.																																	2024-04-11	PPRN:88189927		
J	Sun, Youbang; Li, Zitao; Li, Yaliang; Ding, Bolin				Li, Zitao/AAT-3918-2021						Improving LoRA in Privacy-preserving Federated Learning								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.12313v1	arXiv:2403.12313			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs. The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL. Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.																																	2024-04-12	PPRN:88238685		
J	Yang, Kevin; Klein, Dan; Celikyilmaz, Asli; Peng, Nanyun; Tian, Yuandong										RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment								Arxiv											2	2;2024-03-16;https://www.arxiv.org/abs/2307.12950v3| 1;2023-07-24;https://www.arxiv.org/abs/2307.12950v1	arXiv:2307.12950			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 16 2024	2024	We propose Reinforcement Learning from Contrastive Distillation (RLCD), a method for aligning language models to follow principles expressed in natural language (e.g., to be more harmless) without using human feedback. RLCD creates preference pairs from two contrasting model outputs, one using a positive prompt designed to encourage following the given principles, and one using a negative prompt designed to encourage violating them. Using two different prompts causes model outputs to be more differentiated on average, resulting in cleaner preference labels in the absence of human annotations. We then use the preference pairs to train a preference model, which is in turn used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and when using both 7B and 30B model scales for simulating preference data.																																	2024-04-11	PPRN:74079409		
J	Iyer, Aadhithya; Peng, Zhuoran; Dai, Yinlong; Guzey, Irmak; Haldar, Siddhant; Chintala, Soumith; Pinto, Lerrel				Dai, Yinlong/ORI-0180-2025						OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation								Arxiv											1	1;2024-03-12;https://www.arxiv.org/abs/2403.07870v1	arXiv:2403.07870			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 12 2024	2024	Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines. The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, bimanual manipulation, and applications ranging from logistics to home robotics. However, existing data collection platforms are often proprietary, costly, or tailored to specific robotic morphologies. We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed reality for intuitive robot control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH enables real-time control of various robots, including multi-fingered hands and bimanual arms, through an easy-to-use app. Using natural hand gestures and movements, users can manipulate robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views. We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots. A comprehensive user study indicates significant improvement in teleoperation capability over the AnyTeleop framework. Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous and contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and Allegro platforms, OPEN TEACH is fully open-sourced to promote broader adoption. 																																	2024-04-08	PPRN:88113302		
J	Chung, Hyungjin; Lee, Suhyeon; Ye, Jong Chul				Chung, Hyungjin/AAL-1161-2021; Ye, Jong/C-1623-2011						DECOMPOSED DIFFUSION SAMPLER FOR ACCELERATING LARGE-SCALE INVERSE PROBLEMS								Arxiv											3	3;2024-02-19;https://www.arxiv.org/abs/2303.05754v3| 2;2023-12-15;https://www.arxiv.org/abs/2303.05754v2| 1;2023-03-10;https://www.arxiv.org/abs/2303.05754v1	arXiv:2303.05754			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 19 2024	2024	Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the conjugate gradient method (CG), one of the most popular Krylov subspace methods, is based on the idea of minimizing the residual error in the Krylov subspace. However, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. In this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and Krylov subspace methods. Specifically, we prove that if the tangent space at a denoised sample by Tweedie's formula forms a Krylov subspace, then the CG initialized with the denoised data ensures the data consistency update to remain in the tangent space. This negates the need to compute the manifold-constrained gradient (MCG), leading to a more efficient diffusion sampling method. Our method is applicable regardless of the parametrization and setting (i.e., VE, VP). Notably, we achieve state-of-the-art reconstruction quality on challenging real-world medical inverse imaging problems, including multi-coil MRI reconstruction and 3D CT reconstruction. Moreover, our proposed method achieves more than 80 times faster inference time than the previous state-of-the-art method. Code is available at https://github.com/HJ-harry/DDS																																	2024-03-16	PPRN:46056489		
J	Krajewski, Jakub; Ludziejewski, Jan; Adamczewski, Kamil; Pioro, Maciej; Krutul, Michal; Antoniak, Szymon; Ciebiera, Kamil; Krol, Krystian; Odrzygozdz, Tomasz; Sankowski, Piotr; Cygan, Marek; Jaszczur, Sebastian				Jaszczur, Sebastian/AAP-3541-2020; Cygan, Marek/AAX-7632-2020						Scaling Laws for Fine-Grained Mixture of Experts								Arxiv											1	1;2024-02-12;https://www.arxiv.org/abs/2402.07871v1	arXiv:2402.07871			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 12 2024	2024	Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.																																	2024-05-25	PPRN:87647869		
J	Dou, Shihan; Liu, Yan; Jia, Haoxiang; Xiong, Limao; Zhou, Enyu; Shen, Wei; Shan, Junjie; Huang, Caishuang; Wang, Xiao; Fan, Xiaoran; Xi, Zhiheng; Zhou, Yuhao; Ji, Tao; Zheng, Rui; Zhang, Qi; Huang, Xuanjing; Gui, Tao				Xi, Zhiheng/KUD-1665-2024; Jia, Haoxiang/OEO-2861-2025; Ji, Tao/KIK-1554-2024; Zhou, Yuhao/ABC-4280-2022; Gui, Tao/LWI-6783-2024						StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback								Arxiv											1	1;2024-02-05;https://www.arxiv.org/abs/2402.01391v2	arXiv:2402.01391			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 05 2024	2024	The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.																																	2024-02-19	PPRN:87517713		
J	Wang, Jiawei; Zhang, Yuchen; Zou, Jiaxin; Zeng, Yan; Wei, Guoqiang; Yuan, Liping; Li, Hang				Wang, Jiawei/HSI-2308-2023; Zou, Jiaxin/AAC-1977-2022; Huang, Haoliang/A-4235-2019; Huang, Liyan/JEZ-6158-2023; Zhang, Ziyue/KSL-8395-2024						Boximator: Generating Rich and Controllable Motions for Video Synthesis								Arxiv											1	1;2024-02-02;https://www.arxiv.org/abs/2402.01566v1	arXiv:2402.01566			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 02 2024	2024	Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding box alignment metric. Human evaluation also shows that users favor Boximator generation results over the base model.																																	2024-02-19	PPRN:87509276		
J	Wu, Weijia; Zhao, Yuzhong; Shou, Mike Zheng; Zhou, Hong; Shen, Chunhua				Shou, Mike Zheng/LXW-9197-2024						DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models								Arxiv											3	3;2024-01-21;https://www.arxiv.org/abs/2303.11681v4| 2;2023-12-19;https://www.arxiv.org/abs/2303.11681v3| 1;2023-03-21;https://www.arxiv.org/abs/2303.11681v1	arXiv:2303.11681			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 21 2024	2024	Collecting and annotating images with pixel-wise labels is time-consuming and laborious. In contrast, synthetic data can be freely available using a generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that it is possible to automatically obtain accurate semantic masks of synthetic images generated by the Off-the-shelf Stable Diffusion model, which uses only text-image pairs during training. Our approach, called DiffuMask, exploits the potential of the cross-attention map between text and image, which is natural and seamless to extend the text-driven image synthesis to semantic mask generation. DiffuMask uses text-guided cross-attention information to localize class/word-specific regions, which are combined with practical techniques to create a novel high-resolution and class-discriminative pixel-wise mask. The methods help to reduce data collection and annotation costs obviously. Experiments demonstrate that the existing segmentation methods trained on synthetic data of DiffuMask can achieve a competitive performance over the counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird), DiffuMask presents promising performance, close to the stateof-the-art result of real data (within 3% mIoU gap). Moreover, in the open-vocabulary segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on Unseen class of VOC 2012. The project website can be found at https://weijiawu.github.io/DiffusionMask/.																																	2024-05-25	PPRN:46958810		
J	Hainline, Kevin N.; Johnson, Benjamin D.; Robertson, Brant; Tacchella, Sandro; Helton, Jakob M.; Sun, Fengwu; Eisenstein, Daniel J.; Simmonds, Charlotte; Topping, Michael W.; Whitler, Lily; Willmer, Christopher N.A.; Rieke, Marcia; Suess, Katherine A.; Hviding, Raphael E.; Cameron, Alex J.; Alberts, Stacey; Baker, William M.; Baum, Stefi; Bhatawdekar, Rachana; Bonaventura, Nina; Boyett, Kristan; Bunker, Andrew J.; Carniani, Stefano; Charlot, Stephane; Chevallard, Jacopo; Chen, Zuyi; Curti, Mirko; Curtis-Lake, Emma; D'Eugenio, Francesco; Egami, Eiichi; Endsley, Ryan; Hausen, Ryan; Ji, Zhiyuan; Looser, Tobias J.; Lyu, Jianwei; Maiolino, Roberto; Nelson, Erica; Puskas, David; Rawle, Tim; Sandles, Lester; Saxena, Aayush; Smit, Renske; Stark, Daniel P.; Williams, Christina C.; Willott, Chris; Witstok, Joris				ji, zhiyuan/HGC-6180-2022; Tacchella, Sandro/AAT-1602-2021; Lyu, Jianwei/KGL-5057-2024; Endsley, Ryan/AAJ-5103-2021; Witstok, Joris/GQA-8643-2022; D'Eugenio, Francesco/H-2606-2019; Helton, Jakob/KXS-1907-2024; Smit, Renske/MIK-8564-2025; Baker, William/KUD-6412-2024; Puskás, Dávid/NZO-4934-2025; Robertson, Brant/AAA-6124-2022; BOEKER, TORSTEN/KVC-3022-2024; Nelson, Erica/OUI-1817-2025						The Cosmos in its Infancy: JADES Galaxy Candidates at z > 8 in GOODS-S and GOODS-N								Arxiv											2	2;2024-01-11;https://www.arxiv.org/abs/2306.02468v3| 1;2023-06-28;https://www.arxiv.org/abs/2306.02468v2	arXiv:2306.02468			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 11 2024	2024	We present a catalog of 717 candidate galaxies at z > 8 selected from 125 square arcminutes of NIR-Cam imaging as part of the JWST Advanced Deep Extragalactic Survey (JADES). We combine the full JADES imaging dataset with data from the JEMS and FRESCO JWST surveys along with extremely deep existing observations from HST/ACS for a final filter set that includes fifteen JWST/NIRCam filters and five HST/ACS filters. The high-redshift galaxy candidates were selected from their estimated photometric redshifts calculated using a template fitting approach, followed by visual inspection from seven independent reviewers. We explore these candidates in detail, highlighting interesting resolved or extended sources, sources with very red long-wavelength slopes, and our highest redshift candidates, which extend to zphot ∼ 18. Over 93% of the sources are newly identified from our deep JADES imaging, including 31 new galaxy candidates at zphot > 12. We also investigate potential contamination by stellar objects, and do not find strong evidence from SED fitting that these faint high-redshift galaxy candidates are low-mass stars. Using 42 sources in our sample with measured spectroscopic redshifts from NIRSpec and FRESCO, we find excellent agreement to our photometric redshift estimates, with no catastrophic outliers and an average difference of ⟨∆z = zphot − zspec⟩ = 0.26. These sources comprise one of the most robust samples for probing the early buildup of galaxies within the first few hundred million years of the Universe’s history.																																	2024-01-26	PPRN:73028848		
J	Li, Xinghang; Li, Peiyan; Liu, Minghuan; Wang, Dong; Liu, Jirong; Kang, Bingyi; Ma, Xiao; Kong, Tao; Zhang, Hanbo; Liu, Huaping										Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models								Arxiv											2	2;2024-12-24;https://www.arxiv.org/abs/2412.14058v3| 1;2024-12-18;https://www.arxiv.org/abs/2412.14058v1	arXiv:2412.14058			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Dec 24 2024	2024	Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research.																																	2025-02-05	PPRN:120040988		
J	Taubenfeld, Amir; Dover, Yaniv; Reichart, Roi; Goldstein, Ariel				Dover, Yaniv/F-8916-2016						Systematic Biases in LLM Simulations of Debates								Arxiv											3	3;2024-12-17;https://www.arxiv.org/abs/2402.04049v3| 2;2024-09-28;https://www.arxiv.org/abs/2402.04049v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.04049v1	arXiv:2402.04049			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 17 2024	2024	The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates on topics that are important aspects of people's day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.																																	2025-01-25	PPRN:87529164		
J	Wu, Rundi; Gao, Ruiqi; Poole, Ben; Trevithick, Alex; Zheng, Changxi; Barron, Jonathan T.; Holynski, Aleksander				Gao, Rui-Qi/GRF-3082-2022						CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/2411.18613v1	arXiv:2411.18613			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos.																																	2025-01-10	PPRN:119470093		
J	Shen, Fei; Ye, Hu; Zhang, Jun; Wang, Cong; Han, Xiao; Yang, Wei				Han, Xiao/GQZ-6090-2022; 沈, 飞/HKM-4286-2023						Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models								Arxiv											5	5;2024-11-21;https://www.arxiv.org/abs/2310.06313v4| 4;2024-03-13;https://www.arxiv.org/abs/2310.06313v3| 3;2023-10-16;https://www.arxiv.org/abs/2310.06313v2| 2;2023-10-10;https://www.arxiv.org/abs/2310.06313v1| 1;2023-10-10;https://www.arxiv.org/abs/2310.06313v1	arXiv:2310.06313			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 21 2024	2024	Recent work has showcased the significant potential of diffusion models in pose- guided person image synthesis. However, owing to the inconsistency in pose between the source and target images, synthesizing an image with a distinct pose, relying exclusively on the source image and target pose information, remains a formidable challenge. This paper presents P rogressive C onditional D iffusion M odel s (PCDMs) that incrementally bridge the gap between person images under the target and source poses through three stages. Specifically, in the first stage, we design a simple prior conditional diffusion model that predicts the global features of the target image by mining the global alignment relationship between pose coordinates and image appearance. Then, the second stage establishes a dense correspondence between the source and target images using the global features from the previous stage, and an inpainting conditional diffusion model is proposed to further align and enhance the contextual features, generating a coarse-grained person image. In the third stage, we propose a refining conditional diffusion model to utilize the coarsely generated image from the previous stage as a condition, achieving texture restoration and enhancing fine-detail consistency. The three-stage PCDMs work progressively to generate the final high-quality and high-fidelity synthesized image. Both qualitative and quantitative results demonstrate the consistency and photorealism of our proposed PCDMs under challenging scenarios. The code and model will be available at https://github.com/tencent-ailab/PCDMs.																																	2025-01-03	PPRN:85521383		
J	Zhong, Ming; Shen, Yelong; Wang, Shuohang; Lu, Yadong; Jiao, Yizhu; Ouyang, Siru; Yu, Donghan; Han, Jiawei; Chen, Weizhu				Lu, Yadong/JGL-6374-2023; han, jiawei/GVT-3012-2022; Ouyang, Siru/OKS-7297-2025						Multi-LoRA Composition for Image Generation								Arxiv											1	1;2024-11-19;https://www.arxiv.org/abs/2402.16843v2	arXiv:2402.16843			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 19 2024	2024	Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery. In this paper, we study multi-LoRA composition through a decoding-centric perspective. We present two training-free methods: LORA SWITCH, which alternates between different LoRAs at each denoising step, and LORA COMPOSITE, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights, and all evaluation details are available on our project website.																																	2024-12-28	PPRN:119275023		
J	Ma, Yiyang; Liu, Xingchao; Chen, Xiaokang; Liu, Wen; Wu, Chengyue; Wu, Zhiyu; Pan, Zizheng; Xie, Zhenda; Zhang, Haowei; Yu, Xingkai; Zhao, Liang; Wang, Yisong; Liu, Jiaying; Ruan, Chong				Xie, Zhenda/O-1198-2013; Yu, Xingkai/AAO-5118-2020						JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation								Arxiv											1	1;2024-11-12;https://www.arxiv.org/abs/2411.07975v1	arXiv:2411.07975			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 12 2024	2024	We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.																																	2025-01-11	PPRN:119146795		
J	Mehrabi, Ninareh; Goyal, Palash; Dupuy, Christophe; Hu, Qian; Ghosh, Shalini; Zemel, Richard; Chang, Kai-Wei; Galstyan, Aram; Gupta, Rahul				Galstyan, Aram/G-3660-2011; Chang, Kai-Wei/AAJ-7874-2020						FLIRT: Feedback Loop In-context Red Teaming								Arxiv											2	2;2024-11-07;https://www.arxiv.org/abs/2308.04265v2| 1;2023-08-08;https://www.arxiv.org/abs/2308.04265v1	arXiv:2308.04265			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 07 2024	2024	As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. In this work, we propose an automatic red teaming framework that evaluates a given black-box model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses incontext learning in a feedback loop to red team models and trigger them into unsafe content generation. In particular, taking text-to-image models as target models, we explore different feedback mechanisms to automatically learn effective and diverse adversarial prompts. Our experiments demonstrate that even with enhanced safety features, Stable Diffusion (SD) models are vulnerable to our adversarial prompts, raising concerns on their robustness in practical uses. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models.																																	2024-12-16	PPRN:74310852		
J	Zhang, Yanzhe; Yu, Tao; Yang, Diyi				zhang, yanzhe/O-7292-2015						Attacking Vision-Language Computer Agents via Pop-ups								Arxiv											1	1;2024-11-04;https://www.arxiv.org/abs/2411.02391v1	arXiv:2411.02391			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups, which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing the tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack.																																	2024-12-16	PPRN:119089438		
J	Yang, Rui; Ding, Ruomeng; Lin, Yong; Zhang, Huan; Zhang, Tong				Zhang, tong/IAP-2587-2023; Yang, Rui/LZG-6631-2025						Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs								Arxiv											2	2;2024-10-23;https://www.arxiv.org/abs/2406.10216v2| 1;2024-06-14;https://www.arxiv.org/abs/2406.10216v1	arXiv:2406.10216			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 23 2024	2024	Reward models trained on human preference data have been proven to effectively align Large Language Models (LLMs) with human intent within the framework of reinforcement learning from human feedback (RLHF). However, current reward models have limited generalization capabilities to unseen prompts and responses, which can lead to an unexpected phenomenon known as reward over-optimization, resulting in a decline in actual performance due to excessive optimization of rewards. While previous research has advocated for constraining policy optimization, our study introduces a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states. Specifically, we retain the base model's language model head and incorporate a suite of text-generation losses to preserve the hidden states' text-generation capabilities, while concurrently learning a reward head behind the same hidden states. Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviates the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm.																																	2024-11-26	PPRN:89334165		
J	Doneva, Daniela D.; Ramazanoglu, Fethi M.; Silva, Hector O.; Sotiriou, Thomas P.; Yazadjiev, Stoytcho S.				Silva, Hector/G-4963-2010; Doneva, Daniela/GVT-6721-2022; stoytcho/ABH-4440-2020; Sotiriou, Thomas/ABE-4017-2021						Spontaneous scalarization								Arxiv											1	1;2024-09-10;https://www.arxiv.org/abs/2211.01766v2	arXiv:2211.01766			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 10 2024	2024	Scalarization is a mechanism that endows strongly self-gravitating bodies, such as neutron stars and black holes, with a scalar-field configuration. It resembles a phase transition in that the scalar configuration appears only when a certain quantity that characterizes the compact object, for example, its compactness or spin, is beyond a threshold. A critical and comprehensive review of scalarization, including the mechanism itself, theories that exhibit it, its manifestation in neutron stars, black holes and their binaries, potential extension to other fields, and a thorough discussion of future perspectives, is provided.																																	2024-11-11	PPRN:22462532		
J	Liu, Yizhou; Gao, Pengfei; Wang, Xinchen; Liu, Jie; Shi, Yexuan; Zhang, Zhao; Peng, Chao				Shi, Yexuan/AGK-5636-2022; Peng, Chao/ITT-3850-2023; Liu, Yizhou/ABD-4361-2022						MarsCode Agent: AI-native Automated Bug Fixing								Arxiv											1	1;2024-09-04;https://www.arxiv.org/abs/2409.00899v2	arXiv:2409.00899			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 04 2024	2024	Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.																																	2024-09-13	PPRN:91734304		
J	Chen, Ziyu; Yang, Jiawei; Huang, Jiahui; de Lutio, Riccardo; Esturo, Janick Martinez; Ivanovic, Boris; Litany, Or; Gojcic, Zan; Fidler, Sanja; Pavone, Marco; Song, Li; Wang, Yue				Yang, Jiawei/AGH-2199-2022; Litany, Or/AAI-9678-2020; Chen, Ziyu/ABF-1535-2022						OmniRe: Omni Urban Scene Reconstruction								Arxiv											1	1;2024-08-29;https://www.arxiv.org/abs/2408.16760v1	arXiv:2408.16760			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 29 2024	2024	We introduce OmniRe, a holistic approach for efficiently reconstructing high-fidelity dynamic urban scenes from on-device logs. Recent methods for modeling driving sequences using neural radiance fields or Gaussian Splatting have demonstrated the potential of reconstructing challenging dynamic scenes, but often overlook pedestrians and other non-vehicle dynamic actors, hindering a complete pipeline for dynamic urban scene reconstruction. To that end, we propose a comprehensive 3DGS framework for driving scenes, named OmniRe, that allows for accurate, full-length reconstruction of diverse dynamic objects in a driving log. OmniRe builds dynamic neural scene graphs based on Gaussian representations and constructs multiple local canonical spaces that model various dynamic actors, including vehicles, pedestrians, and cyclists, among many others. This capability is unmatched by existing methods. OmniRe allows us to holistically reconstruct different objects present in the scene, subsequently enabling the simulation of reconstructed scenarios with all actors participating in real-time (~60Hz). Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We believe our work fills a critical gap in driving reconstruction.																																	2024-09-23	PPRN:91783558		
J	Abueidda, Diab W.; Pantidis, Panos; Mobasher, Mostafa E.				Abueidda, Diab/ADG-2231-2022; Mobasher, Mostafa/GRS-5690-2022						DEEPOKAN: DEEP OPERATOR NETWORK BASED ON KOLMOGOROV ARNOLD NETWORKS FOR MECHANICS PROBLEMS								Arxiv											3	3;2024-08-07;https://www.arxiv.org/abs/2405.19143v3| 2;2024-07-11;https://www.arxiv.org/abs/2405.19143v2| 1;2024-05-29;https://www.arxiv.org/abs/2405.19143v1	arXiv:2405.19143			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 07 2024	2024	The modern digital engineering design often requires costly repeated simulations for different scenarios. The prediction capability of neural networks (NNs) makes them suitable surrogates for providing design insights. However, only a few NNs can efficiently handle complex engineering scenario predictions. We introduce a new version of the neural operators called DeepOKAN, which utilizes Kolmogorov Arnold networks (KANs) rather than the conventional neural network architectures. Our DeepOKAN uses Gaussian radial basis functions (RBFs) rather than the B-splines. RBFs offer good approximation properties and are typically computationally fast. The KAN architecture, combined with RBFs, allows DeepOKANs to represent better intricate relationships between input parameters and output fields, resulting in more accurate predictions across various mechanics problems. Specifically, we evaluate DeepOKAN’s performance on several mechanics problems, including 1D sinusoidal waves, 2D orthotropic elasticity, and transient Poisson’s problem, consistently achieving lower training losses and more accurate predictions compared to traditional DeepONets. This approach should pave the way for further improving the performance of neural operators.																																	2024-08-17	PPRN:89104799		
J	Mondorf, Philipp; Plank, Barbara										Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey								Arxiv											2	2;2024-08-06;https://www.arxiv.org/abs/2404.01869v2| 1;2024-04-02;https://www.arxiv.org/abs/2404.01869v1	arXiv:2404.01869			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 06 2024	2024	Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on sophisticated reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.																																	2024-08-13	PPRN:88378412		
J	Lee, Nicholas; Wattanawong, Thanakul; Kim, Sehoon; Mangalam, Karttikeya; Shen, Sheng; Anumanchipalli, Gopala; Mahoney, Michael W.; Keutzer, Kurt; Gholami, Amir										LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement								Arxiv											2	2;2024-07-13;https://www.arxiv.org/abs/2403.15042v2| 1;2024-03-22;https://www.arxiv.org/abs/2403.15042v1	arXiv:2403.15042			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 13 2024	2024	Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model.																																	2024-07-23	PPRN:88263685		
J	Park, Junsoo; Jwa, Seungyeon; Ren, Meiying; Kim, Daeyoung; Choi, Sanghyuk										OffsetBias: Leveraging Debiased Data for Tuning Evaluators								Arxiv											2	2;2024-10-07;https://www.arxiv.org/abs/2407.06551v2| 1;2024-07-01;	arXiv:2407.06551			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Employing Large Language Models (LLMs) to assess the quality of generated responses, such as prompting instruct-tuned models or fine-tuning judge models, has become a widely adopted evaluation method. It is also known that such evaluators are vulnerable to biases, such as favoring longer responses. While it is important to overcome this problem, the specifics of these biases remain under-explored. In this work, we qualitatively identify six types of biases inherent in various judge models. We propose EvalBiasBench as a meta-evaluation collection of hand-crafted test cases for each bias type. Additionally, we present de-biasing dataset construction methods and the associated preference dataset OffsetBias. Experimental results demonstrate that fine-tuning on our dataset significantly enhances the robustness of judge models against biases and improves performance across most evaluation scenarios. We release our datasets and the fine-tuned judge model to public.																																	2024-11-18	PPRN:104376814		
J	Zhang, Haoji; Wang, Yiqin; Tang, Yansong; Liu, Yong; Feng, Jiashi; Dai, Jifeng; Jin, Xiaojie				Feng, Jiashi/AGX-6209-2022; tao, zhang/KHY-8716-2024; Dai, Jifeng/HGU-8741-2022						Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams								Arxiv											2	2;2024-06-30;https://www.arxiv.org/abs/2406.08085v2| 1;2024-06-12;https://www.arxiv.org/abs/2406.08085v1	arXiv:2406.08085			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 30 2024	2024	Benefiting from the advancements in large language models and cross-modal alignment, existing multi-modal video understanding methods have achieved prominent performance in offline scenario. However, online video streams, as one of the most common media forms in the real world, have seldom received attention. Compared to offline videos, the 'dynamic' nature of online video streams poses challenges for the direct application of existing models and introduces new problems, such as the storage of extremely long-term information, interaction between continuous visual content and 'asynchronous' user questions. Therefore, in this paper we present Flash-VStream, a video-language model that simulates the memory mechanism of human. Our model is able to process extremely long video streams in real-time and respond to user queries simultaneously. Compared to existing models, Flash-VStream achieves significant reductions in inference latency and VRAM consumption, which is intimately related to performing understanding of online streaming video. In addition, given that existing video understanding benchmarks predominantly concentrate on offline scenario, we propose VStream-QA, a novel question answering benchmark specifically designed for online video streaming understanding. Comparisons with popular existing methods on the proposed benchmark demonstrate the superiority of our method for such challenging setting. To verify the generalizability of our approach, we further evaluate it on existing video understanding benchmarks and achieves state-of-the-art performance in offline scenarios as well.																																	2024-07-18	PPRN:89291434		
J	Qian, Cheng; Han, Chi; Fung, Yi R.; Qin, Yujia; Liu, Zhiyuan; Ji, Heng				han, chi/LPR-1781-2024						CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models								Arxiv											3	3;2024-06-21;https://www.arxiv.org/abs/2305.14318v3| 2;2023-10-08;https://www.arxiv.org/abs/2305.14318v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.14318v1	arXiv:2305.14318			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 21 2024	2024	Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose C REATOR , a novel framework that enables LLMs to create their own tools using documentation and code realization. C REATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate C REATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CRE- RE - ATOR outperforms existing chain -of -thought, program -of -thought, and tool -using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs’ tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM’s problem -solving paradigm, driving us closer to the next frontier of artificial intelligence. All the codes and data are released.																																	2024-07-11	PPRN:71592582		
J	Debenedetti, Edoardo; Zhang, Jie; Balunovic, Mislav; Beurer-Kellner, Luca; Fischer, Marc; Tramer, Florian										AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents								Arxiv											3	3;2024-11-24;https://www.arxiv.org/abs/2406.13352v3| 2;2024-07-18;https://www.arxiv.org/abs/2406.13352v2| 1;2024-06-19;https://www.arxiv.org/abs/2406.13352v1	arXiv:2406.13352			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 19 2024	2024	AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.																																	2025-08-07	PPRN:89376868		
J	Lee, Jinhyuk; Chen, Anthony; Dai, Zhuyun; Dua, Dheeru; Sachan, Devendra Singh; Boratko, Michael; Luan, Yi; Arnold, Sebastien M.R.; Perot, Vincent; Dalmia, Siddharth; Hu, Hexiang; Lin, Xudong; Pasupat, Panupong; Amini, Aida; Cole, Jeremy R.; Riedel, Sebastian; Naim, Iftekhar; Chang, Ming-Wei; Guu, Kelvin				Luan, Yi/HLH-8516-2023; lin, Xudong/ISA-0071-2023						Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?								Arxiv											1	1;2024-06-19;https://www.arxiv.org/abs/2406.13121v1	arXiv:2406.13121			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 19 2024	2024	Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs' performance on in-context retrieval and reasoning. Our findings reveal LCLMs' surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks. However, LCLMs still face challenges in areas like compositional reasoning that are required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.1																																	2024-07-06	PPRN:89375402		
J	Briesch, Martin; Sobania, Dominik; Rothlauf, Franz				Rothlauf, Franz/O-4704-2016						Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2311.16822v2	arXiv:2311.16822			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	Large Language Models (LLM) are already widely used to generate content for a variety of online platforms. As we are not able to safely distinguish LLM-generated content from human-produced content, LLM-generated content is used to train the next generation of LLMs, giving rise to a self-consuming training loop. From the image generation domain we know that such a self-consuming training loop reduces both quality and diversity of images finally ending in a model collapse. However, it is unclear whether this alarming effect can also be observed for LLMs. Therefore, we present the first study investigating the self-consuming training loop for LLMs. Further, we propose a novel method based on logic expressions that allows us to unambiguously verify the correctness of LLM-generated content, which is difficult for natural language text. We find that the self-consuming training loop produces correct outputs, however, the output declines in its diversity depending on the proportion of the used generated data. Fresh data can slow down this decline, but not stop it. Given these concerning results, we encourage researchers to study methods to negate this process.																																	2025-08-07	PPRN:123164668		
J	Pi, Renjie; Han, Tianyang; Zhang, Jianshu; Xie, Yueqi; Pan, Rui; Lian, Qing; Dong, Hanze; Zhang, Jipeng; Zhang, Tong				Han, Tianyang/KEZ-8361-2024; Lian, Qing/HKN-1590-2023; Xie, Yueqi/JUV-7366-2023; Zhang, Peng/H-9461-2018; Zhang, Tong/HGC-1090-2022						MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance								Arxiv											3	3;2024-06-17;https://www.arxiv.org/abs/2401.02906v3| 2;2024-01-17;https://www.arxiv.org/abs/2401.02906v2| 1;2024-01-05;https://www.arxiv.org/abs/2401.02906v1	arXiv:2401.02906			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a ``foreign language" that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security.																																	2024-07-04	PPRN:86996842		
J	Zhu, Lei; Wei, Fangyun; Lu, Yanye; Chen, Dong				Zhu, Lei/KUD-1330-2024; Lu, Yanye/ABF-8769-2020						Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.11837v1	arXiv:2406.11837			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models. Code and models are available at https://github.com/zh460045050/VQGAN-LC.																																	2024-07-04	PPRN:89350117		
J	Zhang, Yi-Fan; Wen, Qingsong; Fu, Chaoyou; Wang, Xue; Zhang, Zhang; Wang, Liang; Jin, Rong				Wen, Qingsong/LTF-7625-2024						Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models								Arxiv											2	2;2024-06-14;https://www.arxiv.org/abs/2406.08487v3| 1;2024-06-13;https://www.arxiv.org/abs/2406.08487v2	arXiv:2406.08487			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Seeing clearly with high resolution is a foundation of Large Multimodal Models (LMMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the most important tokens accounting for the user question will be further selected by a similarity -based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. . Besides, a significant challenge lies in the training strategy, as simultaneous end -to -end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed LMM with S ophisticated Tasks, L ocal i mage compression, and M ixture of global E xperts (SliME), achieves leading performance across various benchmarks with only 2 million training data.																																	2024-07-02	PPRN:89294071		
J	Goyal, Ankit; Blukis, Valts; Xu, Jie; Guo, Yijie; Chao, Yu-Wei; Fox, Dieter				Blukis, Valts/HGA-0955-2022						RVT-2: Learning Precise Manipulation from Few Demonstrations								Arxiv											1	1;2024-06-12;https://www.arxiv.org/abs/2406.08545v1	arXiv:2406.08545			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 12 2024	2024	In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations.																																	2024-07-10	PPRN:89303051		
J	Xi, Zhiheng; Ding, Yiwen; Chen, Wenxiang; Hong, Boyang; Guo, Honglin; Wang, Junzhe; Yang, Dingwen; Liao, Chenyang; Guo, Xin; He, Wei; Gao, Songyang; Chen, Lu; Zheng, Rui; Zou, Yicheng; Gui, Tao; Zhang, Qi; Qiu, Xipeng; Huang, Xuanjing; Wu, Zuxuan; Jiang, Yu-Gang				Xi, Zhiheng/KUD-1665-2024; Zou, Yicheng/ISU-0863-2023; Wang, Junzhe/OFN-2765-2025; Gui, Tao/LWI-6783-2024; Liao, Chenyang/JQW-3331-2023; Zhang, Qi/JEP-7674-2023; Ding, Yiwen/NZN-8639-2025						AgentGym: Evolving Large Language Model-based Agents across Diverse Environments								Arxiv											1	1;2024-06-06;https://www.arxiv.org/abs/2406.04151v1	arXiv:2406.04151			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 06 2024	2024	Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations.																																	2024-06-22	PPRN:89249370		
J	Wang, Cong; Tian, Kuan; Zhang, Jun; Guan, Yonghang; Luo, Feng; Shen, Fei; Jiang, Zhiwei; Gu, Qing; Han, Xiao; Yang, Wei				沈, 飞/HKM-4286-2023; Han, Xiao/GQZ-6090-2022; Jiang, Zhiwei/JLR-6369-2023						V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation								Arxiv											1	1;2024-06-04;https://www.arxiv.org/abs/2406.02511v1	arXiv:2406.02511			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 04 2024	2024	In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent. A common approach involves leveraging generative models to enhance adapters for controlled generation. However, control signals (e.g., text, audio, reference image, pose, depth map, etc.) can vary in strength. Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions. In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as facial pose and reference image. However, direct training with weak signals often leads to difficulties in convergence. To address this, we propose V-Express, a simple method that balances different control signals through the progressive training and the conditional dropout operation. Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account the facial pose, reference image, and audio. The experimental results demonstrate that our method can effectively generate portrait videos controlled by audio. Furthermore, a potential solution is provided for the simultaneous and effective use of conditions of varying strengths.																																	2024-11-20	PPRN:89259188		
J	Mavi, Vaibhav; Jangra, Anubhav; Jatowt, Adam				Jatowt, Adam/AAT-3213-2020						Multi-hop Question Answering								Arxiv											1	1;2024-05-31;https://www.arxiv.org/abs/2204.09140v2	arXiv:2204.09140			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 31 2024	2024	The task of Question Answering (QA) has attracted significant research interest for long. Its relevance to language understanding and knowledge retrieval tasks, along with the simple setting makes the task of QA crucial for strong AI systems. Recent success on simple QA tasks has shifted the focus to more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most researched tasks over the recent years. In broad terms, MHQA is the task of answering natural language questions that involve extracting and combining multiple pieces of information and doing multiple steps of reasoning. An example of a multi-hop question would be "The Argentine PGA Championship record holder has won how many tournaments worldwide?". Answering the question would need two pieces of information: "Who is the record holder for Argentine PGA Championship tournaments?" and "How many tournaments did [Answer of Sub Q1] win?". The ability to answer multi-hop questions and perform multi step reasoning can significantly improve the utility of NLP systems. Consequently, the field has seen a surge with high quality datasets, models and evaluation strategies. The notion of 'multiple hops' is somewhat abstract which results in a large variety of tasks that require multi-hop reasoning. This leads to different datasets and models that differ significantly from each other and makes the field challenging to generalize and survey. We aim to provide a general and formal definition of the MHQA task, and organize and summarize existing MHQA frameworks. We also outline some best practices for building MHQA datasets. This book provides a systematic and thorough introduction as well as the structuring of the existing attempts to this highly interesting, yet quite challenging task.																																	2024-07-12	PPRN:89131563		
J	Yue, Zihao; Zhang, Liang; Jin, Qin										Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2402.14545v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.14545v1	arXiv:2402.14545			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge.																																	2024-08-25	PPRN:87806341		
J	Qiao, Shuofei; Zhang, Ningyu; Fang, Runnan; Luo, Yujie; Zhou, Wangchunshu; Jiang, Yuchen Eleanor; Lv, Chengfei; Chen, Huajun				Huajun, Chen/B-6340-2013; Zhang, Ningyu/AAQ-7391-2021; Wang, Benyou/Y-5146-2019						AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning								Arxiv											3	3;2024-05-26;https://www.arxiv.org/abs/2401.05268v4| 2;2024-01-17;https://www.arxiv.org/abs/2401.05268v2| 1;2024-01-10;https://www.arxiv.org/abs/2401.05268v1	arXiv:2401.05268			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 26 2024	2024	Language agents have achieved considerable performance on various complex question-answering tasks by planning with external tools. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework for QA that does not rely on large-scale annotated data and synthetic planning trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutoAct generally outperforming that of others1. 																																	2024-06-08	PPRN:87169724		
J	Li, Yafu; Li, Qintong; Cui, Leyang; Bi, Wei; Wang, Zhilin; Wang, Longyue; Yang, Linyi; Shi, Shuming; Zhang, Yue				Li, Qintong/HGA-9822-2022						MAGE: Machine-generated Text Detection in the Wild								Arxiv											2	2;2024-05-21;https://www.arxiv.org/abs/2305.13242v3| 1;2023-05-22;https://www.arxiv.org/abs/2305.13242v1	arXiv:2305.13242			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 21 2024	2024	Large language models (LLMs) have achieved human-level text generation, emphasizing the need for effective AI-generated text detection to mitigate risks like the spread of fake news and plagiarism. Existing research has been constrained by evaluating detection methods on specific domains or particular language models. In practical scenarios, however, the detector faces texts from various domains or LLMs without knowing their sources. To this end, we build a comprehensive testbed by gathering texts from diverse human writings and texts generated by different LLMs. Empirical results show challenges in distinguishing machine-generated texts from human-authored ones across various scenarios, especially out-of-distribution. These challenges are due to the decreasing linguistic distinctions between the two sources. Despite challenges, the top-performing detector can identify 86.54% out-of-domain texts generated by a new LLM, indicating the feasibility for application scenarios. 																																	2024-08-23	PPRN:70823456		
J	Jiang, Ting; Huang, Shaohan; Luo, Shengyue; Zhang, Zihan; Huang, Haizhen; Wei, Furu; Deng, Weiwei; Sun, Feng; Zhang, Qi; Wang, Deqing; Zhuang, Fuzhen				Wang, Deqing/E-4845-2013; Zhang, Zihan/Y-7713-2018; Huang, Shaohan/LDF-3300-2024						MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning								Arxiv											1	1;2024-05-20;https://www.arxiv.org/abs/2405.12130v1	arXiv:2405.12130			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 20 2024	2024	Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.																																	2024-08-25	PPRN:91459609		
J	Liu, Hongwei; Zheng, Zilong; Qiao, Yuxuan; Duan, Haodong; Fei, Zhiwei; Zhou, Fengzhe; Zhang, Wenwei; Zhang, Songyang; Lin, Dahua; Chen, Kai				liu, hongwei/ACJ-6082-2022; Zhang, Wenwei/HKO-4277-2023; Duan, Haodong/ITV-1505-2023; Zhang, Songyang/GPX-5621-2022; Lin, Dahua/W-6576-2019						MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark								Arxiv											1	1;2024-05-20;https://www.arxiv.org/abs/2405.12209v1	arXiv:2405.12209			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 20 2024	2024	Recent advancements in large language models (LLMs) have showcased significant improvements in mathematics. However, traditional math benchmarks like GSM8k offer a unidimensional perspective, falling short in providing a holistic assessment of the LLMs’ math capabilities. To address this gap, we introduce MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large language models. MathBench spans a wide range of mathematical disciplines, offering a detailed evaluation of both theoretical understanding and practical problem-solving skills. The benchmark progresses through five distinct stages, from basic arithmetic to college mathematics, and is structured to evaluate models at various depths of knowledge. Each stage includes theoretical questions and application problems, allowing us to measure a model’s mathematical proficiency and its ability to apply concepts in practical scenarios. MathBench aims to enhance the evaluation of LLMs’ mathematical abilities, providing a nuanced view of their knowledge understanding levels and problem solving skills in a bilingual context. The project is released at https://github. com/open-compass/MathBench. .																																	2024-06-15	PPRN:89094472		
J	Pei, Zehua; Zhen, Hui-Ling; Yuan, Mingxuan; Huang, Yu; Yu, Bei				PEI, Zehua/LBI-5961-2024						BetterV: Controlled Verilog Generation with Discriminative Guidance								Arxiv											3	3;2024-05-02;https://www.arxiv.org/abs/2402.03375v3| 2;2024-04-28;https://www.arxiv.org/abs/2402.03375v2| 1;2024-02-03;https://www.arxiv.org/abs/2402.03375v1	arXiv:2402.03375			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen rising research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes the large language models (LLMs) on processed domainspecific datasets and incorporates generative discriminators for guidance on particular design demands. The Verilog modules are collected, filtered and processed from internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tune the LLMs to understand the knowledge about Verilog. Furthermore, data are augmented to enrich the training set and also used to train a generative discriminator on particular downstream task, which leads a guidance for the LLMs to optimize the Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, which can outperform GPT-4 on the VerilogEval benchmark. With the help of task -specific generative discriminator, BetterV can achieve remarkable improvement on various electronic design automation (EDA) downstream tasks, including the netlist node reduction for synthesis and verification runtime reduction with Boolean Satisfiability (SAT) solving.																																	2024-05-20	PPRN:87533776		
J	Phuong, Mary; Aitchison, Matthew; Catt, Elliot; Cogan, Sarah; Kaskasoli, Alexandre; Krakovna, Victoria; Lindner, David; Rahtz, Matthew; Assael, Yannis; Hodkinson, Sarah; Howard, Heidi; Lieberum, Tom; Kumar, Ramana; Raad, Maria Abi; Webson, Albert; Ho, Lewis; Lin, Sharon; Farquhar, Sebastian; Hutter, Marcus; Deletang, Gregoire; Ruoss, Anian; El-Sayed, Seliem; Brown, Sasha; Dragan, Anca; Shah, Rohin; Dafoe, Allan; Shevlane, Toby				Assael, Yannis/E-8160-2013; Dafoe, Allan/G-2505-2014						Evaluating Frontier Models for Dangerous Capabilities								Arxiv											2	2;2024-04-05;https://www.arxiv.org/abs/2403.13793v2| 1;2024-03-20;https://www.arxiv.org/abs/2403.13793v1	arXiv:2403.13793			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 05 2024	2024	To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new "dangerous capability" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.																																	2024-04-19	PPRN:88243682		
J	Lee, Seongyun; Park, Sue Hyun; Jo, Yongrae; Seo, Minjoon										Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision								Arxiv											2	2;2024-04-02;https://www.arxiv.org/abs/2311.07362v4| 1;2023-11-14;https://www.arxiv.org/abs/2311.07362v2	arXiv:2311.07362			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	Large multimodal models suffer from multi-modal hallucination, where they provide in-correct responses misaligned with the given visual information. Recent works have con-jectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visualcues. Building on this approach, we introduce VOLCANO, a multimodal self-feedback guided revision model. VOLCANO generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. VOLCANO effectively reduces multi-modal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Veta nd MM Bench. Through qualitative analysis, we show that VOLCANO’s feedback is properly grounded on the image than the initial response. This indicates that VOLCANO can provide itself with richer visual information through feedback generation, leading to self-correct hallucinations. We publicly release our model, data																																	2024-04-18	PPRN:86200504		
J	Parmar, Gaurav; Park, Taesung; Narasimhan, Srinivasa; Zhu, Jun-Yan				Zhu, Jun-Yan/V-7271-2018; Parmar, Gaurav/AAZ-7824-2021						One-Step Image Translation with Text-to-Image Models								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.12036v1	arXiv:2403.12036			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end -to -end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion -based methods for various scene translation tasks, such as day -to -night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like ControlNet for Sketch2Photo and Edge2Image, but with a single-step inference. This work suggests that single-step diffusion models can serve as strong backbones for a range of GAN learning objectives																																	2024-04-11	PPRN:88198948		
J	Wang, Qineng; Wang, Zihao; Su, Ying; Tong, Hanghang; Song, Yangqiu				Song, Yangqiu/OHU-0096-2025						Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18272v1	arXiv:2402.18272			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.																																	2024-03-28	PPRN:87988654		
J	Long, Stephanie; Schuster, Tibor; Piche, Alexandre				Schuster, Tibor/LQJ-4546-2024						Can large language models build causal graphs?								Arxiv											3	3;2024-02-23;https://www.arxiv.org/abs/2303.05279v2| 2;2023-03-07;https://www.arxiv.org/abs/2303.05279v1| 1;2023-03-07;https://www.arxiv.org/abs/2303.05279v1	arXiv:2303.05279			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.																																	2024-11-09	PPRN:44840032		
J	Bonetti, Federico; Del Zotto, Michele; Minasian, Ruben				Bonetti, Federico/HJA-7787-2022; del zotto, michele/C-4685-2018						SymTFTs for Continuous non-Abelian Symmetries								Arxiv											1	1;2024-02-19;https://www.arxiv.org/abs/2402.12347v1	arXiv:2402.12347			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 19 2024	2024	Topological defects and operators give a far-reaching generalization of symmetries of quantum fields. An auxiliary topological field theory in one dimension higher than the QFT of interest, known as the SymTFT, provides a natural way for capturing such operators. This gives a new perspective on several applications of symmetries, but fails to capture continuous non -Ab elian symmetries. The main aim of this work is to fill this gap. Guided by geometric engineering and holography, we recover various known features of representation theory of the non -Ab elian symmetry from a SymTFT viewpoint. Central to our approach is a duality between (flat) free Yang-Mills and non -Ab elian BF theories. Our results extend naturally to models without supersymmetry.																																	2024-03-16	PPRN:87760125		
J	Hernandez, Evan; Sharma, Arnab Sen; Haklay, Tal; Meng, Kevin; Wattenberg, Martin; Andreas, Jacob; Belinkov, Yonatan; Bau, David				Bau, David/KGM-5427-2024						Linearity of Relation Decoding in Transformer Language Models								Arxiv											2	2;2024-02-15;https://www.arxiv.org/abs/2308.09124v2| 1;2023-08-17;https://www.arxiv.org/abs/2308.09124v1	arXiv:2308.09124			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 15 2024	2024	Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.																																	2024-03-14	PPRN:85992757		
J	Sitawarin, Chawin; Mu, Norman; Wagner, David; Araujo, Alexandre				Sitawarin, Chawin/KIB-4488-2024						PAL: Proxy-Guided Black-Box Attack on Large Language Models								Arxiv											1	1;2024-02-15;https://www.arxiv.org/abs/2402.09674v1	arXiv:2402.09674			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 15 2024	2024	Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy -Guided Attack on LLMs (PAL), the first optimization -based attack on LLMs in a black -box query -only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real -world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT3.5-Turbo and 48% on Llama -2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white -box Llama -2-7B, and the Random -Search Attack on LLMs (RAL), a strong but simple baseline for query -based attacks. We believe the techniques proposed in this work will enable more comprehensive safety testing of LLMs and, in the long term, the development of better security guardrails. The code can be found athttps://github.com/chawins/pal.																																	2024-03-13	PPRN:87704088		
J	Shen, Weizhou; Li, Chenliang; Chen, Hongzhan; Yan, Ming; Quan, Xiaojun; Chen, Hehong; Zhang, Ji; Huang, Fei				Chen, Chih-Ming/I-2464-2015; Yan, Ming/LDT-2692-2024; Chen, Hui/AAE-6559-2019						Small LLMs Are Weak Tool Learners: A Multi-LLM Agent								Arxiv											2	2;2024-02-01;https://www.arxiv.org/abs/2401.07324v2| 1;2024-01-14;https://www.arxiv.org/abs/2401.07324v1	arXiv:2401.07324			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 01 2024	2024	Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.																																	2024-05-25	PPRN:87189238		
J	Apruzzi, Fabio; Bedogna, Francesco; Dondi, Nicola				Apruzzi, Fabio/AAY-3418-2020						SymTh for non-finite symmetries								Arxiv											2	2;2024-12-31;https://www.arxiv.org/abs/2402.14813v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.14813v1	arXiv:2402.14813			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 31 2024	2024	Symmetry topological field theory (SymTFT) is a convenient tool for studying finite generalized symmetries of a given quantum field theory (QFT). In particular, SymTFTs encode all the symmetry structures and properties, including anomalies. Recently, this tool has been applied for non-finite symmetries as well. In this paper, we take a different route, which consists of considering a free theory rather than a topological field theory in the bulk. We call it Symmetry Theory (SymTh). We study its topological operators together with the free boundary conditions. We also propose a procedure that is analogous to the sandwich construction of SymTFTs and allows us to obtain the physical QFT. We apply this to many examples, ranging from abelian p-form symmetries to 2-groups, and the (solvable) case of group-like symmetries in quantum mechanics. Finally, we provide a derivation of the SymTh of Q/Z non-invertible symmetries from the dimensional reduction of IIB supergravity on the conifold. In addition, we give an ultraviolet interpretation of the quantum Hall states dressing the non-invertible Q/Z topological defects, in terms of branes in the IIB supergravity background.																																	2025-02-15	PPRN:87800621		
J	Karras, Tero; Aittala, Miika; Kynkaeaenniemi, Tuomas; Lehtinen, Jaakko; Aila, Timo; Laine, Samuli				Lehtinen, Jaakko/G-2328-2013						Guiding a Diffusion Model with a Bad Version of Itself								Arxiv											3	3;2024-12-19;https://www.arxiv.org/abs/2406.02507v3| 2;2024-12-09;https://www.arxiv.org/abs/2406.02507v2| 1;2024-06-04;https://www.arxiv.org/abs/2406.02507v1	arXiv:2406.02507			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 19 2024	2024	The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.																																	2025-01-26	PPRN:89262288		
J	Ouyang, Hao; Wang, Qiuyu; Xiao, Yuxi; Bai, Qingyan; Zhang, Juntao; Zheng, Kecheng; Zhou, Xiaowei; Chen, Qifeng; Shen, Yujun				xiao, yuxi/KPA-1941-2024; Wang, qiuyu/LDO-6178-2024						CoDeF: Content Deformation Fields for Temporally Consistent Video Processing								Arxiv											2	2;2024-12-12;https://www.arxiv.org/abs/2308.07926v2| 1;2023-08-15;https://www.arxiv.org/abs/2308.07926v1	arXiv:2308.07926			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 12 2024	2024	We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video. With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field. We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog.																																	2025-01-22	PPRN:77937731		
J	Mccabe, Michael; Regaldo-Saint Blancard, Bruno; Parker, Liam Holden; Ohana, Ruben; Cranmer, Miles; Bietti, Alberto; Eickenberg, Michael; Golkar, Siavash; Krawezik, Geraud; Lanusse, Francois; Pettee, Mariel; Tesileanu, Tiberiu; Cho, Kyunghyun; Ho, Shirley; Polymathic AI Collaboration				Cranmer, Miles/ABE-2188-2021						Multiple Physics Pretraining for Physical Surrogate Models								Arxiv											2	2;2024-12-10;https://www.arxiv.org/abs/2310.02994v2| 1;2023-10-04;https://www.arxiv.org/abs/2310.02994v1	arXiv:2310.02994			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 10 2024	2024	We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling of spatiotemporal systems with transformers. In MPP, rather than training one model on a specific physical system, we train a backbone model to predict the dynamics of multiple heterogeneous physical systems simultaneously in order to learn features that are broadly useful across systems and facilitate transfer. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on systems with previously unseen physical components or higher dimensional systems compared to training from scratch or finetuning pre- trained video foundation models. We open-source our code and model weights trained at multiple scales for reproducibility.																																	2025-01-19	PPRN:85398788		
J	Sreenivas, Sharath Turuvekere; Muralidharan, Saurav; Joshi, Raviraj; Chochowski, Marcin; Mahabaleshwarkar, Ameya Sunil; Shen, Gerald; Zeng, Jiaqi; Chen, Zijia; Suhara, Yoshi; Diao, Shizhe; Yu, Chenhan; Chen, Wei-Chun; Ross, Hayley; Olabiyi, Oluwatobi; Aithal, Ashwath; Kuchaiev, Oleksii; Korzekwa, Daniel; Molchanov, Pavlo; Patwary, Mostofa; Shoeybi, Mohammad; Kautz, Jan; Catanzaro, Bryan				Korzekwa, Daniel/CAG-6093-2022; Diao, Shizhe/JXY-7398-2024						LLM Pruning and Distillation in Practice: The Minitron Approach								Arxiv											3	3;2024-12-09;https://www.arxiv.org/abs/2408.11796v4| 2;2024-11-30;https://www.arxiv.org/abs/2408.11796v3| 1;2024-08-21;https://www.arxiv.org/abs/2408.11796v1	arXiv:2408.11796			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 09 2024	2024	We present a comprehensive report on compressing the Llama 3.1 8B and Mistral NeMo 12B models to 4B and 8B parameters, respectively, using pruning and distillation. We explore two distinct pruning strategies: (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning, and evaluate the results on common benchmarks from the LM Evaluation Harness. The models are then aligned with NeMo Aligner and tested in instruct-tuned versions. This approach produces a compelling 4B model from Llama 3.1 8B and a state-of-the-art Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo 12B. We found that with no access to the original data, it is beneficial to slightly fine-tune teacher models on the distillation dataset. We open-source our base model weights on Hugging Face with a permissive license.																																	2025-01-17	PPRN:91499832		
J	Wen, Jiaxin; Zhong, Ruiqi; Khan, Akbir; Perez, Ethan; Steinhardt, Jacob; Huang, Minlie; Bowman, Samuel R.; He, He; Feng, Shi				Zhong, Ruiqi/HGA-7351-2022						Language Models Learn to Mislead Humans via RLHF								Arxiv											3	3;2024-12-08;https://www.arxiv.org/abs/2409.12822v3| 2;2024-09-25;https://www.arxiv.org/abs/2409.12822v2| 1;2024-09-19;https://www.arxiv.org/abs/2409.12822v1	arXiv:2409.12822			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 08 2024	2024	Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at convincing humans that they are right even when they are wrong. We study this phenomenon under a standard RLHF pipeline, calling it "U-SOPHISTRY" since it is Unintended by model developers. Specifically, we ask time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task (QuALITY) and programming task (APPS), RLHF makes LMs better at convincing our subjects but not at completing the task correctly. RLHF also makes the model harder to evaluate: our subjects' false positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored LMs), does not generalize to U-SOPHISTRY. Our results highlight an important failure mode of RLHF and call for more research in assisting humans to align them.																																	2025-01-17	PPRN:92374142		
J	Xu, Guangkai; Ge, Yongtao; Liu, Mingyu; Fan, Chengxiang; Xie, Kangyang; Zhao, Zhiyue; Chen, Hao; Shen, Chunhua										What Matters When Repurposing Diffusion Models for General Dense Perception Tasks?								Arxiv											3	3;2024-12-01;https://www.arxiv.org/abs/2403.06090v4| 2;2024-03-15;https://www.arxiv.org/abs/2403.06090v2| 1;2024-03-10;https://www.arxiv.org/abs/2403.06090v1	arXiv:2403.06090			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 01 2024	2024	Extensive pre-training with large data is indispensable for downstream geometry and semantic visual perception tasks. Thanks to large-scale text-to-image (T2I) pretraining, recent works show promising results by simply fine-tuning T2I diffusion models for a few dense perception tasks. However, several crucial design decisions in this process still lack comprehensive justification, encompassing the necessity of the multi-step diffusion mechanism, training strategy, inference ensemble strategy, and fine-tuning data quality. In this work, we conduct a thorough investigation into critical factors that affect transfer efficiency and performance when using diffusion priors. Our key findings are: 1) High-quality fine-tuning data is paramount for both semantic and geometry perception tasks. 2) As a special case of the diffusion scheduler by setting its hyper-parameters, the multi-step generation can be simplified to a one-step fine-tuning paradigm without any loss of performance. 3) Apart from fine-tuning the diffusion model with only latent space supervision, task-specific supervision is beneficial to enhance fine-grained details. These observations culminate in the development of GenPercept, an effective deterministic one-step fine-tuning paradigm tailored for dense visual perception tasks exploiting diffusion priors. Different from the previous multi-step methods, our paradigm offers a much faster inference speed, and can be seamlessly integrated with customized perception decoders and loss functions for task- specific supervision, which can be critical for improving the fine-grained details of predictions. Comprehensive experiments on a diverse set of dense visual perceptual tasks, including monocular depth estimation, surface normal estimation, image segmentation, and matting, are performed to demonstrate the remarkable adaptability and effectiveness of our proposed method.																																	2025-01-16	PPRN:88107565		
J	Kumar, Tanishq; Ankner, Zachary; Spector, Benjamin F.; Bordelon, Blake; Muennighoff, Niklas; Paul, Mansheej; Pehlevan, Cengiz; Re, Christopher; Raghunathan, Aditi				Pehlevan, Cengiz/I-2259-2013						Scaling Laws for Precision								Arxiv											2	2;2024-11-30;https://www.arxiv.org/abs/2411.04330v2| 1;2024-11-07;https://www.arxiv.org/abs/2411.04330v1	arXiv:2411.04330			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 30 2024	2024	Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise “precision-aware” scaling laws for both training and inference. We propose that training in lower precision reduces the model’s effective parameter count, allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.																																	2025-01-11	PPRN:119070113		
J	Ji, Shengpeng; Chen, Yifu; Fang, Minghui; Zuo, Jialong; Lu, Jingyu; Wang, Hanting; Jiang, Ziyue; Zhou, Long; Liu, Shujie; Cheng, Xize; Yang, Xiaoda; Wang, Zehan; Yang, Qian; Li, Jian; Jiang, Yidi; He, Jingzhen; Chu, Yunfei; Xu, Jin; Zhao, Zhou				Chen, Yifu/LBI-0793-2024; Lu, Jingyu/JXL-5617-2024; Chu, Yunfei/C-8002-2013; jiang, ziyue/GSI-9122-2022; yang, qian/LRT-5966-2024						WavChat: A Survey of Spoken Dialogue Models								Arxiv											2	2;2024-11-26;https://www.arxiv.org/abs/2411.13577v2| 1;2024-11-15;https://www.arxiv.org/abs/2411.13577v1	arXiv:2411.13577			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 26 2024	2024	Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. 																																	2025-01-08	PPRN:119318173		
J	Li, Panfeng; Abouelenien, Mohamed; Mihalcea, Rada; Ding, Zhicheng; Yang, Qikai; Zhou, Yiming				Yang, Qikai/OMK-9174-2025; Zhou, Yiming/LQK-0943-2024; Ding, Zhicheng/KGK-9412-2024; Li, Panfeng/LBG-9988-2024						Deception Detection from Linguistic and Physiological Data Streams Using Bimodal Convolutional Neural Networks								Arxiv											5	5;2024-11-12;https://www.arxiv.org/abs/2311.10944v5| 4;2024-06-26;https://www.arxiv.org/abs/2311.10944v4| 3;2024-05-20;https://www.arxiv.org/abs/2311.10944v3| 2;2023-12-23;https://www.arxiv.org/abs/2311.10944v2| 1;2023-11-18;https://www.arxiv.org/abs/2311.10944v1	arXiv:2311.10944			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 12 2024	2024	Deception detection is gaining increasing interest due to ethical and security concerns. This paper explores the application of convolutional neural networks for the purpose of multimodal deception detection. We use a dataset built by interviewing 104 subjects about two topics, with one truthful and one falsified response from each subject about each topic. In particular, we make three main contributions. First, we extract linguistic and physiological features from this data to train and construct the neural network models. Second, we propose a fused convolutional neural network model using both modalities in order to achieve an improved overall performance. Third, we compare our new approach with earlier methods designed for multimodal deception detection. We find that our system outperforms regular classification methods; our results indicate the feasibility of using neural networks for deception detection even in the presence of limited amounts of data.																																	2024-12-18	PPRN:86214736		
J	Tao, Chaofan; Liu, Qian; Dou, Longxu; Muennighoff, Niklas; Wan, Zhongwei; Luo, Ping; Lin, Min; Wong, Ngai				Wan, Zhongwei/JDM-4369-2023; Luo, Ping/HGE-7623-2022						Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies								Arxiv											3	3;2024-11-01;https://www.arxiv.org/abs/2407.13623v3| 2;2024-07-26;https://www.arxiv.org/abs/2407.13623v2| 1;2024-07-18;https://www.arxiv.org/abs/2407.13623v1	arXiv:2407.13623			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 01 2024	2024	Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training. 																																	2024-12-06	PPRN:90882205		
J	Xhonneux, Sophie; Sordoni, Alessandro; Guennemann, Stephan; Gidel, Gauthier; Schwinn, Leo				Schwinn, Leo/AGZ-7273-2022						Efficient Adversarial Training in LLMs with Continuous Attacks								Arxiv											3	3;2024-11-01;https://www.arxiv.org/abs/2405.15589v3| 2;2024-06-21;https://www.arxiv.org/abs/2405.15589v2| 1;2024-05-24;https://www.arxiv.org/abs/2405.15589v1	arXiv:2405.15589			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 01 2024	2024	Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.																																	2024-12-06	PPRN:89009515		
J	Orgad, Hadas; Toker, Michael; Gekhman, Zorik; Reichart, Roi; Szpektor, Idan; Kotek, Hadas; Belinkov, Yonatan										LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations								Arxiv											3	3;2024-10-28;https://www.arxiv.org/abs/2410.02707v3| 2;2024-10-07;https://www.arxiv.org/abs/2410.02707v2| 1;2024-10-03;https://www.arxiv.org/abs/2410.02707v1	arXiv:2410.02707			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 28 2024	2024	Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims—truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.1																																	2024-12-06	PPRN:102572529		
J	Schuster, Thomas; Yin, Chao; Gao, Xun; Yao, Norman Y.				Schuster, Thomas/AAF-3430-2020; Yao, Norman/A-3929-2009						A polynomial-time classical algorithm for noisy quantum circuits								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2407.12768v2| 1;2024-07-17;https://www.arxiv.org/abs/2407.12768v1	arXiv:2407.12768			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	We provide a polynomial-time classical algorithm for noisy quantum circuits. The algorithm computes the expectation value of any observable for any circuit, with a small average error over input states drawn from an ensemble (e.g. the computational basis). Our approach is based upon the intuition that noise exponentially damps non-local correlations relative to local correlations. This enables one to classically simulate a noisy quantum circuit by only keeping track of the dynamics of local quantum information. Our algorithm also enables sampling from the output distribution of a circuit in quasi-polynomial time, so long as the distribution anti-concentrates. A number of practical implications are discussed, including a fundamental limit on the efficacy of noise mitigation strategies: for constant noise rates, any quantum circuit for which error mitigation is efficient on most input states, is also classically simulable on most input states.																																	2024-11-07	PPRN:90872270		
J	Chen, Tong; Wang, Hongwei; Chen, Sihao; Yu, Wenhao; Ma, Kaixin; Zhao, Xinran; Zhang, Hongming; Yu, Dong				Zhang, Hongming/ABF-8690-2021; Wang, Hongwei/HFT-3345-2022						Dense X Retrieval: What Retrieval Granularity Should We Use?								Arxiv											2	2;2024-10-04;https://www.arxiv.org/abs/2312.06648v3| 1;2023-12-12;https://www.arxiv.org/abs/2312.06648v2	arXiv:2312.06648			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 04 2024	2024	Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.																																	2024-10-24	PPRN:86557346		
J	Aiyappa, Rachith; An, Jisun; Kwak, Haewoon; Ahn, Yong-Yeol				Ahn, Yong-Yeol/C-6334-2011						Can we trust the evaluation on ChatGPT?								Arxiv											3	3;2024-08-22;https://www.arxiv.org/abs/2303.12767v2| 2;2023-03-22;https://www.arxiv.org/abs/2303.12767v1| 1;2023-03-22;https://www.arxiv.org/abs/2303.12767v1	arXiv:2303.12767			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 22 2024	2024	ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.																																	2024-08-31	PPRN:46977188		
J	Furtak, Lukas J.; Labbe, Ivo; Zitrin, Adi; Greene, Jenny E.; Dayal, Pratika; Chemerynska, Iryna; Kokorev, Vasily; Miller, Tim B.; Goulding, Andy D.; de Graaff, Anna; Bezanson, Rachel; Brammer, Gabriel B.; Cutler, Sam E.; Leja, Joel; Pan, Richard; Price, Sedona H.; Wang, Bingjie; Weaver, John R.; Whitaker, Katherine E.; Atek, Hakim; Bogdan, Akos; Charlot, Stephane; Curtis-Lake, Emma; van Dokkum, Pieter; Endsley, Ryan; Feldmann, Robert; Fudamoto, Yoshinobu; Fujimoto, Seiji; Glazebrook, Karl; Juneau, Stephanie; Marchesini, Danilo; Maseda, Michael V.; Nelson, Erica; Oesch, Pascal A.; Plat, Adele; Setton, David J.; Stark, Daniel P.; Williams, Christina C.				Nelson, Erica/OUI-1817-2025; Yoshinobu, Fudamoto/JBR-7809-2023; Kokorev, Vasily/GPK-2541-2022; Labbe, Ivo/B-1408-2016; Brammer, Gabriel/AAB-4859-2020; Oesch, Pascal/AFN-4775-2022; Glazebrook, Karl/N-3488-2015; Leja, Joel/JPL-7942-2023; dayal, Pratika/AAD-4237-2019; Endsley, Ryan/AAJ-5103-2021; Wang, Bingjie/AAD-6128-2022						A high black hole to host mass ratio in a lensed AGN in the early Universe								Arxiv											2	2;2024-08-15;https://www.arxiv.org/abs/2308.05735v2| 1;2023-08-10;https://www.arxiv.org/abs/2308.05735v1	arXiv:2308.05735			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 15 2024	2024	Early JWST observations have uncovered a new population of red sources that might represent a previously overlooked phase of supermassive black hole growth [1–3]. One of the most intriguing examples is an extremely red, point-like object that was found to be triply-imaged by the strong lensing (SL) cluster Abell 2744 [4]. Here we present deep JWST/NIRSpec observations of this object, Abell2744-QSO1. The spectroscopy confirms that the three images are of the same object, and that it is a highly reddened ( A V ^ 3 ) broad emission line Active Galactic Nucleus (AGN) at a redshift of z spec = 7.0451 ± 0.0005 . From the width of H β ( FWHM = 2800 ± 250 km s ) we derive a black hole mass of M BH = 4 +2 − 1 × 107 M⊙ . We infer a very high ratio of black hole to galaxy mass of at least 3 %, an order of magnitude more than is seen in local galaxies [5], and possibly as high as 100 %. The lack of strong metal lines in the spectrum together with the high bolometric luminosity ( L bol = (1.1 ± 0.3) × 10 45 erg s ) indicate that we are seeing the black hole in a phase of rapid growth, accreting at 30 % of the Eddington limit. The rapid growth and high black hole to galaxy mass ratio of A2744-QSO1 suggest that it may represent the missing link between black hole seeds [6] and the first luminous quasars [7].																																	2024-08-25	PPRN:75260656		
J	Yu, Botao; Baker, Frazier N.; Chen, Ziqi; Ning, Xia; Sun, Huan				Chen, Ziqi/HPE-6145-2023; Yu, Botao/LIH-7233-2024						LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset								Arxiv											4	4;2024-08-10;https://www.arxiv.org/abs/2402.09391v4| 3;2024-04-01;https://www.arxiv.org/abs/2402.09391v3| 2;2024-02-17;https://www.arxiv.org/abs/2402.09391v2| 1;2024-02-14;https://www.arxiv.org/abs/2402.09391v1	arXiv:2402.09391			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 10 2024	2024	Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, , comprehensive, , and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs named as LlaSMol, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.																																	2024-08-22	PPRN:87688979		
J	Zhou, Boran; Yang, Hui; Zhang, Ya-Hui										Fractional quantum anomalous Hall effects in rhombohedral multilayer graphene in the moiréless limit and in Coulomb imprinted superlattice								Arxiv											4	4;2024-08-09;https://www.arxiv.org/abs/2311.04217v4| 3;2024-07-18;https://www.arxiv.org/abs/2311.04217v3| 2;2023-11-23;https://www.arxiv.org/abs/2311.04217v2| 1;2023-11-07;https://www.arxiv.org/abs/2311.04217v1	arXiv:2311.04217			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 09 2024	2024	The standard theoretical framework for fractional quantum anomalous Hall effect (FQAH) assumes an isolated flat Chern band in the single particle level. In this paper we challenges this paradigm for the FQAH recently observed in the pentalayer rhombohedral stacked graphene aligned with hexagon boron nitride (hBN). We show that the external moiré superlattice potential is simply a perturbation in a model with continuous translation symmetry. Through Hartree Fock calculation, we find that interaction opens a sizable remote band gap, resulting an isolated narrow C=1 Chern band at filling ν=1. From exact diagonalization (ED) we identify FQAH phases at various fillings. But they exist also in the calculations without any external moiré potential. We suggest that the QAH insulator at ν=1 should be viewed as an interaction driven topological Wigner crystal with QAH effect, which is then pinned by a small moiré potential. The C=1 QAH crystal is robust with a crystal period around 10nm in 4-layer, 5-layer, 6-layer and 7-layer graphene systems. Our work suggests a new direction to exploring the interplay of topology and FQAH with spontaneous crystal formation in the vanishing moiré potential limit. We also propose a new system to generate and control both honeycomb and triangular moiré superlattice potential through Coulomb interaction from another control layer, which can stabilize or suppress the QAH crystal depending on the density of the control layer.																																	2024-08-21	PPRN:86067377		
J	Yoran, Ori; Wolfson, Tomer; Bogin, Ben; Katz, Uri; Deutch, Daniel; Berant, Jonathan										Answering Questions by Meta-Reasoning over Multiple Chains of Thought								Arxiv											3	3;2024-08-02;https://www.arxiv.org/abs/2304.13007v4| 2;2023-10-17;https://www.arxiv.org/abs/2304.13007v3| 1;2023-04-25;https://www.arxiv.org/abs/2304.13007v1	arXiv:2304.13007			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 02 2024	2024	Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.																																	2024-08-11	PPRN:65330763		
J	Tramer, Florian; Kamath, Gautam; Carlini, Nicholas										Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining								Arxiv											2	2;2024-07-17;https://www.arxiv.org/abs/2212.06470v3| 1;2024-06-03;https://www.arxiv.org/abs/2212.06470v2	arXiv:2212.06470			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 17 2024	2024	The performance of differentially private machine learning can be boosted significantly by leveraging the transfer learning capabilities of non-private models pretrained on large public datasets. We critically review this approach. We primarily question whether the use of large Web-scraped datasets should be viewed as differential-privacy-preserving. We caution that publicizing these models pretrained on Web data as "private" could lead to harm and erode the public's trust in differential privacy as a meaningful definition of privacy. Beyond the privacy considerations of using public data, we further question the utility of this paradigm. We scrutinize whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains, which may be poorly represented in public Web data. Finally, we notice that pretraining has been especially impactful for the largest available models -- models sufficiently large to prohibit end users running them on their own devices. Thus, deploying such models today could be a net loss for privacy, as it would require (private) data to be outsourced to a more compute-powerful third party. We conclude by discussing potential paths forward for the field of private learning, as public pretraining becomes more popular and powerful.																																	2024-07-25	PPRN:89131985		
J	Lyu, Yuanjie; Li, Zhiyu; Niu, Simin; Xiong, Feiyu; Tang, Bo; Wang, Wenjin; Wu, Hao; Liu, Huanyong; Xu, Tong; Chen, Enhong				Lyu, Yuanjie/LEM-1532-2024; Zheng, Yefeng/ABG-7053-2020						CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models								Arxiv											3	3;2024-07-15;https://www.arxiv.org/abs/2401.17043v3| 2;2024-02-19;https://www.arxiv.org/abs/2401.17043v2| 1;2024-01-30;https://www.arxiv.org/abs/2401.17043v1	arXiv:2401.17043			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 15 2024	2024	Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the range of RAG applications into four distinct types-Create, Read, Update, and Delete (CRUD), each representing a unique use case. "Create" refers to scenarios requiring the generation of original, varied content. "Read" involves responding to intricate questions in knowledge-intensive situations. "Update" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. "Delete" pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed comprehensive datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, the context length, the knowledge base construction, and the LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios.																																	2024-07-23	PPRN:87420087		
J	Raina, Vyas; Liusie, Adian; Gales, Mark										Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment								Arxiv											2	2;2024-07-04;https://www.arxiv.org/abs/2402.14016v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.14016v1	arXiv:2402.14016			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios. 1																																	2024-07-20	PPRN:87788123		
J	Alkhamissi, Badr; Elnokrashy, Muhammad; Alkhamissi, Mai; Diab, Mona										Investigating Cultural Alignment of Large Language Models								Arxiv											2	2;2024-02-20;https://www.arxiv.org/abs/2402.13231v1| 1;2024-07-01;	arXiv:2402.13231			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 01 2024	2024	The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions—firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer. 																																	2024-11-17	PPRN:87806441		
J	Jiang, Liwei; Rao, Kavel; Han, Seungju; Ettinger, Allyson; Brahman, Faeze; Kumar, Sachin; Mireshghallah, Niloofar; Lu, Ximing; Sap, Maarten; Choi, Yejin; Dziri, Nouha				Jiang, Alicia/X-3945-2019; Lu, Ximing/LLL-7542-2024						WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models								Arxiv											1	1;2024-06-26;https://www.arxiv.org/abs/2406.18510v1	arXiv:2406.18510			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 26 2024	2024	We introduce WildTeaming, an automatic LLM safety red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes multiple tactics for systematic exploration of novel jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system. WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods. While many datasets exist for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed even when model weights are open. With WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that resemble harmful queries in form but contain no harm. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive experiments, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All components of WildJailbeak contribute to achieving balanced safety behaviors of models.																																	2024-07-15	PPRN:89911059		
J	Zhu, Tong; Qu, Xiaoye; Dong, Daize; Ruan, Jiacheng; Tong, Jingqi; He, Conghui; Cheng, Yu				Ruan, Jiacheng/KAM-3815-2024; He, Conghui/AAZ-3323-2021; Zhu, Tong/MIJ-7955-2025						LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training								Arxiv											1	1;2024-06-24;https://www.arxiv.org/abs/2406.16554v1	arXiv:2406.16554			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 24 2024	2024	Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated by this limit, we investigate building MoE models from existing dense large language models. Specifically, based on the well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert Construction, which partitions the parameters of original Feed-Forward Networks (FFNs) into multiple experts; (2) Continual Pre-training, which further trains the transformed MoE model and additional gate networks. In this paper, we comprehensively explore different methods for expert construction and various data sampling strategies for continual pre-training. After these stages, our LLaMA-MoE models could maintain language abilities and route the input tokens to specific experts with part of the parameters activated. Empirically, by training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense models that contain similar activation parameters. 																																	2024-07-12	PPRN:89406806		
J	Dong, Guanting; Lu, Keming; Li, Chengpeng; Xia, Tingyu; Yu, Bowen; Zhou, Chang; Zhou, Jingren				李, 程鹏/HLP-9229-2023; Bowen, Yu/MFH-7462-2025; dong, guanting/JGL-9364-2023; Zhou, Mingyuan/AAE-8717-2021						Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models								Arxiv											2	2;2024-07-18;https://www.arxiv.org/abs/2406.13542v3| 1;2024-06-19;https://www.arxiv.org/abs/2406.13542v1	arXiv:2406.13542			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 19 2024	2024	One core capability of large language models (LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to check the correctness of the instruction responses, and unit test samples to verify the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Our code is publicly available at https://github.com/QwenLM/AutoIF.																																	2025-08-07	PPRN:89379205		
J	Kiamari, Mehrdad; Kiamari, Mohammad; Krishnamachari, Bhaskar										GKAN: Graph Kolmogorov-Arnold Networks								Arxiv											1	1;2024-06-10;https://www.arxiv.org/abs/2406.06470v1	arXiv:2406.06470			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural network architecture that extends the principles of the recently proposed Kolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the unique characteristics of KANs, notably the use of learnable univariate functions instead of fixed linear weights, we develop a powerful model for graph-based learning tasks. Unlike traditional Graph Convolutional Networks (GCNs) that rely on a fixed convolutional architecture, GKANs implement learnable spline-based functions between layers, transforming the way information is processed across the graph structure. We present two different ways to incorporate KAN layers into GKAN: architecture 1 — where the learnable functions are applied to input features after aggregation and architecture 2 — where the learnable functions are applied to input features before aggregation. We evaluate GKAN empirically using a semi-supervised graph learning task on a real-world dataset (Cora). We find that architecture generally performs better. We find that GKANs achieve higher accuracy in semi-supervised learning tasks on graphs compared to the traditional GCN model. For example, when considering 100 features, GCN provides an accuracy of 53.5 while a GKAN with a comparable number of parameters gives an accuracy of 61.76; with 200 features, GCN provides an accuracy of 61.24 while a GKAN with a comparable number of parameters gives an accuracy of 67.66. We also present results on the impact of various parameters such as the number of hidden nodes, grid-size, and the polynomial-degree of the spline on the performance of GKAN.																																	2024-06-22	PPRN:89265798		
J	Saha, Swarnadeep; Levy, Omer; Celikyilmaz, Asli; Bansal, Mohit; Weston, Jason; Li, Xian				Bansal, Mohit/Q-9105-2016						Branch-Solve-Merge Improves Large Language Model Evaluation and Generation								Arxiv											2	2;2024-06-07;https://www.arxiv.org/abs/2310.15123v2| 1;2023-10-23;https://www.arxiv.org/abs/2310.15123v1	arXiv:2310.15123			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model’s lack of coherence and inability to plan and decompose the problem. We propose B RANCH-S OLVE-M ERGE (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, , solve, , and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.																																	2024-11-20	PPRN:85758823		
J	Dong, Yijiang River; Hu, Tiancheng; Collier, Nigel										Can LLM be a Personalized Judge?								Arxiv											1	1;2024-06-01;	arXiv:2406.11657			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.																																	2024-11-17	PPRN:89348571		
J	Chen, Ling-Hao; Lu, Shunlin; Zeng, Ailing; Zhang, Hao; Wang, Benyou; Zhang, Ruimao; Zhang, Lei				Chen, Ling-Hao/JXX-2220-2024						MotionLLM: Understanding Human Behaviors from Human Motions and Videos								Arxiv											1	1;2024-05-30;https://www.arxiv.org/abs/2405.20340v1	arXiv:2405.20340			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	This study delves into the realm of multi-modality ( i.e. , video and motion modalities) human behavior understanding by leveraging the powerful capabilities of Large Language Models (LLMs). Diverging from recent LLMs designed for videoonly or motion-only understanding, we argue that understanding human behavior necessitates joint modeling from both videos and motion sequences ( e.g. , SMPL sequences) to capture nuanced body part dynamics and semantics effectively. In light of this, we present MotionLLM, a straightforward yet effective framework for human motion understanding, captioning, and reasoning. Specifically, MotionLLM adopts a unified video-motion training strategy that leverages the complementary advantages of existing coarse video-text data and fine-grained motion-text data to glean rich spatial-temporal insights. Furthermore, we collect a substantial dataset, MoVid, comprising diverse videos, motions, captions, and instructions. Additionally, we propose the MoVid-Bench, with carefully manual annotations, for better evaluation of human behavior understanding on video and motion. Extensive experiments show the superiority of MotionLLM in the caption, spatial-temporal comprehension, and reasoning ability.																																	2024-11-10	PPRN:89113570		
J	Li, Nian; Gao, Chen; Li, Mingyu; Li, Yong; Liao, Qingmin				Gao, Chen/M-4009-2018						EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities								Arxiv											3	3;2024-05-24;https://www.arxiv.org/abs/2310.10436v4| 2;2024-05-22;https://www.arxiv.org/abs/2310.10436v3| 1;2023-10-16;https://www.arxiv.org/abs/2310.10436v1	arXiv:2310.10436			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	The advent of artificial intelligence has led to a growing emphasis on data-driven modeling in macroeconomics, with agent-based modeling (ABM) emerging as a prominent bottom-up simulation paradigm. In ABM, agents ( e.g. , households, firms) interact within a macroeconomic environment, collectively generating market dynamics. Existing agent modeling typically employs predetermined rules or learningbased neural networks for decision-making. However, customizing each agent presents significant challenges, complicating the modeling of agent heterogeneity. Additionally, the influence of multi-period market dynamics and multifaceted macroeconomic factors are often overlooked in decision-making processes. In this work, we introduce EconAgent, , a large language model-empowered agent with humanlike characteristics for macroeconomic simulation. We first construct a simulation environment that incorporates various market dynamics driven by agents’ decisions regarding work and consumption. Through the perception module, we create heterogeneous agents with distinct decision-making mechanisms. Furthermore, we model the impact of macroeconomic trends using a memory module, which allows agents to reflect on past individual experiences and market dynamics. Simulation experiments show that EconAgent can make realistic decisions, leading to more reasonable macroeconomic phenomena compared to existing rule-based or learning-based agents. Our codes are released at https://github.com/ tsinghua-fib-lab/ACL24-EconAgent. .																																	2024-06-08	PPRN:85660208		
J	Guo, Xudong; Huang, Kaixuan; Liu, Jiale; Fan, Wenhui; Velez, Natalia; Wu, Qingyun; Wang, Huazheng; Griffiths, Thomas L.; Wang, Mengdi				Wang, Huazheng/GWZ-4217-2022; Wu, Qing-Yun/G-9976-2017; liu, jiale/HTO-3772-2023; Guo, Xudong/CAG-5824-2022						Embodied LLM Agents Learn to Cooperate in Organized Teams								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2403.12482v2| 1;2024-03-19;https://www.arxiv.org/abs/2403.12482v1	arXiv:2403.12482			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency1. 1 .																																	2024-06-04	PPRN:88240676		
J	Chen, Zhiyu Zoey; Ma, Jing; Zhang, Xinlu; Hao, Nan; Yan, An; Nourbakhsh, Armineh; Yang, Xianjun; Mcauley, Julian; Petzold, Linda; Wang, William Yang				Yan, An/GLT-4045-2022; MA, JINGXUAN/L-7502-2019						A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law								Arxiv											1	1;2024-05-02;https://www.arxiv.org/abs/2405.01769v1	arXiv:2405.01769			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	In the fast-evolving domain of artificial intelligence, large language models (LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance, healthcare, and law: domains characterized by their reliance on professional expertise, challenging data acquisition, high-stakes, and stringent regulatory compliance. This survey offers a detailed exploration of the methodologies, applications, challenges, and forward-looking opportunities of LLMs within these high-stakes sectors. We highlight the instrumental role of LLMs in enhancing diagnostic and treatment methodologies in healthcare, innovating financial analytics, and refining legal interpretation and compliance strategies. Moreover, we critically examine the ethics for LLM applications in these fields, pointing out the existing ethical concerns and the need for transparent, fair, and robust AI systems that respect regulatory norms. By presenting a thorough review of current literature and practical applications, we showcase the transformative impact of LLMs, and outline the imperative for interdisciplinary cooperation, methodological advancements, and ethical vigilance. Through this lens, we aim to spark dialogue and inspire future research dedicated to maximizing the benefits of LLMs while mitigating their risks in these precision-dependent sectors. To facilitate future research on LLMs in these critical societal domains, we also initiate a reading list that tracks the latest advancements under this topic, which will be continually updated: url{https://github.com/czyssrs/LLM_X_papers}.																																	2024-05-21	PPRN:88761103		
J	Zhou, Siyuan; Du, Yilun; Chen, Jiaben; Li, Yandong; Yeung, Dit-Yan; Gan, Chuang				Li, Jiaqi/HHN-8236-2022						RoboDreamer: Learning Compositional World Models for Robot Imagination								Arxiv											1	1;2024-04-18;https://www.arxiv.org/abs/2404.12377v1	arXiv:2404.12377			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 18 2024	2024	Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation. However, one major issue in such models is generalization -- models are limited to synthesizing videos subject to language instructions similar to those seen at training time. This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments. To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation. We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos. We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components. We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image. Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation.																																	2024-04-28	PPRN:88562420		
J	Song, Bowen; Kwon, Soo Min; Zhang, Zecheng; Hu, Xinyu; Qu, Qing; Shen, Liyue				Zhang, Zecheng/LYN-9921-2024; Qu, Qing/AAA-8226-2019; Shen, Liyue/ABH-5222-2020; Hu, Xinyu/KIK-7663-2024						Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency								Arxiv											3	3;2024-04-16;https://www.arxiv.org/abs/2307.08123v3| 2;2023-10-10;https://www.arxiv.org/abs/2307.08123v2| 1;2023-07-16;https://www.arxiv.org/abs/2307.08123v1	arXiv:2307.08123			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 16 2024	2024	Diffusion models have recently emerged as powerful generative priors for solving inverse problems. However, training diffusion models in the pixel space are both data-intensive and computationally demanding, which restricts their applicability as priors for high-dimensional real-world data such as medical images. Latent diffusion models, which operate in a much lower-dimensional space, offer a solution to these challenges. However, incorporating latent diffusion models to solve inverse problems remains a challenging problem due to the nonlinearity of the encoder and decoder. To address these issues, we propose textit{ReSample}, an algorithm that can solve general inverse problems with pre-trained latent diffusion models. Our algorithm incorporates data consistency by solving an optimization problem during the reverse sampling process, a concept that we term as hard data consistency. Upon solving this optimization problem, we propose a novel resampling scheme to map the measurement-consistent sample back onto the noisy data manifold and theoretically demonstrate its benefits. Lastly, we apply our algorithm to solve a wide range of linear and nonlinear inverse problems in both natural and medical images, demonstrating that our approach outperforms existing state-of-the-art approaches, including those based on pixel-space diffusion models.																																	2024-04-26	PPRN:73950074		
J	Prasad, Archiki; Koller, Alexander; Hartmann, Mareike; Clark, Peter; Sabharwal, Ashish; Bansal, Mohit; Khot, Tushar				Bansal, Mohit/Q-9105-2016						ADaPT: As-Needed Decomposition and Planning with Language Models								Arxiv											2	2;2024-04-08;https://www.arxiv.org/abs/2311.05772v2| 1;2023-11-08;https://www.arxiv.org/abs/2311.05772v1	arXiv:2311.05772			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 08 2024	2024	Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.																																	2024-04-22	PPRN:86125133		
J	Sun, Haoran; Liu, Lixin; Li, Junjie; Wang, Fengyu; Dong, Baohua; Lin, Ran; Huang, Ruohui				Dong, Baohua/ACD-2830-2022; Li, Junjie/KFA-8634-2024; Liu, Lixin/AAL-5634-2020; Sun, Solomon/JUV-1311-2023						Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models								Arxiv											1	1;2024-04-03;https://www.arxiv.org/abs/2404.02823v1	arXiv:2404.02823			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 03 2024	2024	The ability of large language models (LLMs) to follow instructions is crucial to real -world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi -level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy -to -hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction -following abilities, especially for instructions with complex constraints. On several instruction -following benchmarks, our 7B model outperforms the state-of-the-art opensource 7B models, even exceeds the performance of models 10 times larger on certain metrics. 																																	2024-04-18	PPRN:88396556		
J	Chuang, Yun-Shiuan; Goyal, Agam; Harlalka, Nikunj; Suresh, Siddharth; Hawkins, Robert; Yang, Sijia; Shah, Dhavan; Hu, Junjie; Rogers, Timothy T.				Yang, Sijia/AHH-9486-2022; Hawkins, Robert/JNT-6100-2023; Chuang, Yun-Shiuan/JQJ-1192-2023; Hu, Junjie/HHR-9040-2022						Simulating Opinion Dynamics with Networks of LLM-based Agents								Arxiv											2	2;2024-04-01;https://www.arxiv.org/abs/2311.09618v4| 1;2023-11-16;https://www.arxiv.org/abs/2311.09618v1	arXiv:2311.09618			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.1																																	2024-04-17	PPRN:86176705		
J	Zhai, Bohan; Yang, Shijia; Xu, Chenfeng; Shen, Sheng; Keutzer, Kurt; Li, Chunyuan; Li, Manling				Li, Chunyuan/KHY-0771-2024						HallE-Control: Controlling Object Hallucination in Large Multimodal Models								Arxiv											3	3;2024-03-28;https://www.arxiv.org/abs/2310.01779v3| 2;2023-12-03;https://www.arxiv.org/abs/2310.01779v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.01779v1	arXiv:2310.01779			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 28 2024	2024	Current Large Multimodal Models (LMMs) achieve remarkable progress, yet there remains significant uncertainty regarding their ability to accurately apprehend visual details, that is, in performing detailed captioning. To address this, we introduce CCEval, a GPT-4 assisted evaluation method for detailed captioning. Interestingly, while LMMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations. In this paper, we make the first attempt to investigate such hallucination from different aspects, including image resolution, the language decoder size, and instruction data amount, quality, granularity.Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can ground or verify, thus inducing hallucination. To control such hallucinations, we further attribute the reliability of captioning to contextual knowledge (involving only contextually grounded objects) and parametric knowledge (containing inferred objects by the model). Thus, we introduce HallE-Control, a controllable LMM in terms of Hallucination in object Existence. HallE-Control can condition the captioning to shift between (i) exclusively depicting contextual knowledge for grounded objects and (ii) blending it with parametric knowledge to imagine inferred objects. Our method reduces hallucination by 44% compared to LLaVA7B and maintains the object coverage. Our code is publicly available at https://github.com/bronyayang/HallE_Control																																	2024-04-15	PPRN:85378615		
J	Lu, Chen; Pan, Zhiming; Yang, Fan; Wu, Congjun				Pan, Zhiming/MIQ-9253-2025						Interlayer Coupling Driven High-Temperature Superconductivity in La3Ni2O7 Under Pressure								Arxiv											2	2;2024-03-20;https://www.arxiv.org/abs/2307.14965v3| 1;2023-07-27;https://www.arxiv.org/abs/2307.14965v1	arXiv:2307.14965			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 20 2024	2024	The newly discovered high -temperature superconductivity in La3Ni2O7 under pressure has attracted a great deal of attentions. The essential ingredient characterizing the electronic properties is the bilayer NiO2 planes coupled by the interlayer bonding of 3dz2 orbitals through the intermediate oxygen -atoms. In the strong coupling limit, the low energy physics is described by an intralayer antiferromagnetic spin -exchange interaction J∥ between 3dx2−y2 orbitals and an interlayer one J⊥ between 3dz2 orbitals. Taking into account Hund’s rule on each site and integrating out the 3dz2 spin degree of freedom, the system reduces to a single -orbital bilayer t -J model based on the 3dx2−y2 orbital. By employing the slave -boson approach, the self -consistent equations for the bonding and pairing order parameters are solved. Near the physically relevant 1/4 -filling regime (doping δ = 0.3 ∼ 0.5), the interlayer coupling J⊥ tunes the conventional single -layer d -wave superconducting state to the s -wave one. A strong J⊥ could enhance the inter -layer superconducting order, leading to a dramatically increased Tc. Interestingly, there could exist a finite regime in which an s + id state emerges.																																	2024-04-12	PPRN:74129191		
J	Yao, Yifan; Duan, Jinhao; Xu, Kaidi; Cai, Yuanfang; Sun, Zhibo; Zhang, Yue				sun, eric/Q-3171-2019						A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly								Arxiv											3	3;2024-03-20;https://www.arxiv.org/abs/2312.02003v3| 2;2024-01-26;https://www.arxiv.org/abs/2312.02003v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02003v1	arXiv:2312.02003			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Mar 20 2024	2024	Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.																																	2024-04-13	PPRN:86378842		
J	Clymer, Joshua; Gabrieli, Nick; Krueger, David; Larsen, Thomas										Safety Cases: How to Justify the Safety of Advanced AI Systems								Arxiv											2	2;2024-03-18;https://www.arxiv.org/abs/2403.10462v2| 1;2024-03-15;https://www.arxiv.org/abs/2403.10462v1	arXiv:2403.10462			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and - if AI systems become much more powerful - deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.																																	2024-05-03	PPRN:88164209		
J	Liu, Jiuming; Yu, Ruiji; Wang, Yian; Zheng, Yu; Deng, Tianchen; Ye, Weicai; Wang, Hesheng				Wang, Hesheng/P-3192-2015; Deng, Tianchen/KBB-4126-2024; Zheng, Yu/LTD-1583-2024; Liu, Jiuming/LKL-2382-2024						Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.06467v2	arXiv:2403.06467			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Recently, state space model (SSM) has gained great attention due to its promising performance, linear complexity, and long sequence modeling ability in both language and image domains. However, it is non-trivial to extend SSM to the point cloud field, because of the causality requirement of SSM and the disorder and irregularity nature of point clouds. In this paper, we propose a novel SSM-based point cloud processing backbone, named Point Mamba, with a causality-aware ordering mechanism. To construct the causal dependency relationship, we design an octree-based ordering strategy on raw irregular points, globally sorting points in a z -order sequence and also retaining their spatial proximity. Our method achieves state-of-the-art performance compared with transformer-based counterparts, with 93.4% accuracy and 75.7 mIOU respectively on the ModelNet40 classification dataset and ScanNet semantic segmentation dataset. Furthermore, our Point Mamba has linear complexity, which is more efficient than transformer-based methods. Our method demonstrates the great potential that SSM can serve as a generic backbone in point cloud understanding. Codes are released at https://github.com/IRMVLab/Point-Mamba.																																	2024-04-11	PPRN:88190151		
J	Agrawal, Garima; Kumarage, Tharindu; Alghamdi, Zeyad; Liu, Huan				Agrawal, Garima/JFJ-6864-2023; Kumarage, Tharindu/MBH-7074-2025						Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey								Arxiv											1	1;2024-03-16;https://www.arxiv.org/abs/2311.07914v2	arXiv:2311.07914			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Mar 16 2024	2024	The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.																																	2024-04-11	PPRN:86743412		
J	Liu, Xian; Zhan, Xiaohang; Tang, Jiaxiang; Shan, Ying; Zeng, Gang; Lin, Dahua; Liu, Xihui; Liu, Ziwei				Zhan, Xiaohang/KMY-6662-2024; Liu, Ziwei/AAG-6939-2021; Liu, Xihui/LHA-5141-2024; Lin, Dahua/W-6576-2019						HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2311.17061v2| 1;2023-11-28;https://www.arxiv.org/abs/2311.17061v1	arXiv:2311.17061			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.																																	2024-04-11	PPRN:86309909		
J	Huang, Haoxu; Lin, Fanqi; Hu, Yingdong; Wang, Shengjie; Gao, Yang										CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models								Arxiv											1	1;2024-03-13;https://www.arxiv.org/abs/2403.08248v1	arXiv:2403.08248			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 13 2024	2024	Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training.																																	2024-04-08	PPRN:88127112		
J	Li, Xiaopeng; Li, Shasha; Song, Shezheng; Yang, Jing; Ma, Jun; Yu, Jie										PMET: Precise Model Editing in a Transformer								Arxiv											6	6;2024-03-11;https://www.arxiv.org/abs/2308.08742v6| 5;2024-02-20;https://www.arxiv.org/abs/2308.08742v5| 4;2023-12-20;https://www.arxiv.org/abs/2308.08742v4| 3;2023-12-13;https://www.arxiv.org/abs/2308.08742v3| 2;2023-08-22;https://www.arxiv.org/abs/2308.08742v2| 1;2023-08-17;https://www.arxiv.org/abs/2308.08742v1	arXiv:2308.08742			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 11 2024	2024	Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we introduce PMET, which simultaneously optimizes Transformer Component (TC, namely MHSA and FFN) hidden states, while only using the optimized TC hidden states of FFN to precisely update FFN weights. Our experiments demonstrate that PMET exhibits state-of-the-art performance on both the COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the effectiveness of our enhancements, further reinforcing the finding that the MHSA encodes certain general knowledge extraction patterns and indicating its storage of a small amount of factual knowledge. Our code is available at https://github.com/xpq-tech/PMET.																																	2024-04-08	PPRN:82319889		
J	Wang, Yuxia; Mansurov, Jonibek; Ivanov, Petar; Su, Jinyan; Shelmanov, Artem; Tsvigun, Akim; Whitehouse, Chenxi; Afzal, Osama Mohammed; Mahmoud, Tarek; Sasaki, Toru; Arnold, Thomas; Aji, Alham Fikri; Habash, Nizar; Gurevych, Iryna; Nakov, Preslav				Shelmanov, Artem/JOK-8394-2023; Tsvigun, Akim/HTT-2881-2023						M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection								Arxiv											2	2;2024-03-10;https://www.arxiv.org/abs/2305.14902v2| 1;2023-05-24;https://www.arxiv.org/abs/2305.14902v1	arXiv:2305.14902			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 10 2024	2024	Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark textbf{M4}, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem.																																	2024-04-08	PPRN:72712971		
J	Zhong, Yaofeng Desmond; Dey, Biswadip; Chakraborty, Amit				Zhong, Yaofeng Desmond/GPF-9280-2022						Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control								Arxiv											2	2;2024-03-01;https://www.arxiv.org/abs/1909.12077v5| 1;2020-04-30;https://www.arxiv.org/abs/1909.12077v4	arXiv:1909.12077			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 01 2024	2024	In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.																																	2024-03-28	PPRN:14796147		
J	Ji, Jiabao; Hou, Bairu; Robey, Alexander; Pappas, George J.; Hassani, Hamed; Zhang, Yang; Wong, Eric; Chang, Shiyu				ji, jiabao/NSU-9039-2025						Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing								Arxiv											2	2;2024-02-28;https://www.arxiv.org/abs/2402.16192v2| 1;2024-02-25;https://www.arxiv.org/abs/2402.16192v1	arXiv:2402.16192			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 28 2024	2024	Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.																																	2024-03-28	PPRN:87889556		
J	Zeng, Shenglai; Zhang, Jiankun; He, Pengfei; Xing, Yue; Liu, Yiding; Xu, Han; Ren, Jie; Wang, Shuaiqiang; Yin, Dawei; Chang, Yi; Tang, Jiliang				Zeng, Shenglai/HTR-1153-2023; Yin, Dawei/JOR-9201-2023						The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/2402.16893v1| 1;2024-02-23;https://www.arxiv.org/abs/2402.16893v1	arXiv:2402.16893			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Retrieval -augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under -explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs’ training data. Overall, we provide new insights in this paper for privacy protection of retrievalaugmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAGprivacy.																																	2024-03-28	PPRN:87921226		
J	Jeong, Jaeseok; Kim, Junho; Choi, Yunjey; Lee, Gayoung; Uh, Youngjung										Visual Style Prompting with Swapping Self-Attention								Arxiv											1	1;2024-02-21;https://www.arxiv.org/abs/2402.12974v2	arXiv:2402.12974			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 21 2024	2024	In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately.																																	2024-11-09	PPRN:87795704		
J	Aldaco, Jorge; Armstrong, Travis; Baruch, Robert; Bingham, Jeff; Chan, Sanky; Draper, Kenneth; Dwibedi, Debidatta; Finn, Chelsea; Florence, Pete; Goodrich, Spencer; Gramlich, Wayne; Hage, Torr; Herzog, Alexander; Hoech, Jonathan; Nguyen, Thinh; Storz, Ian; Tabanpour, Baruch; Takayama, Leila; Tompson, Jonathan; Wahid, Ayzaan; Wahrburg, Ted; Xu, Sichun; Yaroshenko, Sergey; Zakka, Kevin; Zhao, Tony Z.		Aloha 2 Team		Yaroshenko, Sergey/HTN-2625-2023						ALOHA 2: An Enhanced Low-Cost Hardware for Bimanual Teleoperation								Arxiv											1	1;2024-02-07;https://www.arxiv.org/abs/2405.02292v1	arXiv:2405.02292			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 07 2024	2024	Diverse demonstration datasets have powered significant advances in robot learning, but the dexterity and scale of such data can be limited by the hardware cost, the hardware robustness, and the ease of teleoperation. We introduce ALOHA 2 , an enhanced version of ALOHA that has greater performance, ergonomics, and robustness compared to the original design. To accelerate research in large-scale bimanual manipulation, we open source all hardware designs of ALOHA 2 with a detailed tutorial, together with a MuJoCo model of ALOHA 2 with system identification.																																	2024-05-25	PPRN:88791336		
J	Chee, Jerry; Cai, Yaohui; Kuleshov, Volodymyr; De Sa, Christopher				Cai, Yaohui/HGB-4859-2022						QuIP: 2-Bit Quantization of Large Language Models With Guarantees								Arxiv											2	2;2024-01-15;https://www.arxiv.org/abs/2307.13304v2| 1;2023-07-25;https://www.arxiv.org/abs/2307.13304v1	arXiv:2307.13304			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 15 2024	2024	This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from $textit{incoherent}$ weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. 																																	2024-05-25	PPRN:74114787		
J	Tu, Quan; Fan, Shilong; Tian, Zihang; Yan, Rui										CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation								Arxiv											2	2;2024-01-09;https://www.arxiv.org/abs/2401.01275v2| 1;2024-01-02;https://www.arxiv.org/abs/2401.01275v1	arXiv:2401.01275			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 09 2024	2024	Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in CharacterEval, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at https://github.com/ morecry/CharacterEval.																																	2024-05-25	PPRN:86914142		
J	Piet, Julien; Alrashed, Maha; Sitawarin, Chawin; Chen, Sizhe; Wei, Zeming; Sun, Elizabeth; Alomair, Basel; Wagner, David				Sitawarin, Chawin/KIB-4488-2024; Wei, Zeming/JDW-5614-2023; Chen, Sizhe/ACB-4737-2022						Jatmo: Prompt Injection Defense by Task-Specific Finetuning								Arxiv											2	2;2024-01-08;https://www.arxiv.org/abs/2312.17673v2| 1;2023-12-29;https://www.arxiv.org/abs/2312.17673v1	arXiv:2312.17673			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 08 2024	2024	Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks. However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones. In this work, we introduce Jatmo, a method for generating task-specific models resilient to prompt-injection attacks. Jatmo leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. It harnesses a teacher instruction-tuned model to generate a task-specific dataset, which is then used to fine-tune a base model (i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs. For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases none at all, to produce a fully synthetic dataset. Our experiments on seven tasks show that Jatmo models provide similar quality of outputs on their specific task as standard LLMs, while being resilient to prompt injections. The best attacks succeeded in less than 0.5% of cases against our models, versus 87% success rate against GPT-3.5-Turbo. We release Jatmo at https://github.com/wagner-group/prompt-injection-defense.																																	2024-05-25	PPRN:86870740		
J	Wang, Heng; Feng, Shangbin; He, Tianxing; Tan, Zhaoxuan; Han, Xiaochuang; Tsvetkov, Yulia				He, Tianxing/MDT-2168-2025; Wang, Heng/JXN-7037-2024						Can Language Models Solve Graph Problems in Natural Language?								Arxiv											3	3;2024-01-06;https://www.arxiv.org/abs/2305.10037v3| 2;2023-10-30;https://www.arxiv.org/abs/2305.10037v2| 1;2023-05-17;https://www.arxiv.org/abs/2305.10037v1	arXiv:2305.10037			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 06 2024	2024	Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph.																																	2024-01-23	PPRN:70025586		
J	Weyssow, Martin; Zhou, Xin; Kim, Kisub; Lo, David; Sahraoui, Houari				Zhou, Xin/KJL-7156-2024; LO, David/A-2493-2012; Kim, Kisub/GMX-3152-2022						Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models								Arxiv											3	3;2024-12-27;https://www.arxiv.org/abs/2308.10462v3| 2;2024-01-18;https://www.arxiv.org/abs/2308.10462v2| 1;2023-08-21;https://www.arxiv.org/abs/2308.10462v1	arXiv:2308.10462			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 27 2024	2024	Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.																																	2025-02-15	PPRN:81812799		
J	Xia, Chunqiu Steven; Wei, Yuxiang; Zhang, Lingming				Wei, Yuxiang/KYQ-5192-2024						Practical Program Repair in the Era of Large Pre-trained Language Models								Arxiv											1	1;2024-12-09;https://www.arxiv.org/abs/2210.14179v2	arXiv:2210.14179			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 09 2024	2024	Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (PLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged PLMs for APR without relying on any bug- fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art PLMs or was not evaluated on realistic datasets. Thus, the true power of modern PLMs on the important APR problem is yet to be revealed. In this work, we perform the first extensive study on directly applying PLMs for APR. We select 9 recent state-of-the-art PLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use PLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the PLMs under these repair settings on 5 datasets across 3 different languages and compare different PLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the PLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-ofthe-art PLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied PLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the PLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that PLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.																																	2025-01-24	PPRN:119815278		
J	Weber, Mark; Yu, Lijun; Yu, Qihang; Deng, Xueqing; Shen, Xiaohui; Cremers, Daniel; Chen, Liang-Chieh				Yu, Qihang/NXC-6226-2025; Yu, Lijun/AAJ-6691-2020						MaskBit: Embedding-free Image Generation via Bit Tokens								Arxiv											2	2;2024-12-08;https://www.arxiv.org/abs/2409.16211v2| 1;2024-09-24;https://www.arxiv.org/abs/2409.16211v1	arXiv:2409.16211			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 08 2024	2024	Masked transformer models for class-conditional image generation have become a compelling alternative to diffusion models. Typically comprising two stages - an initial VQGAN model for transitioning between latent space and image space, and a subsequent Transformer model for image generation within latent space - these frameworks offer promising avenues for image synthesis. In this study, we present two primary contributions: Firstly, an empirical and systematic examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel embedding-free generation network operating directly on bit tokens - a binary quantized representation of tokens with rich semantics. The first contribution furnishes a transparent, reproducible, and high-performing VQGAN model, enhancing accessibility and matching the performance of current state-of-the-art methods while revealing previously undisclosed details. The second contribution demonstrates that embedding-free image generation using bit tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256 benchmark, with a compact generator model of mere 305M parameters. 																																	2025-01-17	PPRN:98863228		
J	Konoplya, R.A.; Stashko, O.S.				Stashko, Oleksandr/X-3937-2018; Konoplya, Roman/A-1519-2008						Probing the Effective Quantum Gravity via Quasinormal Modes and Shadows of Black Holes								Arxiv											3	3;2024-11-27;https://www.arxiv.org/abs/2408.02578v3| 2;2024-08-25;https://www.arxiv.org/abs/2408.02578v2| 1;2024-08-05;https://www.arxiv.org/abs/2408.02578v1	arXiv:2408.02578			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 27 2024	2024	Two quantum-corrected black hole models have recently been proposed within the Hamiltonian constraints approach to quantum gravity, maintaining general covariance [1]. We have studied the quasinormal spectra of these black holes using four methods: the higher-order WKB approach with Fadé approximants, time-domain integration, Frobenius, and pseudospectral methods. The Frobenius method, in particular, allows us to determine precise values of the frequencies, including the overtones. The two models differ in their choice of quantum parameter ξ, and we can distinguish them by their quasinormal spectra. In the first model, increasing the quantum parameter results in higher real oscillation frequencies and damping rates of the fundamental mode. In contrast, the second model shows a decrease in the oscillation frequency of the least-damped mode when the quantum parameter is introduced. We have shown that, while the fundamental mode changes relatively gradually with the quantum parameter, the first few overtones deviate from their Schwarzschild limits at an increasing rate. This results in a qualitatively new behavior: the real parts of the frequencies of the first and higher overtones tend to zero as the quantum parameter increases. In addition to the branch of modes that are perturbative in the quantum parameter, we observe some non-perturbative modes at moderate values of the quantum parameter. Additionally, we have calculated the radii of the shadows cast by these black holes and discussed possible constraints based on observations of Sgt A*. As a byproduct, we tested the method of calculating quasinormal modes of this kind based on a recent parametrization of effective potentials, and showed that while the parametrized formalism could be used for estimating the fundamental mode at small values of the coupling, its accuracy is highly dependent on the particular spacetime under consideration and is insufficient even for the lowest overtones.																																	2025-01-08	PPRN:91245154		
J	Wang, Yuxuan; Wu, Haixu; Dong, Jiaxiang; Qin, Guo; Zhang, Haoran; Liu, Yong; Qiu, Yunzhong; Wang, Jianmin; Long, Mingsheng				Qiu, Yunzhong/GRR-8013-2022						TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables								Arxiv											4	4;2024-11-11;https://www.arxiv.org/abs/2402.19072v4| 3;2024-11-02;https://www.arxiv.org/abs/2402.19072v3| 2;2024-10-24;https://www.arxiv.org/abs/2402.19072v2| 1;2024-02-29;https://www.arxiv.org/abs/2402.19072v1	arXiv:2402.19072			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 11 2024	2024	Deep models have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous variables can provide valuable external information for endogenous variables. Thus, unlike well-established multivariate or univariate forecasting paradigms that either treat all the variables equally or ignore exogenous information, this paper focuses on a more practical setting: time series forecasting with exogenous variables. We propose a novel approach, TimeXer, to ingest external information to enhance the forecasting of endogenous variables. With deftly designed embedding layers, TimeXer empowers the canonical Transformer with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are used simultaneously. Moreover, global endogenous tokens are learned to effectively bridge the causal information underlying exogenous series into endogenous temporal patches. Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability. Code is available at this repository: https://github.com/thuml/TimeXer.																																	2024-12-19	PPRN:87990801		
J	Li, Baiqi; Lin, Zhiqiu; Pathak, Deepak; Li, Jiayao; Fei, Yixin; Wu, Kewen; Ling, Tiffany; Xia, Xide; Zhang, Pengchuan; Neubig, Graham; Ramanan, Deva				Li, Baiqi/HKN-0957-2023; Li, Jiayao/KJL-0230-2024						GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual Generation								Arxiv											3	3;2024-11-03;https://www.arxiv.org/abs/2406.13743v3| 2;2024-06-21;https://www.arxiv.org/abs/2406.13743v2| 1;2024-06-19;https://www.arxiv.org/abs/2406.13743v1	arXiv:2406.13743			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 03 2024	2024	While text-to-visual models now produce photo-realistic images and videos, they struggle with compositional text prompts involving attributes, relationships, and higher-order reasoning such as logic and comparison. In this work, we conduct an extensive human study on GenAI-Bench to evaluate the performance of leading image and video generation models in various aspects of compositional text-to-visual generation. We also compare automated evaluation metrics against our collected human ratings and find that VQAScore – a metric measuring the likelihood that a VQA model views an image as accurately depicting the prompt – significantly outperforms previous metrics such as CLIPScore. In addition, VQAScore can improve generation in a black-box manner (without finetuning) via simply ranking a few (3 to 9) candidate images. Ranking by VQAScore is 2x to 3x more effective than other scoring methods like PickScore, HPSv2, and ImageReward at improving human alignment ratings for DALL-E 3 and Stable Diffusion, especially on compositional prompts that require advanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with over 40,000 human ratings to evaluate scoring metrics on ranking images generated from the same prompt. Lastly, we discuss promising areas for improvement in VQAScore, such as addressing fine-grained visual details. We will release all human ratings (over 80,000) to facilitate scientific benchmarking of both generative models and automated metrics.																																	2024-12-16	PPRN:89378369		
J	Li, Muyang; Lin, Yujun; Zhang, Zhekai; Cai, Tianle; Li, Xiuyu; Guo, Junxian; Xie, Enze; Meng, Chenlin; Zhu, Jun-Yan; Han, Song				Lin, Yujun/AAR-9588-2020; Meng, Chenlin/HKF-5727-2023; li, muyang/NVM-4065-2025; Zhu, Jun-Yan/V-7271-2018						SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models								Arxiv											2	2;2024-11-07;https://www.arxiv.org/abs/2411.05007v1| 1;2024-11-01;	arXiv:2411.05007			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, anew 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, naïvely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-Σ, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5×, achieving 3.0× speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs.																																	2025-01-08	PPRN:119074114		
J	Shen, Yongliang; Song, Kaitao; Tan, Xu; Zhang, Wenqi; Ren, Kan; Yuan, Siyu; Lu, Weiming; Li, Dongsheng; Zhuang, Yueting				Shen, Yongliang/GWC-1883-2022; Song, Kaitao/JKJ-5832-2023						TaskBench: Benchmarking Large Language Models for Task Automation								Arxiv											4	4;2024-11-01;https://www.arxiv.org/abs/2311.18760v4| 3;2024-10-31;https://www.arxiv.org/abs/2311.18760v3| 2;2023-12-09;https://www.arxiv.org/abs/2311.18760v2| 1;2023-11-30;https://www.arxiv.org/abs/2311.18760v1	arXiv:2311.18760			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TaskBench, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TaskEval, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TaskBench effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TaskBench offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents.																																	2024-12-09	PPRN:86341744		
J	Zhang, Bowen; Cheng, Yiji; Yang, Jiaolong; Wang, Chunyu; Zhao, Feng; Tang, Yansong; Chen, Dong; Guo, Baining				Zhao, Feng/NGQ-9015-2025						GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling								Arxiv											4	4;2024-10-31;https://www.arxiv.org/abs/2403.19655v4| 3;2024-05-23;https://www.arxiv.org/abs/2403.19655v3| 2;2024-04-05;https://www.arxiv.org/abs/2403.19655v2| 1;2024-03-28;https://www.arxiv.org/abs/2403.19655v1	arXiv:2403.19655			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling. Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods. We derive GaussianCube by first using a novel densification-constrained Gaussian fitting algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians, and then rearranging these Gaussians into a predefined voxel grid via Optimal Transport. Since GaussianCube is a structured grid representation, it allows us to use standard 3D U-Net as our backbone in diffusion modeling without elaborate designs. More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude. The compactness of GaussianCube greatly eases the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling.																																	2024-12-06	PPRN:88334151		
J	Zong, Zhuofan; Ma, Bingqi; Shen, Dazhong; Song, Guanglu; Shao, Hao; Jiang, Dongzhi; Li, Hongsheng; Liu, Yu				Ma, Bingqi/AAA-4578-2022; Li, Hongsheng/AES-5328-2022						MoVA: Adapting Mixture of Vision Experts to Multimodal Context								Arxiv											2	2;2024-10-31;https://www.arxiv.org/abs/2404.13046v2| 1;2024-04-19;https://www.arxiv.org/abs/2404.13046v1	arXiv:2404.13046			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content. Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, e.g., the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content. To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts. This benefits from the powerful model function understanding ability of the large language model (LLM). In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts. This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability. We conduct extensive experiments to evaluate the effectiveness of the proposed approach. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of challenging multimodal benchmarks.																																	2024-12-06	PPRN:88589346		
J	He, Zhengfu; Shu, Wentao; Ge, Xuyang; Chen, Lingjie; Wang, Junxuan; Zhou, Yunhua; Liu, Frances; Guo, Qipeng; Huang, Xuanjing; Wu, Zuxuan; Jiang, Yu-Gang; Qiu, Xipeng				ZHOU, YUN/ISA-9160-2023; Shu, Wentao/MGU-9855-2025; Ge, xuyang/HOH-5791-2023						LLAMA SCOPE : EXTRACTING MILLIONS OF FEATURES FROM LLAMA-3.1-8B WITH SPARSE AUTOEN-CODERS								Arxiv											1	1;2024-10-27;https://www.arxiv.org/abs/2410.20526v1	arXiv:2410.20526			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 27 2024	2024	Sparse Autoencoders (SAEs) have emerged as a powerful unsupervised method for extracting sparse representations from language models, yet scalable training remains a significant challenge. We introduce a suite of 256 SAEs, trained on each layer and sublayer of the Llama-3.1-8B-Base model, with 32K and 128K features. Modifications to a state-of-the-art SAE variant, Top-K SAEs, are evaluated across multiple dimensions. In particular, we assess the generalizability of SAEs trained on base models to longer contexts and fine-tuned models. Additionally, we analyze the geometry of learned SAE latents, confirming that emph{feature splitting} enables the discovery of new features. The Llama Scope SAE checkpoints are publicly available at https://huggingface.co/fnlp/Llama-Scope, alongside our scalable training, interpretation, and visualization tools at  https://github.com/OpenMOSS/Language-Model-SAEs. These contributions aim to advance the open-source Sparse Autoencoder ecosystem and support mechanistic interpretability research by reducing the need for redundant SAE training.																																	2024-12-06	PPRN:118942524		
J	Wang, Evan; Cassano, Federico; Wu, Catherine; Bai, Yunfeng; Song, Will; Nath, Vaskar; Han, Ziwen; Hendryx, Sean; Yue, Summer; Zhang, Hugh				Bai, yunfeng/JQJ-2582-2023						Planning In Natural Language Improves LLM Search For Code Generation								Arxiv											1	1;2024-10-18;https://www.arxiv.org/abs/2409.03733v2	arXiv:2409.03733			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 18 2024	2024	While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PlanSearch, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PlanSearch generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PlanSearch explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PlanSearch on top of Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on LiveCodeBench, outperforming both the best score achieved without search (pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains due to search as a direct function of the diversity over generated ideas. 																																	2024-11-20	PPRN:118747950		
J	Hagele, Alexander; Bakouch, Elie; Kosson, Atli; Ben Allal, Loubna; Von Werra, Leandro; Jaggi, Martin										Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations								Arxiv											2	2;2024-10-17;https://www.arxiv.org/abs/2405.18392v3| 1;2024-05-29;https://www.arxiv.org/abs/2405.18392v2	arXiv:2405.18392			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 17 2024	2024	Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative - constant learning rate and cooldowns -- and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs. 																																	2024-11-12	PPRN:89104196		
J	Ren, Qibing; Li, Hao; Liu, Dongrui; Xie, Zhanxu; Lu, Xiaoya; Qiao, Yu; Sha, Lei; Yan, Junchi; Ma, Lizhuang; Shao, Jing				Liu, DongRui/NLN-8967-2025						Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues								Arxiv											1	1;2024-10-14;https://www.arxiv.org/abs/2410.10700v1	arXiv:2410.10700			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 14 2024	2024	This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired by actor-network theory, which models a network of semantically linked actors as attack clues to generate diverse and effective attack paths toward harmful targets. ActorAttack addresses two main challenges in multi-turn attacks: (1) concealing harmful intents by creating an innocuous conversation topic about the actor, and (2) uncovering diverse attack paths towards the same harmful target by leveraging LLMs' knowledge to specify the correlated actors as various attack clues. In this way, ActorAttack outperforms existing single-turn and multi-turn attack methods across advanced aligned LLMs, even for GPT-o1. We will publish a dataset called SafeMTData, which includes multi-turn adversarial prompts and safety alignment data, generated by ActorAttack. We demonstrate that models safety-tuned using our safety dataset are more robust to multi-turn attacks. 																																	2024-11-05	PPRN:112581201		
J	Lai, Hanyu; Liu, Xiao; Iong, Iat Long; Yao, Shuntian; Chen, Yuxuan; Shen, Pengbo; Yu, Hao; Zhang, Hanchen; Zhang, Xiaohan; Dong, Yuxiao; Tang, Jie				Zhang, Hanchen/GYJ-7012-2022; 航航, 张/KBC-0720-2024						AutoWebGLM: A Large Language Model-based Web Navigating Agent								Arxiv											2	2;2024-10-12;https://www.arxiv.org/abs/2404.03648v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.03648v1	arXiv:2404.03648			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 12 2024	2024	Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark -- AutoWebBench -- for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at url{https://github.com/THUDM/AutoWebGLM}.																																	2024-11-06	PPRN:88412988		
J	Ankner, Zachary; Parthasarathy, Rishab; Nrusimha, Aniruddha; Rinard, Christopher; Ragan-Kelley, Jonathan; Brandon, William				Parthasarathy, Rishab/ABB-5494-2021						Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding								Arxiv											2	2;2024-10-07;https://www.arxiv.org/abs/2402.05109v2| 1;2024-02-07;https://www.arxiv.org/abs/2402.05109v1	arXiv:2402.05109			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 07 2024	2024	To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of lightweight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads: a sequentially-dependent drop-in replacement for standard draft heads that significantly improves the accuracy of draft head speculation. We further explore the design space of Hydra head training objectives and architectures, and propose a carefully tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by up to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding respectively. Overall, Hydra heads are a simple and well-motivated intervention on standard draft heads that significantly improve the end-to-end speed of draft head-based speculative decoding. 																																	2024-10-28	PPRN:87561735		
J	Esteban, Ivan; Gonzalez-Garcia, M.C.; Maltoni, Michele; Martinez-Soler, Ivan; Pinheiro, Joao Paulo; Schwetz, Thomas				González-Garcia, Maria/N-7359-2018						NuFit-6.0: Updated global analysis of three-flavor neutrino oscillations								Arxiv											3	3;2024-10-07;https://www.arxiv.org/abs/2410.05380v1| 2;2024-10-07;https://www.arxiv.org/abs/2410.05380v1| 1;2024-10-07;https://www.arxiv.org/abs/2410.05380v1	arXiv:2410.05380			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 07 2024	2024	We present an updated global analysis of neutrino oscillation data as of September 2024. The parameters θ12,  θ13,  Δm212, and |Δm3ℓ2| (ℓ = 1, 2) are well-determined with relative precision at 3σ of about 13%, 8%, 15%, and 6%, respectively. The third mixing angle θ23 still suffers from the octant ambiguity, with no clear indication of whether it is larger or smaller than 45º. The determination of the leptonic CP phase δCP depends on the neutrino mass ordering: for normal ordering the global fit is consistent with CP conservation within 1σ, whereas for inverted ordering CP-violating values of δCP around 270º are favored against CP conservation at more than 3.6σ. While the present data has in principle 2.5--3σ sensitivity to the neutrino mass ordering, there are different tendencies in the global data that reduce the discrimination power: T2K and NOvA appearance data individually favor normal ordering, but they are more consistent with each other for inverted ordering. Conversely, the joint determination of |Δm3ℓ2| from global disappearance data prefers normal ordering. Altogether, the global fit including long-baseline, reactor and IceCube atmospheric data results into an almost equally good fit for both orderings. Only when the χ2 table for atmospheric neutrino data from Super-Kamiokande is added to our χ2, the global fit prefers normal ordering with Δχ2 = 6.1. We provide also updated ranges and correlations for the effective parameters sensitive to the absolute neutrino mass from β-decay, neutrinoless double-beta decay, and cosmology.																																	2025-02-22	PPRN:105765607		
J	Gupta, Shailja; Ranjan, Rajesh; Singh, Surya Narayan				Gupta, Shailja/NRX-9489-2025						A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions								Arxiv											1	1;2024-10-03;https://www.arxiv.org/abs/2410.12837v1	arXiv:2410.12837			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 03 2024	2024	This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing. [GRAPHICS].																																	2024-11-12	PPRN:115400761		
J	Tao, Stone; Xiang, Fanbo; Shukla, Arth; Qin, Yuzhe; Hinrichsen, Xander; Yuan, Xiaodi; Bao, Chen; Lin, Xinsong; Liu, Yulin; Chan, Tse-kai; Gao, Yuan; Li, Xuanlin; Mu, Tongzhou; Xiao, Nan; Gurha, Arnav; Huang, Zhiao; Calandra, Roberto; Chen, Rui; Luo, Shan; Su, Hao				Mu, Tong/OOK-2179-2025; Calandra, Roberto/X-1789-2019; Qin, Yuzhe/IXN-5900-2023; Luo, Shan/JEP-7010-2023; Xiao, Nan/OEN-5695-2025; Su, Hao/HHZ-1048-2022; Lin, Xinsong/G-4535-2015; XUANLIN, LI/AAO-6594-2021; Bao, Chen/LQK-7168-2024						ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI								Arxiv											1	1;2024-10-01;https://www.arxiv.org/abs/2410.00425v1	arXiv:2410.00425			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 01 2024	2024	Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage than other platforms, achieving up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation for tasks such as drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.																																	2024-10-12	PPRN:100750819		
J	Zhang, Haotian; Gao, Mingfei; Gan, Zhe; Dufter, Philipp; Wenzel, Nina; Huang, Forrest; Shah, Dhruti; Du, Xianzhi; Zhang, Bowen; Li, Yanghao; Dodge, Sam; You, Keen; Yang, Zhen; Timofeev, Aleksei; Xu, Mingze; Chen, Hong-You; Fauconnier, Jean-Philippe; Lai, Zhengfeng; You, Haoxuan; Wang, Zirui; Dehghan, Afshin; Grasch, Peter; Yang, Yinfei				Dodge, Somayeh/D-3293-2013; Zhang, Haotian/CAH-0725-2022; 王, 梓睿/IUN-3605-2023; Zhang, Bowen/CAG-9533-2022						MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning								Arxiv											1	1;2024-09-30;https://www.arxiv.org/abs/2409.20566v1	arXiv:2409.20566			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 30 2024	2024	We present MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised finetuning. Our models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, we introduce two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, we provide detailed insights into the training processes and decisions that inform our final designs, offering valuable guidance for future research in MLLM development.																																	2024-10-10	PPRN:100732999		
J	Liang, Xun; Song, Shichao; Zheng, Zifan; Wang, Hanyu; Yu, Qingchen; Li, Xunkai; Li, Rong-Hua; Wang, Yi; Wang, Zhonghao; Xiong, Feiyu; Li, Zhiyu				Li, Xunkai/KFQ-2783-2024; Song, Shichao/GZA-6212-2022; Wang, Zhonghao/JVO-1763-2024; Zheng, Zifan/OCL-2681-2025						Internal Consistency and Self-Feedback in Large Language Models: A Survey								Arxiv											3	3;2024-09-18;https://www.arxiv.org/abs/2407.14507v3| 2;2024-08-29;https://www.arxiv.org/abs/2407.14507v2| 1;2024-07-19;https://www.arxiv.org/abs/2407.14507v1	arXiv:2407.14507			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 18 2024	2024	Large language models (LLMs) often exhibit deficient reasoning or generate hallucinations. To address these, studies prefixed with "Self-" such as Self-Consistency, Self-Improve, and Self-Refine have been initiated. They share a commonality: involving LLMs evaluating and updating themselves. Nonetheless, these efforts lack a unified perspective on summarization, as existing surveys predominantly focus on categorization. In this paper, we use a unified perspective of internal consistency, offering explanations for reasoning deficiencies and hallucinations. Internal consistency refers to the consistency in expressions among LLMs' latent, decoding, or response layers based on sampling methodologies. Then, we introduce an effective theoretical framework capable of mining internal consistency, named Self-Feedback. This framework consists of two modules: Self-Evaluation and Self-Update. The former captures internal consistency signals, while the latter leverages the signals to enhance either the model's response or the model itself. This framework has been employed in numerous studies. We systematically classify these studies by tasks and lines of work; summarize relevant evaluation methods and benchmarks; and delve into the concern, "Does Self-Feedback Really Work?" We also propose several critical viewpoints, including the "Hourglass Evolution of Internal Consistency", "Consistency Is (Almost) Correctness" hypothesis, and "The Paradox of Latent and Explicit Reasoning". 																																	2024-09-30	PPRN:91010932		
J	Guo, Hao-Han; Liu, Kun; Shen, Fei-Yu; Wu, Yi-Chen; Xie, Feng-Long; Xie, Kun; Xu, Kai-Tuo		FireRed Team		Wu, Yichen/LDF-3232-2024; Guo, Haohan/MGU-0040-2025						FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications								Arxiv											1	1;2024-09-05;https://www.arxiv.org/abs/2409.03283v1	arXiv:2409.03283			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 05 2024	2024	This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.																																	2024-09-18	PPRN:91754874		
J	Wu, Qitian; Zhang, Hengrui; Yan, Junchi; Wipf, David				Yan, Junchi/ADK-0658-2022						Handling Distribution Shifts on Graphs: An Invariance Perspective								Arxiv											2	2;2024-08-16;https://www.arxiv.org/abs/2202.02466v5| 1;2022-05-07;https://www.arxiv.org/abs/2202.02466v4	arXiv:2202.02466			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 16 2024	2024	There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution1.																																	2024-08-25	PPRN:13739365		
J	Lu, Jiarui; Holleis, Thomas; Zhang, Yizhe; Aumayer, Bernhard; Nan, Feng; Bai, Felix; Ma, Shuang; Ma, Shen; Li, Mengyu; Yin, Guoli; Wang, Zirui; Pang, Ruoming				Yin, Guoli/GQA-4968-2022						ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities								Arxiv											1	1;2024-08-08;https://www.arxiv.org/abs/2408.04682v1	arXiv:2408.04682			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 08 2024	2024	Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities.																																	2024-08-21	PPRN:91320198		
J	Wang, Yizheng; Sun, Jia; Bai, Jinshuai; Anitescu, Cosmin; Eshaghi, Mohammad Sadegh; Zhuang, Xiaoying; Rabczuk, Timon; Liu, Yinghua				Zhuang, Xiaoying/G-4754-2011; Bai, Jinshuai/JHT-4500-2023; Anitescu, Cosmin/N-8620-2018; Eshaghi Khanghah, Mohammad Sadegh/AAZ-4672-2021; Wang, Yizheng/LNQ-7729-2024						Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks								Arxiv											2	2;2024-08-04;https://www.arxiv.org/abs/2406.11045v2| 1;2024-06-16;https://www.arxiv.org/abs/2406.11045v1	arXiv:2406.11045			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 04 2024	2024	AI for partial differential equations (PDEs) has garnered significant attention, particularly with the emergence of Physics-informed neural networks (PINNs). The recent advent of Kolmogorov-Arnold Network (KAN) indicates that there is potential to revisit and enhance the previously MLP-based PINNs. Compared to MLPs, KANs offer interpretability and require fewer parameters. PDEs can be described in various forms, such as strong form, energy form, and inverse form. While mathematically equivalent, these forms are not computationally equivalent, making the exploration of different PDE formulations significant in computational physics. Thus, we propose different PDE forms based on KAN instead of MLP, termed Kolmogorov-Arnold-Informed Neural Network (KINN) for solving forward and inverse problems. We systematically compare MLP and KAN in various numerical examples of PDEs, including multi-scale, singularity, stress concentration, nonlinear hyperelasticity, heterogeneous, and complex geometry problems. Our results demonstrate that KINN significantly outperforms MLP regarding accuracy and convergence speed for numerous PDEs in computational solid mechanics, except for the complex geometry problem. This highlights KINN's potential for more efficient and accurate PDE solutions in AI for PDEs.																																	2024-08-11	PPRN:89344397		
J	Goncharov, Alexander B; Shen, Linhui										QUANTUM GEOMETRY OF MODULI SPACES OF LOCAL SYSTEMS AND REPRESENTATION THEORY								Arxiv											2	2;2024-07-31;https://www.arxiv.org/abs/1904.10491v4| 1;2019-04-23;https://www.arxiv.org/abs/1904.10491v3	arXiv:1904.10491			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 31 2024	2024	Let G be a split semi-simple adjoint group over Q , and S a colored decorated surface, , given by an oriented surface with punctures, special boundary points, and a specified collection of boundary intervals. We introduce a moduli space YG,S parametrizing G-local systems on S with certain boundary data, and prove that it carries a cluster Poisson structure, equivariant under the action of a discrete cluster modular group Γ G , S . The group ΓG,S contains the mapping class group of S , the product of Weyl groups over punctures, the product of generalized braid groups over boundary components, and the group of outer automorphisms of G. We prove that the dual moduli space AG,S carries a ΓG,S-equivariant G cluster K2- structure, so that the pair (AG,S , PG , S ) is a cluster ensemble. This generalizes the works of V. Fock & the first author, and of I. Le. The main theorem has many applications, including the following three, established in the paper. • Combining with the quantization of cluster Poisson varieties, we get a ΓG,S-equivariant cluster quantization of PG , S . It is given by a *-algebra Ah(PG,S) and its ΓG,S-equivariant G principal series *representation in a Hilbert space. The Langlands modular duality is built in the construction. • When S is punctured disc with two special points, we get a geometric construction of the principal series *–representations of the Langlands modular double of the quantum group Uq(g). • Using the cluster Poisson structure of PG , S , we proved that its Donaldson-Thomas transformation is a cluster transformation, and compute it explicitly. Combing this with the Gross-Hacking-KeelKontsevich construction, we finally prove the Duality Conjecture for PG , S , including a canonical linear basis in O(PG,S). For a genus g surface S with n > 0 punctures, we conjecture a canonical nondegenerate continuous pairing between the two infinite dimensional vector bundles on the space Mgn : (1) The bundle of conformal blocks in the Liouville/Toda Conformal Field Theory. It is a bundle of discrete vector spaces, given by the coinvariants of oscillatory representations of the W-algebra of g∨ . (2) The bundle of topological vector spaces, provided by the cluster quantization of PG , S . The second bundle in the genus zero case is identified by Modular Functor Conjecture with: (3) Tensor product invariants of the principal series representations of the quantum group Uq(g). The Modular Functor Conjecture is claimed for PGL2 by Teschner and for PGLm m by Schrader-Shapiro. The conjectural relation 1) ↔ 3) between representations of W −algebras and principal series representations of quantum groups is a continuous analog of the Kazhdan-Lusztig Theory. 																																	2024-08-08	PPRN:12393445		
J	Lauriere, Mathieu; Perrin, Sarah; Perolat, Julien; Girgin, Sertan; Muller, Paul; Elie, Romuald; Geist, Matthieu; Pietquin, Olivier				Lauriere, Mathieu/KUD-1349-2024; Pietquin, Olivier/L-3108-2015						Learning in Mean Field Games: A Survey								Arxiv											3	3;2024-07-26;https://www.arxiv.org/abs/2205.12944v4| 2;2024-02-20;https://www.arxiv.org/abs/2205.12944v3| 1;2022-05-25;https://www.arxiv.org/abs/2205.12944v2	arXiv:2205.12944			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 26 2024	2024	Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malhamé, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of MFGs. We then present a general framework for classical iterative methods (based on best-response computation or policy evaluation) to solve MFGs in an exact way. Building on these algorithms and the connection with Markov Decision Processes, we explain how RL can be used to learn MFG solutions in a model-free way. Last, we present numerical illustrations on a benchmark problem, and conclude with some perspectives.																																	2024-08-08	PPRN:12199907		
J	Boccaletti, A.; Borsanyi, Sz.; Davier, M.; Fodor, Z.; Frech, F.; Gerardin, A.; Giusti, D.; Kotov, A.Yu.; Lellouch, L.; Lippert, Th.; Lupo, A.; Malaescu, B.; Mutzel, S.; Portelli, A.; Risch, A.; Sjo, M.; Stokes, F.; Szabo, K.K.; Toth, B.C.; Wang, G.; Zhang, Z.				Lupo, Alessandro/HGF-0055-2022; Kotov, Andrey/A-8386-2014; Davier, Michel/HTM-1466-2023; Giusti, Davide/IQW-1797-2023						High precision calculation of the hadronic vacuum polarisation contribution to the muon anomaly								Arxiv											1	1;2024-07-15;https://www.arxiv.org/abs/2407.10913v1	arXiv:2407.10913			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 15 2024	2024	For fifty years, the standard model of particle physics has been hugely successful in describing subatomic phenomena. Recently, this statement appeared to be contradicted by the strong disagreement between measurements of the anomalous magnetic moment of the muon [1], a µ , and the reference standard-model prediction [2]. Such a large discrepancy should signal the discovery of interactions or particles not present in the standard model. However, two independent determinations [3, 4] of the most uncertain contribution to the standard-model prediction display significantly less tension with the measurement. Here we present a new first-principle calculation of this contribution. We reduce uncertainties compared to our earlier computation [3] by 40%.  To reach this unprecedented level of precision we improve on many aspects of the calculation. Namely, we perform large-scale lattice QCD simulations on finer lattices than in Ref. [3], allowing for an even more accurate continuum extrapolation. We also include a small, long-distance contribution obtained using input from experiments in a low-energy regime where they all agree. Combined with the extensive calculations of other standard model contributions summarised in Ref. [2], our result leads to a prediction that differs from the measurement of a µ by only 0.9 standard deviations. This provides a remarkable validation of the standard model to 0.37ppm.																																	2024-07-23	PPRN:90822810		
J	Tan, Weihao; Zhang, Wentao; Xu, Xinrun; Xia, Haochong; Ding, Ziluo; Li, Boyu; Zhou, Bohan; Yue, Junpeng; Jiang, Jiechuan; Li, Yewen; An, Ruyi; Qin, Molei; Zong, Chuqiao; Zheng, Longtao; Wu, Yujie; Chai, Xiaoqiang; Bi, Yifei; Xie, Tianbao; Gu, Pengjie; Li, Xiyun; Zhang, Ceyao; Tian, Long; Wang, Chaojie; Wang, Xinrun; Karlsson, Boerje F.; An, Bo; Yan, Shuicheng; Lu, Zongqing				Karlsson, Börje/B-4046-2010; Wu, Yujie/HSG-5270-2023; yan, shuicheng/HCH-9860-2022						Cradle: Empowering Foundation Agents Towards General Computer Control								Arxiv											3	3;2024-07-02;https://www.arxiv.org/abs/2403.03186v3| 2;2024-03-07;https://www.arxiv.org/abs/2403.03186v2| 1;2024-03-05;https://www.arxiv.org/abs/2403.03186v1	arXiv:2403.03186			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 02 2024	2024	Despite the success in specific scenarios, existing foundation agents still struggle to generalize across various virtual scenarios, mainly due to the dramatically different encapsulations of environments with manually designed observation and action spaces. To handle this issue, we propose the General Computer Control (GCC) setting to restrict foundation agents to interact with software through the most unified and standardized interface, i.e., using screenshots as input and keyboard and mouse actions as output. We introduce CRADLE, a modular and flexible LMMpowered framework, as a preliminary attempt towards GCC. Enhanced by six key modules: Information Gathering, Self-Reflection, Task Inference, Skill Curation, Action Planning, and Memory, CRADLE is able to understand input screenshots and output executable code for low-level keyboard and mouse control after highlevel planning, so that CRADLE can interact with any software and complete long-horizon complex tasks without relying on any built-in APIs. Experimental results show that CRADLE exhibits remarkable generalizability and impressive performance across four previously unexplored commercial video games, five software applications, and a comprehensive benchmark, OSWorld. To our best knowledge, CRADLE is the first to enable foundation agents to follow the main storyline and complete 40 -minute -long real missions in the complex AAA game Red Dead Redemption 2 (RDR2). CRADLE can also create a city of a thousand people in Cities: Skylines, farm and harvest parsnips in Stardew Valley, and trade and bargain with a maximal weekly total profit of 87% in Dealer’s Life 2. CRADLE can not only operate daily software, like Chrome, Outlook, and Feishu, but also edit images and videos using Meitu and CapCut. With a unified interface to interact with any software, CRADLE greatly extends the reach of foundation agents by enabling the easy conversion of any software, especially complex games, into benchmarks to evaluate agents’ various abilities and facilitate further data collection, thus paving the way for generalist agents.																																	2024-08-02	PPRN:88033052		
J	Liu, Chengkai; Lin, Jianghao; Wang, Jianling; Liu, Hanzhou; Caverlee, James				Liu, Chengkai/KYR-6035-2024						Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models								Arxiv											2	2;2024-06-29;https://www.arxiv.org/abs/2403.03900v2| 1;2024-03-06;https://www.arxiv.org/abs/2403.03900v1	arXiv:2403.03900			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 29 2024	2024	Sequential recommendation aims to estimate the dynamic user preferences and sequential dependencies among historical user behaviors. Although Transformer-based models have proven to be effective for sequential recommendation, they suffer from the inference inefficiency problem stemming from the quadratic computational complexity of attention operators, especially for long behavior sequences. Inspired by the recent success of state space models (SSMs), we propose Mamba4Rec, which is the first work to explore the potential of selective SSMs for efficient sequential recommendation. Built upon the basic Mamba block which is a selective SSM with an efficient hardware-aware parallel algorithm, we design a series of sequential modeling techniques to further promote model performance while maintaining inference efficiency. Through experiments on public datasets, we demonstrate how Mamba4Rec effectively tackles the effectiveness-efficiency dilemma, outperforming both RNN- and attention-based baselines in terms of both effectiveness and efficiency. 																																	2024-07-18	PPRN:88043585		
J	Wang, Xiaofei; Thakker, Manthan; Chen, Zhuo; Kanda, Naoyuki; Eskimez, Sefik Emre; Chen, Sanyuan; Tang, Min; Liu, Shujie; Li, Jinyu; Yoshioka, Takuya				Thakker, Manthan/KVY-3021-2024; jinyu, Li/JQV-7729-2023; Eskimez, Sefik Emre/AAB-3665-2022; Chen, Sanyuan/GLR-3754-2022						SpeechX: Neural Codec Language Model as a Versatile Speech Transformer								Arxiv											2	2;2024-06-25;https://www.arxiv.org/abs/2308.06873v2| 1;2023-08-14;https://www.arxiv.org/abs/2308.06873v1	arXiv:2308.06873			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 25 2024	2024	Recent advancements in generative speech models based on audio -text prompts have enabled remarkable innovations like high -quality zero -shot text -to -speech. However, existing models still face limitations in handling diverse audio -text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero -shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi -task learning using taskdependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX’s efficacy in various tasks, including zeroshot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.																																	2024-07-15	PPRN:77332935		
J	Wang, Zora Zhiruo; Asai, Akari; Yu, Xinyan Velocity; Xu, Frank F.; Xie, Yiqing; Neubig, Graham; Fried, Daniel				Xie, Yiqing/KBC-0228-2024						CodeRAG-Bench: Can Retrieval Augment Code Generation?								Arxiv											2	2;2025-02-26;https://www.arxiv.org/abs/2406.14497v2| 1;2024-06-20;https://www.arxiv.org/abs/2406.14497v1	arXiv:2406.14497			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 20 2024	2024	While language models (LMs) have proven remarkably adept at generating code, many programs are challenging for LMs to generate using their parametric knowledge alone. Providing external contexts such as library documentation can facilitate generating accurate and functional code. Despite the success of retrieval-augmented generation (RAG) in various text-oriented tasks, its potential for improving code generation remains under-explored. In this work, we conduct a systematic, large-scale analysis by asking: in what scenarios can retrieval benefit code generation models? and what challenges remain? We first curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three categories of code generation tasks, including basic programming, open-domain, and repository-level problems. We aggregate documents from five sources for models to retrieve contexts: competition solutions, online tutorials, library documentation, StackOverflow posts, and GitHub repositories. We examine top-performing models on CodeRAG-Bench by providing contexts retrieved from one or multiple sources. While notable gains are made in final code generation by retrieving high-quality contexts across various settings, our analysis reveals room for improvement -- current retrievers still struggle to fetch useful contexts especially with limited lexical overlap, and generators fail to improve with limited context lengths or abilities to integrate additional contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage further development of advanced code-oriented RAG methods.																																	2025-08-07	PPRN:89378597		
J	Yang, Zonglin; Du, Xinya; Li, Junxian; Zheng, Jie; Poria, Soujanya; Cambria, Erik				Cambria, Erik/C-2103-2013; Yang, Zonglin/AGA-2162-2022; Zheng, Jie/AAX-1454-2021; PORIA, SOUJANYA/KIJ-4789-2024						Large Language Models for Automated Open-domain Scientific Hypotheses Discovery								Arxiv											3	3;2024-06-12;https://www.arxiv.org/abs/2309.02726v3| 2;2024-02-16;https://www.arxiv.org/abs/2309.02726v2| 1;2023-09-06;https://www.arxiv.org/abs/2309.02726v1	arXiv:2309.02726			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 12 2024	2024	Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation. To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (''not existing in literature'') and valid (''reflecting reality'') scientific hypotheses1.																																	2024-07-04	PPRN:84787297		
J	Zhou, Xiaoyu; Ran, Xingjian; Xiong, Yajiao; He, Jinlin; Lin, Zhiwei; Wang, Yongtao; Sun, Deqing; Yang, Ming-Hsuan				Wang, Yongtao/LOR-4168-2024; Sun, Deqing/KLD-7402-2024; Yang, Ming-Hsuan/T-9533-2019; Lin, ZhiWei Lin/KBC-8548-2024						GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting								Arxiv											2	2;2024-06-11;https://www.arxiv.org/abs/2402.07207v2| 1;2024-02-11;https://www.arxiv.org/abs/2402.07207v1	arXiv:2402.07207			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 11 2024	2024	We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an instance-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. 																																	2024-07-02	PPRN:87636830		
J	Zhang, Wenqi; Tang, Ke; Wu, Hai; Wang, Mengna; Shen, Yongliang; Hou, Guiyang; Tan, Zeqi; Li, Peng; Zhuang, Yueting; Lu, Weiming				Shen, Yongliang/GWC-1883-2022; Xue, Wang/KMA-5868-2024						Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization								Arxiv											3	3;2024-06-06;https://www.arxiv.org/abs/2402.17574v3| 2;2024-03-27;https://www.arxiv.org/abs/2402.17574v2| 1;2024-02-27;https://www.arxiv.org/abs/2402.17574v1	arXiv:2402.17574			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.																																	2024-07-04	PPRN:87918109		
J	Dong, Yi; Mu, Ronghui; Zhang, Yanghao; Sun, Siqi; Zhang, Tianle; Wu, Changshun; Jin, Gaojie; Qi, Yi; Hu, Jinwei; Meng, Jie; Bensalem, Saddek; Huang, Xiaowei				sun, siqi/KCZ-2643-2024; Xiaowei, Huang/JKI-6599-2023; Zhang, Tianle/V-8753-2019; Dong, Yi/HOF-1502-2023						Safeguarding Large Language Models: A Survey								Arxiv											1	1;2024-06-03;https://www.arxiv.org/abs/2406.02622v1	arXiv:2406.02622			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as "safeguards" or "guardrails", has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.																																	2024-07-04	PPRN:89263414		
J	Wang, Qingyun; Downey, Doug; Ji, Heng; Hope, Tom										SciMON: Scientific Inspiration Machines Optimized for Novelty								Arxiv											5	5;2024-06-03;https://www.arxiv.org/abs/2305.14259v7| 4;2024-05-28;https://www.arxiv.org/abs/2305.14259v6| 3;2024-02-17;https://www.arxiv.org/abs/2305.14259v5| 2;2024-01-22;https://www.arxiv.org/abs/2305.14259v4| 1;2023-05-23;https://www.arxiv.org/abs/2305.14259v1	arXiv:2305.14259			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link prediction--severely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of "inspirations" from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature																																	2025-09-18	PPRN:71592379		
J	Fang, Richard; Bindu, Rohan; Gupta, Akul; Zhan, Qiusi; Kang, Daniel										Teams of LLM Agents can Exploit Zero-Day Vulnerabilities								Arxiv											1	1;2024-06-02;https://www.arxiv.org/abs/2406.01637v1	arXiv:2406.01637			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 02 2024	2024	LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities). In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and show that our team of agents improve over prior work by up to 4.5 ×.																																	2024-07-04	PPRN:89246265		
J	Qiao, Bo; Li, Liqun; Zhang, Xu; He, Shilin; Kang, Yu; Zhang, Chaoyun; Yang, Fangkai; Dong, Hang; Zhang, Jue; Wang, Lu; Ma, Minghua; Zhao, Pu; Qin, Si; Qin, Xiaoting; Du, Chao; Xu, Yong; Lin, Qingwei; Rajmohan, Saravan; Zhang, Dongmei				Wang, Chao/JHT-6081-2023; Ma, Minghua/JFA-3786-2023; 林, 青伟/IXN-4340-2023						TaskWeaver: A Code-First Agent Framework								Arxiv											2	2;2023-12-01;https://www.arxiv.org/abs/2311.17541v2| 1;2024-06-01;	arXiv:2311.17541			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	Large Language Models (LLMs) have shown impressive abilities in natural language understanding and generation, leading to their widespread use in applications such as chatbots and virtual assistants. However, existing LLM frameworks face limitations in handling domain-specific data analytics tasks with rich data structures. Moreover, they struggle with flexibility to meet diverse user requirements. To address these issues, TaskWeaver is proposed as a code-first framework for building LLM-powered autonomous agents. It converts user requests into executable code and treats user-defined plugins as callable functions. TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic. It also incorporates domain-specific knowledge through examples and ensures the secure execution of generated code. TaskWeaver offers a powerful and flexible framework for creating intelligent conversational agents that can handle complex tasks and adapt to domain-specific scenarios. The code is open sourced at https://github.com/microsoft/TaskWeaver/.																																	2025-11-07	PPRN:86358452		
J	Chambon, Pierre; Delbrouck, Jean-Benoit; Sounack, Thomas; Huang, Shih-Cheng; Chen, Zhihong; Varma, Maya; Truong, Steven QH; Chuong, Chu The; Langlotz, Curtis P.				Chambon, Pierre/HSI-0354-2023						CheXpert Plus: Hundreds of Thousands of Aligned Radiology Texts, Images and Patients								Arxiv											1	1;2024-05-29;https://www.arxiv.org/abs/2405.19538v1	arXiv:2405.19538			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	Since the release of the original CheXpert paper (Irvin et al., 2019) five years ago, CheXpert has become one of the most widely used and cited clinical AI datasets. The emergence of vision language models has sparked an increase in demands for sharing reports linked to CheXpert images, along with a growing interest among AI fairness researchers in obtaining demographic data. To address this, C HE X PERT P LUS serves as a new collection of radiology data sources, made publicly available to enhance the scaling, performance, robustness, and fairness of models for all subsequent machine learning tasks in the field of radiology. C HE X PERT P LUS is the largest text dataset publicly released in radiology, with a total of 36 million text tokens, including 13 million impression tokens. To the best of our knowledge, it represents the largest text de-identification effort in radiology, with almost 1 million PHI spans anonymized. It is only the second time that a large-scale English paired dataset has been released in radiology, thereby enabling, for the first time, cross-institution training at scale. All reports are paired with high-quality images in DICOM format, along with numerous image and patient metadata covering various clinical and socio-economic groups, as well as many pathology labels and RadGraph annotations. We hope this dataset will boost research for AI models that can further assist radiologists and help improve medical care. 																																	2024-11-10	PPRN:89113162		
J	Chen, Xinyun; Chi, Ryan A.; Wang, Xuezhi; Zhou, Denny				Chen, Xinyun/ABZ-9877-2022						Premise Order Matters in Reasoning with Large Language Models								Arxiv											3	3;2024-05-28;https://www.arxiv.org/abs/2402.08939v3| 2;2024-03-04;https://www.arxiv.org/abs/2402.08939v2| 1;2024-02-14;https://www.arxiv.org/abs/2402.08939v1	arXiv:2402.08939			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, , despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model’s accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.																																	2024-06-14	PPRN:87684167		
J	Liang, Hanwen; Yin, Yuyang; Xu, Dejia; Liang, Hanxue; Wang, Zhangyang; Plataniotis, Konstantinos N.; Zhao, Yao; Wei, Yunchao				Liang, Hanwen/GWQ-5112-2022; Zhihua, Wang/AFO-5263-2022; YIN, Yuyang/HTL-3947-2023						Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models								Arxiv											1	1;2024-05-26;https://www.arxiv.org/abs/2405.16645v1	arXiv:2405.16645			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 26 2024	2024	The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation. Most prior approaches rely on multiple image or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision. However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues. Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models. Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation. Specifically, we present a novel framework, Diffusion4D, , for efficient and scalable 4D content generation. Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics. After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarseto-fine manner. The synthesized multi-view consistent 4D image set enables us to swiftly generate high-fidelity and diverse 4D assets within just several minutes. Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities.																																	2024-06-11	PPRN:89071732		
J	Li, Juncheng; Pan, Kaihang; Ge, Zhiqi; Gao, Minghe; Ji, Wei; Zhang, Wenqiao; Chua, Tat-Seng; Tang, Siliang; Zhang, Hanwang; Zhuang, Yueting				Wang, Meng/AEZ-9059-2022; zhao, zhao/JAC-1686-2023						Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions								Arxiv											3	3;2024-05-25;https://www.arxiv.org/abs/2308.04152v4| 2;2023-09-30;https://www.arxiv.org/abs/2308.04152v3| 1;2023-08-08;https://www.arxiv.org/abs/2308.04152v1	arXiv:2308.04152			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 25 2024	2024	Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image -caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image -captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs’ underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module ( VPG-C ), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, , eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, , a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero -shot performance across all tasks of DEMON. . Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. . The code and models are available at https://github.com/DCDmllm/Cheetah. .																																	2024-06-11	PPRN:74482787		
J	Golchin, Shahriar; Surdeanu, Mihai										Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models								Arxiv											5	5;2024-05-24;https://www.arxiv.org/abs/2311.06233v6| 4;2024-02-10;https://www.arxiv.org/abs/2311.06233v5| 3;2024-02-03;https://www.arxiv.org/abs/2311.06233v4| 2;2023-11-17;https://www.arxiv.org/abs/2311.06233v2| 1;2023-11-10;https://www.arxiv.org/abs/2311.06233v1	arXiv:2311.06233			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each subsampled instance from a specific dataset partition (e.g., GSM8k test set) are created. These changes only include word -level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the possibility of selecting none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it in its pre -training phase—a trait intrinsic to LLMs. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the model being examined with the dataset partition to which the quiz pertains. Applied to various datasets with GPT4 and GPT-3.5, our findings—while fully lacking access to pre -training data and model parameters—suggest that DCQ achieves state-of-the-art results and uncovers greater contamination/memorization levels compared to existing methods and proficiently bypasses more safety filters, especially those set to avoid generating copyrighted contents.																																	2024-06-08	PPRN:86126356		
J	Chen, Huanran; Dong, Yinpeng; Wang, Zhengyi; Yang, Xiao; Duan, Chengqi; Su, Hang; Zhu, Jun				Dong, Yinpeng/KBA-4751-2024						Robust Classification via a Single Diffusion Model								Arxiv											2	2;2024-05-21;https://www.arxiv.org/abs/2305.15241v2| 1;2023-05-24;https://www.arxiv.org/abs/2305.15241v1	arXiv:2305.15241			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 21 2024	2024	Diffusion models have been applied to improve adversarial robustness of image classifiers by purifying the adversarial noises or generating realistic data for adversarial training. However, diffusion-based purification can be evaded by stronger adaptive attacks while adversarial training does not perform well under unseen threats, exhibiting inevitable limitations of these methods. To better harness the expressive power of diffusion models, this paper proposes Robust Diffusion Classifier (RDC), a generative classifier that is constructed from a pre-trained diffusion model to be adversarially robust. RDC first maximizes the data likelihood of a given input and then predicts the class probabilities of the optimized input using the conditional likelihood estimated by the diffusion model through Bayes' theorem. To further reduce the computational cost, we propose a new diffusion backbone called multi-head diffusion and develop efficient sampling strategies. As RDC does not require training on particular adversarial attacks, we demonstrate that it is more generalizable to defend against multiple unseen threats. In particular, RDC achieves 75.67% robust accuracy against various ℓ∞ norm-bounded adaptive attacks with ϵ∞ = 8/255 on CIFAR-10, surpassing the previous state-of-the-art adversarial training models by +4.77%. The results highlight the potential of generative classifiers by employing pre-trained diffusion models for adversarial robustness compared with the commonly studied discriminative classifiers. 																																	2024-06-15	PPRN:72717837		
J	Yu, Weihao; Wang, Xinchao				Yu, Weihao/HJH-1824-2023						MambaOut: Do We Really Need Mamba for Vision?								Arxiv											1	1;2024-05-20;https://www.arxiv.org/abs/2405.07992v3	arXiv:2405.07992			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 20 2024	2024	Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks. The code is available at https://github.com/yuweihao/MambaOut																																	2024-06-15	PPRN:89094474		
J	Liu, Aiwei; Pan, Leyi; Hu, Xuming; Meng, Shiao; Wen, Lijie				Hu, Xuming/HTS-1538-2023						A Semantic Invariant Robust Watermark for Large Language Models								Arxiv											2	2;2024-05-19;https://www.arxiv.org/abs/2310.06356v3| 1;2024-02-29;https://www.arxiv.org/abs/2310.06356v2	arXiv:2310.06356			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 19 2024	2024	Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. 																																	2024-06-01	PPRN:88755972		
J	Gu, Xinyang; Wang, Yen-Jen; Chen, Jianyu										Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer								Arxiv											2	2;2024-05-18;https://www.arxiv.org/abs/2404.05695v2| 1;2024-04-08;https://www.arxiv.org/abs/2404.05695v1	arXiv:2404.05695			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 18 2024	2024	Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer. 																																	2024-06-15	PPRN:88448959		
J	Grace, Katja; Stewart, Harlan; Sandkuehler, Julia Fabienne; Thomas, Stephen; Weinstein-Raun, Ben; Brauner, Jan										Thousands of AI Authors on the Future of AI								Arxiv											2	2;2024-04-30;https://www.arxiv.org/abs/2401.02843v2| 1;2024-01-05;https://www.arxiv.org/abs/2401.02843v1	arXiv:2401.02843			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 30 2024	2024	In the largest survey of its kind, we surveyed 2,778 researchers who had published in top-tier artificial intelligence (AI) venues, asking for their predictions on the pace of AI progress and the nature and impacts of advanced AI systems. The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey). Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 37.8% and 51.4% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that “substantial” or “extreme” concern is warranted about six different AI-related scenarios, including spread of false information, authoritarian population control, and worsened inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.																																	2024-05-19	PPRN:86996610		
J	Liu, Shengcai; Chen, Caishun; Qu, Xinghua; Tang, Ke; Ong, Yew-Soon				Chen, Caishun/KIL-3554-2024; Liu, Shengcai/ABA-9832-2020						Large Language Models as Evolutionary Optimizers								Arxiv											3	3;2024-04-26;https://www.arxiv.org/abs/2310.19046v3| 2;2023-11-01;https://www.arxiv.org/abs/2310.19046v2| 1;2023-10-29;https://www.arxiv.org/abs/2310.19046v1	arXiv:2310.19046			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems. However, EAs often demand carefully -designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model. This approach is referred to as LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions. Then, LMEA evaluates these new solutions and include them into the population for the next generation. LMEA is equipped with a self -adaptation mechanism that controls the temperature of the LLM. This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima. We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research. Notably, the results show that LMEA performs competitively to traditional heuristics in finding high -quality solutions on TSP instances with up to 20 nodes. Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the selfadaptation mechanism in evolutionary search. In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems. We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges.																																	2024-05-07	PPRN:85888862		
J	Cen, Jiazhong; Fang, Jiemin; Zhou, Zanwei; Yang, Chen; Xie, Lingxi; Zhang, Xiaopeng; Shen, Wei; Tian, Qi				Xie, Lingxi/ABF-6996-2020						Segment Anything in 3D with Radiance Fields								Arxiv											3	3;2024-04-16;https://www.arxiv.org/abs/2304.12308v5| 2;2023-11-28;https://www.arxiv.org/abs/2304.12308v4| 1;2023-04-24;https://www.arxiv.org/abs/2304.12308v2	arXiv:2304.12308			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	The Segment Anything Model (SAM) emerges as a powerful vision foundation model to generate high-quality 2D segmentation results. This paper aims to generalize SAM to segment 3D objects. Rather than replicating the data acquisition and annotation procedure which is costly in 3D, we design an efficient solution, leveraging the radiance field as a cheap and off-the-shelf prior that connects multi-view 2D images to the 3D space. We refer to the proposed solution as SA3D, short for Segment Anything in 3D. With SA3D, the user is only required to provide a 2D segmentation prompt (e.g., rough points) for the target object in a single view, which is used to generate its corresponding 2D mask with SAM. Next, SA3D alternately performs mask inverse rendering and cross-view self-prompting across various views to iteratively refine the 3D mask of the target object. For one view, mask inverse rendering projects the 2D mask obtained by SAM into the 3D space with guidance of the density distribution learned by the radiance field for 3D mask refinement; Then, cross-view self-prompting extracts reliable prompts automatically as the input to SAM from the rendered 2D mask of the inaccurate 3D mask for a new view. We show in experiments that SA3D adapts to various scenes and achieves 3D segmentation within seconds. Our research reveals a potential methodology to lift the ability of a 2D segmentation model to 3D. 																																	2024-04-26	PPRN:65249701		
J	Renduchintala, Adithya; Konuk, Tugrul; Kuchaiev, Oleksii										Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying								Arxiv											2	2;2024-04-12;https://www.arxiv.org/abs/2311.09578v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09578v1	arXiv:2311.09578			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 12 2024	2024	We introduce Tied-LoRA, a novel paradigm leveraging weight tying and selective training to enhance the parameter efficiency of Low-rank Adaptation (LoRA). Our exploration encompasses different plausible combinations of parameter training and freezing, coupled with weight tying, aimed at identifying the optimal trade-off between performance and the count of trainable parameters. Across 5 diverse tasks and two foundational language models with different parameter counts, our experiments provide comprehensive insights into the inherent trade-offs between efficiency and performance. Our findings reveal a specific Tied-LoRA configuration that distinguishes itself by showcasing comparable performance to LoRA across multiple tasks while utilizing only a fraction of the parameters employed by the standard LoRA method, particularly at elevated ranks. This underscores the efficacy of Tied-LoRA in achieving impressive results with significantly reduced model complexity.																																	2024-04-25	PPRN:86177307		
J	Charatan, David; Li, Sizhe; Tagliasacchi, Andrea; Sitzmann, Vincent				Li, Sizhe/LKK-5558-2024						pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction								Arxiv											4	4;2024-04-04;https://www.arxiv.org/abs/2312.12337v4| 3;2024-04-02;https://www.arxiv.org/abs/2312.12337v3| 2;2023-12-21;https://www.arxiv.org/abs/2312.12337v2| 1;2023-12-19;https://www.arxiv.org/abs/2312.12337v1	arXiv:2312.12337			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	We introduce pixelSplat, a feed -forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory -efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back -propagate gradients through the Gaussian splatting representation. We benchmark our method on wide -baseline novel view synthesis on the real -world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field. Additional materials can be found on the project website. 1																																	2024-04-20	PPRN:86725789		
J	Qin, Minghan; Li, Wanhua; Zhou, Jiawei; Wang, Haoqian; Pfister, Hanspeter				Li, Wanhua/AAE-9197-2021; Pfister, Hanspeter/NJR-9877-2025; Zhou, Jiawei/AAH-3779-2019						LangSplat: 3D Language Gaussian Splatting								Arxiv											2	2;2024-03-31;https://www.arxiv.org/abs/2312.16084v2| 1;2023-12-26;https://www.arxiv.org/abs/2312.16084v1	arXiv:2312.16084			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 31 2024	2024	Humans live in a 3D world and commonly use natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open -vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile -based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene -wise language autoencoder and then learns language features on the scene -specific latent space,thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experimental results show that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a 199 × speedup compared to LERF at the resolution of 1440 × 1080. We strongly recommend readers to check out our video results athttps://langsplat.github.io/.																																	2024-04-17	PPRN:86828460		
J	Zhang, Yi-Fan; Yu, Weichen; Wen, Qingsong; Wang, Xue; Zhang, Zhang; Wang, Liang; Jin, Rong; Tan, Tieniu				Wen, Qingsong/LTF-7625-2024						Debiasing Multimodal Large Language Models								Arxiv											3	3;2025-08-14;https://www.arxiv.org/abs/2403.05262v3| 2;2024-03-27;https://www.arxiv.org/abs/2403.05262v2| 1;2024-03-08;https://www.arxiv.org/abs/2403.05262v1	arXiv:2403.05262			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 27 2024	2024	In the realms of computer vision and natural language processing, Large Vision-Language Models (LVLMs) have become indispensable tools, proficient in generating textual descriptions based on visual inputs. Despite their advancements, our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image. Our empirical experiments underscore the persistence of this bias, as LVLMs often provide confident answers even in the absence of relevant images or given incongruent visual input. To rectify these biases and redirect the model’s focus toward vision information, we introduce two simple, training-free strategies. Firstly, for tasks such as classification or multi-choice question-answering (QA), we propose a “calibration” step through affine transformation to adjust the output distribution. This “Post -Ho c debias” approach ensures uniform scores for each answer when the image is absent, serving as an effective regularization technique to alleviate the influence of LLM priors. For more intricate open-ended generation tasks, we extend this method to “Debias sampling”, drawing inspirations from contrastive decoding methods. Furthermore, our investigation sheds light on the instability of LVLMs across various decoding configurations. Through systematic exploration of different settings, we significantly enhance performance, surpassing reported results and raising concerns about the fairness of existing evaluations. Comprehensive experiments substantiate the effectiveness of our proposed strategies in mitigating biases. These strategies not only prove beneficial in minimizing hallucinations but also contribute to the generation of more helpful and precise illustrations.																																	2024-04-15	PPRN:88081755		
J	Yu, Zhiyuan; Liu, Xiaogeng; Liang, Shunning; Cameron, Zach; Xiao, Chaowei; Zhang, Ning				Xiao, Chaowei/AAT-8772-2021; Yu, Zhiyuan/JPX-8768-2023; Liu, Xiaogeng/KIJ-1671-2024						Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models								Arxiv											2	2;2024-09-30;https://www.arxiv.org/abs/2403.17336v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17336v1	arXiv:2403.17336			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 26 2024	2024	Recent advancements in generative AI have enabled ubiquitous access to large language models (LLMs). Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers. To overcome such protection, jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited. Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak prompts, we systemized existing prompts and measured their jailbreak effectiveness empirically. Further, we conducted a user study involving 92 participants with diverse backgrounds to unveil the process of manually creating jailbreak prompts. We observed that users often succeeded in jailbreak prompts generation regardless of their expertise in LLMs. Building on the insights from the user study, we also developed a system using AI as the assistant to automate the process of jailbreak prompt generation.																																	2025-08-07	PPRN:88296148		
J	Garibi, Daniel; Patashnik, Or; Voynov, Andrey; Averbuch-Elor, Hadar; Cohen-Or, Daniel				Averbuch-Elor, Hadar/AAO-4246-2021; Patashnik, Or/ADQ-4024-2022						ReNoise: Real Image Inversion Through Iterative Noising								Arxiv											1	1;2024-03-21;https://www.arxiv.org/abs/2403.14602v1	arXiv:2403.14602			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 21 2024	2024	Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained diffusion model. Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the diffusion sampling process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward diffusion trajectory, by iteratively applying the pretrained diffusion model, and averaging these predictions. We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated diffusion models. Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed. Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images.																																	2024-04-13	PPRN:88261398		
J	Liao, Weibin; Zhu, Yinghao; Wang, Xinyuan; Pan, Chengwei; Wang, Yasha; Ma, Liantao				Ma, Liantao/MGV-9545-2025; pan, chengwei/HDM-8386-2022; Zhu, Yinghao/JNT-3876-2023						LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation								Arxiv											1	1;2024-03-11;https://www.arxiv.org/abs/2403.05246v2	arXiv:2403.05246			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 11 2024	2024	UNet and its variants have been widely used in medical image segmentation. However, these models, especially those based on Transformer architectures, pose challenges due to their large number of parameters and computational loads, making them unsuitable for mobile health applications. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as competitive alternatives to CNN and Transformer architectures. Building upon this, we employ Mamba as a lightweight substitute for CNN and Transformer within UNet, aiming at tackling challenges stemming from computational resource limitations in real medical settings. To this end, we introduce the Lightweight Mamba UNet (LightM-UNet) that integrates Mamba and UNet in a lightweight framework. Specifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure Mamba fashion to extract deep semantic features and model long-range spatial dependencies, with linear computational complexity. Extensive experiments conducted on two real-world 2D/3D datasets demonstrate that LightM-UNet surpasses existing state-of-the-art literature. Notably, when compared to the renowned nnU-Net, LightM-UNet achieves superior segmentation performance while drastically reducing parameter and computation costs by 116x and 21x, respectively. This highlights the potential of Mamba in facilitating model lightweighting. Our code implementation is publicly available at https://github.com/MrBlankness/LightM-UNet.																																	2024-04-08	PPRN:88106259		
J	Wang, Samson; Czarnik, Piotr; Arrasmith, Andrew; Cerezo, M.; Cincio, Lukasz; Coles, Patrick J.				Czarnik, Piotr/A-9031-2015						Can Error Mitigation Improve Trainability of Noisy Variational Quantum Algorithms?								Arxiv											1	1;2024-03-08;https://www.arxiv.org/abs/2109.01051v2	arXiv:2109.01051			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 08 2024	2024	Variational Quantum Algorithms (VQAs) are often viewed as the best hope for near-term quantum advantage. However, recent studies have shown that noise can severely limit the trainability of VQAs, e.g., by exponentially flattening the cost landscape and suppressing the magnitudes of cost gradients. Error Mitigation (EM) shows promise in reducing the impact of noise on near-term devices. Thus, it is natural to ask whether EM can improve the trainability of VQAs. In this work, we first show that, for a broad class of EM strategies, exponential cost concentration cannot be resolved without committing exponential resources elsewhere. This class of strategies includes as special cases Zero Noise Extrapolation, Virtual Distillation, Probabilistic Error Cancellation, and Clifford Data Regression. Second, we perform analytical and numerical analysis of these EM protocols, and we find that some of them (e.g., Virtual Distillation) can make it harder to resolve cost function values compared to running no EM at all. As a positive result, we do find numerical evidence that Clifford Data Regression (CDR) can aid the training process in certain settings where cost concentration is not too severe. Our results show that care should be taken in applying EM protocols as they can either worsen or not improve trainability. On the other hand, our positive results for CDR highlight the possibility of engineering error mitigation methods to improve trainability.																																	2025-01-24	PPRN:88086186		
J	Wang, Zihao; Liu, Anji; Lin, Haowei; Li, Jiaqi; Ma, Xiaojian; Liang, Yitao				Liu, Anji/LEM-6968-2024; Wang, Zihao/S-7875-2019						RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation								Arxiv											1	1;2024-03-08;https://www.arxiv.org/abs/2403.05313v1	arXiv:2403.05313			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 08 2024	2024	We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models’ reasoning and generation ability in long -horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method — retrieval -augmented thoughts (RAT) — revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero -shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long -horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found in https://craftjarvis.github.io/RAT.																																	2024-04-07	PPRN:88085774		
J	Yang, Jonathan; Glossop, Catherine; Bhorkar, Arjun; Shah, Dhruv; Vuong, Quan; Finn, Chelsea; Sadigh, Dorsa; Levine, Sergey				Vuong, Quan-Hoang/F-2115-2010						Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19432v1	arXiv:2402.19432			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Feb 29 2024	2024	Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website http://extreme-cross-embodiment.github.io.																																	2024-05-20	PPRN:88756262		
J	Looser, Tobias J.; D'Eugenio, Francesco; Maiolino, Roberto; Witstok, Joris; Sandles, Lester; Curtis-Lake, Emma; Chevallard, Jacopo; Tacchella, Sandro; Johnson, Benjamin D.; Baker, William M.; Suess, Katherine A.; Carniani, Stefano; Ferruit, Pierre; Arribas, Santiago; Bonaventura, Nina; Bunker, Andrew J.; Cameron, Alex J.; Charlot, Stephane; Curti, Mirko; Graaff, Anna de; Maseda, Michael V.; Rawle, Tim; Rix, Hans-Walter; Del Pino, Bruno Rodriguez; Smit, Renske; Ubler, Hannah; Willott, Chris; Alberts, Stacey; Egami, Eiichi; Eisenstein, Daniel J.; Endsley, Ryan; Hausen, Ryan; Rieke, Marcia; Robertson, Brant; Shivaei, Irene; Williams, Christina C.; Boyett, Kristan; Chen, Zuyi; Ji, Zhiyuan; Jones, Gareth C.; Kumari, Nimisha; Nelson, Erica; Perna, Michele; Saxena, Aayush; Scholtz, Jan				Endsley, Ryan/AAJ-5103-2021; Baker, William/KUD-6412-2024; Kumari, Nimisha/AFS-1631-2022; Venturi, Giacomo/AAB-4352-2021; Perna, Michele/GNP-0399-2022; Nelson, Erica/OUI-1817-2025; Robertson, Brant/AAA-6124-2022; Del Pino, Bruno/C-3326-2017; Tacchella, Sandro/AAT-1602-2021; Jones, Gareth/AAD-7663-2022; Witstok, Joris/GQA-8643-2022; Smit, Renske/MIK-8564-2025; BOEKER, TORSTEN/KVC-3022-2024; D'Eugenio, Francesco/H-2606-2019; Arribas, Santiago/F-9277-2015						A recently quenched galaxy 700 million years after the Big Bang								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/2302.14155v3| 1;2023-02-27;https://www.arxiv.org/abs/2302.14155v2	arXiv:2302.14155			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	Local and low-redshift (z<3) galaxies are known to broadly follow a bimodal distribution: actively star forming galaxies with relatively stable star-formation rates, and passive systems. These two populations are connected by galaxies in relatively slow transition. In contrast, theory predicts that star formation was stochastic at early cosmic times and in low-mass systems: these galaxies transitioned rapidly between starburst episodes and phases of suppressed star formation, potentially even causing temporary quiescence -- so-called mini-quenching events. However, the regime of star-formation burstiness is observationally highly unconstrained. Directly observing mini-quenched galaxies in the primordial Universe is therefore of utmost importance to constrain models of galaxy formation and transformation. Early quenched galaxies have been identified out to redshift z<5, and these are all found to be massive (M∗>1010 M⊙) and relatively old. Here we report a (mini-)quenched galaxy at z=7.3, when the Universe was only 700~Myr old. The JWST/NIRSpec spectrum is very blue (U-V=0.16±0.03~mag), but exhibits a Balmer break and no nebular emission lines. The galaxy experienced a short starburst followed by rapid quenching; its stellar mass (4-6×108 M⊙) falls in a range that is sensitive to various feedback mechanisms, which can result in perhaps only temporary quenching.																																	2024-11-09	PPRN:42585144		
J	Manvi, Rohin; Khanna, Samar; Mai, Gengchen; Burke, Marshall; Lobell, David; Ermon, Stefano				Mai, Gengchen/ABF-8620-2020						GEOLLM: EXTRACTING GEOSPATIAL KNOWLEDGE FROM LARGE LANGUAGE MODELS								Arxiv											2	2;2024-02-24;https://www.arxiv.org/abs/2310.06213v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06213v1	arXiv:2310.06213			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 24 2024	2024	The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson’s r2) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well.																																	2024-03-27	PPRN:85525247		
J	Yue, Yuxuan; Yuan, Zhihang; Duanmu, Haojie; Zhou, Sifan; Wu, Jianlong; Nie, Liqiang				Yuan, Zhihang/HDN-8259-2022; zhou, sifan/HPC-7625-2023						WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More								Arxiv											1	1;2024-02-20;https://www.arxiv.org/abs/2402.12065v2	arXiv:2402.12065			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 20 2024	2024	Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto -regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low -bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past -only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross -block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weightactivation quantization, while also approaching the performance of weight -only quantization.																																	2024-03-19	PPRN:87777073		
J	Jiang, Jinhao; Zhou, Kun; Zhao, Wayne Xin; Song, Yang; Zhu, Chen; Zhu, Hengshu; Wen, Ji-Rong				Xia, Lianghao/IWV-0954-2023						KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph								Arxiv											1	1;2024-02-17;https://www.arxiv.org/abs/2402.11163v1	arXiv:2402.11163			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 17 2024	2024	In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.																																	2024-03-19	PPRN:87761675		
J	Gupta, Ish; Afle, Chaitanya; Arun, K.G.; Bandopadhyay, Ananya; Baryakhtar, Masha; Biscoveanu, Sylvia; Borhanian, Ssohrab; Broekgaarden, Floor; Corsi, Alessandra; Dhani, Arnab; Evans, Matthew; Hall, Evan D.; Hannuksela, Otto A.; Kacanja, Keisi; Kashyap, Rahul; Khadkikar, Sanika; Kuns, Kevin; Li, Tjonnie G.F.; Miller, Andrew L.; Nitz, Alexander Harvey; Owen, Benjamin J.; Palomba, Cristiano; Pearce, Anthony; Phurailatpam, Hemantakumar; Rajbhandari, Binod; Read, Jocelyn; Romano, Joseph D.; Sathyaprakash, Bangalore S.; Shoemaker, David H.; Singh, Divya; Vitale, Salvatore; Barsotti, Lisa; Berti, Emanuele; Cahillane, Craig; Chen, Hsin-Yu; Fritschel, Peter; Haster, Carl-Johan; Landry, Philippe; Lovelace, Geoffrey; Mcclelland, David; Slagmolen, Bram J J; Smith, Joshua R; Soares-Santos, Marcelle; Sun, Ling; Tanner, David; Yamamoto, Hiro; Zucker, Michael				Lovelace, Geoffrey/LUY-6382-2024; Berti, Emanuele/AAI-1513-2019; Shoemaker, David/JKI-5322-2023; Rajbhandari, Binod/MDT-7627-2025; Smith, Joshua/HGV-2348-2022; Villa-Ortega, Verónica/ABG-4912-2021; Kashyap, Rahul/HGE-2165-2022; Slagmolen, Bram/AAY-1638-2020; Gupta, Ish/JGM-1363-2023; Zucker, Michael/AAB-1348-2021; SINGH, DIVYA/KXR-9679-2024						Characterizing Gravitational Wave Detector Networks: From A♯ to Cosmic Explorer								Arxiv											2	2;2024-02-02;https://www.arxiv.org/abs/2307.10421v2| 1;2023-07-19;https://www.arxiv.org/abs/2307.10421v1	arXiv:2307.10421			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	Gravitational-wave observations by the Laser Interferometer Gravitational-Wave Observatory (LIGO) and Virgo have provided us a new tool to explore the Universe on all scales from nuclear physics to the cosmos and have the massive potential to further impact fundamental physics, astrophysics, and cosmology for decades to come. In this paper we have studied the science capabilities of a network of LIGO detectors when they reach their best possible sensitivity, called A♯, given the infrastructure in which they exist and a new generation of observatories that are factor of 10 to 100 times more sensitive (depending on the frequency), in particular a pair of L-shaped Cosmic Explorer observatories (one 40 km and one 20 km arm length) in the US and the triangular Einstein Telescope with 10 km arms in Europe. We use a set of science metrics derived from the top priorities of several funding agencies to characterize the science capabilities of different networks. The presence of one or two A♯ observatories in a network containing two or one next generation observatories, respectively, will provide good localization capabilities for facilitating multimessenger astronomy and precision measurement of the Hubble parameter. Two Cosmic Explorer observatories are indispensable for achieving precise localization of binary neutron star events, facilitating detection of electromagnetic counterparts and transforming multimessenger astronomy. Their combined operation is even more important in the detection and localization of high-redshift sources, such as binary neutron stars, beyond the star-formation peak, and primordial black hole mergers, which may occur roughly 100 million years after the Big Bang. The addition of the Einstein Telescope to a network of two Cosmic Explorer observatories is critical for accomplishing all the identified science metrics including the nuclear equation of state, cosmological parameters, the growth of black holes through cosmic history, but also make new discoveries such as the presence of dark matter within or around neutron stars and black holes, continuous gravitational waves from rotating neutron stars, transient signals from supernovae, and the production of stellar -mass black holes in the early Universe. For most metrics the triple network of next generation terrestrial observatories are a factor 100 better than what can be accomplished by a network of three A♯ observatories.																																	2024-03-14	PPRN:74077148		
J	Li, Xiaoyu; Zhang, Qi; Kang, Di; Cheng, Weihao; Gao, Yiming; Zhang, Jingbo; Liang, Zhihao; Liao, Jing; Cao, Yan-Pei; Shan, Ying				Zhang, Jingbo/GRX-3761-2022; 梁, 志浩/GQQ-1269-2022; cheng, weihao/MUO-0523-2025; li, xiaoyu/HSG-5635-2023; Zhang, Qi/ABD-3983-2020						Advances in 3D Generation: A Survey								Arxiv											1	1;2024-01-31;https://www.arxiv.org/abs/2401.17807v1	arXiv:2401.17807			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 31 2024	2024	Generating 3D models lies at the core of computer graphics and has been the focus of decades of research. With the emergence of advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling the creation of increasingly high-quality and diverse 3D models. The rapid growth of this field makes it difficult to stay abreast of all recent developments. In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and establish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications. Specifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. Lastly, we discuss available datasets, applications, and open challenges. We hope this survey will help readers explore this exciting topic and foster further advancements in the field of 3D content generation.																																	2024-05-01	PPRN:87437284		
J	Zhang, Jianyi; Vahidian, Saeed; Kuo, Martin; Li, Chunyuan; Zhang, Ruiyi; Yu, Tong; Zhou, Yufan; Wang, Guoyin; Chen, Yiran				Zhang, Jianyi/AAE-8396-2020; Chen, Yiran/L-4812-2017; Zhou, Yufan/AFR-9024-2022; Li, Chunyuan/AAG-1303-2020; Zhang, Ruiyi/JNR-1096-2023						Towards Building the Federated GPT: Federated Instruction Tuning								Arxiv											2	2;2024-01-29;https://www.arxiv.org/abs/2305.05644v2| 1;2023-05-09;https://www.arxiv.org/abs/2305.05644v1	arXiv:2305.05644			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	While "instruction-tuned" generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality instructions, especially when it comes to human-written instructions, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts. To tackle this issue, our study introduces a new approach called Federated Instruction Tuning (FedIT), which leverages federated learning (FL) as the learning framework for the instruction tuning of LLMs. This marks the first exploration of FL-based instruction tuning for LLMs. This is especially important since text data is predominantly generated by end users. For example, collecting extensive amounts of everyday user conversations in different languages can be a useful approach to improving the generalizability of LLMs, allowing them to generate authentic and natural responses. Therefore, it is imperative to design and adapt FL approaches to effectively leverage these users’ diverse instructions stored on local devices, while preserving privacy and ensuring data security. In the current paper, by conducting widely used GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous and diverse sets of instructions on the client’s end with the proposed framework FedIT, we improved the performance of LLMs compared to centralized training with only limited local instructions. Further, in this paper, we developed a Github repository named Shepherd. This repository offers a foundational framework for exploring federated fine-tuning of LLMs using heterogeneous instructions across diverse categories. The framework is designed for ease of use, adaptability, and scalability to accommodate large datasets. Additionally, it facilitates the seamless integration of novel algorithms and configurations, making it a convenient tool for researchers and practitioners in the NLP community.																																	2024-05-25	PPRN:68808427		
J	Bsharat, Sondos Mahmoud; Myrzakhan, Aidar; Shen, Zhiqiang										Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4								Arxiv											2	2;2024-01-18;https://www.arxiv.org/abs/2312.16171v2| 1;2023-12-26;https://www.arxiv.org/abs/2312.16171v1	arXiv:2312.16171			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 18 2024	2024	This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.																																	2024-05-25	PPRN:86820335		
J	Kurakin, Alexey; Ponomareva, Natalia; Syed, Umar; Macdermed, Liam; Terzis, Andreas				Terzis, Andreas/A-3348-2010						Harnessing large-language models to generate private synthetic text								Arxiv											2	2;2024-01-11;https://www.arxiv.org/abs/2306.01684v2| 1;2023-06-02;https://www.arxiv.org/abs/2306.01684v1	arXiv:2306.01684			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	Differentially private training algorithms like DP-SGD protect sensitive training data by ensuring that trained models do not reveal private information. An alternative approach, which this paper studies, is to use a sensitive dataset to generate synthetic data that is differentially private with respect to the original data, and then non-privately training a model on the synthetic data. Doing so has several advantages: synthetic data can be reused for other tasks (including for hyper parameter tuning), retained indefinitely, and shared with third parties without sacrificing privacy. However, generating private synthetic data is much harder than training a private model. To improve performance on text data, recent work has utilized public data by starting with a pre-trained generative language model and privately fine-tuning it on sensitive data. This model can be used to sample a DP synthetic dataset. While this strategy seems straightforward, executing it has proven problematic. Previous approaches either show significant performance loss, or have, as we show, critical design flaws. In this paper we demonstrate that a proper training objective along with tuning fewer parameters results in excellent DP synthetic data quality. Our approach is competitive with direct DP-training of downstream classifiers in terms of performance on downstream tasks. Further, we demonstrate that our DP synthetic data is not only useful for downstream classifier training, but also to tune those same models.																																	2024-01-26	PPRN:72841173		
J	Zhang, Ceyao; Yang, Kaijie; Hu, Siyi; Wang, Zihao; Li, Guanghe; Sun, Yihang; Zhang, Cheng; Zhang, Zhaowei; Liu, Anji; Zhu, Song-Chun; Chang, Xiaojun; Zhang, Junge; Yin, Feng; Liang, Yitao; Yang, Yaodong				Hu, Siyi/AAG-4361-2020; Liu, Anji/LEM-6968-2024; Wang, Zihao/S-7875-2019; Yang, Kaijie/HJP-4591-2023; Chang, Xiaojun/A-2055-2015						ProAgent: Building Proactive Cooperative Agents with Large Language Models								Arxiv											3	3;2024-01-11;https://www.arxiv.org/abs/2308.11339v3| 2;2023-08-28;https://www.arxiv.org/abs/2308.11339v2| 1;2023-08-22;https://www.arxiv.org/abs/2308.11339v1	arXiv:2308.11339			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art method.																																	2024-01-26	PPRN:82346252		
J	Xia, Wenhan; Qin, Chengwei; Hazan, Elad				qin, chengwei/MBV-9309-2025						Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning								Arxiv											1	1;2024-01-08;https://www.arxiv.org/abs/2401.04151v1	arXiv:2401.04151			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 08 2024	2024	Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model’s scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks. We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as empirical results to validate the effectiveness of our algorithm. Across various models (OPT and llama-2) and seven bench marking tasks, we demonstrate that COLA can consistently outperform LoRA without additional computational or memory costs.																																	2024-01-25	PPRN:87077827		
J	Hu, Cunchen; Huang, Heyang; Hu, Junhao; Xu, Jiang; Chen, Xusheng; Xie, Tao; Wang, Chenxi; Wang, Sa; Bao, Yungang; Sun, Ninghui; Shan, Yizhou				Shan, Yizhou/AAF-3021-2020						MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool								Arxiv											2	2;2024-12-21;https://www.arxiv.org/abs/2406.17565v3| 1;2024-06-26;https://www.arxiv.org/abs/2406.17565v2	arXiv:2406.17565			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 21 2024	2024	Large language model (LLM) serving has transformed from stateless to stateful systems, utilizing techniques like context caching and disaggregated inference. These optimizations extend the lifespan and domain of the KV cache, necessitating a new architectural approach. We present MemServe, a unified system that integrates both inter-request and intra-request optimizations. MemServe introduces MemPool, an elastic memory pool managing distributed memory and KV caches across serving instances. Using MemPool APIs, MemServe combines context caching with disaggregated inference for the first time, supported by a global scheduler that enhances cache reuse through a global prompt tree-based locality-aware policy. Tests show that MemServe significantly improves job completion time and time-to-first-time.																																	2025-01-31	PPRN:89872794		
J	Bertoni, Christian; Haferkamp, Jonas; Hinsche, Marcel; Ioannou, Marios; Eisert, Jens; Pashayan, Hakop				Pashayan, Hakop/IAP-7526-2023						Shallow shadows: Expectation estimation using low-depth random Clifford circuits								Arxiv											3	3;2024-12-20;https://www.arxiv.org/abs/2209.12924v3| 2;2023-04-11;https://www.arxiv.org/abs/2209.12924v2| 1;2022-09-26;https://www.arxiv.org/abs/2209.12924v2	arXiv:2209.12924			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 20 2024	2024	We provide practical and powerful schemes for learning many properties of an unknown n-qubit quantum state using a sparing number of copies of the state. Specifically, we present a depth-modulated randomized measurement scheme that interpolates between two known classical shadows schemes based on random Pauli measurements and random Clifford measurements. These can be seen within our scheme as the special cases of zero and infinite depth respectively. We focus on the regime where depth scales logarithmically in n and provide evidence that this retains the desirable properties of both extremal schemes whilst, in contrast to the random Clifford scheme, also being experimentally feasible. We present methods for two key tasks; estimating expectation values of certain observables from generated classical shadows and, computing upper bounds on the depth-modulated shadow norm, thus providing rigorous guarantees on the accuracy of the output estimates. We consider observables that can be written as a linear combination of poly(n) Paulis and observables that can be written as a low bond dimension matrix product operator. For the former class of observables both tasks are solved efficiently in n . For the latter class, we do not guarantee efficiency but present a tensor network method that works in practice; by variationally computing a heralded approximate inverse of a tensor network.																																	2025-01-29	PPRN:58027885		
J	Liu, Haoxiong; Zhang, Yifan; Luo, Yifan; Yao, Andrew Chi-Chih				Zhang, Yifan/KMY-8838-2024; Luo, Yifan/AAY-9039-2020						Augmenting Math Word Problems via Iterative Question Composing								Arxiv											5	5;2024-12-16;https://www.arxiv.org/abs/2401.09003v5| 4;2024-02-11;https://www.arxiv.org/abs/2401.09003v4| 3;2024-01-30;https://www.arxiv.org/abs/2401.09003v3| 2;2024-01-20;https://www.arxiv.org/abs/2401.09003v2| 1;2024-01-17;https://www.arxiv.org/abs/2401.09003v1	arXiv:2401.09003			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 16 2024	2024	Despite the advancements in large language models (LLMs) for mathematical reasoning, solving competition-level math problems remains a significant challenge, especially for open-source LLMs without external tools. We introduce the MMIQC dataset, comprising a mixture of processed web data and synthetic question-response pairs, aimed at enhancing the mathematical reasoning capabilities of base language models. Models fine-tuned on MMIQC consistently surpass their counterparts in performance on the MATH benchmark across various model sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding the previous open-source state-of-the-art by 8.2% and outperforming the initial version GPT-4 released in 2023. Extensive evaluation results on Hungarian high school finals suggest that such improvement can generalize to unseen data. Our ablation study on MMIQC reveals that a large part of the improvement can be attributed to our novel augmentation method, Iterative Question Composing (IQC), which involves iteratively composing new questions from seed problems using an LLM and applying rejection sampling through another LLM.																																	2025-01-24	PPRN:87209857		
J	Bussmann, Bart; Leask, Patrick; Nanda, Neel										BatchTopK Sparse Autoencoders								Arxiv											1	1;2024-12-09;https://www.arxiv.org/abs/2412.06410v1	arXiv:2412.06410			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 09 2024	2024	Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting language model activations by decomposing them into sparse, interpretable features. A popular approach is the TopK SAE, that uses a fixed number of the most active latents per sample to reconstruct the model activations. We introduce BatchTopK SAEs, a training method that improves upon TopK SAEs by relaxing the top-k constraint to the batch-level, allowing for a variable number of latents to be active per sample. As a result, BatchTopK adaptively allocates more or fewer latents depending on the sample, improving reconstruction without sacrificing average sparsity. We show that BatchTopK SAEs consistently outperform TopK SAEs in reconstructing activations from GPT-2 Small and Gemma 2 2B, and achieve comparable performance to state-of-the-art JumpReLU SAEs. However, an advantage of BatchTopK is that the average number of latents can be directly specified, rather than approximately tuned through a costly hyperparameter sweep. 																																	2025-01-17	PPRN:119791294		
J	Gallotta, Roberto; Todd, Graham; Zammit, Marvin; Earle, Sam; Liapis, Antonios; Togelius, Julian; Yannakakis, Georgios N.				Gallotta, Roberto/HLW-5877-2023						Large Language Models and Games: A Survey and Roadmap								Arxiv											5	5;2024-12-09;https://www.arxiv.org/abs/2402.18659v5| 4;2024-10-01;https://www.arxiv.org/abs/2402.18659v4| 3;2024-09-12;https://www.arxiv.org/abs/2402.18659v3| 2;2024-07-15;https://www.arxiv.org/abs/2402.18659v2| 1;2024-02-28;https://www.arxiv.org/abs/2402.18659v1	arXiv:2402.18659			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 09 2024	2024	Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.																																	2025-01-17	PPRN:88007147		
J	Dong, Juechu; Feng, Boyuan; Guessous, Driss; Liang, Yanbo; He, Horace				Liang, Yanbo/JWA-2953-2024						Flex Attention: A Programming Model for Generating Optimized Attention Kernels								Arxiv											1	1;2024-12-07;https://www.arxiv.org/abs/2412.05496v1	arXiv:2412.05496			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 07 2024	2024	Over the past 7 years, attention has become one of the most important primitives in deep learning. The primary approach to optimize attention is FlashAttention, which fuses the operation together, drastically improving both the runtime and the memory consumption. However, the importance of FlashAttention combined with its monolithic nature poses a problem for researchers aiming to try new attention variants -- a "software lottery". This problem is exacerbated by the difficulty of writing efficient fused attention kernels, resisting traditional compiler-based approaches. We introduce FlexAttention, a novel compiler-driven programming model that allows implementing the majority of attention variants in a few lines of idiomatic PyTorch code. We demonstrate that many existing attention variants (e.g. Alibi, Document Masking, PagedAttention, etc.) can be implemented via FlexAttention, and that we achieve competitive performance compared to these handwritten kernels. Finally, we demonstrate how FlexAttention allows for easy composition of attention variants, solving the combinatorial explosion of attention variants.																																	2025-01-17	PPRN:119792743		
J	Xu, Yuzhuang; Han, Xu; Yang, Zonghan; Wang, Shuo; Zhu, Qingfu; Liu, Zhiyuan; Liu, Weidong; Che, Wanxiang				Liu, Weidong/AAE-9153-2019; 朱, 庆福/HGE-1466-2022; XIN, WANG/KGK-5385-2024; Liu, Zhiyuan/I-2233-2014						OneBit: Towards Extremely Low-bit Large Language Models								Arxiv											5	5;2024-11-29;https://www.arxiv.org/abs/2402.11295v6| 4;2024-11-04;https://www.arxiv.org/abs/2402.11295v5| 3;2024-10-28;https://www.arxiv.org/abs/2402.11295v4| 2;2024-05-22;https://www.arxiv.org/abs/2402.11295v3| 1;2024-02-17;https://www.arxiv.org/abs/2402.11295v1	arXiv:2402.11295			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 29 2024	2024	Model quantification uses low bit-width values to represent the weight matrices of existing models to be quantized, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, current quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit model compressing framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the quantization framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 81% of the non-quantized performance on LLaMA models) with robust training processes when only using 1-bit weight matrices.																																	2025-01-11	PPRN:87762435		
J	Chi, Jianfeng; Karn, Ujjwal; Zhan, Hongyuan; Smith, Eric; Rando, Javier; Zhang, Yiming; Plawiak, Kate; Coudert, Zacharie Delpierre; Upasani, Kartikeya; Pasupuleti, Mahesh				Karn, Ujjwal/KMY-3539-2024						Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations								Arxiv											1	1;2024-11-15;https://www.arxiv.org/abs/2411.10414v1	arXiv:2411.10414			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 15 2024	2024	We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for human-AI conversations that involves image understanding: it can be used to safeguard content for both multimodal LLM inputs (prompt classification) and outputs (response classification). Unlike the previous text-only Llama Guard versions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed to support image reasoning use cases and is optimized to detect harmful multimodal (text and image) prompts and text responses to these prompts. Llama Guard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong performance on the internal benchmarks using the MLCommons taxonomy. We also test its robustness against adversarial attacks. We believe that Llama Guard 3 Vision serves as a good starting point to build more capable and robust content moderation tools for human-AI conversation with multimodal capabilities.																																	2024-12-25	PPRN:119242489		
J	Zhang, Zhexin; Yang, Junxiao; Ke, Pei; Cui, Shiyao; Zheng, Chujie; Wang, Hongning; Huang, Minlie				Zheng, Chujie/CAG-9031-2022; Cui, Shiyao/AAP-2707-2021; Wang, Hongning/GPK-7527-2022						Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks								Arxiv											2	2;2024-11-05;https://www.arxiv.org/abs/2407.02855v2| 1;2024-07-03;https://www.arxiv.org/abs/2407.02855v1	arXiv:2407.02855			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 05 2024	2024	LLMs are known to be vulnerable to jailbreak attacks, even after safety alignment. An important observation is that, while different types of jailbreak attacks can generate significantly different queries, they mostly result in similar responses that are rooted in the same harmful knowledge (e.g., detailed steps to make a bomb). Therefore, we conjecture that directly unlearn the harmful knowledge in the LLM can be a more effective way to defend against jailbreak attacks than the mainstream supervised fine-tuning (SFT) approaches. Our extensive experiments demonstrate the surprising generalizability of our unlearning-based approach: using only 20 raw harmful questions without any jailbreak prompt during training, our solution reduced the Attack Success Rate (ASR) in Vicuna-7B from 82.6% to 7.7% on out-of-distribution (OOD) harmful questions wrapped with various complex jailbreak prompts . This significantly outperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but still has an ASR of 21.9% even under the help of an additional safety system prompt. Further analysis reveals that the generalization ability of our solution may stem from the intrinsic relatedness among harmful responses across harmful questions (e.g., response patterns, shared steps and actions in response, and similarity among their learned representations in the LLM). 																																	2024-12-09	PPRN:90691094		
J	Chen, Jwo-Sy; Nielsen, Erik; Ebert, Matthew; Inlek, Volkan; Wright, Kenneth; Chaplin, Vandiver; Maksymov, Andrii; Paez, Eduardo; Poudel, Amrit; Maunz, Peter; Gamble, John				Maksymov, Andrii/JCD-5777-2023						Benchmarking a trapped-ion quantum computer with 30 qubits								Arxiv											2	2;2024-11-02;https://www.arxiv.org/abs/2308.05071v2| 1;2023-08-09;https://www.arxiv.org/abs/2308.05071v1	arXiv:2308.05071			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 02 2024	2024	Quantum computers are rapidly becoming more capable, with dramatic increases in both qubit count [1] and quality [2]. Among different hardware approaches, trapped-ion quantum processors are a leading technology for quantum computing, with established high-fidelity operations and architectures with promising scaling. Here, we demonstrate and thoroughly benchmark the IonQ Forte system: configured as a single- chain 30-qubit trapped-ion quantum computer with all-to-all operations. We assess the performance of our quantum computer operation at the component level via direct randomized benchmarking (DRB) across all (30 2) = 435 gate pairs. We then show the results of application-oriented [3, 4] benchmarks and show that the system passes the suite of algorithmic qubit (AQ) benchmarks up to #AQ 29. Finally, we use our component-level benchmarking to build a system-level model to predict the application benchmarking data through direct simulation. While we find that the system-level model correlates with the experiment in predicting application circuit performance, we note quantitative discrepancies indicating significant out-of-model errors, leading to higher predicted performance than what is observed. This highlights that as quantum computers move toward larger and higher- quality devices, characterization becomes more challenging, suggesting future work required to push performance further.																																	2024-12-16	PPRN:74922588		
J	Miller, Evan										Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations								Arxiv											1	1;2024-11-01;https://www.arxiv.org/abs/2411.00640v1	arXiv:2411.00640			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.																																	2024-12-09	PPRN:119007163		
J	Li, Xiner; Zhao, Yulai; Wang, Chenyu; Scalia, Gabriele; Eraslan, Gokcen; Nair, Surag; Biancalani, Tommaso; Ji, Shuiwang; Regev, Aviv; Levine, Sergey; Uehara, Masatoshi				ZHAO, YULAI/JLL-5344-2023; Biancalani, Tommaso/Q-2010-2016; Nair, Surag/AFM-6683-2022; Li, Xiner/IUO-5870-2023; Uehara, Masatoshi/ISS-6270-2023; Scalia, Gabriele/AAS-9964-2020						Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding								Arxiv											4	4;2024-10-25;https://www.arxiv.org/abs/2408.08252v5| 3;2024-10-03;https://www.arxiv.org/abs/2408.08252v4| 2;2024-09-12;https://www.arxiv.org/abs/2408.08252v3| 1;2024-08-15;https://www.arxiv.org/abs/2408.08252v1	arXiv:2408.08252			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 25 2024	2024	Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require ``differentiable'' proxy models (textit{e.g.}, classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (textit{e.g.}, classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. 																																	2024-11-30	PPRN:91414948		
J	Parthasarathy, Venkatesh Balavadhani; Zafar, Ahtsham; Khan, Aafaq; Shahid, Arsalan				Shahid, Arsalan/J-7446-2019; Zafar, Ahtsham/HDM-9425-2022						The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities								Arxiv											2	2;2024-10-21;https://www.arxiv.org/abs/2408.13296v2| 1;2024-08-23;https://www.arxiv.org/abs/2408.13296v1	arXiv:2408.13296			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 21 2024	2024	This technical report thoroughly examines the process of fine-tuning Large Language Models (LLMs), integrating theoretical insights and practical applications. It begins by tracing the historical development of LLMs, emphasising their evolution from traditional Natural Language Processing (NLP) models and their pivotal role in modern AI systems. The analysis differentiates between various fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, underscoring their respective implications for specific tasks. A structured seven-stage pipeline for LLM fine-tuning is introduced, covering the complete lifecycle from data preparation to model deployment. Key considerations include data collection strategies, handling of imbalanced datasets, model initialisation, and optimisation techniques, with a particular focus on hyperparameter tuning. The report also highlights parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with optimal model performance. The exploration extends to advanced fine-tuning techniques and configurations like memory finetuning, Mixture of Experts (MoE) and Mixture of Agents (MoA), demonstrating how these methods harness specialised networks and multi-agent collaboration for improved outcomes. Proximal Policy Optimisation (PPO) and Direct Preference Optimisation (DPO) are discussed as innovative approaches to aligning models with human preferences, while the benefits of pruning and routing optimisations are examined for enhancing efficiency. In the latter sections, the report delves into validation frameworks, post-deployment monitoring, and optimisation techniques for inference. It also addresses the deployment of LLMs on distributed and cloud-based platforms. Additionally, cutting-edge topics such as multimodal LLMs and fine-tuning for audio and speech processing are covered, alongside emerging challenges related to scalability, privacy, and accountability. This report aims to serve as a comprehensive guide for researchers and practitioners, offering actionable insights into fine-tuning LLMs while navigating the challenges and opportunities inherent in this rapidly evolving field.																																	2024-11-20	PPRN:91546669		
J	Rouze, Cambyse; Franca, Daniel Stilck; Alhambra, Alvaro M.				Alhambra, Alvaro/AHE-1977-2022						Efficient thermalization and universal quantum computing with quantum Gibbs samplers								Arxiv											2	2;2024-10-21;https://www.arxiv.org/abs/2403.12691v2| 1;2024-03-19;https://www.arxiv.org/abs/2403.12691v1	arXiv:2403.12691			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 21 2024	2024	The preparation of thermal states of matter is a crucial task in quantum simulation. In this work, we prove that a recently introduced, efficiently implementable dissipative evolution thermalizes to the Gibbs state in time scaling polynomially with system size at high enough temperatures for any Hamiltonian that satisfies a Lieb-Robinson bound, such as local Hamiltonians on a lattice. Furthermore, we show the efficient adiabatic preparation of the associated purifications or "thermofield double" states. To the best of our knowledge, these are the first results rigorously establishing the efficient preparation of high-temperature Gibbs states and their purifications. In the low-temperature regime, we show that implementing this family of dissipative evolutions for inverse temperatures polynomial in the system's size is computationally equivalent to standard quantum computations. On a technical level, for high temperatures, our proof makes use of the mapping of the generator of the evolution into a Hamiltonian, and then connecting its convergence to that of the infinite temperature limit. For low temperature, we instead perform a perturbation at zero temperature and resort to circuit-to-Hamiltonian mappings akin to the proof of universality of quantum adiabatic computing. Taken together, our results show that a family of quasi-local dissipative evolutions efficiently prepares a large class of quantum many-body states of interest, and has the potential to mirror the success of classical Monte Carlo methods for quantum many-body systems.																																	2024-11-20	PPRN:88240031		
J	Wang, Ke; Zhu, Jiahui; Ren, Minjie; Liu, Zeming; Li, Shiwei; Zhang, Zongye; Zhang, Chenkai; Wu, Xiaoyu; Zhan, Qiqi; Liu, Qingjie; Wang, Yunhong				Liu, Qingjie/IVH-7937-2023; Wang, Yunhong/MZS-0685-2025; Ren, Minjie/GQQ-3784-2022; Wang, Ke/AAW-4079-2020; Jiahui, Zhu/KGK-7056-2024; Zhang, Zongye/JCE-7720-2023						A Survey on Data Synthesis and Augmentation for Large Language Models								Arxiv											1	1;2024-10-16;https://www.arxiv.org/abs/2410.12896v1	arXiv:2410.12896			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 16 2024	2024	The success of Large Language Models (LLMs) is inherently linked to the availability of vast, diverse, and high-quality data for training and evaluation. However, the growth rate of high-quality data is significantly outpaced by the expansion of training datasets, leading to a looming data exhaustion crisis. This underscores the urgent need to enhance data efficiency and explore new data sources. In this context, synthetic data has emerged as a promising solution. Currently, data generation primarily consists of two major approaches: data augmentation and synthesis. This paper comprehensively reviews and summarizes data generation techniques throughout the lifecycle of LLMs, including data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and applications. Furthermore, We discuss the current constraints faced by these methods and investigate potential pathways for future development and research. Our aspiration is to equip researchers with a clear understanding of these methodologies, enabling them to swiftly identify appropriate data generation strategies in the construction of LLMs, while providing valuable insights for future exploration.																																	2024-11-13	PPRN:115477580		
J	Lee, Donghyun; Tiwari, Mo										Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems								Arxiv											1	1;2024-10-09;https://www.arxiv.org/abs/2410.07283v1	arXiv:2410.07283			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 09 2024	2024	As Large Language Models (LLMs) grow increasingly powerful, multi-agent systems are becoming more prevalent in modern AI applications. Most safety research, however, has focused on vulnerabilities in single-agent LLMs. These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim's application. In this paper, we reveal a more dangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We introduce Prompt Infection, a novel attack where malicious prompts self-replicate across interconnected agents, behaving much like a computer virus. This attack poses severe threats, including data theft, scams, misinformation, and system-wide disruption, all while propagating silently through the system. Our extensive experiments demonstrate that multi-agent systems are highly susceptible, even when agents do not publicly share all communications. To address this, we propose LLM Tagging, a defense mechanism that, when combined with existing safeguards, significantly mitigates infection spread. This work underscores the urgent need for advanced security measures as multi-agent LLM systems become more widely adopted.																																	2024-11-02	PPRN:105778288		
J	Zhang, Yimeng; Chen, Xin; Jia, Jinghan; Zhang, Yihua; Fan, Chongyu; Liu, Jiancheng; Hong, Mingyi; Ding, Ke; Liu, Sijia				Liu, Sijia/HOC-2459-2023; Liu, JC/LPI-0149-2024						Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models								Arxiv											3	3;2024-10-09;https://www.arxiv.org/abs/2405.15234v3| 2;2024-06-14;https://www.arxiv.org/abs/2405.15234v2| 1;2024-05-24;https://www.arxiv.org/abs/2405.15234v1	arXiv:2405.15234			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 09 2024	2024	Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning , also known as concept erasing , have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn . However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs’ image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn . Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at https://github.com/OPTML-Group/AdvUnlearn .																																	2024-10-30	PPRN:89018797		
J	Chen, Guoxin; Liao, Minpeng; Li, Chengxi; Fan, Kai										Step-level Value Preference Optimization for Mathematical Reasoning								Arxiv											2	2;2024-09-27;https://www.arxiv.org/abs/2406.10858v2| 1;2024-06-16;https://www.arxiv.org/abs/2406.10858v1	arXiv:2406.10858			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 27 2024	2024	Direct Preference Optimization (DPO) using an implicit reward model has proven to be an effective alternative to reinforcement learning from human feedback (RLHF) for fine-tuning preference aligned large language models (LLMs). However, the overall preference annotations of responses do not fully capture the fine-grained quality of model outputs in complex multi-step reasoning tasks, such as mathematical reasoning. To address this limitation, we introduce a novel algorithm called Step-level Value Preference Optimization (SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically annotate step-level preferences for multi-step reasoning. Furthermore, from the perspective of learning-to-rank, we train an explicit value model to replicate the behavior of the implicit reward model, complementing standard preference optimization. This value model enables the LLM to generate higher reward responses with minimal cost during inference. Experimental results demonstrate that our method achieves state-of-the-art performance on both in-domain and out-of-domain mathematical reasoning benchmarks. 																																	2024-10-09	PPRN:89351950		
J	Martins, Pedro Henrique; Fernandes, Patrick; Alves, Joao; Guerreiro, Nuno M.; Rei, Ricardo; Alves, Duarte M.; Pombal, Jose; Farajian, Amin; Faysse, Manuel; Klimaszewski, Mateusz; Colombo, Pierre; Haddow, Barry; de Souza, Jose G.C.; Birch, Alexandra; Martins, Andre F.T.				Torres Martins, Andre Filipe/JXL-9782-2024; Alves, João/AAH-7435-2019						EuroLLM: Multilingual Language Models for Europe								Arxiv											1	1;2024-09-24;https://www.arxiv.org/abs/2409.16235v1	arXiv:2409.16235			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 24 2024	2024	The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on multilingual general benchmarks and machine translation.																																	2024-10-07	PPRN:98862239		
J	Miao, Wang; Shi, Xu; Li, Yilin; Tchetgen, Eric Tchetgen				Miao, Wang/Z-6061-2019; Tchetgen, Eric/AAF-6160-2021; Shi, Xu/E-6789-2019; Li, Yilin/AAW-9533-2021						A confounding bridge approach for double negative control inference on causal effects								Arxiv											2	2;2024-09-06;https://www.arxiv.org/abs/1808.04945v4| 1;2020-09-18;https://www.arxiv.org/abs/1808.04945v3	arXiv:1808.04945			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 06 2024	2024	Unmeasured confounding is a key challenge for causal inference. In this paper, we establish a framework for unmeasured confounding adjustment with negative control variables. A negative control outcome is associated with the confounder but not causally affected by the exposure in view, and a negative control exposure is correlated with the primary exposure or the confounder but does not causally affect the outcome of interest. We introduce an outcome confounding bridge function that depicts the relationship between the confounding effects on the primary outcome and the negative control outcome, and we incorporate a negative control exposure to identify the bridge function and the average causal effect. We also consider the extension to the positive control setting by allowing for nonzero causal effect of the primary exposure on the control outcome. We illustrate our approach with simulations and apply it to a study about the short-term effect of air pollution on mortality. Although a standard analysis shows a significant acute effect of PM2.5 on mortality, our analysis indicates that this effect may be confounded, and after double negative control adjustment, the effect is attenuated toward zero.																																	2024-09-23	PPRN:13121073		
J	Yan, Hanshu; Liu, Xingchao; Pan, Jiachun; Liew, Jun Hao; Liu, Qiang; Feng, Jiashi				chiang, liu/HKN-0883-2023						PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator								Arxiv											4	4;2024-09-02;https://www.arxiv.org/abs/2405.07510v5| 3;2024-08-13;https://www.arxiv.org/abs/2405.07510v4| 2;2024-05-29;https://www.arxiv.org/abs/2405.07510v3| 1;2024-05-22;https://www.arxiv.org/abs/2405.07510v2	arXiv:2405.07510			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 02 2024	2024	We present Piecewise Rectified Flow (PeRFlow), a flow-based method for accelerating diffusion models. PeRFlow divides the sampling process of generative flows into several time windows and straightens the trajectories in each interval via the reflow operation, thereby approaching piecewise linear flows. PeRFlow achieves superior performance in a few-step generation. Moreover, through dedicated parameterizations, the PeRFlow models inherit knowledge from the pretrained diffusion models. Thus, the training converges fast and the obtained models show advantageous transfer ability, serving as universal plug-and-play accelerators that are compatible with various workflows based on the pre-trained diffusion models. 1																																	2024-09-11	PPRN:88988962		
J	Jin, Ye; Yang, Ruoxuan; Yi, Zhijie; Shen, Xiaoxi; Peng, Huiling; Liu, Xiaoan; Qin, Jingli; Li, Jiayang; Xie, Jintao; Gao, Peizhong; Zhou, Guyue; Gong, Jiangtao										SurrealDriver: Designing LLM-powered Generative Driver Agent Framework based on Human Drivers' Driving-thinking Data								Arxiv											2	2;2024-07-22;https://www.arxiv.org/abs/2309.13193v2| 1;2023-09-22;https://www.arxiv.org/abs/2309.13193v1	arXiv:2309.13193			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 22 2024	2024	Leveraging advanced reasoning capabilities and extensive world knowledge of large language models (LLMs) to construct generative agents for solving complex real-world problems is a major trend. However, LLMs inherently lack embodiment as humans, resulting in suboptimal performance in many embodied decision-making tasks. In this paper, we introduce a framework for building human-like generative driving agents using post-driving self-report driving-thinking data from human drivers as both demonstration and feedback. To capture high-quality, natural language data from drivers, we conducted urban driving experiments, recording drivers' verbalized thoughts under various conditions to serve as chain-of-thought prompts and demonstration examples for the LLM-Agent. The framework's effectiveness was evaluated through simulations and human assessments. Results indicate that incorporating expert demonstration data significantly reduced collision rates by 81.04% and increased human likeness by 50% compared to a baseline LLM-based agent. Our study provides insights into using natural language-based human demonstration data for embodied tasks.																																	2024-07-27	PPRN:85199772		
J	Ye, Gen; Martinelli, Matteo; Hu, Bin; Silvestri, Alessandra				Silvestri, Alessandra/HHC-1275-2022; Ye, Gen/IXN-6931-2023; Martinelli, Matteo/U-7309-2018						Non-minimally coupled gravity as a physically viable fit to DESI 2024 BAO								Arxiv											1	1;2024-07-22;https://www.arxiv.org/abs/2407.15832v1	arXiv:2407.15832			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 22 2024	2024	The recent measurements of baryon acoustic oscillations (BAO) from the DESI collaboration have presented an indication for dynamical dark energy, when adopting the (w0 , wa) parametrization of the equation of state. The associated posterior constraints imply a crossing of the phantom divide. The latter, however, has profound theoretical implications because not all models can do so without developing incurable instabilities. Simple quintessence models of dark energy, for instance, would be ruled out if such a crossing is confirmed. We perform a non-parametric reconstruction of the equation of state, and confirm that crossing of the phantom divide is required by the DESI BAO data. We then explore the theory space of Horndeski gravity employing a reconstruction method based on the effective field theory of dark energy, and show that for most of the models it is still difficult to safely cross the divide. We identify non-minimal coupling to gravity as the key modification which sustains a stable phantom crossing in the general Horndeski theory space and fits DESI observations. Guided by these insights, we propose the Thawing Gravity model which has the same number of parameters as w0 wa CDM and naturally realizes non-minimal coupling when dark energy becomes non-negligible. Thawing Gravity improves the fit over ΛCDM for DESI BAO, CMB as well as type Ia Supernovae.																																	2024-07-28	PPRN:91025907		
J	Wang, Zheng; Jin, Boxiao; Yu, Zhongzhi; Zhang, Minjia				Zhang, Minjia/MFH-5718-2025						Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks								Arxiv											2	2;2024-07-21;https://www.arxiv.org/abs/2407.08454v2| 1;2024-07-11;https://www.arxiv.org/abs/2407.08454v1	arXiv:2407.08454			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 21 2024	2024	Large Language Models (LLMs) have attracted remarkable attention due to their unprecedented performance across a wide range of tasks. However, how to efficiently serve LLMs has become a pressing issue because of their huge computational cost in their autoregressive generation process. To mitigate computational costs, LLMs often employ the KV Cache technique to improve the generation speed. While improving the computational efficiency, the storage requirements of the KV cache are substantial, particularly in long-context scenarios, leading to significant memory consumption. Existing KV cache eviction methods often degrade the performance of LLMs in long-context scenarios due to the information loss introduced by eviction. In this paper, we propose a novel KV cache merging approach, called KVMerger, , to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets. Our approach is inspired by the intriguing observation that key states exhibit high similarity at the token level within a single sequence. To facilitate merging, we develop an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging. Our merging set identification algorithm stimulates the second observation that KV cache sparsity, from similarity perspective, is independent of the dataset and remains persistent at the model level. Subsequently, we propose a Gaussian kernel weighted merging algorithm to selectively merge all states within each merging set. We conduct extensive experiments to demonstrate the effectiveness of KVMerger for longcontext tasks under constrained memory budgets, applying it to models including Llama2-7B/13B-chat and Mistral-7B-instruct. Using the LongBench and ZeroScroll benchmarks, we compare our method with other KV cache compression techniques, including H2O and CaM, showing that our method achieves superior performance across tasks with both 50% and 35% KV cache budgets.																																	2024-07-28	PPRN:90769887		
J	Fu, Qichen; Cho, Minsik; Merth, Thomas; Mehta, Sachin; Rastegari, Mohammad; Najibi, Mahyar										LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference								Arxiv											1	1;2024-07-19;https://www.arxiv.org/abs/2407.14057v1	arXiv:2407.14057			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 19 2024	2024	The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.																																	2024-07-27	PPRN:91010720		
J	Bai, Tianyi; Liang, Hao; Wan, Binwang; Xu, Yanran; Li, Xi; Li, Shiyu; Yang, Ling; Li, Bozhou; Wang, Yifan; Cui, Bin; Huang, Ping; Shan, Jiulong; He, Conghui; Yuan, Binhang; Zhang, Wentao				Zhang, Wentao/AGC-4653-2022; He, Conghui/AAZ-3323-2021; Yuan, Binhang/OGP-8450-2025						A Survey of Multimodal Large Language Model from A Data-centric Perspective								Arxiv											2	2;2024-07-18;https://www.arxiv.org/abs/2405.16640v2| 1;2024-05-26;https://www.arxiv.org/abs/2405.16640v1	arXiv:2405.16640			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 18 2024	2024	Multimodal large language models (MLLMs) enhance the capabilities of standard large language models by integrating and processing data from multiple modalities, including text, vision, audio, video, and 3D environments. Data plays a pivotal role in the development and refinement of these models. In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective. Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs. Additionally, we analyze the evaluation methods for the datasets and review the benchmarks for evaluating MLLMs. Our survey also outlines potential future research directions. This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.																																	2024-07-26	PPRN:89047643		
J	Chauhan, Vinod Kumar; Zhou, Jiandong; Lu, Ping; Molaei, Soheila; Clifton, David A.				Chauhan, Vinod Kumar/O-5926-2018; Zhou, Jiandong/D-3087-2017						A Brief Review of Hypernetworks in Deep Learning								Arxiv											2	2;2024-07-13;https://www.arxiv.org/abs/2306.06955v3| 1;2023-06-12;https://www.arxiv.org/abs/2306.06955v1	arXiv:2306.06955			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 13 2024	2024	Hypernetworks, or hypernets for short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning. Despite their success across different problem settings, there is currently no comprehensive review available to inform researchers about the latest developments and to assist in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example of training deep neural networks using hypernets and propose categorizing hypernets based on five design criteria: inputs, outputs, variability of inputs and outputs, and the architecture of hypernets. We also review applications of hypernets across different deep learning problem settings, followed by a discussion of general scenarios where hypernets can be effectively employed. Finally, we discuss the challenges and future directions that remain underexplored in the field of hypernets. We believe that hypernetworks have the potential to revolutionize the field of deep learning. They offer a new way to design and train neural networks, and they have the potential to improve the performance of deep learning models on a variety of tasks. Through this review, we aim to inspire further advancements in deep learning through hypernetworks.																																	2024-07-23	PPRN:73301459		
J	Li, Shufan; Singh, Harkanwar; Grover, Aditya				Li, Shuangning/GLT-4682-2022						Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data								Arxiv											4	4;2024-07-13;https://www.arxiv.org/abs/2402.05892v5| 3;2024-03-20;https://www.arxiv.org/abs/2402.05892v4| 2;2024-03-14;https://www.arxiv.org/abs/2402.05892v3| 1;2024-02-08;https://www.arxiv.org/abs/2402.05892v1	arXiv:2402.05892			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 13 2024	2024	In recent years, Transformers have become the de-facto architecture for sequence modeling on text and a variety of multi-dimensional data, such as images and video. However, the use of self-attention layers in a Transformer incurs prohibitive compute and memory complexity that scales quadratically w.r.t. the sequence length. A recent architecture, Mamba, based on state space models has been shown to achieve comparable performance for modeling text sequences, while scaling linearly with the sequence length. In this work, we present Mamba-ND, a generalized design extending the Mamba architecture to arbitrary multi-dimensional data. Our design alternatively unravels the input data across different dimensions following row-major orderings. We provide a systematic comparison of Mamba-ND with several other alternatives, based on prior multi-dimensional extensions such as Bi-directional LSTMs and S4ND. Empirically, we show that Mamba-ND demonstrates performance competitive with the state-of-the-art on a variety of multi-dimensional benchmarks, including ImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather forecasting and BTCV 3D segmentation.																																	2024-07-23	PPRN:87572724		
J	Lu, Jinliang; Pang, Ziliang; Xiao, Min; Zhu, Yaochen; Xia, Rui; Zhang, Jiajun				Zhu, Yaochen/KGM-8619-2024						Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models								Arxiv											1	1;2024-07-08;https://www.arxiv.org/abs/2407.06089v1	arXiv:2407.06089			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 08 2024	2024	The remarkable success of Large Language Models (LLMs) has ushered natural language processing (NLP) research into a new era. Despite their diverse capabilities, LLMs trained on different corpora exhibit varying strengths and weaknesses, leading to challenges in maximizing their overall efficiency and versatility. To address these challenges, recent studies have explored collaborative strategies for LLMs. This paper provides a comprehensive overview of this emerging research area, highlighting the motivation behind such collaborations. Specifically, we categorize collaborative strategies into three primary approaches: Merging , Ensemble , and Cooperation . Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation leverages different LLMs to allow full play to their diverse capabilities for specific tasks. We provide in-depth introductions to these methods from different perspectives and discuss their potential applications. Additionally, we outline future research directions, hoping this work will catalyze further studies on LLM collaborations and paving the way for advanced NLP applications.																																	2024-07-21	PPRN:90741356		
J	Chen, Zhaorun; Du, Yichao; Wen, Zichen; Zhou, Yiyang; Cui, Chenhang; Weng, Zhenzhen; Tu, Haoqin; Wang, Chaoqi; Tong, Zhengwei; Huang, Qinglan; Chen, Canyu; Ye, Qinghao; Zhu, Zhihong; Zhang, Yuqing; Zhou, Jiawei; Zhao, Zhuokai; Rafailov, Rafael; Finn, Chelsea; Yao, Huaxiu				Yao, Huaxiu/V-3516-2019; Chen, Zhaorun/AAT-1611-2021; Zhao, Zhuokai/JLL-0434-2023; Du, Yichao/MSX-1374-2025; Chen, Canyu/KFT-0519-2024; Zhu, Zhihong/OIR-6326-2025						MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?								Arxiv											1	1;2024-07-05;https://www.arxiv.org/abs/2407.04842v1	arXiv:2407.04842			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 05 2024	2024	While text-to-image models like DALLE-3 and Stable Diffusion are rapidly proliferating, they often encounter challenges such as hallucination, bias, and the production of unsafe, low-quality output. To effectively address these issues, it is crucial to align these models with desired behaviors based on feedback from a multimodal judge. . Despite their significance, current multimodal judges frequently undergo inadequate evaluation of their capabilities and limitations, potentially leading to misalignment and unsafe fine-tuning outcomes. To address this issue, we introduce MJ-BENCH, ENCH , a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, we evaluate a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-BENCH. ENCH . 																																	2024-07-21	PPRN:90738443		
J	He, Xu Owen										Mixture of A Million Experts								Arxiv											1	1;2024-07-04;https://www.arxiv.org/abs/2407.04153v1	arXiv:2407.04153			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.																																	2025-08-07	PPRN:123172114		
J	Ying, Zonghao; Liu, Aishan; Zhang, Tianyuan; Yu, Zhengmin; Liang, Siyuan; Liu, Xianglong; Tao, Dacheng				Liang, Siyuan/KHW-1891-2024; Liu, Xianglong/NTQ-2427-2025; Tao, Dacheng/A-5449-2012; Zhang, Tianyuan/JZE-1561-2024						Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt								Arxiv											2	2;2024-07-01;https://www.arxiv.org/abs/2406.04031v2| 1;2024-06-06;https://www.arxiv.org/abs/2406.04031v1	arXiv:2406.04031			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi -Modal Adversarial Prompt Attack ( BAP ), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally adversarial perturbations in an image, guided by a few-shot query-agnostic corpus ( e . g ., affirmative prefixes and negative inhibitions). This process ensures that the adversarial image prompt LVLMs to respond positively to harmful queries. Subsequently, leveraging the image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our BAP significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black -box commercial LVLMs, such as Gemini and ChatGLM. Our code is available at https://github.com/NY1024/BAP-Jailbreak-Vision-Lan guage-Models-via-Bi-Modal-Adversarial-Prompt .																																	2024-07-18	PPRN:89260020		
J	Tihanyi, Norbert; Jain, Ridhi; Charalambous, Yiannis; Ferrag, Mohamed Amine; Sun, Youcheng; Cordeiro, Lucas C.				Jain, Ridhi/LLM-7092-2024; FERRAG, Mohamed/M-2909-2016; Tihanyi, Norbert/GNW-6860-2022; Cordeiro, Lucas/IVV-3044-2023						A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification								Arxiv											2	2;2024-06-27;https://www.arxiv.org/abs/2305.14752v2| 1;2023-05-24;https://www.arxiv.org/abs/2305.14752v1	arXiv:2305.14752			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 27 2024	2024	This paper introduces an innovative approach that combines Large Language Models (LLMs) with Formal Verification strategies for automatic software vulnerability repair. Initially, we employ Bounded Model Checking (BMC) to identify vulnerabilities and extract counterexamples. These counterexamples are supported by mathematical proofs and the stack trace of the vulnerabilities. Using a specially designed prompt, we combine the original source code with the identified vulnerability, including its stack trace and counterexample that specifies the line number and error type. This combined information is then fed into an LLM, which is instructed to attempt to fix the code. The new code is subsequently verified again using BMC to ensure the fix succeeded. We present the ESBMC-AI framework as a proof of concept, leveraging the well-recognized and industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained transformer model to detect and fix errors in C programs, particularly in critical software components. We evaluated our approach on 50,000 C programs randomly selected from the FormAI dataset with their respective vulnerability classifications. Our results demonstrate ESBMC-AI's capability to automate the detection and repair of issues such as buffer overflow, arithmetic overflow, and pointer dereference failures with high accuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC techniques, offering potential integration into the continuous integration and deployment (CI/CD) process within the software development lifecycle.																																	2024-07-17	PPRN:72717974		
J	Huang, Wen; Liu, Hongbin; Guo, Minxin; Gong, Neil Zhenqiang				Liu, Hongbin/MXK-1341-2025						Visual Hallucinations of Multi-modal Large Language Models								Arxiv											2	2;2024-06-16;https://www.arxiv.org/abs/2402.14683v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.14683v1	arXiv:2402.14683			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 16 2024	2024	Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks.																																	2024-07-04	PPRN:87804513		
J	Zhang, Yimeng; Jia, Jinghan; Chen, Xin; Chen, Aochuan; Zhang, Yihua; Liu, Jiancheng; Ding, Ke; Liu, Sijia				Liu, JC/LPI-0149-2024						To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now								Arxiv											1	1;2024-06-15;https://www.arxiv.org/abs/2310.11868v3	arXiv:2310.11868			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 15 2024	2024	The recent advances in diffusion models (DMs) have revolutionized the generation of realistic and complex images. However, these models also introduce potential safety hazards, such as producing harmful content and infringing data copyrights. Despite the development of safety-driven unlearning techniques to counteract these challenges, doubts about their efficacy persist. To tackle this issue, we introduce an evaluation framework that leverages adversarial prompts to discern the trustworthiness of these safety-driven DMs after they have undergone the process of unlearning harmful concepts. Specifically, we investigated the adversarial robustness of DMs, assessed by adversarial prompts, when eliminating unwanted concepts, styles, and objects. We develop an effective and efficient adversarial prompt generation approach for DMs, termed UnlearnDiffAtk. This method capitalizes on the intrinsic classification abilities of DMs to simplify the creation of adversarial prompts, thereby eliminating the need for auxiliary classification or diffusion models.Through extensive benchmarking, we evaluate the robustness of five widely-used safety-driven unlearned DMs (i.e., DMs after unlearning undesirable concepts, styles, or objects) across a variety of tasks. Our results demonstrate the effectiveness and efficiency merits of UnlearnDiffAtk over the state-of-the-art adversarial prompt generation method and reveal the lack of robustness of current safety-driven unlearning techniques when applied to DMs. Codes are available at https://github.com/OPTML-Group/Diffusion-MU-Attack. WARNING: This paper contains model outputs that may be offensive in nature.																																	2025-08-07	PPRN:123164671		
J	Wang, Yufei; Sun, Zhanyi; Zhang, Jesse; Xian, Zhou; Biyik, Erdem; Held, David; Erickson, Zackory				Bıyık, Erdem/P-8897-2017						RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback								Arxiv											3	3;2024-06-14;https://www.arxiv.org/abs/2402.03681v4| 2;2024-02-10;https://www.arxiv.org/abs/2402.03681v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.03681v1	arXiv:2402.03681			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/																																	2024-07-04	PPRN:87534023		
J	Zhao, Yang; Xu, Yanwu; Xiao, Zhisheng; Jia, Haolin; Hou, Tingbo				Xu, Yanwu/AAG-2725-2019; Hou, Tingbo/H-6978-2012; Jia, Haolin/PBN-3893-2025						MobileDiffusion: Instant Text-to-Image Generation on Mobile Devices								Arxiv											2	2;2024-06-12;https://www.arxiv.org/abs/2311.16567v2| 1;2023-11-28;https://www.arxiv.org/abs/2311.16567v1	arXiv:2311.16567			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 12 2024	2024	The deployment of large-scale text-to-image diffusion models on mobile devices is impeded by their substantial model size and high latency. In this paper, we present MobileDiffusion , an ultra -efficient text-to-image diffusion model obtained through extensive optimizations in both architecture and sampling techniques. We conduct a comprehensive examination of model architecture design to minimize model size and FLOPs, while preserving image generation quality. Additionally, we revisit the advanced sampling technique by diffusion-GAN, and make one-step sampling compatible to downstream applications trained on the base model. Empirical studies, conducted both quantitatively and qualitatively, demonstrate the effectiveness of our proposed technologies. With them, MobileDiffusion achieves instant text-to-image generation on mobile devices, establishing a new state of the art.																																	2024-07-04	PPRN:86311601		
J	Aghaei, Alireza Afzal				Afzal Aghaei, Alireza/IQW-6272-2023						fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions								Arxiv											1	1;2024-06-11;https://www.arxiv.org/abs/2406.07456v1	arXiv:2406.07456			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision. This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractionalorthogonal Jacobi function as its basis function. By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures ef lcient learning and enhanced accuracy. The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning. Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis. Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations. The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications.																																	2024-07-12	PPRN:89278861		
J	Thaker, Pratiksha; Maurya, Yash; Hu, Shengyuan; Wu, Zhiwei Steven; Smith, Virginia										Guardrail Baselines for Unlearning in LLMs								Arxiv											3	3;2024-06-11;https://www.arxiv.org/abs/2403.03329v3| 2;2024-06-06;https://www.arxiv.org/abs/2403.03329v2| 1;2024-03-05;https://www.arxiv.org/abs/2403.03329v1	arXiv:2403.03329			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 11 2024	2024	Recent work has demonstrated that finetuning is a promising approach to 'unlearn' concepts from large language models. However, finetuning can be expensive, as it requires both generating a set of examples and running iterations of finetuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to finetuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive finetuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. finetuning, and highlights scenarios where guardrails expose possible unintended behavior in existing metrics and benchmarks.																																	2024-07-11	PPRN:88048758		
J	Montanez-Barrera, J.A.; Michielsen, Kristel										Towards a universal QAOA protocol: Evidence of a scaling advantage in solving some combinatorial optimization problems								Arxiv											2	2;2024-06-07;https://www.arxiv.org/abs/2405.09169v2| 1;2024-05-15;https://www.arxiv.org/abs/2405.09169v1	arXiv:2405.09169			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	The quantum approximate optimization algorithm (QAOA) is a promising algorithm for solving combinatorial optimization problems (COPs). In this algorithm, there are alternating layers consisting of a mixer and a problem Hamiltonian. Each layer i = 0 , ... , p − 1 is parameterized by βi and γi . How to find these parameters has been an open question with the majority of the research focused on finding them using classical algorithms. In this work, we present evidence that fixed linear ramp schedules constitute a universal set of QAOA parameters, i.e., a set of γ and β parameters that rapidly approximate the optimal solution, x∗ , independently of the COP selected, and that the success probability of finding it, probability (x∗), increases with the number of QAOA layers p . We simulate linear ramp QAOA protocols (LR-QAOA) involving up to N q = 42 qubits and p = 400 layers on random instances of 9 different COPs. The results suggest that probability (x∗) ≈ 1 / 2 (ηN q /p ) for a constant η . We extend the analysis in 4 COPs with p = N q and show that probability (x∗) seems to be constant for general cases. For example, when implementing LR-QAOA with p = 42, the probability (x∗) for 42-qubit Weighted MaxCut problems (W-MaxCut) increases from 2 / 242 ≈ 10 − 13 to an average of 0.13. We compare LR-QAOA, simulated annealing (SA), and branch-and-bound (B&B) finding a scaling improvement in LR-QAOA. We test LR-QAOA on real hardware using IonQ Aria, Quantinuum H2-1, IBM Brisbane, IBM Kyoto, and IBM Osaka, encoding random weighted MaxCut (W-MaxCut) problems from 5 to 109 qubits and p = 3 to 100. We find that even for the largest case, Nq = 109 qubits and p = 100, information about the LR-QAOA optimization protocol is still present. The circuit involved requires 21200 CNOT gates and a time of 132 µs . The resilience of LR-QAOA to noise is attributed to its characteristic of bringing the system towards a minimal energy state despite noise driving it towards a maximally mixed state. These results show that LR-QAOA effectively finds high-quality solutions for a large variety of COPs and suggest a scaling advantage of quantum computation for combinatorial optimization.																																	2024-06-22	PPRN:89076780		
J	Wang, Xintong; Pan, Jingheng; Ding, Liang; Biemann, Chris				Wang, Xintong/JJE-1189-2023; Ding, Liang/IXD-6099-2023						Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding								Arxiv											2	2;2024-06-05;https://www.arxiv.org/abs/2403.18715v2| 1;2024-03-27;https://www.arxiv.org/abs/2403.18715v1	arXiv:2403.18715			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 05 2024	2024	Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.																																	2024-06-22	PPRN:88333104		
J	Ma, Ruochen; Turzillo, Alex										Symmetry Protected Topological Phases of Mixed States in the Doubled Space								Arxiv											3	3;2024-06-02;https://www.arxiv.org/abs/2403.13280v3| 2;2024-04-09;https://www.arxiv.org/abs/2403.13280v2| 1;2024-03-20;https://www.arxiv.org/abs/2403.13280v1	arXiv:2403.13280			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 02 2024	2024	The interplay of symmetry and topology in quantum many-body mixed states has recently garnered significant interest. In a phenomenon not seen in pure states, mixed states can exhibit average symmetries – symmetries that act on component states while leaving the ensemble invariant. In this work, we systematically characterize symmetry protected topological (SPT) phases of short -range entangled (SRE) mixed states of spin systems – protected by both average and exact symmetries – by studying their pure Choi states in a doubled Hilbert space, where the familiar notions and tools for SRE and SPT pure states apply. This advantage of the doubled space comes with a price: extra symmetries as well as subtleties around how hermiticity and positivity of the original density matrix constrain the possible SPT invariants. Nevertheless, the doubled space perspective allows us to obtain a systematic classification of mixed -state SPT (MSPT) phases. We also investigate the robustness of MSPT invariants under symmetric finite-depth quantum channels, the bulk-boundary correspondence for MSPT phases, and the consequences of the MSPT invariants for the separability of mixed states and the symmetry-protected sign problem. In addition to MSPT phases, we study the patterns of spontaneous symmetry breaking (SSB) of mixed states, including the phenomenon of exact-to-average SSB, and the order parameters that detect them. Mixed state SSB is related to an ingappability constraint on symmetric Lindbladian dynamics.																																	2024-11-10	PPRN:88248686		
J	Xie, Tengyang; Foster, Dylan J.; Krishnamurthy, Akshay; Rosset, Corby; Awadallah, Ahmed; Rakhlin, Alexander				Xie, Tengyang/ABM-6089-2022						Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF								Arxiv											1	1;2024-05-31;https://www.arxiv.org/abs/2405.21046v1	arXiv:2405.21046			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 31 2024	2024	Reinforcement learning from human feedback (RLHF) has emerged as a central tool for language model alignment. We consider online exploration in RLHF, which exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, maximally informative responses. By allowing RLHF to confidently stray from the pre-trained model, online exploration offers the possibility of novel, potentially super-human capabilities, but its full potential as a paradigm for language model training has yet to be realized, owing to computational and statistical bottlenecks in directly adapting existing reinforcement learning techniques. We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical - a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al., 2023) - yet enjoys the strongest known provable guarantees and promising empirical performance. XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model and human feedback data. In theory, we show that XPO is provably sample-efficient and converges to a near-optimal language model policy under natural exploration conditions, irrespective of whether the initial model has good coverage. Our analysis, which builds on the observation that DPO implicitly performs a form of Q*-approximation (or, Bellman error minimization), combines previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the perspective of KL-regularized Markov decision processes. Empirically, we find that XPO is more sample-efficient than non-exploratory DPO variants in a preliminary evaluation.																																	2024-06-19	PPRN:89131281		
J	Huang, Nan; Wei, Xiaobao; Zheng, Wenzhao; An, Pengju; Lu, Ming; Zhan, Wei; Tomizuka, Masayoshi; Keutzer, Kurt; Zhang, Shanghang				Wei, Xiaobao/HDL-9165-2022; Zhang, Lisa/AAW-9795-2021						S3Gaussian: Self-Supervised Street Gaussians for Autonomous Driving								Arxiv											1	1;2024-05-30;https://www.arxiv.org/abs/2405.20323v1	arXiv:2405.20323			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	Photorealistic 3D reconstruction of street scenes is a critical technique for developing real-world simulators for autonomous driving. Despite the efficacy of Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting (3DGS) emerges as a promising direction due to its faster speed and more explicit representation. However, most existing street 3DGS methods require tracked 3D vehicle bounding boxes to decompose the static and dynamic elements for effective reconstruction, limiting their applications for in-the-wild scenarios. To facilitate efficient 3D scene reconstruction without costly annotations, we propose a self-supervised street Gaussian (S3 Gaussian) method to decompose dynamic and static elements from 4D consistency. We represent each scene with 3D Gaussians to preserve the explicitness and further accompany them with a spatial-temporal field network to compactly model the 4D dynamics. We conduct extensive experiments on the challenging Waymo-Open dataset to evaluate the effectiveness of our method. Our S3 Gaussian demonstrates the ability to decompose static and dynamic scenes and achieves the best performance without using 3D annotations.																																	2024-06-16	PPRN:89112927		
J	Wang, Lening; Zheng, Wenzhao; Ren, Yilong; Jiang, Han; Cui, Zhiyong; Yu, Haiyang; Lu, Jiwen				Ren, Yilong/MXL-4162-2025; Lin, Hongyu/LXA-3658-2024; Cui, Zhiyong/AGV-6207-2022; Jiang, Han/ACK-6738-2022						OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving								Arxiv											1	1;2024-05-30;https://www.arxiv.org/abs/2405.20337v1	arXiv:2405.20337			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	Understanding the evolution of 3D scenes is important for effective autonomous driving. While conventional methods mode scene development with the motion of individual instances, world models emerge as a generative framework to describe the general scene dynamics. However, most existing methods adopt an autoregressive framework to perform next-token prediction, which suffer from inefficiency in modeling long-term temporal evolutions. To address this, we propose a diffusion-based 4D occupancy generation model, OccSora, to simulate the development of the 3D world for autonomous driving. We employ a 4D scene tokenizer to obtain compact discrete spatial-temporal representations for 4D occupancy input and achieve high-quality reconstruction for long-sequence occupancy videos. We then learn a diffusion transformer on the spatial-temporal representations and generate 4D occupancy conditioned on a trajectory prompt. We conduct extensive experiments on the widely used nuScenes dataset with Occ3D occupancy annotations. OccSora can generate 16s-videos with authentic 3D layout and temporal consistency, demonstrating its ability to understand the spatial and temporal distributions of driving scenes. With trajectory-aware 4D generation, OccSora has the potential to serve as a world simulator for the decision-making of autonomous driving. 																																	2024-06-16	PPRN:89113386		
J	Lou, Renze; Zhang, Kai; Yin, Wenpeng				Zhang, Kai/KOD-2592-2024; Yin, Wenpeng/AFK-3230-2022						Large Language Model Instruction Following: A Survey of Progresses and Challenges								Arxiv											4	4;2024-05-25;https://www.arxiv.org/abs/2303.10475v8| 3;2024-01-07;https://www.arxiv.org/abs/2303.10475v7| 2;2023-11-24;https://www.arxiv.org/abs/2303.10475v6| 1;2023-03-18;https://www.arxiv.org/abs/2303.10475v2	arXiv:2303.10475			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 25 2024	2024	Task semantics can be expressed by a set of input-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning to follow task instructions, i.e., instruction following . Despite its impressive progress, there are some unsolved research equations that the community struggles with. This survey paper tries to summarize and provide insights to the current research on instruction following, particularly, by answering the following questions: (i) What is task instruction, and what instruction types exist? (ii) How to model instructions? (iii) What are popular instruction following datasets and evaluation metrics? (iv) What factors influence and explain the instructions’ performance? (v) What challenges remain in instruction following? To our knowledge, this is the first comprehensive survey about instruction following. 1																																	2024-06-11	PPRN:46958952		
J	Li, Xiaoxi; Jin, Jiajie; Zhou, Yujia; Zhang, Yuyao; Zhang, Peitian; Zhu, Yutao; Dou, Zhicheng				李, 晓曦/AEG-7480-2022; Dou, Zhicheng/OBO-6932-2025						From Matching to Generation: A Survey on Generative Information Retrieval								Arxiv											2	2;2024-05-16;https://www.arxiv.org/abs/2404.14851v3| 1;2024-04-23;https://www.arxiv.org/abs/2404.14851v1	arXiv:2404.14851			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 16 2024	2024	Information Retrieval (IR) systems are crucial tools for users to access information, widely applied in scenarios like search engines, question answering, and recommendation systems. Traditional IR methods, based on similarity matching to return ranked lists of documents, have been reliable means of information acquisition, dominating the IR field for years. With the advancement of pre-trained language models, generative information retrieval (GenIR) has emerged as a novel paradigm, gaining increasing attention in recent years. Currently, research in GenIR can be categorized into two aspects: generative document retrieval (GR) and reliable response generation. GR leverages the generative model's parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. Reliable response generation, on the other hand, employs language models to directly generate the information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching, offering more flexibility, efficiency, and creativity, thus better meeting practical needs. This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training, document identifier, incremental learning, downstream tasks adaptation, multi-modal GR and generative recommendation, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, generating response with citations and personal information assistant. We also review the evaluation, challenges and future prospects in GenIR systems. This review aims to offer a comprehensive reference for researchers in the GenIR field, encouraging further development in this area.																																	2024-06-01	PPRN:88621676		
J	Song, Dingjie; Chen, Shunian; Chen, Guiming Hardy; Yu, Fei; Wan, Xiang; Wang, Benyou				Wang, Benyou/Y-5146-2019						MileBench: Benchmarking MLLMs in Long Context								Arxiv											2	2;2024-05-15;https://www.arxiv.org/abs/2404.18532v2| 1;2024-04-29;https://www.arxiv.org/abs/2404.18532v1	arXiv:2404.18532			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 15 2024	2024	Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs. To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, obtained from testing 22 models, revealed that while the closed-source GPT-4o outperforms others, most open-source MLLMs struggle in long-context situations. Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images.																																	2024-06-12	PPRN:88967256		
J	Zhang, Quanjun; Fang, Chunrong; Xie, Yang; Ma, Yuxiang; Sun, Weisong; Yang, Yun; Chen, Zhenyu				Fang, Chunrong/GPW-9872-2022; ZHANG, QUANJUN/LPP-9143-2024; ma, yuxiang/HLG-5117-2023; Sun, Weisong/AAU-9503-2020						A Systematic Literature Review on Large Language Models for Automated Program Repair								Arxiv											2	2;2024-05-12;https://www.arxiv.org/abs/2405.01466v2| 1;2024-05-02;https://www.arxiv.org/abs/2405.01466v1	arXiv:2405.01466			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 12 2024	2024	Automated Program Repair (APR) attempts to patch software bugs and reduce manual debugging efforts. Very recently, with the advances in Large Language Models (LLMs), an increasing number of APR techniques have been proposed, facilitating software development and maintenance and demonstrating remarkable performance. However, due to ongoing explorations in the LLM-based APR field, it is challenging for researchers to understand the current achievements, challenges, and potential opportunities. This work provides the first systematic literature review to summarize the applications of LLMs in APR between 2020 and 2024. We analyze 127 relevant papers from LLMs, APR and their integration perspectives. First, we categorize existing popular LLMs that are applied to support APR and outline three types of utilization strategies for their deployment. Besides, we detail some specific repair scenarios that benefit from LLMs, e.g., semantic bugs and security vulnerabilities. Furthermore, we discuss several critical aspects of integrating LLMs into APR research, e.g., input forms and open science. Finally, we highlight a set of challenges remaining to be investigated and the potential guidelines for future research. Overall, our paper provides a systematic overview of the research landscape to the APR community, helping researchers gain a comprehensive understanding of achievements and promote future research.																																	2024-06-08	PPRN:88724433		
J	Huang, Yiming; Liu, Xiao; Gong, Yeyun; Gou, Zhibin; Shen, Yelong; Duan, Nan; Chen, Weizhu				Duan, Nan/AAR-2231-2020; Huang, Yiming/HMV-3013-2023						Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning								Arxiv											2	2;2024-05-08;https://www.arxiv.org/abs/2403.02333v3| 1;2024-04-17;https://www.arxiv.org/abs/2403.02333v2	arXiv:2403.02333			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 08 2024	2024	Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% PASS@1 accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 70B range and best commercial models like GPT-4 across multiple math reasoning datasets.																																	2024-05-27	PPRN:88554303		
J	Zhang, Xiyuan; Chowdhury, Ranak Roy; Gupta, Rajesh K.; Shang, Jingbo				Shang, Jingbo/T-4207-2019						Large Language Models for Time Series: A Survey								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2402.01801v3| 1;2024-02-06;https://www.arxiv.org/abs/2402.01801v2	arXiv:2402.01801			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.																																	2024-06-04	PPRN:87533938		
J	Cheng, Jie; Chen, Yingbing; Chen, Qifeng										PLUTO: Pushing the Limit of Imitation Learning-based Planning for Autonomous Driving								Arxiv											1	1;2024-04-22;https://www.arxiv.org/abs/2404.14327v1	arXiv:2404.14327			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 22 2024	2024	We present PLUTO, a powerful framework that pushes the limit of imitation learning-based planning for autonomous driving. Our improvements stem from three pivotal aspects: a longitudinal-lateral aware model architecture that enables flexible and diverse driving behaviors; An innovative auxiliary loss computation method that is broadly applicable and efficient for batch-wise calculation; A novel training framework that leverages contrastive learning, augmented by a suite of new data augmentations to regulate driving behaviors and facilitate the understanding of underlying interactions. We assessed our framework using the large-scale real-world nuPlan dataset and its associated standardized planning benchmark. Impressively, PLUTO achieves state-of-the-art closed-loop performance, beating other competing learning-based methods and surpassing the current top-performed rule-based planner for the first time.																																	2024-05-01	PPRN:88610854		
J	Shen, Yikang; Guo, Zhen; Cai, Tianle; Qin, Zengyi				Qin, Zengyi/CAJ-0463-2022						JetMoE: Reaching Llama2 Performance with 0.1M Dollars								Arxiv											1	1;2024-04-11;https://www.arxiv.org/abs/2404.07413v1	arXiv:2404.07413			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 11 2024	2024	Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models. This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs.																																	2024-05-22	PPRN:88500597		
J	Shao, Yijia; Jiang, Yucheng; Kanell, Theodore A.; Xu, Peter; Khattab, Omar; Lam, Monica S.				Jiang, Yucheng/K-5772-2013						Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models								Arxiv											1	1;2024-04-08;https://www.arxiv.org/abs/2402.14207v2	arXiv:2402.14207			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 08 2024	2024	We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.																																	2024-05-22	PPRN:88443348		
J	Agashe, Saaket; Fan, Yue; Reyna, Anthony; Wang, Xin Eric				Wang, Xin/ABD-3905-2020						LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models								Arxiv											3	3;2024-04-02;https://www.arxiv.org/abs/2310.03903v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03903v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03903v1	arXiv:2310.03903			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by Large Language Models (LLMs) make them promising candidates for developing coordination agents. In this study, we introduce a new LLM-Coordination Benchmark aimed at a detailed analysis of LLMs within the context of Pure Coordination Games, where participating agents need to cooperate for the most gain. This benchmark evaluates LLMs through two distinct tasks: (1) emph{Agentic Coordination}, where LLMs act as proactive participants for cooperation in 4 pure coordination games; (2) emph{Coordination Question Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice questions from the 4 games for evaluation of three key reasoning abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture for Coordination (CAC) framework that can easily integrate different LLMs as plug-and-play modules for pure coordination games. Our findings indicate that LLM agents equipped with GPT-4-turbo achieve comparable performance to state-of-the-art reinforcement learning methods in games that require commonsense actions based on the environment. Besides, zero-shot coordination experiments reveal that, unlike RL methods, LLM agents are robust to new unseen partners. However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs. The analysis also sheds light on how the ability of LLMs to understand their environment and their partner's beliefs and intentions plays a part in their ability to plan for coordination. 																																	2024-04-18	PPRN:85518974		
J	Ishibashi, Yoichi; Nishimura, Yoshimasa										Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization								Arxiv											1	1;2024-04-02;https://www.arxiv.org/abs/2404.02183v1	arXiv:2404.02183			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.																																	2024-04-18	PPRN:88389655		
J	Huang, Hai; Zhao, Zhengyu; Backes, Michael; Shen, Yun; Zhang, Yang										Composite Backdoor Attacks Against Large Language Models								Arxiv											2	2;2024-03-30;https://www.arxiv.org/abs/2310.07676v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07676v1	arXiv:2310.07676			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 30 2024	2024	Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third -party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with 3% poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a 100% Attack Success Rate (ASR) with a False Triggered Rate (FTR) below 2.06% and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.1																																	2024-04-18	PPRN:85540137		
J	Kim, Wonkyun; Choi, Changin; Lee, Wonseok; Rhee, Wonjong				Rhee, Wonjong/JMB-2765-2023						An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM								Arxiv											1	1;2024-03-27;https://www.arxiv.org/abs/2403.18406v1	arXiv:2403.18406			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 27 2024	2024	Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in a grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten zero-shot video question answering benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks.																																	2024-04-15	PPRN:88335049		
J	Yu, Mulin; Lu, Tao; Xu, Linning; Jiang, Lihan; Xiangli, Yuanbo; Dai, Bo										GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction								Arxiv											2	2;2024-10-13;https://www.arxiv.org/abs/2403.16964v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16964v1	arXiv:2403.16964			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry.																																	2025-08-07	PPRN:88279047		
J	Kalai, Adam Tauman; Vempala, Santosh S.										Calibrated Language Models Must Hallucinate								Arxiv											3	3;2024-03-20;https://www.arxiv.org/abs/2311.14648v3| 2;2023-12-03;https://www.arxiv.org/abs/2311.14648v2| 1;2023-11-24;https://www.arxiv.org/abs/2311.14648v1	arXiv:2311.14648			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 20 2024	2024	Recent language models generate false but plausible -sounding text with surprising frequency. Such “hallucinations” are an obstacle to the usability of language -based AI systems and can harm people who rely upon their outputs. This work shows that there is an inherent statistical lower -bound on the rate that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality. For “arbitrary” facts whose veracity cannot be determined from the training data, we show that hallucinations must occur at a certain rate for language models that satisfy a statistical calibration condition appropriate for generative language models. Specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a “Good -Turing” estimate), even assuming ideal training data without errors. One conclusion is that models pretrained to be sufficiently good predictors (i.e., calibrated) may require post -training to mitigate hallucinations on the type of arbitrary facts that tend to appear once in the training set. However, our analysis also suggests that there is no statistical reason that pretraining will lead to hallucination on facts that tend to appear more than once in the training data (like references to publications such as articles and books, whose hallucinations have been particularly notable and problematic) or on systematic facts (like arithmetic calculations). Therefore, different architectures and learning algorithms may mitigate these latter types of hallucinations.																																	2024-04-12	PPRN:86280650		
J	Liu, Xian; Ren, Jian; Siarohin, Aliaksandr; Skorokhodov, Ivan; Li, Yanyu; Lin, Dahua; Liu, Xihui; Liu, Ziwei; Tulyakov, Sergey				Liu, Ziwei/AAG-6939-2021; Liu, Xihui/LHA-5141-2024; Lin, Dahua/W-6576-2019; Ren, Jian/AAP-2636-2021; Войнов, Олег/KUF-2109-2024						HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion								Arxiv											2	2;2024-03-15;https://www.arxiv.org/abs/2310.08579v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08579v1	arXiv:2310.08579			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 15 2024	2024	Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task. Existing models like Stable Diffusion and DALL-E 2 tend to generate human images with incoherent parts or unnatural poses. To tackle these challenges, our key insight is that human image is inherently structural over multiple granularities, from the coarse-level body skeleton to fine-grained spatial geometry. Therefore, capturing such correlations between the explicit appearance and latent structure in one model is essential to generate coherent and natural human images. To this end, we propose a unified framework, HyperHuman, that generates in-the-wild human images of high realism and diverse layouts. Specifically, 1) we first build a large-scale human-centric dataset, named HumanVerse, which consists of 340M images with comprehensive annotations like human pose, depth, and surface normal. 2) Next, we propose a Latent Structural Diffusion Model that simultaneously denoises the depth and surface normal along with the synthesized RGB image. Our model enforces the joint learning of image appearance, spatial relationship, and geometry in a unified network, where each branch in the model complements to each other with both structural awareness and textural richness. 3) Finally, to further boost the visual quality, we propose a Structure-Guided Refiner to compose the predicted conditions for more detailed generation of higher resolution. Extensive experiments demonstrate that our framework yields the state-of-the-art performance, generating hyper-realistic human images under diverse scenarios. 																																	2024-04-11	PPRN:85605358		
J	Hu, Xueyu; Zhao, Ziyu; Wei, Shuang; Chai, Ziwei; Ma, Qianli; Wang, Guoyin; Wang, Xuwu; Su, Jing; Xu, Jingjing; Zhu, Ming; Cheng, Yao; Yuan, Jianbo; Li, Jiwei; Kuang, Kun; Yang, Yang; Yang, Hongxia; Wu, Fei				Yang, Hongxia/OIS-1418-2025; 袁, 剑波/JVO-4766-2024; Wu, Fan/AID-2699-2022; Su, Jing/B-3171-2013; zhao, ziyu/MIQ-5218-2025						InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks								Arxiv											3	3;2024-03-11;https://www.arxiv.org/abs/2401.05507v3| 2;2024-02-12;https://www.arxiv.org/abs/2401.05507v2| 1;2024-01-10;https://www.arxiv.org/abs/2401.05507v1	arXiv:2401.05507			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 11 2024	2024	In this paper, we introduce InfiAgent-DABench, the first benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. These tasks require agents to end -to -end solving complex tasks by interacting with an execution environment. This benchmark contains DAEval, a dataset consisting of 257 data analysis questions derived from 52 CSV files, and an agent framework which incorporates LLMs to serve as data analysis agents for both serving and evaluation. Since data analysis questions are often open-ended and hard to evaluate without human supervision, we adopt a format -prompting technique to convert each question into a closed -form format so that they can be automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks. In addition, building on top of our agent framework, we develop a specialized agent, DAAgent, which surpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for InfiAgent-DABench are released at https: //github.com/InfiAgent/InfiAgent.																																	2024-04-08	PPRN:87128282		
J	Edelman, Benjamin L.; Edelman, Ezra; Goel, Surbhi; Malach, Eran; Tsilivis, Nikolaos										The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains								Arxiv											1	1;2024-02-16;https://www.arxiv.org/abs/2402.11004v1	arXiv:2402.11004			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 16 2024	2024	Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in -context learning (ICL) capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form statistical induction heads which compute accurate next -token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub -optimally predict using in -context single -token statistics (unigrams); then, there is a rapid phase transition to the correct in -context bigram solution. We conduct an empirical and theoretical investigation of this multi -phase process, showing how successful learning results from the interaction between the transformer’s layers, and uncovering evidence that the presence of the simpler unigram solution may delay formation of the final bigram solution. We examine how learning is affected by varying the prior distribution over Markov chains, and consider the generalization of our in -context learning of Markov chains (ICL-MC) task to n -grams for n ≥ 2.																																	2024-03-19	PPRN:87761938		
J	Fung, Yi R; Zhao, Ruining; Doo, Jae; Sun, Chenkai; Ji, Heng										Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking								Arxiv											1	1;2024-02-14;https://www.arxiv.org/abs/2402.09369v1	arXiv:2402.09369			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Feb 14 2024	2024	Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain.1																																	2024-03-01	PPRN:87683886		
J	Benton, Joe; Deligiannidis, George; Doucet, Arnaud				Doucet, Arnaud/B-2473-2013						Error Bounds for Flow Matching Methods								Arxiv											2	2;2024-02-11;https://www.arxiv.org/abs/2305.16860v2| 1;2023-05-26;https://www.arxiv.org/abs/2305.16860v1	arXiv:2305.16860			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 11 2024	2024	Score -based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDEs). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODEs) rather than SDEs. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE -based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the L2 loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an L2 bound on the approximation error and a certain regularity condition on the data distributions.																																	2024-02-27	PPRN:72729063		
J	Niu, Runliang; Li, Jindong; Wang, Shiqi; Fu, Yali; Hu, Xiyu; Leng, Xueyuan; Kong, He; Chang, Yi; Wang, Qi				Hu, Xiyu/GRJ-7069-2022; Wang, Qi/CAH-9366-2022; Wang, Shiqi/LEM-7616-2024						ScreenAgent: A Vision Language Model-driven Computer Control Agent								Arxiv											1	1;2024-02-09;https://www.arxiv.org/abs/2402.07945v1	arXiv:2402.07945			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 09 2024	2024	Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code is available at url{https://github.com/niuzaisheng/ScreenAgent}.																																	2025-08-27	PPRN:123529408		
J	Xiong, Haoyu; Mendonca, Russell; Shaw, Kenneth; Pathak, Deepak										Adaptive Mobile Manipulation for Articulated Objects In the Open World								Arxiv											2	2;2024-01-28;https://www.arxiv.org/abs/2401.14403v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.14403v1	arXiv:2401.14403			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 28 2024	2024	Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus. With less than an hour of online learning for each object, the system is able to increase success rate from 50% of BC pre-training to 95% using online adaptation. 																																	2024-02-15	PPRN:87331485		
J	Xu, Mengwei; Cai, Dongqi; Wu, Yaozong; Li, Xiang; Wang, Shangguang				Xu, Mengwei/AAE-5567-2020						FwdLLM: Efficient FedLLM using Forward Gradient								Arxiv											2	2;2024-01-20;https://www.arxiv.org/abs/2308.13894v2| 1;2023-08-26;https://www.arxiv.org/abs/2308.13894v1	arXiv:2308.13894			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 20 2024	2024	Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in finetuning LLMs to downstream mobile tasks, i.e., FedLLM. A vital challenge of FedLLM is the tension between LLM complexity and resource constraint of mobile devices. In response to this challenge, this work introduces FwdLLM1, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM is to employ backpropagation (BP)-free training methods, requiring devices only to execute “perturbed inferences”. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with parameter-efficient training methods, an essential way to scale the approach to the LLM era; (2) it systematically and adaptively allocates computational loads across devices, striking a careful balance between convergence speed and accuracy; (3) it discriminatively samples perturbed predictions that are more valuable to model convergence. Comprehensive experiments illustrate FwdLLM’s significant advantages over conventional methods, including up to three orders of magnitude faster convergence and a 14.6× reduction in memory footprint. Uniquely, FwdLLM paves the way for federated billion-parameter LLMs such as LLaMA on COTS mobile devices – a feat previously unattained.																																	2024-02-06	PPRN:84257938		
J	Chai, Linzheng; Yang, Jian; Sun, Tao; Guo, Hongcheng; Liu, Jiaheng; Wang, Bing; Liang, Xiannian; Bai, Jiaqi; Li, Tongliang; Peng, Qiyao; Li, Zhoujun				Sun, Tao/ORK-0166-2025; Sun, Tao/AGJ-5617-2022; Yang, Jian/JXX-7911-2024						xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning								Arxiv											1	1;2024-01-13;https://www.arxiv.org/abs/2401.07037v1	arXiv:2401.07037			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 13 2024	2024	Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (xCOT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (xCOT-INSTRUCT) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL)) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results on previous benchmarks demonstrate the superior performance of xCoT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap.																																	2024-05-25	PPRN:87186731		
J	Zhang, Bing				Zhang, Bing/AAG-2824-2019						The Physics of Fast Radio Bursts								Arxiv											3	3;2024-01-03;https://www.arxiv.org/abs/2212.03972v2| 2;2022-12-07;https://www.arxiv.org/abs/2212.03972v1| 1;2022-12-07;https://www.arxiv.org/abs/2212.03972v1	arXiv:2212.03972			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 03 2024	2024	Fast radio bursts (FRBs), millisecond-duration bursts prevailing in the radio sky, are the latest big puzzle in the universe and have been a subject of intense observational and theoretical investigations in recent years. The rapid accumulation of the observational data has painted the following sketch about the physical origin of FRBs: They predominantly originate from cosmological distances so that their sources produce the most extreme coherent radio emission in the universe; at least some, probably most, FRBs are repeating sources that do not invoke cataclysmic events; and at least some FRBs are produced by magnetars, neutron stars with the strongest magnetic fields in the universe. Many open questions regarding the physical origin(s) and mechanism(s) of FRBs remain. This article reviews the phenomenology and possible underlying physics of FRBs. Topics include: a summary of the observational data, basic plasma physics, general constraints on FRB models from the data, radiation mechanisms, source and environment models, propagation effects, as well as FRBs as cosmological probes. Current pressing problems and future prospects are also discussed.																																	2024-01-12	PPRN:25188243		
J	Jiang, Jinhao; Chen, Zhipeng; Min, Yingqian; Chen, Jie; Cheng, Xiaoxue; Wang, Jiapeng; Tang, Yiru; Sun, Haoxiang; Deng, Jia; Zhao, Wayne Xin; Liu, Zheng; Yan, Dong; Xie, Jian; Wang, Zhongyuan; Wen, Ji-Rong				haoxiang, sun/ABF-1642-2022; Wang, Jiapeng/HLG-3222-2023						Enhancing LLM Reasoning with Reward-guided Tree Search								Arxiv											4	4;2024-12-31;https://www.arxiv.org/abs/2411.11694v4| 3;2024-12-22;https://www.arxiv.org/abs/2411.11694v3| 2;2024-12-11;https://www.arxiv.org/abs/2411.11694v2| 1;2024-11-18;https://www.arxiv.org/abs/2411.11694v1	arXiv:2411.11694			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 31 2024	2024	Recently, test-time scaling has garnered significant attention from the research community, largely due to the substantial advancements of the o1 model released by OpenAI. By allocating more computational resources during the inference phase, large language models~(LLMs) can extensively explore the solution space by generating more thought tokens or diverse solutions, thereby producing more accurate responses. However, developing an o1-like reasoning approach is challenging, and researchers have been making various attempts to advance this open area of research. In this paper, we present a preliminary exploration into enhancing the reasoning abilities of LLMs through reward-guided tree search algorithms. This framework is implemented by integrating the policy model, reward model, and search algorithm. It is primarily constructed around a tree search algorithm, where the policy model navigates a dynamically expanding tree guided by a specially trained reward model. The implemented framework is denoted as textbf{STILL-1}. We thoroughly explore various design considerations necessary for implementing this framework and provide a detailed report of the technical aspects. To assess the effectiveness of our approach, we focus on mathematical reasoning tasks and conduct extensive evaluations on four challenging datasets, significantly enhancing the reasoning abilities of LLMs.																																	2025-02-15	PPRN:119260101		
J	Ding, Tong; Wagner, Sophia J.; Song, Andrew H.; Chen, Richard J.; Lu, Ming Y.; Zhang, Andrew; Vaidya, Anurag J.; Jaume, Guillaume; Shaban, Muhammad; Kim, Ahrong; Williamson, Drew F.K.; Chen, Bowen; Almagro-Perez, Cristina; Doucet, Paul; Sahai, Sharifa; Chen, Chengkuan; Komura, Daisuke; Kawabe, Akihiro; Ishikawa, Shumpei; Gerber, Georg; Peng, Tingying; Le, Long Phi; Mahmood, Faisal				Song, Andrew/ODK-2632-2025; MAHMOOD, FAISAL/KVZ-2096-2024; Williamson, Drew/KLC-8523-2024; Chen, Bowen/HOF-5018-2023; Lu, Ming Yang/JCE-3271-2023; Ishikawa, Shumpei/NBW-9562-2025; Ding, Tong/KXQ-5862-2024; Almagro-Pérez, Cristina/JUU-7925-2023; Wagner, Sophia/HII-6467-2022; Komura, Daisuke/AAF-2481-2019; Zhang, Andrew/KII-1242-2024						Multimodal Whole Slide Foundation Model for Pathology								Arxiv											1	1;2024-11-29;https://www.arxiv.org/abs/2411.19666v1	arXiv:2411.19666			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 29 2024	2024	The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL). However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose TITAN, a multimodal whole slide foundation model pretrained using 335,645 WSIs via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any finetuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that TITAN outperforms both ROI and slide foundation models across machine learning settings such as linear probing, few-shot and zero-shot classification, rare cancer retrieval and cross-modal retrieval, and pathology report generation.																																	2025-01-11	PPRN:119588776		
J	Fu, Wenjie; Wang, Huandong; Gao, Chen; Liu, Guanghua; Li, Yong; Jiang, Tao				Jiang, Tao/AGX-8391-2022; Gao, Chongming/AAC-7707-2021; Liu, Guanghua/AFK-5493-2022						Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration								Arxiv											4	4;2024-11-26;https://www.arxiv.org/abs/2311.06062v4| 3;2024-06-25;https://www.arxiv.org/abs/2311.06062v3| 2;2023-12-12;https://www.arxiv.org/abs/2311.06062v2| 1;2023-11-10;https://www.arxiv.org/abs/2311.06062v1	arXiv:2311.06062			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 26 2024	2024	Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios. We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs. Furthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding. Comprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. 																																	2025-01-08	PPRN:86126562		
J	Huang, Tiansheng; Hu, Sihao; Liu, Ling										Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack								Arxiv											6	6;2024-11-24;https://www.arxiv.org/abs/2402.01109v6| 5;2024-10-29;https://www.arxiv.org/abs/2402.01109v5| 4;2024-08-22;https://www.arxiv.org/abs/2402.01109v4| 3;2024-02-29;https://www.arxiv.org/abs/2402.01109v3| 2;2024-02-18;https://www.arxiv.org/abs/2402.01109v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01109v1	arXiv:2402.01109			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 24 2024	2024	The new paradigm of fine-tuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the fine-tuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a harmful embedding drift phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users fine-tuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the fine-tuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. 																																	2025-01-08	PPRN:87507369		
J	Cortinhal, Tiago; Tzelepis, George; Aksoy, Eren Erdal										SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving								Arxiv											2	2;2024-11-20;https://www.arxiv.org/abs/2003.03653v4| 1;2020-07-09;https://www.arxiv.org/abs/2003.03653v3	arXiv:2003.03653			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 20 2024	2024	In this paper, we introduce SalsaNext for the uncertainty-aware semantic segmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next version of SalsaNet [1] which has an encoder-decoder architecture where the encoder unit has a set of ResNet blocks and the decoder part combines upsampled features from the residual blocks. In contrast to SalsaNet, we introduce a new context module, replace the ResNet encoder blocks with a new residual dilated convolution stack with gradually increasing receptive fields and add the pixel-shuffle layer in the decoder. Additionally, we switch from stride convolution to average pooling and also apply central dropout treatment. To directly optimize the Jaccard index, we further combine the weighted cross-entropy loss with Lovasz-Softmax loss [2]. We finally inject a Bayesian treatment to compute the epistemic and aleatoric uncertainties for each point in the cloud. We provide a thorough quantitative evaluation on the Semantic-KITTI dataset [3], which demonstrates that the proposed SalsaNext outperforms other state-of-the-art semantic segmentation networks and ranks first on the Semantic-KITTI leaderboard. 																																	2024-12-31	PPRN:22519207		
J	Chen, Yilun; Yang, Shuai; Huang, Haifeng; Wang, Tai; Xu, Runsen; Lyu, Ruiyuan; Lin, Dahua; Pang, Jiangmiao				huang, haifeng/G-9256-2012; Lin, Dahua/W-6576-2019; Wang, Tai/MVV-1100-2025; Chen, Yilun/IWV-1091-2023						Grounded 3D-LLM with Referent Tokens								Arxiv											2	2;2024-11-18;https://www.arxiv.org/abs/2405.10370v2| 1;2024-05-16;https://www.arxiv.org/abs/2405.10370v1	arXiv:2405.10370			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Nov 18 2024	2024	Prior studies on 3D scene understanding have primarily developed specialized models for specific tasks or required task-specific fine-tuning. In this study, we propose Grounded 3D-LLM, which explores the potential of 3D large multi-modal models (3D LMMs) to consolidate various 3D vision tasks within a unified generative framework. The model uses scene referent tokens as special noun phrases to reference 3D scenes, enabling it to handle sequences that interleave 3D and textual data. Per-task instruction-following templates are employed to ensure natural and diversity in translating 3D vision tasks into language formats. To facilitate the use of referent tokens in subsequent language modeling, we provide a large-scale, automatically curated grounded scene-text dataset with over 1 million phrase-to-region correspondences and introduce Contrastive Language-Scene Pre-training (CLASP) to perform phrase-level scene-text alignment using this data. Our comprehensive evaluation covers open-ended tasks like dense captioning and 3D question answering, alongside close-ended tasks such as object detection and language grounding. Experiments across multiple 3D benchmarks reveal the leading performance and the broad applicability of Grounded 3D-LLM. Code and datasets are available at the project page.																																	2024-12-28	PPRN:89092267		
J	Li, Xirui; Wang, Ruochen; Cheng, Minhao; Zhou, Tianyi; Hsieh, Cho-Jui				wang, ruochen/ITU-5925-2023						DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers								Arxiv											1	1;2024-11-11;https://www.arxiv.org/abs/2402.16914v3	arXiv:2402.16914			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 11 2024	2024	Safety-aligned Large Language Models (LLMs) are still vulnerable to some manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, existing jailbreaking methods usually view a harmful prompt as a whole but they are not effective at reducing LLMs’ attention on combinations of words with malice, which well-aligned LLMs can easily reject. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively reduce LLMs’ attention on harmful words by presenting them to LLMs in a fragmented form, thereby addressing these limitations and improving attack effectiveness. We introduce an automatic prompt Decomposition and Reconstruction framework for jailbreaking Attack (DrAttack). DrAttack consists of three key components: (a) ‘Decomposition’ of the original prompt into sub-prompts, (b) ‘Reconstruction’ of these sub-prompts implicitly by In-Context Learning with semantically similar but benign reassembling example, and (c) ‘Synonym Search’ of sub-prompts, aiming to find sub-prompts’ synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with fewer queries, DrAttack obtains a substantial gain of success rate on powerful LLMs over prior SOTA attackers. Notably, the success rate of 80% on GPT-4 surpassed previous art by 65%. 																																	2024-12-18	PPRN:119141806		
J	Zhao, Xuandong; Zhang, Kexun; Su, Zihao; Vasan, Saastha; Grishchenko, Ilya; Kruegel, Christopher; Vigna, Giovanni; Wang, Yu-Xiang; Li, Lei				Zhao, Xuandong/LIG-4204-2024						Invisible Image Watermarks Are Provably Removable Using Generative AI								Arxiv											2	2;2024-10-31;https://www.arxiv.org/abs/2306.01953v3| 1;2023-06-02;https://www.arxiv.org/abs/2306.01953v1	arXiv:2306.01953			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack. Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality. However, watermarks that keep the image semantically similar can be an alternative defense against our attacks. Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks.																																	2024-12-06	PPRN:72854801		
J	Wang, Boshi; Yue, Xiang; Su, Yu; Sun, Huan										Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization								Arxiv											2	2;2024-10-30;https://www.arxiv.org/abs/2405.15071v3| 1;2024-05-27;https://www.arxiv.org/abs/2405.15071v2	arXiv:2405.15071			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 30 2024	2024	We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model’s internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.																																	2024-12-06	PPRN:89063698		
J	Li, Ziming; Zang, Qianbo; Ma, David; Guo, Jiawei; Zheng, Tuney; Liu, Minghao; Niu, Xinyao; Wang, Yue; Yang, Jian; Liu, Jiaheng; Zhong, Wanjun; Zhou, Wangchunshu; Huang, Wenhao; Zhang, Ge				Huang, Wenhao/GWU-9337-2022; Liu, Minghao/GLU-5065-2022; guo, jiawei/LKL-3487-2024						AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions								Arxiv											1	1;2024-10-29;https://www.arxiv.org/abs/2410.20424v2	arXiv:2410.20424			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 29 2024	2024	Data science tasks involving tabular data present complex challenges that require sophisticated problem-solving approaches. We propose AutoKaggle, a powerful and user-centric framework that assists data scientists in completing daily data pipelines through a collaborative multi-agent system. AutoKaggle implements an iterative development process that combines code execution, debugging, and comprehensive unit testing to ensure code correctness and logic consistency. The framework offers highly customizable workflows, allowing users to intervene at each phase, thus integrating automated intelligence with human expertise. Our universal data science toolkit, comprising validated functions for data cleaning, feature engineering, and modeling, forms the foundation of this solution, enhancing productivity by streamlining common tasks. We selected 8 Kaggle competitions to simulate data processing workflows in real-world application scenarios. Evaluation results demonstrate that AutoKaggle achieves a validation submission rate of 0.85 and a comprehensive score of 0.82 in typical data science pipelines, fully proving its effectiveness and practicality in handling complex data science tasks.																																	2024-12-10	PPRN:119043077		
J	He, Tairan; Xiao, Wenli; Lin, Toru; Luo, Zhengyi; Xu, Zhenjia; Jiang, Zhenyu; Kautz, Jan; Liu, Changliu; Shi, Guanya; Wang, Xiaolong; Fan, Linxi Jim; Zhu, Yuke				zy, j/JYP-2387-2024						HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots								Arxiv											1	1;2024-10-28;https://www.arxiv.org/abs/2410.21229v1	arXiv:2410.21229			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 28 2024	2024	Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.																																	2024-12-06	PPRN:118942069		
J	Meth, Michael; Haase, Jan F.; Zhang, Jinglei; Edmunds, Claire; Postler, Lukas; Jena, Andrew J.; Steiner, Alex; Dellantonio, Luca; Blatt, Rainer; Zoller, Peter; Monz, Thomas; Schindler, Philipp; Muschik, Christine; Ringbauer, Martin				Dellantonio, Luca/P-5369-2015; Blatt, Rainer/H-9527-2014; Meth, Michael/IZQ-0883-2023; Muschik, Christine/LMN-5923-2024; Monz, Thomas/E-1285-2017; Schindler, Philipp/E-1485-2017; Zoller, Peter/O-1639-2014; Ringbauer, Martin/I-5839-2012						Simulating 2D lattice gauge theories on a qudit quantum computer								Arxiv											3	3;2024-10-24;https://www.arxiv.org/abs/2310.12110v3| 2;2024-04-29;https://www.arxiv.org/abs/2310.12110v2| 1;2023-10-18;https://www.arxiv.org/abs/2310.12110v1	arXiv:2310.12110			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 24 2024	2024	Particle physics underpins our understanding of the world at a fundamental level by describing the interplay of matter and forces through gauge theories. Yet, despite their unmatched success, the intrinsic quantum nature of gauge theories makes important problem classes notoriously difficult for classical computational techniques [1, 2]. A promising way to overcome these roadblocks is offered by quantum computers, which are based on the same laws that make the classical computations so difficult [3–8]. Here we demonstrate two essential requirements for gauge theory calculations with quantum computers. Firstly, we perform a quantum computation of the properties of the basic building block of two-dimensional lattice quantum electrodynamics, involving both gauge fields and matter [9, 10]. Secondly, we show how to refine the gauge field discretization beyond its minimal representation. These computations are made possible by the use of a trapp ed-ion qudit quantum processor [11], where quantum information is encoded in d different states per ion, rather than in two states as in qubits. Such qudits are ideally suited for describing gauge fields, which are naturally high-dimensional, leading to a dramatic reduction in the register size and circuit complexity. Using a variational quantum eigensolver [12–16] we prepare the ground state of the model. We then observe the effect of dynamical matter on quantized magnetic fields and show how to seamlessly observe the effect of different gauge field truncations by controlling the qudit dimension. Our results open the door for hardware-efficient quantum simulations with qudits in near-term quantum devices.																																	2024-11-27	PPRN:85699793		
J	Deng, Kangle; Liu, Andrew; Zhu, Jun-Yan; Ramanan, Deva				Zhu, Jun-Yan/V-7271-2018						Depth-supervised NeRF: Fewer Views and Faster Training for Free								Arxiv											2	2;2024-10-17;https://www.arxiv.org/abs/2107.02791v3| 1;2022-04-29;https://www.arxiv.org/abs/2107.02791v2	arXiv:2107.02791			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as "free" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.																																	2024-11-14	PPRN:13597839		
J	Ye, Seonghyeon; Jang, Joel; Jeon, Byeongguk; Joo, Sejune; Yang, Jianwei; Peng, Baolin; Mandlekar, Ajay; Tan, Reuben; Chao, Yu-Wei; Lin, Bill Yuchen; Liden, Lars; Lee, Kimin; Gao, Jianfeng; Zettlemoyer, Luke; Fox, Dieter; Seo, Minjoon				Gao, Jianfeng/AAP-8200-2021; Peng, Baolin/F-2278-2019						Latent Action Pretraining from Videos								Arxiv											1	1;2024-10-15;https://www.arxiv.org/abs/2410.11758v1	arXiv:2410.11758			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 15 2024	2024	We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model. We open-source the model checkpoints and code at latentactionpretraining.github.io.																																	2024-11-07	PPRN:112920339		
J	Rout, Litu; Chen, Yujia; Ruiz, Nataniel; Caramanis, Constantine; Shakkottai, Sanjay; Chu, Wen-Sheng				Rout, Litu/LCD-6775-2024; Chu, Wen-Sheng/AAF-6871-2019; yujia, chen/JLM-5613-2023						Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations								Arxiv											1	1;2024-10-14;https://www.arxiv.org/abs/2410.10792v1	arXiv:2410.10792			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.																																	2024-11-05	PPRN:112580116		
J	Geng, Zhengyang; Pokle, Ashwini; Luo, William; Lin, Justin; Kolter, J. Zico				Luo, Weijian/LGZ-3495-2024						Consistency Models Made Easy								Arxiv											2	2;2024-10-10;https://www.arxiv.org/abs/2406.14548v2| 1;2024-06-20;https://www.arxiv.org/abs/2406.14548v1	arXiv:2406.14548			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 10 2024	2024	Consistency models (CMs) offer faster sampling than traditional diffusion models, but their training is resource-intensive. For example, as of 2024, training a state-of-the-art CM on CIFAR-10 takes one week on 8 GPUs. In this work, we propose an effective scheme for training CMs that largely improves the efficiency of building such models. Specifically, by expressing CM trajectories via a particular differential equation, we argue that diffusion models can be viewed as a special case of CMs. We can thus fine-tune a consistency model starting from a pretrained diffusion model and progressively approximate the full consistency condition to stronger degrees over the training process. Our resulting method, which we term Easy Consistency Tuning (ECT), achieves vastly reduced training times while improving upon the quality of previous methods: for example, ECT achieves a 2-step FID of 2.73 on CIFAR10 within 1 hour on a single A100 GPU, matching Consistency Distillation trained for hundreds of GPU hours. Owing to this computational efficiency, we investigate the scaling laws of CMs under ECT, showing that they obey the classic power law scaling, hinting at their ability to improve efficiency and performance at larger scales. 																																	2024-11-04	PPRN:89376938		
J	Jin, Bowen; Xie, Chulin; Zhang, Jiawei; Roy, Kashob Kumar; Zhang, Yu; Li, Zheng; Li, Ruirui; Tang, Xianfeng; Wang, Suhang; Meng, Yu; Han, Jiawei				Tang, Xianfeng/IWM-0393-2023; Li, Ruirui/IUQ-8509-2023; Meng, Yu/ABH-2615-2020; Wang, Suhang/AAH-1378-2019; Jin, Bowen/HTR-0099-2023						Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs								Arxiv											3	3;2024-10-03;https://www.arxiv.org/abs/2404.07103v3| 2;2024-07-15;https://www.arxiv.org/abs/2404.07103v2| 1;2024-04-10;https://www.arxiv.org/abs/2404.07103v1	arXiv:2404.07103			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected ( e.g. , academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBENCH , containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each G RAPH-COT iteration consists of three substeps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRB ENCH , where GRAPH-COT outperforms the baselines consistently. 																																	2024-10-18	PPRN:88478496		
J	Kumar, Ishita; Viswanathan, Snigdha; Yerra, Sushrita; Salemi, Alireza; Rossi, Ryan A.; Dernoncourt, Franck; Deilamsalehy, Hanieh; Chen, Xiang; Zhang, Ruiyi; Agarwal, Shubham; Lipka, Nedim; Van Nguyen, Chein; Zamani, Hamed; Nguyen, Thien Huu				Rossi, Ryan/C-7974-2013; Salemi, Alireza/IWM-0238-2023; Zhang, Ruiyi/AAB-8402-2021						LongLaMP: A Benchmark for Personalized Long-form Text Generation								Arxiv											2	2;2024-09-27;https://www.arxiv.org/abs/2407.11016v2| 1;2024-06-27;https://www.arxiv.org/abs/2407.11016v1	arXiv:2407.11016			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 27 2024	2024	Long-text generation is seemingly ubiquitous in real-world applications of large language models such as generating an email or writing a review. Despite the fundamental importance and prevalence of long-text generation in many practical applications, existing work on personalized generation has focused on the generation of very short text. To overcome these limitations, we study the problem of personalized long-text generation, that is, generating long-text that is personalized for a specific user while being practically useful for the vast majority of real-world applications that naturally require the generation of longer text. In this work, we demonstrate the importance of user-specific personalization for long-text generation tasks and develop the Long-text Language Model Personalization (LongLaMP) Benchmark. LongLaMP provides a comprehensive and diverse evaluation framework for personalized long-text generation. Extensive experiments on LongLaMP for zero-shot and fine-tuned language tasks demonstrate the effectiveness of the proposed benchmark and its utility for developing and evaluating techniques for personalized long-text generation across a wide variety of long-text generation tasks. The results highlight the importance of personalization across a wide variety of long-text generation tasks. Finally, we release the benchmark for others to use for this important problem.																																	2024-10-09	PPRN:90851617		
J	Bharadhwaj, Homanga; Dwibedi, Debidatta; Gupta, Abhinav; Tulsiani, Shubham; Doersch, Carl; Xiao, Ted; Shah, Dhruv; Xia, Fei; Sadigh, Dorsa; Kirmani, Sean				Bharadhwaj, Homanga/LQK-7079-2024; Xia, Fei/AAW-8782-2021						Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation								Arxiv											1	1;2024-09-24;https://www.arxiv.org/abs/2409.16283v1	arXiv:2409.16283			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 24 2024	2024	How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video. To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data. 																																	2024-11-10	PPRN:98864162		
J	Hu, Anwen; Xu, Haiyang; Zhang, Liang; Ye, Jiabo; Yan, Ming; Zhang, Ji; Jin, Qin; Huang, Fei; Zhou, Jingren				Zhou, Mingyuan/AAE-8717-2021; Xu, Haiyang/AAC-2095-2021						mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding								Arxiv											2	2;2024-09-09;https://www.arxiv.org/abs/2409.03420v2| 1;2024-09-05;https://www.arxiv.org/abs/2409.03420v1	arXiv:2409.03420			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 09 2024	2024	Multimodel Large Language Models(MLLMs) have achieved promising OCR-free Document Understanding performance by increasing the supported resolution of document images. However, this comes at the cost of generating thousands of visual tokens for a single document image, leading to excessive GPU memory and slower inference times, particularly in multi-page document comprehension. In this work, to address these challenges, we propose a High-resolution DocCompressor module to compress each high-resolution document image into 324 tokens, guided by low-resolution global visual features. With this compression module, to strengthen multi-page document comprehension ability and balance both token efficiency and question-answering performance, we develop the DocOwl2 under a three-stage training framework: Single-image Pretraining, Multi-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new state-of-the-art across multi-page document understanding benchmarks and reduces first token latency by more than 50%, demonstrating advanced capabilities in multi-page questioning answering, explanation with evidence pages, and cross-page structure understanding. Additionally, compared to single-image MLLMs trained on similar data, our DocOwl2 achieves comparable single-page understanding performance with less than 20% of the visual tokens.																																	2024-09-26	PPRN:91750372		
J	Reichardt, Ben W.; Aasen, David; Chao, Rui; Chernoguzov, Alex; van Dam, Wim; Gaebler, John P.; Gresh, Dan; Lucchetti, Dominic; Mills, Michael; Moses, Steven A.; Neyenhuis, Brian; Paetznick, Adam; Paz, Andres; Siegfried, Peter E.; da Silva, Marcus P.; Svore, Krysta M.; Wang, Zhenghan; Zanner, Matt				Chao, Rui/ABC-9051-2020; van der Aalst, Wil/G-1248-2011; Paz, Andrés/GQH-9815-2022; Wang, Zhenghan/LXV-5298-2024						Demonstration of quantum computation and error correction with a tesseract code								Arxiv											1	1;2024-09-06;https://www.arxiv.org/abs/2409.04628v1	arXiv:2409.04628			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 06 2024	2024	A critical milestone for quantum computers is to demonstrate fault-tolerant computation that outperforms computation on physical qubits. The tesseract subsystem color code protects four logical qubits in 16 physical qubits, to distance four. Using the tesseract code on Quantinuum's trapped-ion quantum computers, we prepare high-fidelity encoded graph states on up to 12 logical qubits, beneficially combining for the first time fault-tolerant error correction and computation. We also protect encoded states through up to five rounds of error correction. Using performant quantum software and hardware together allows moderate-depth logical quantum circuits to have an order of magnitude less error than the equivalent unencoded circuits.																																	2024-09-23	PPRN:91803906		
J	Angrisani, Armando; Schmidhuber, Alexander; Rudolph, Manuel S.; Cerezo, M.; Holmes, Zoe; Huang, Hsin-Yuan				Cerezo, Marco/ABD-9254-2020						Classically estimating observables of noiseless quantum circuits								Arxiv											1	1;2024-09-03;https://www.arxiv.org/abs/2409.01706v1	arXiv:2409.01706			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 03 2024	2024	We present a classical algorithm for estimating expectation values of arbitrary observables on most quantum circuits across all circuit architectures and depths, including those with all-to-all connectivity. We prove that for any architecture where each circuit layer is equipped with a measure invariant under single-qubit rotations, our algorithm achieves a small error ε on all circuits except for a small fraction δ. The computational time is polynomial in qubit count and circuit depth for any small constant ε, δ, and quasi-polynomial for inverse-polynomially small ε, δ. For non-classicallysimulable input states or observables, the expectation values can be estimated by augmenting our algorithm with classical shadows of the relevant state or observable. Our approach leverages a Pauli-path method under Heisenberg evolution. While prior works are limited to noisy quantum circuits, we establish classical simulability in noiseless regimes. Given that most quantum circuits in an architecture exhibit chaotic and locally scrambling behavior, our work demonstrates that estimating observables of such quantum dynamics is classically tractable across all geometries.																																	2024-09-12	PPRN:91717567		
J	Peng, Shuai; Fu, Di; Gao, Liangcai; Zhong, Xiuqin; Fu, Hongguang; Tang, Zhi				gao, liangcai/P-8338-2017						MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models								Arxiv											1	1;2024-08-30;https://www.arxiv.org/abs/2409.00147v1	arXiv:2409.00147			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 30 2024	2024	The rapid development of large language models (LLMs) has spurred extensive research into their domain-specific capabilities, particularly mathematical reasoning. However, most open-source LLMs focus solely on mathematical reasoning, neglecting the integration with visual injection, despite the fact that many mathematical tasks rely on visual inputs such as geometric diagrams, charts, and function plots. To fill this gap, we introduce MultiMath-7B, , a multimodal large language model that bridges the gap between math and vision. MultiMath-7B is trained through a four-stage process, focusing on vision-language alignment, visual and math instruction-tuning, and process-supervised reinforcement learning. We also construct a novel, diverse and comprehensive multimodal mathematical dataset, MultiMath300K, , which spans K-12 levels with image captions and step-wise solutions. MultiMath-7B achieves state-of-the-art (SOTA) performance among open-source models on existing multimodal mathematical benchmarks and also excels on text-only mathematical benchmarks. 																																	2024-09-12	PPRN:91720575		
J	Ahamed, Md Atik; Cheng, Qiang				Ahamed, Md Atik/MXK-8750-2025						TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting								Arxiv											2	2;2024-08-22;https://www.arxiv.org/abs/2403.09898v2| 1;2024-03-14;https://www.arxiv.org/abs/2403.09898v1	arXiv:2403.09898			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 22 2024	2024	Long-term time-series forecasting remains challenging due to the difficulty in capturing long-term dependencies, achieving linear scalability, and maintaining computational efficiency. We introduce TimeMachine, an innovative model that leverages Mamba, a state-space model, to capture long-term dependencies in multivariate time series data while maintaining linear scalability and small memory footprints. TimeMachine exploits the unique properties of time series data to produce salient contextual cues at multi-scales and leverage an innovative integrated quadruple-Mamba architecture to unify the handling of channel-mixing and channel-independence situations, thus enabling effective selection of contents for prediction against global and local contexts at different scales. Experimentally, TimeMachine achieves superior performance in prediction accuracy, scalability, and memory efficiency, as extensively validated using benchmark datasets. 																																	2024-09-04	PPRN:88166080		
J	Zang, Yuhang; Li, Wei; Han, Jun; Zhou, Kaiyang; Loy, Chen Change				Zhou, Kaiyang/KTI-8952-2024; Li, Wei/JFB-3448-2023; Zang, Yuhang/AES-3018-2022						Contextual Object Detection with Multimodal Large Language Models								Arxiv											2	2;2024-08-12;https://www.arxiv.org/abs/2305.18279v2| 1;2023-05-29;https://www.arxiv.org/abs/2305.18279v1	arXiv:2305.18279			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 12 2024	2024	Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection - understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. 																																	2024-08-21	PPRN:72756911		
J	Wang, Junxiong; Gangavarapu, Tushaar; Yan, Jing Nathan; Rush, Alexander M.				Gangavarapu, Tushaar/AAE-3425-2019						MambaByte: Token-free Selective State Space Model								Arxiv											3	3;2024-08-09;https://www.arxiv.org/abs/2401.13660v3| 2;2024-04-03;https://www.arxiv.org/abs/2401.13660v2| 1;2024-01-24;https://www.arxiv.org/abs/2401.13660v1	arXiv:2401.13660			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 09 2024	2024	Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.																																	2024-08-21	PPRN:87316647		
J	Sun, Hanshi; Chen, Zhuoming; Yang, Xinyu; Tian, Yuandong; Chen, Beidi				Sun, Hanshi/OEO-7890-2025						TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding								Arxiv											3	3;2024-08-04;https://www.arxiv.org/abs/2404.11912v3| 2;2024-04-23;https://www.arxiv.org/abs/2404.11912v2| 1;2024-04-18;https://www.arxiv.org/abs/2404.11912v1	arXiv:2404.11912			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 04 2024	2024	With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the autoregressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TRIFORCE, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TRI- RI- F ORCE not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31× on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, T RI F ORCE achieves 0.108s/token—only half as slow as the auto-regressive baseline on an A100, which attains 7.78× on our optimized offloading system. Additionally, T RI F ORCE performs 4.86× than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. T RI F ORCE ’s robustness is highlighted by its consistently outstanding performance across various temperatures.																																	2024-08-09	PPRN:88562197		
J	Evans, Zach; Parker, Julian D.; Carr, CJ; Zukowski, Zack; Taylor, Josiah; Pons, Jordi										LONG-FORM MUSIC GENERATION WITH LATENT DIFFUSION								Arxiv											2	2;2024-07-29;https://www.arxiv.org/abs/2404.10301v2| 1;2024-04-16;https://www.arxiv.org/abs/2404.10301v1	arXiv:2404.10301			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 29 2024	2024	Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure from text prompts. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m 45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5 Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.																																	2024-08-04	PPRN:88539071		
J	Vatsal, Shubham; Dubey, Harsh				Dubey, Harsh/ODL-2253-2025; Vatsal, Shubham/NTR-1000-2025						A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks								Arxiv											2	2;2024-07-24;https://www.arxiv.org/abs/2407.12994v2| 1;2024-07-17;https://www.arxiv.org/abs/2407.12994v1	arXiv:2407.12994			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 24 2024	2024	Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs’ knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.																																	2024-07-31	PPRN:90880993		
J	Teng, Yao; Wu, Yue; Shi, Han; Ning, Xuefei; Dai, Guohao; Wang, Yu; Li, Zhenguo; Liu, Xihui				Liu, Xihui/LHA-5141-2024; Lv, Zhengtong/AAW-9611-2020						DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis								Arxiv											2	2;2024-07-10;https://www.arxiv.org/abs/2405.14224v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14224v1	arXiv:2405.14224			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 10 2024	2024	Diffusion models have achieved great success in image generation, with the backbone evolving from U-Net to Vision Transformers. However, the computational cost of Transformers is quadratic to the number of tokens, leading to significant challenges when dealing with high-resolution images. In this work, we propose Diffusion Mamba (DiM), which combines the efficiency of Mamba, a sequence model based on State Space Models (SSM), with the expressive power of diffusion models for efficient high-resolution image synthesis. To address the challenge that Mamba cannot generalize to 2D signals, we make several architecture designs including multi-directional scans, learnable padding tokens at the end of each row and column, and lightweight local feature enhancement. Our DiM architecture achieves inference-time efficiency for high-resolution images. In addition, to further improve training efficiency for high-resolution image generation with DiM, we investigate “weak-to-strong” training strategy that pretrains DiM on low-resolution images (256 × 256) and then finetune it on high-resolution images (512 × 512). We further explore training-free upsampling strategies to enable the model to generate higher-resolution images ( e.g. ,1024 × 1024 and 1536 × 1536) without further fine-tuning. Experiments demonstrate the effectiveness and efficiency of our DiM. 																																	2024-07-21	PPRN:88987885		
J	Kande, Rahul; Pearce, Hammond; Tan, Benjamin; Dolan-Gavitt, Brendan; Thakur, Shailja; Karri, Ramesh; Rajendran, Jeyavijayan				Kande, Rahul/ONK-3480-2025; Tan, Benjamin/Q-8521-2019; Pearce, Hammond/AAJ-1986-2020						(Security) Assertions by Large Language Models								Arxiv											1	1;2024-07-09;https://www.arxiv.org/abs/2306.14027v2	arXiv:2306.14027			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 09 2024	2024	The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce System-Verilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of prompts, and we create a benchmark suite comprising real-world hardware designs and corresponding golden reference assertions that we want to generate with the LLM.																																	2024-07-21	PPRN:90760773		
J	Lu, Xing Han										BM25S: Orders of magnitude faster lexical search via eager sparse scoring								Arxiv											1	1;2024-07-04;https://www.arxiv.org/abs/2407.03618v1	arXiv:2407.03618			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	We introduce BM25S, an efficient Python-based implementation of BM25 that only depends on Numpy1 and Scipy2. BM25S achieves up to a 500x speedup compared to the most popular Python-based framework by eagerly computing BM25 scores during indexing and storing them into sparse matrices. It also achieves considerable speedups compared to highly optimized Java-based implementations, which are used by popular commercial products. Finally, BM25S reproduces the exact implementation of five BM25 variants based on Kamphuis et al. (2020) by extending eager scoring to non-sparse variants using a novel score shifting method.																																	2024-07-20	PPRN:90726682		
J	Arkani-Hamed, Nima; Baumann, Daniel; Hillman, Aaron; Joyce, Austin; Lee, Hayden; Pimentel, Guilherme L.										Differential Equations for Cosmological Correlators								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2312.05303v2| 1;2023-12-08;https://www.arxiv.org/abs/2312.05303v1	arXiv:2312.05303			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	Cosmological fluctuations retain a memory of the physics that generated them in their spatial correlations. The strength of correlations varies smoothly as a function of external kinematics, which is encoded in differential equations satisfied by cosmological correlation functions. In this work, we provide a broader perspective on the origin and structure of these differential equations. As a concrete example, we study conformally coupled scalar fields in a power-law cosmology. The wavefunction coefficients in this model have integral representations, with the integrands being the product of the corresponding flat-space results and “twist factors” that depend on the cosmological evolution. Similar twisted integrals arise for loop amplitudes in dimensional regularization, and their recent study has led to the discovery of rich mathematical structures and powerful new tools for computing multi-loop Feynman integrals in quantum field theory. The integrals of interest in cosmology are also part of a finite-dimensional basis of master integrals, which satisfy a system of first-order differential equations. We develop a formalism to derive these differential equations for arbitrary tree graphs. The results can be represented in graphical form by associating the singularities of the differential equations with a set of graph tubings. Upon differentiation, these tubings grow in a local and predictive fashion. In fact, a few remarkably simple rules allow us to predict—by hand—the equations for all tree graphs. While the rules of this “kinematic flow” are defined purely in terms of data on the boundary of the spacetime, they reflect the physics of bulk time evolution. We also study the analogous structures in tr ϕ3 theory, and see some glimpses of hidden structure in the sum over planar graphs. This suggests that there is an autonomous combinatorial or geometric construction from which cosmological correlations, and the associated spacetime, emerge.																																	2024-08-05	PPRN:86548145		
J	Mitra, Arindam; Del Corro, Luciano; Zheng, Guoqing; Mahajan, Shweti; Rouhana, Dany; Codas, Andres; Lu, Yadong; Chen, Wei-ge; Vrousgos, Olga; Rosset, Corby; Silva, Fillipe; Khanpour, Hamed; Lara, Yash; Awadallah, Ahmed				Zheng, Guo-qing/A-4412-2019						AgentInstruct: Toward Generative Teaching with Agentic Flows								Arxiv											1	1;2024-07-03;https://www.arxiv.org/abs/2407.03502v1	arXiv:2407.03502			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 03 2024	2024	Synthetic data is becoming increasingly important for accelerating the development of language models, both large and small. Despite several successful use cases, researchers also raised concerns around model collapse and drawbacks of imitating other models. This discrepancy can be attributed to the fact that synthetic data varies in quality and diversity. Effective use of synthetic data usually requires significant human effort in curating the data. We focus on using synthetic data for post-training, specifically creating data by powerful models to teach a new skill or behavior to another model, we refer to this setting as Generative Teaching. We introduce AgentInstruct, an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data. AgentInstruct can create both the prompts and responses, using only raw data sources like text documents and code files as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset of 25M pairs to teach language models different skills, such as text editing, creative writing, tool usage, coding, reading comprehension, etc. The dataset can be used for instruction tuning of any base model. We post-train Mistral-7b with the data. When comparing the resulting model Orca-3 to Mistral-7b-Instruct (which uses the same base model), we observe significant improvements across many benchmarks. For example, 40% improvement on AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and 45% improvement on AlpacaEval. Additionally, it consistently outperforms other models such as LLAMA-8B-instruct and GPT-3.5-turbo.																																	2024-07-20	PPRN:90726915		
J	Dong, Hongyuan; Li, Jiawen; Wu, Bohong; Wang, Jiacong; Zhang, Yuan; Guo, Haoyuan				Guo, Haoyuan/KZU-9578-2024						Benchmarking and Improving Detail Image Caption								Arxiv											3	3;2024-05-31;https://www.arxiv.org/abs/2405.19092v3| 2;2024-05-30;https://www.arxiv.org/abs/2405.19092v2| 1;2024-07-01;	arXiv:2405.19092			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 01 2024	2024	Image captioning has long been regarded as a fundamental task in visual understanding. Recently, however, few large vision-language model (LVLM) research discusses model's image captioning performance because of the outdated short-caption benchmarks and unreliable evaluation metrics. In this work, we propose to benchmark detail image caption task by curating high-quality evaluation datasets annotated by human experts, GPT-4V and Gemini-1.5-Pro. We also design a more reliable caption evaluation metric called CAPTURE (CAPtion evaluation by exTracting and coUpling coRE information). CAPTURE extracts visual elements, e.g., objects, attributes and relations from captions, and then matches these elements through three stages, achieving the highest consistency with expert judgements over other rule-based or model-based caption metrics. The proposed benchmark and metric provide reliable evaluation for LVLM's detailed image captioning ability. Guided by this evaluation, we further explore to unleash LVLM's detail caption capabilities by synthesizing high-quality data through a five-stage data construction pipeline. Our pipeline only uses a given LVLM itself and other open-source tools, without any human or GPT-4V annotation in the loop. Experiments show that the proposed data construction strategy significantly improves model-generated detail caption data quality for LVLMs with leading performance, and the data quality can be further improved in a self-looping paradigm. 																																	2024-11-17	PPRN:89113426		
J	Mukhoti, Jishnu; Gal, Yarin; Torr, Philip H.S.; Dokania, Puneet K.										Fine-tuning can cripple your foundation model; preserving features may be the solution								Arxiv											3	3;2024-07-01;https://www.arxiv.org/abs/2308.13320v3| 2;2024-02-04;https://www.arxiv.org/abs/2308.13320v2| 1;2023-08-25;https://www.arxiv.org/abs/2308.13320v1	arXiv:2308.13320			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Pre-trained foundation models, due to their enormous capacity and exposure to vast amounts of data during pre-training, are known to have learned plenty of real-world concepts. An important step in making these pre-trained models effective on downstream tasks is to fine-tune them on related datasets. While various fine-tuning methods have been devised and have been shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks different from the downstream one is reduced significantly compared to its pre-trained counterpart. This is an undesirable effect of fine-tuning as a substantial amount of resources was used to learn these pre-trained concepts in the first place. We call this phenomenon ''concept forgetting'' and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we propose a simple fix to this problem by designing a new fine-tuning method called LDIFS (short for ℓ2 distance in feature space) that, while learning new concepts related to the downstream task, allows a model to preserve its pre-trained knowledge as well. Through extensive experiments on 10 fine-tuning tasks we show that LDIFS significantly reduces concept forgetting. Additionally, we show that LDIFS is highly effective in performing continual fine-tuning on a sequence of tasks as well, in comparison with both fine-tuning as well as continual learning baselines.																																	2024-07-18	PPRN:83968076		
