PT	AU	BA	CA	GP	RI	OI	BE	Z2	AU	AA	TI	X1	Y1	Z1	FT	PN	AE	Z3	SO	S1	SE	BS	VL	IS	SI	MA	BP	EP	AR	VN	VH	DI	D2	L1	L2	L3	EA	SU	DT	PD	PY	AB	X4	Y4	Z4	AK	CT	CY	SP	CL	TC	Z8	ZR	ZA	ZB	ZS	Z9	U1	U2	SN	EI	BN	G1	NR	CR	LA	AS	AC	CG	DG	C1	C3	EC	DE	DA	UT	PM	
J	Di Bari, Pasquale; Rahat, Moinul Hossain										The split majoron model confronts the NANOGrav signal and cosmological tensions								Arxiv											3	3;2024-07-30;https://www.arxiv.org/abs/2307.03184v3| 2;2023-09-21;https://www.arxiv.org/abs/2307.03184v2| 1;2023-07-06;https://www.arxiv.org/abs/2307.03184v1	arXiv:2307.03184			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 30 2024	2024	In the light of the evidence of a gravitational wave background from the NANOGrav 15yr data set, we reconsider the split majoron model as a new physics extension of the standard model able to generate a needed contribution to solve the current tension between the data and the standard interpretation in terms of inspiraling supermassive black hole massive binaries. In the split majoron model the seesaw right-handed neutrinos acquire Majorana masses from spontaneous symmetry breaking of global U(1)B-L in a strong first order phase transition of a complex scalar field occurring above the electroweak scale. The final vacuum expectation value couples to a second complex scalar field undergoing a low scale phase transition occurring after neutrino decoupling. Such a coupling enhances the strength of this second low scale first order phase transition and can generate a sizeable primordial gravitational wave background contributing to the NANOGrav 15yr signal. Some amount of extra-radiation is generated after neutron-to-proton ration freeze-out but prior to nucleosynthesis. This can be either made compatible with current upper bound from primordial deuterium measurements or even be used to solve a potential deuterium problem. Moreover, the free streaming length of light neutrinos can be suppressed by their interactions with the resulting majoron background and this mildly ameliorates existing cosmological tensions. Thus cosmological observations nicely provide independent motivations for the model.																																	2024-08-06	PPRN:73806725		
J	Ren, Jie; Xu, Han; He, Pengfei; Cui, Yingqian; Zeng, Shenglai; Zhang, Jiankun; Wen, Hongzhi; Ding, Jiayuan; Huang, Pei; Lyu, Lingjuan; Liu, Hui; Chang, Yi; Tang, Jiliang				huang, pei/LOR-9384-2024; WEN, Hongzhi/JFJ-3882-2023; Zeng, Shenglai/HTR-1153-2023; Cui, Yingqian/KBA-8229-2024						Copyright Protection in Generative AI: A Technical Perspective								Arxiv											3	3;2024-07-24;https://www.arxiv.org/abs/2402.02333v2| 2;2024-02-04;https://www.arxiv.org/abs/2402.02333v1| 1;2024-02-04;https://www.arxiv.org/abs/2402.02333v1	arXiv:2402.02333			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 24 2024	2024	Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.																																	2024-07-31	PPRN:87523603		
J	Pan, Zijie; Yang, Zeyu; Zhu, Xiatian; Zhang, Li				Pan, Zijie/MEQ-2078-2025; Zhu, Xiatian/Y-1601-2019; Yang, Zeyu/OVX-7271-2025						Efficient4D: Fast Dynamic 3D Object Generation from a Single-view Video								Arxiv											3	3;2024-07-22;https://www.arxiv.org/abs/2401.08742v3| 2;2024-03-27;https://www.arxiv.org/abs/2401.08742v2| 1;2024-01-16;https://www.arxiv.org/abs/2401.08742v1	arXiv:2401.08742			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 22 2024	2024	Generating dynamic 3D object from a single-view video is challenging due to the lack of 4D labeled data. An intuitive approach is to extend previous image-to-3D pipelines by transferring off-the-shelf image generation models such as score distillation sampling.However, this approach would be slow and expensive to scale due to the need for back-propagating the information-limited supervision signals through a large pretrained model. To address this, we propose an efficient video-to-4D object generation framework called Efficient4D. It generates high-quality spacetime-consistent images under different camera views, and then uses them as labeled data to directly reconstruct the 4D content through a 4D Gaussian splatting model. Importantly, our method can achieve real-time rendering under continuous camera trajectories. To enable robust reconstruction under sparse views, we introduce inconsistency-aware confidence-weighted loss design, along with a lightly weighted score distillation loss. Extensive experiments on both synthetic and real videos show that Efficient4D offers a remarkable 10-fold increase in speed when compared to prior art alternatives while preserving the quality of novel view synthesis. For example, Efficient4D takes only 10 minutes to model a dynamic object, vs 120 minutes by the previous art model Consistent4D.																																	2024-07-28	PPRN:87209399		
J	Antoniades, Antonis; Wang, Xinyi; Elazar, Yanai; Amayuelas, Alfonso; Albalak, Alon; Zhang, Kexun; Wang, William Yang				Zhang, kexun/JKH-7881-2023; Wang, Xinyi/IAM-0594-2023						Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data								Arxiv											1	1;2024-07-20;https://www.arxiv.org/abs/2407.14985v1	arXiv:2407.14985			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 20 2024	2024	Despite the proven utility of large language models (LLMs) in real-world applications, there remains a lack of understanding regarding how they leverage their large-scale pretraining text corpora to achieve such capabilities. In this work, we investigate the interplay between generalization and memorization in pretrained LLMs at scale, through a comprehensive n-gram analysis of their training data. Our experiments focus on three general task types: translation, question-answering, and multiple-choice reasoning. With various sizes of open-source LLMs and their pretraining corpora, we observe that as the model size increases, the task-relevant n-gram pair data becomes increasingly important, leading to improved task performance, decreased memorization, stronger generalization, and emergent abilities. Our results support the hypothesis that LLMs’ capabilities emerge from a delicate balance of memorization and generalization with sufficient task-related pretraining data, and point the way to larger-scale analyses that could further improve our understanding of these models.																																	2024-07-28	PPRN:91025559		
J	Weng, Yuetian; Han, Mingfei; He, Haoyu; Chang, Xiaojun; Zhuang, Bohan				Han, Mingfei/AAX-3413-2021; ZHANG, JING/HKF-4837-2023; Chang, Xiaojun/F-8887-2014						LongVLM: Efficient Long Video Understanding via Large Language Models								Arxiv											3	3;2024-07-20;https://www.arxiv.org/abs/2404.03384v3| 2;2024-04-10;https://www.arxiv.org/abs/2404.03384v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.03384v1	arXiv:2404.03384			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 20 2024	2024	Empowered by Large Language Models (LLMs), recent advancements in Video-based LLMs (VideoLLMs) have driven progress in various video understanding tasks. These models encode video representations through pooling or query aggregation over a vast number of visual tokens, making computational and memory costs affordable. Despite successfully providing an overall comprehension of video content, existing VideoLLMs still face challenges in achieving detailed understanding due to overlooking local information in long-term videos. To tackle this challenge, we introduce LongVLM, a simple yet powerful VideoLLM for long video understanding, building upon the observation that long videos often consist of sequential key events, complex actions, and camera movements. Our approach proposes to decompose long videos into multiple short-term segments and encode local features for each segment via a hierarchical token merging module. These features are concatenated in temporal order to maintain the storyline across sequential short-term segments. Additionally, we propose to integrate global semantics into each local feature to enhance context understanding. In this way, we encode video representations that incorporate both local and global information, enabling the LLM to generate comprehensive responses for long-term videos. Experimental results on the VideoChatGPT benchmark and zero-shot video question-answering datasets demonstrate the superior capabilities of our model over the previous state-of-the-art methods. Qualitative examples show that our model produces more precise responses for long video understanding. 																																	2024-07-28	PPRN:88413776		
J	Li, Baolin; Jiang, Yankai; Gadepally, Vijay; Tiwari, Devesh				Li, Baolin/N-8884-2019; Jiang, Yankai/ABI-6024-2020						LLM Inference Serving: Survey of Recent Advances and Opportunities								Arxiv											1	1;2024-07-17;https://www.arxiv.org/abs/2407.12391v1	arXiv:2407.12391			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 17 2024	2024	This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.																																	2024-07-26	PPRN:90868671		
J	Li, Jiaman; Clegg, Alexander; Mottaghi, Roozbeh; Wu, Jiajun; Puig, Xavier; Liu, C. Karen				Wu, Jiajun/C-4123-2013; Li, Jiaman/LSJ-4828-2024						Controllable Human-Object Interaction Synthesis								Arxiv											2	2;2024-07-14;https://www.arxiv.org/abs/2312.03913v2| 1;2023-12-06;https://www.arxiv.org/abs/2312.03913v1	arXiv:2312.03913			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 14 2024	2024	Synthesizing semantic-aware, long-horizon, human-object interaction is critical to simulate realistic human behaviors. In this work, we address the challenging problem of generating synchronized object motion and human motion guided by language descriptions in 3D scenes. We propose Controllable Human-Object Interaction Synthesis (CHOIS), an approach that generates object motion and human motion simultaneously using a conditional diffusion model given a language description, initial object and human states, and sparse object waypoints. Here, language descriptions inform style and intent, and waypoints, which can be effectively extracted from high-level planning, ground the motion in the scene. Naively applying a diffusion model fails to predict object motion aligned with the input waypoints; it also cannot ensure the realism of interactions that require precise hand-object and human-floor contact. To overcome these problems, we introduce an object geometry loss as additional supervision to improve the matching between generated object motion and input object waypoints; we also design guidance terms to enforce contact constraints during the sampling process of the trained diffusion model. We demonstrate that our learned interaction module can synthesize realistic human-object interactions, adhering to provided textual descriptions and sparse waypoint conditions. Additionally, our module seamlessly integrates with a path planning module, enabling the generation of long-term interactions in 3D environments.																																	2024-07-23	PPRN:86435729		
J	Kumar, Ashutosh; Murthy, Shiv Vignesh; Singh, Sagarika; Ragupathy, Swathy										The Ethics of Interaction: Mitigating Security Threats in LLMs								Arxiv											2	2;2024-07-10;https://www.arxiv.org/abs/2401.12273v2| 1;2024-01-22;https://www.arxiv.org/abs/2401.12273v1	arXiv:2401.12273			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 10 2024	2024	This paper comprehensively explores the ethical challenges arising from security threats to Large Language Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats—prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hate-based content—going beyond mere identification to assess their critical ethical consequences and the urgency they create for robust defensive strategies. The escalating reliance on LLMs underscores the crucial need for ensuring these systems operate within the bounds of ethical norms, particularly as their misuse can lead to significant societal and individual harm. We propose conceptualizing and developing an evaluative tool tailored for LLMs, which would serve a dual purpose: guiding developers and designers in preemptive fortification of backend systems and scrutinizing the ethical dimensions of LLM chatbot responses during the testing phase. By comparing LLM responses with those expected from humans in a moral context, we aim to discern the degree to which AI behaviors align with the ethical values held by a broader society. Ultimately, this paper not only underscores the ethical troubles presented by LLMs; it also highlights a path toward cultivating trust in these systems.																																	2024-07-21	PPRN:87300902		
J	Fang, Chuan; Dong, Yuan; Luo, Kunming; Hu, Xiaotao; Shrestha, Rakesh; Tan, Ping				胡, 小涛/HZH-4894-2023						Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints								Arxiv											4	4;2024-07-02;https://www.arxiv.org/abs/2310.03602v3| 3;2023-10-09;https://www.arxiv.org/abs/2310.03602v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03602v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03602v1	arXiv:2310.03602			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 02 2024	2024	Text-driven 3D indoor scene generation is useful for gaming, the film industry, and AR/VR applications. However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room. To address these problems, we present Ctrl-Room, which can generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. Our key insight is to separate the modeling of layouts and appearance. Our proposed method consists of two stages: a Layout Generation Stage and an Appearance Generation Stage. The Layout Generation Stage trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. Next, the Appearance Generation Stage employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt. We thus achieve a high-quality 3D room generation with convincing layouts and lively textures. Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive edit-specific training. Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from natural language prompts.																																	2024-07-19	PPRN:85436127		
J	Selvaraju, Pratheba; Ding, Tianyu; Chen, Tianyi; Zharkov, Ilya; Liang, Luming				Selvaraju, Pratheba/JUF-6465-2023; Liang, Luming/E-3371-2016						FORA: Fast-Forward Caching in Diffusion Transformer Acceleration								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.01425v1	arXiv:2407.01425			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Diffusion transformers (DiT) have become the de facto choice for generating high-quality images and videos, largely due to their scalability, which enables the construction of larger models for enhanced performance. However, the increased size of these models leads to higher inference costs, making them less attractive for real-time applications. We present F ast-F OR ward C A ching (FORA), a simple yet effective approach designed to accelerate DiT by exploiting the repetitive nature of the diffusion process. FORA implements a caching mechanism that stores and reuses intermediate outputs from the attention and MLP layers across denoising steps, thereby reducing computational overhead. This approach does not require model retraining and seamlessly integrates with existing transformerbased diffusion models. Experiments show that FORA can speed up diffusion transformers several times over while only minimally affecting performance metrics such as the IS Score and FID. By enabling faster processing with minimal tradeoffs in quality, FORA represents a significant advancement in deploying diffusion transformers for real-time applications. Code will be made publicly available at: https://github.com/prathebaselva/FORA .																																	2024-07-18	PPRN:90656245		
J	Wang, Shaowen; Yu, Linxi; Li, Jian										LoRA-GA: Low-Rank Adaptation with Gradient Approximation								Arxiv											2	2;2024-07-16;https://www.arxiv.org/abs/2407.05000v2| 1;2024-07-01;	arXiv:2407.05000			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 01 2024	2024	Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. LoRA, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters. Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, LoRA-GA (Low Rank Adaptation with Gradient Approximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA, validating its effectiveness in accelerating convergence and enhancing model performance. 																																	2024-11-17	PPRN:90851200		
J	Yan, Ge; Wu, Wenjie; Chen, Yuheng; Pan, Kaisen; Lu, Xudong; Zhou, Zixiang; Wang, Yuhan; Wang, Ruocheng; Yan, Junchi				Zhou, Zixiang/IAM-7266-2023; Yan, Junchi/ADK-0658-2022						Quantum Circuit Synthesis and Compilation Optimization: Overview and Prospects								Arxiv											1	1;2024-06-30;https://www.arxiv.org/abs/2407.00736v1	arXiv:2407.00736			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 30 2024	2024	Quantum computing is regarded as a promising paradigm that may overcome the current computational power bottlenecks in the post -Moore era. The increasing maturity of quantum processors, especially superconducting ones, provides more possibilities for the development and implementation of quantum algorithms. As the crucial stages for quantum algorithm implementation, the logic circuit design and quantum compiling have also received significant attention, which covers key technologies such as quantum logic circuit synthesis (also widely known as quantum architecture search) and optimization, as well as qubit mapping and routing. Recent studies suggest that the scale and precision of related algorithms are steadily increasing, especially with the integration of artificial intelligence methods. In this survey, we systematically review and summarize a vast body of literature, exploring the feasibility of an integrated design and optimization scheme that spans from the algorithmic level to quantum hardware, combining the steps of logic circuit design and compilation optimization. Leveraging the exceptional cognitive and learning capabilities of AI algorithms, one can reduce manual design costs, enhance the precision and efficiency of execution, and facilitate the implementation and validation of the superiority of quantum algorithms on hardware.																																	2024-07-18	PPRN:90656305		
J	Yang, Qu; Ye, Mang; Du, Bo				Shao, Ling/D-3535-2011; Ye, Mang/HCJ-0591-2022						EmoLLM: Multimodal Emotional Understanding Meets Large Language Models								Arxiv											2	2;2024-06-29;https://www.arxiv.org/abs/2406.16442v2| 1;2024-06-24;https://www.arxiv.org/abs/2406.16442v1	arXiv:2406.16442			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 29 2024	2024	Multi-modal large language models (MLLMs) have achieved remarkable performance on objective multimodal perception tasks, but their ability to interpret subjective, emotionally nuanced multimodal content remains largely unexplored. Thus, it impedes their ability to effectively understand and react to the intricate emotions expressed by humans through multimodal media. To bridge this gap, we introduce EmoBench, the first comprehensive benchmark designed specifically to evaluate the emotional capabilities of MLLMs across five popular emotional tasks, using a diverse dataset of 287k images and videos paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a novel model for multimodal emotional understanding, incorporating with two core techniques. 1) Multi-perspective Visual Projection, it captures diverse emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it guides MLLMs to reason about emotions in the correct direction. Experimental results demonstrate that EmoLLM significantly elevates multimodal emotional understanding performance, with an average improvement of 12.1% across multiple foundation models on EmoBench. Our work contributes to the advancement of MLLMs by facilitating a deeper and more nuanced comprehension of intricate human emotions, paving the way for the development of artificial emotional intelligence capabilities with wide-ranging applications in areas such as human-computer interaction, mental health support, and empathetic AI systems. Code, data, and model will be released.																																	2024-07-18	PPRN:89407183		
J	Wu, Junfei; Liu, Qiang; Wang, Ding; Zhang, Jinghao; Wu, Shu; Wang, Liang; Tan, Tieniu				Wu, Junfei/OAJ-8855-2025						Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models								Arxiv											2	2;2024-06-28;https://www.arxiv.org/abs/2402.11622v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11622v1	arXiv:2402.11622			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 28 2024	2024	Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality1.																																	2024-07-17	PPRN:87761654		
J	Zhang, Wentao; Zhao, Lingxuan; Xia, Haochong; Sun, Shuo; Sun, Jiaze; Qin, Molei; Li, Xinyi; Zhao, Yuqing; Zhao, Yilei; Cai, Xinyu; Zheng, Longtao; Wang, Xinrun; An, Bo				CAI, XINYU/IWV-0949-2023; Li, Xinyi/HDO-8414-2022						A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist								Arxiv											2	2;2024-06-28;https://www.arxiv.org/abs/2402.18485v3| 1;2024-02-29;https://www.arxiv.org/abs/2402.18485v2	arXiv:2402.18485			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 28 2024	2024	Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent’s market intelligence module processes a diverse range of data—numerical, textual, and visual—to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent’s ability to learn from historical data and improve decision-making processes. The agent’s emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data -driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.																																	2024-07-17	PPRN:87987334		
J	Akhound-Sadegh, Tara; Rector-Brooks, Jarrid; Bose, Avishek Joey; Mittal, Sarthak; Lemos, Pablo; Liu, Cheng-Hao; Sendera, Marcin; Ravanbakhsh, Siamak; Gidel, Gauthier; Bengio, Yoshua; Malkin, Nikolay; Tong, Alexander				Liu, Cheng-Hao/JAX-3401-2023; Lemos, Pedro/G-5758-2013						Iterated Denoising Energy Matching for Sampling from Boltzmann Densities								Arxiv											2	2;2024-06-26;https://www.arxiv.org/abs/2402.06121v2| 1;2024-02-09;https://www.arxiv.org/abs/2402.06121v1	arXiv:2402.06121			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 26 2024	2024	Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many -body systems, is a foundational problem in science. In this paper, we propose I TERATED D ENOISING E NERGY M ATCHING (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient—and no data samples—to train a diffusion -based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion -based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation -free, , and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging from standard synthetic energy functions to invariant n -body particle systems. We show that the proposed approach achieves state-of-the-art performance on all metrics and trains 2 − 5× × faster, which allows it to be the first method to train using energy on the challenging 55 -particle Lennard-Jones system.																																	2024-07-15	PPRN:87618757		
J	Kang, Jikun; Li, Derek; Chen, Xi; Kazemi, Amirreza; Sun, Qianyi; Chen, Boxing; Li, Dong; He, Xu; He, Quan; Wen, Feng; Hao, Jianye; Yao, Jun				zhang, jiahao/KEE-9357-2024						MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time								Arxiv											4	4;2024-06-26;https://www.arxiv.org/abs/2405.16265v4| 3;2024-06-21;https://www.arxiv.org/abs/2405.16265v3| 2;2024-06-17;https://www.arxiv.org/abs/2405.16265v2| 1;2024-05-25;https://www.arxiv.org/abs/2405.16265v1	arXiv:2405.16265			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 26 2024	2024	Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method -- MindStar (M*). This method formulates reasoning tasks as searching problems and proposes two search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs. [GRAPHICS]																																	2024-07-15	PPRN:89070696		
J	Liu, Zhihan; Hu, Hao; Zhang, Shenao; Guo, Hongyi; Ke, Shuqi; Liu, Boyi; Wang, Zhaoran				Liu, Zhihan/NGS-3762-2025; Hu, Hao/KMX-7258-2024; Liu, Boyi/C-9181-2012; Zhang, Shenao/HLW-0562-2023; Ke, Shuqi/LRS-8847-2024						Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency								Arxiv											3	3;2024-06-24;https://www.arxiv.org/abs/2309.17382v3| 2;2023-10-11;https://www.arxiv.org/abs/2309.17382v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17382v1	arXiv:2309.17382			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 24 2024	2024	Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it is unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose the first framework with provable regret guarantees to orchestrate reasoning and acting, which we call “reason for future, act for now” ( RAFA ). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (“reason for future”). At each step, the LLM agent takes the initial action of the planned trajectory (“act for now”), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs with the memory buffer to estimate the unknown environment (learning) and generate an optimal trajectory for multiple future steps that maximize a value function (planning). The learning and planning subroutines are performed in an “in context” manner to emulate the actor -critic update for MDPs. Our theoretical √ analysis establishes a T regret, while our experimental validation demonstrates superior empirical performance. Here, T denotes the number of online interactions. 																																	2024-07-18	PPRN:85338115		
J	Stan, Gabriela Ben Melech; Aflalo, Estelle; Rohekar, Raanan Yehezkel; Bhiwandiwalla, Anahita; Shao-Yen, Tseng; Olson, Matthew Lyle; Gurwicz, Yaniv; Wu, Chenfei; Duan, Nan; Lal, Vasudev				Wu, Chenfei/AAJ-5232-2020						LVLM-Interpret: An Interpretability Tool for Large Vision-Language Models								Arxiv											3	3;2024-06-24;https://www.arxiv.org/abs/2404.03118v3| 2;2024-06-04;https://www.arxiv.org/abs/2404.03118v2| 1;2024-04-03;https://www.arxiv.org/abs/2404.03118v1	arXiv:2404.03118			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 24 2024	2024	In the rapidly evolving landscape of artificial intelligence, multi-modal large language models are emerging as a significant area of interest. These models, which combine various forms of data input, are becoming increasingly popular. However, understanding their internal mechanisms remains a complex task. Numerous advancements have been made in the field of explainability tools and mechanisms, yet there is still much to explore. In this work, we present a novel interactive application aimed towards understanding the internal mechanisms of large vision-language models. Our interface is designed to enhance the interpretability of the image patches, which are instrumental in generating an answer, and assess the efficacy of the language model in grounding its output in the image. With our application, a user can systematically investigate the model and uncover system limitations, paving the way for enhancements in system capabilities. Finally, we present a case study of how our application can aid in understanding failure mechanisms in a popular large multi-modal model: LLaVA.																																	2024-07-15	PPRN:88405497		
J	Chen, Jiaqi; Lin, Bingqian; Xu, Ran; Chai, Zhenhua; Liang, Xiaodan; Wong, Kwan-Yee K.				Wong, Kwan-Yee/C-1577-2009; Li, Xiaofeng/LIC-9574-2024; Chai, Zhenhua/HTM-6798-2023						MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation								Arxiv											3	3;2024-06-20;https://www.arxiv.org/abs/2401.07314v3| 2;2024-02-25;https://www.arxiv.org/abs/2401.07314v2| 1;2024-01-14;https://www.arxiv.org/abs/2401.07314v1	arXiv:2401.07314			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	Embodied agents equipped with GPT as their brains have exhibited extraordinary decisionmaking and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt GPT-4 to select potential locations within localized environments, without constructing an effective “global-view” for the agent to understand the overall environment. In this work, we present a novel map -guided GPT -based agent, dubbed 3 MapGPT , which introduces an online linguistic-formed map to encourage global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate nodes or sub-goals step by step. Extensive experiments demonstrate that our MapGPT is applicable to both GPT-4 and GPT-4V, achieving state-of-the-art zero-shot performance on R2R and REVERIE simultaneously ( ∼ 10% and ∼ 12% improvements in SR), and showcasing the newly emergent global thinking and path planning abilities of the GPT.																																	2024-07-06	PPRN:87185563		
J	Zhang, Shuo; Pan, Liangming; Zhao, Junzhou; Wang, William Yang				Pan, Liangming/LIF-2753-2024; Zhao, Junzhou/U-7287-2018						The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models								Arxiv											3	3;2024-06-13;https://www.arxiv.org/abs/2305.13669v3| 2;2023-11-03;https://www.arxiv.org/abs/2305.13669v2| 1;2023-05-23;https://www.arxiv.org/abs/2305.13669v1	arXiv:2305.13669			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 13 2024	2024	Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce M IX A LIGN , a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. M IX - A LIGN employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the effectiveness of M IX A LIGN in improving knowledge alignment by producing high-quality, user-centered clarifications. ‡																																	2024-07-02	PPRN:71594638		
J	Zhou, Zhenhong; Yu, Haiyang; Zhang, Xinghua; Xu, Rongwu; Huang, Fei; Li, Yongbin				XIAOJUAN, HU/GLQ-6536-2022; Li, Yongbin/GWM-7528-2022; 张, 兴华/HPG-1538-2023						How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States								Arxiv											2	2;2024-06-13;https://www.arxiv.org/abs/2406.05644v2| 1;2024-06-09;https://www.arxiv.org/abs/2406.05644v1	arXiv:2406.05644			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 13 2024	2024	Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns. 																																	2024-07-02	PPRN:89268735		
J	Liu, Fangfu; Wang, Hanyang; Yao, Shunyu; Zhang, Shengjun; Zhou, Jie; Duan, Yueqi				Liu, Fangfu/KPQ-4616-2024; Wang, Hanyang/JFB-3017-2023						Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion								Arxiv											3	3;2024-06-11;https://www.arxiv.org/abs/2406.04338v3| 2;2024-06-07;https://www.arxiv.org/abs/2406.04338v2| 1;2024-06-06;https://www.arxiv.org/abs/2406.04338v1	arXiv:2406.04338			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 11 2024	2024	In recent years, there has been rapid development in 3D generation models, opening up new possibilities for applications such as simulating the dynamic movements of 3D objects and customizing their behaviors. However, current 3D generative models tend to focus only on surface features such as color and shape, neglecting the inherent physical properties that govern the behavior of objects in the real world. To accurately simulate physics-aligned dynamics, it is essential to predict the physical properties of materials and incorporate them into the behavior prediction process. Nonetheless, predicting the diverse materials of real -world objects is still challenging due to the complex nature of their physical attributes. In this paper, we propose Physics3D , a novel method for learning various physical properties of 3D objects through a video diffusion model. Our approach involves designing a highly generalizable physical simulation system based on a viscoelastic material model, which enables us to simulate a wide range of materials with highfidelity capabilities. Moreover, we distill the physical priors from a video diffusion model that contains more understanding of realistic object materials. Extensive experiments demonstrate the effectiveness of our method with both elastic and plastic materials. Physics3D shows great potential for bridging the gap between the physical world and virtual neural space, providing a better integration and application of realistic physical principles in virtual environments. Project page: https://liuff19.github.io/Physics3D .																																	2024-07-02	PPRN:89232378		
J	Huang, Langwen; Gianinazzi, Lukas; Yu, Yuejiang; Dueben, Peter D.; Hoefler, Torsten				Hoefler, Torsten/HKF-3023-2023						DiffDA: a Diffusion Model for Weather-scale Data Assimilation								Arxiv											3	3;2024-06-10;https://www.arxiv.org/abs/2401.05932v3| 2;2024-03-05;https://www.arxiv.org/abs/2401.05932v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.05932v1	arXiv:2401.05932			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jun 10 2024	2024	The generation of initial conditions via accurate data assimilation is crucial for weather forecasting and climate modeling. We propose DiffDA as a denoising diffusion model capable of assimilating atmospheric variables using predicted states and sparse observations. Acknowledging the similarity between a weather forecast model and a denoising diffusion model dedicated to weather applications, we adapt the pretrained GraphCast neural network as the backbone of the diffusion model. Through experiments based on simulated observations from the ERA5 reanalysis dataset, our method can produce assimilated global atmospheric data consistent with observations at 0.25 deg (~30km) resolution globally. This marks the highest resolution achieved by ML data assimilation models. The experiments also show that the initial conditions assimilated from sparse observations (less than 0.96% of gridded data) and 48-hour forecast can be used for forecast models with a loss of lead time of at most 24 hours compared to initial conditions from state-of-the-art data assimilation in ERA5. This enables the application of the method to real-world applications, such as creating reanalysis datasets with autoregressive data assimilation.																																	2024-07-04	PPRN:87128334		
J	Kim, Seungone; Suk, Juyoung; Cho, Ji Yong; Longpre, Shayne; Kim, Chaeeun; Yoon, Dongkeun; Son, Guijin; Cho, Yejin; Shafayat, Sheikh; Baek, Jinheon; Park, Sue Hyun; Hwang, Hyeonbin; Jo, Jinkyung; Cho, Hyowon; Shin, Haebin; Lee, Seongyun; Oh, Hanseok; Lee, Noah; Ho, Namgyu; Joo, Se June; Ko, Miyoung; Lee, Yoonjoo; Chae, Hyungjoo; Shin, Jamin; Jang, Joel; Ye, Seonghyeon; Lin, Bill Yuchen; Welleck, Sean; Neubig, Graham; Lee, Moontae; Lee, Kyungjae; Seo, Minjoon				Baek, Jinheon/HLX-1814-2023						The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models								Arxiv											1	1;2024-06-09;https://www.arxiv.org/abs/2406.05761v1	arXiv:2406.05761			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 09 2024	2024	As language models (LMs) become capable of handling a wide range of tasks, their evaluation is becoming as challenging as their development. Most generation benchmarks currently assess LMs using abstract evaluation criteria—like —like helpfulness and harmlessness—which often lack the flexibility and granularity of human assessment. Additionally, these benchmarks tend to focus disproportionately on specific capabilities such as instruction following, leading to coverage bias. . To overcome these limitations, we introduce the BIGGEN I GG EN B ENCH , a principled generation benchmark designed to thoroughly evaluate nine distinct capabilities of LMs across 77 diverse tasks. A key feature of the BIGGEN I GG EN B ENCH is its use of instance -specific evaluation criteria, closely mirroring the nuanced discernment of human evaluation. We apply this benchmark to assess 103 frontier LMs using five evaluator LMs. Our code, data, and evaluation results are all publicly available 2 .																																	2024-07-04	PPRN:89266274		
J	Zhang, Wenqi; Shen, Yongliang; Wu, Linjuan; Peng, Qiuying; Wang, Jun; Zhuang, Yueting; Lu, Weiming				Shen, Yongliang/GWC-1883-2022; 吴, 林娟/GSD-1472-2022						Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives								Arxiv											3	3;2024-06-06;https://www.arxiv.org/abs/2401.02009v3| 2;2024-03-27;https://www.arxiv.org/abs/2401.02009v2| 1;2024-01-04;https://www.arxiv.org/abs/2401.02009v1	arXiv:2401.02009			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.																																	2024-06-22	PPRN:86965160		
J	Gardner, Josh; Durand, Simon; Stoller, Daniel; Bittner, Rachel M.				Stoller, Daniel/ADU-8049-2022						LLark: A Multimodal Instruction-Following Language Model for Music								Arxiv											3	3;2024-06-03;https://www.arxiv.org/abs/2310.07160v3| 2;2024-02-08;https://www.arxiv.org/abs/2310.07160v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07160v1	arXiv:2310.07160			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for emph{music} understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, reasoning), we show that LLark matches or outperforms existing baselines in music understanding, and that humans show a high degree of agreement with its responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper.																																	2024-06-19	PPRN:85540686		
J	Yang, Jiangang; Sun, Hualei; Hu, Xunwu; Xie, Yuyang; Miao, Taimin; Luo, Hailan; Chen, Hao; Liang, Bo; Zhu, Wenpei; Qu, Gexing; Chen, Cui-Qun; Huo, Mengwu; Huang, Yaobo; Zhang, Shenjin; Zhang, Fengfeng; Yang, Feng; Wang, Zhimin; Peng, Qinjun; Mao, Hanqing; Liu, Guodong; Xu, Zuyan; Qian, Tian; Yao, Dao-Xin; Wang, Meng; Zhao, Lin; Zhou, X.J.				Wenpei, Zhu/JDD-3726-2023; WANG, ZHIMIN/AAJ-8452-2021; Hu, Xunwu/JNS-4199-2023; Hu, Yong/GRJ-3794-2022						Orbital-Dependent Electron Correlation in Double-Layer Nickelate La3Ni2O7								Arxiv											2	2;2024-06-02;https://www.arxiv.org/abs/2309.01148v2| 1;2023-09-03;https://www.arxiv.org/abs/2309.01148v1	arXiv:2309.01148			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 02 2024	2024	The latest discovery of high temperature superconductivity near 80K in La3Ni2O7 under high pressure has attracted much attention. Many proposals are put forth to understand the origin of super conductivity.The determination of electronic structures is a prerequisite to establish theories to understand superconductivity in nickelates but is still lacking. Here we report our direct measurement of the electronic structures of La3Ni2O7 by high-resolution angle resolved photoemission spectroscopy. The Fermi surface and band structures of La3Ni2O7 are observed and compared with the band structure calculations. Strong electron correlations are revealed which are orbital- and momentum dependent. A flat band is formed from the Ni-3dz2 orbitals around the zone corner which is ~50meV below the Fermi level and exhibits the strongest electron correlation. In many theoretical proposals, this band is expected to play the dominant role in generating superconductivity in La3Ni2O7. Our observations provide key experimental information to understand the electronic structure and origin of high temperature superconductivity in La3Ni2O7.																																	2024-06-19	PPRN:84764490		
J	Yang, Zhaorui; Pang, Tianyu; Feng, Haozhe; Wang, Han; Chen, Wei; Zhu, Minfeng; Liu, Qian				Zhu, Minfeng/R-6788-2019; Tianyu, Pang/AAW-2653-2020						Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning								Arxiv											2	2;2024-05-28;https://www.arxiv.org/abs/2402.13669v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.13669v1	arXiv:2402.13669			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instructionfollowing abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft. .																																	2024-06-13	PPRN:87795868		
J	Tian, Chunlin; Shi, Zhan; Guo, Zhijiang; Li, Li; Xu, Chengzhong				XU, CHENGZHONG/AAX-1707-2020						HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2404.19245v2| 1;2024-04-30;https://www.arxiv.org/abs/2404.19245v1	arXiv:2404.19245			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, , a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases.																																	2024-06-05	PPRN:88700380		
J	Zhang, Mingjin; Cao, Jiannong; Shen, Xiaoming; Cui, Zeyang				Cao, Jian-Nong/ABE-5890-2020						EdgeShard: Efficient LLM Inference via Collaborative Edge Computing								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14371v1	arXiv:2405.14371			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Large language models (LLMs) have shown great potential in natural language processing and content generation. However, current LLMs heavily rely on cloud computing, leading to prolonged latency, high bandwidth cost, and privacy concerns. Edge computing is promising to address such concerns by deploying LLMs on edge devices, closer to data sources. Some works try to leverage model quantization to reduce the model size to fit the resource -constraint edge devices, but they lead to accuracy loss. Other works use cloud -edge collaboration, suffering from unstable network connections. In this work, we leverage collaborative edge computing to facilitate the collaboration among edge devices and cloud servers for jointly performing efficient LLM inference. We propose a general framework to partition the LLM model into shards and deploy on distributed devices. To achieve efficient LLM inference, we formulate an adaptive joint device selection and model partition problem and design an efficient dynamic programming algorithm to optimize the inference latency and throughput, respectively. Experiments of Llama2 serial models on a heterogeneous physical prototype demonstrate that EdgeShard achieves up to 50% latency reduction and 2x throughput improvement over baseline methods.																																	2024-06-05	PPRN:88989715		
J	Didolkar, Aniket; Goyal, Anirudh; Ke, Nan Rosemary; Guo, Siyuan; Valko, Michal; Lillicrap, Timothy; Rezende, Danilo; Bengio, Yoshua; Mozer, Michael; Arora, Sanjeev				Guo, Siyuan/OSI-9741-2025						Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving								Arxiv											1	1;2024-05-20;https://www.arxiv.org/abs/2405.12205v1	arXiv:2405.12205			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 20 2024	2024	Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.																																	2024-06-14	PPRN:89092396		
J	Xin, Detai; Tan, Xu; Shen, Kai; Ju, Zeqian; Yang, Dongchao; Wang, Yuancheng; Takamichi, Shinnosuke; Saruwatari, Hiroshi; Liu, Shujie; Li, Jinyu; Zhao, Sheng				Wang, Yuancheng/GLR-2067-2022; Yang, Dongchao/Y-8059-2018; jinyu, Li/JQV-7729-2023						RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis								Arxiv											3	3;2024-05-19;https://www.arxiv.org/abs/2404.03204v3| 2;2024-04-06;https://www.arxiv.org/abs/2404.03204v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.03204v1	arXiv:2404.03204			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	May 19 2024	2024	We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from 5.6% .6% (without reranking) and 1.7% .7% (with reranking) to 2.5% .5% and 1.0%, .0%, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from 68% to 4%.																																	2024-06-15	PPRN:88405031		
J	Feffer, Michael; Sinha, Anusha; Deng, Wesley Hanwen; Lipton, Zachary C.; Heidari, Hoda										Red-Teaming for Generative AI: Silver Bullet or Security Theater?								Arxiv											2	2;2024-05-15;https://www.arxiv.org/abs/2401.15897v2| 1;2024-01-29;https://www.arxiv.org/abs/2401.15897v1	arXiv:2401.15897			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 15 2024	2024	In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.																																	2024-06-01	PPRN:87392831		
J	Zhang, Kai; Mo, Lingbo; Chen, Wenhu; Sun, Huan; Su, Yu				Zhang, Kai/KOD-2592-2024						MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing								Arxiv											3	3;2024-05-15;https://www.arxiv.org/abs/2306.10012v3| 2;2023-11-29;https://www.arxiv.org/abs/2306.10012v2| 1;2023-06-16;https://www.arxiv.org/abs/2306.10012v1	arXiv:2306.10012			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 15 2024	2024	Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triplets (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate current image editing baselines from multiple dimensions including quantitative, qualitative, and human evaluations. The results reveal the challenging nature of our dataset and the gap between current baselines and real-world editing needs.																																	2024-06-01	PPRN:73400043		
J	Li, Zhengqi; Tucker, Richard; Snavely, Noah; Holynski, Aleksander				Tucker, Richard/ABA-3004-2020						Generative Image Dynamics								Arxiv											2	2;2024-05-14;https://www.arxiv.org/abs/2309.07906v3| 1;2023-11-06;https://www.arxiv.org/abs/2309.07906v2	arXiv:2309.07906			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 14 2024	2024	We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences depicting natural, oscillatory dynamics such as trees, flowers, candles, and clothes swaying in the wind. We model this dense, long-term motion prior in the Fourier domain:given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a spectral volume, which can be converted into a motion texture that spans an entire video. Along with an image-based rendering module, these trajectories can be used for a number of downstream applications, such as turning still images into seamlessly looping videos, or allowing users to realistically interact with objects in real pictures by interpreting the spectral volumes as image-space modal bases, which approximate object dynamics.																																	2024-06-01	PPRN:86052520		
J	Zhou, Hong-Yu; Adithan, Subathra; Acosta, Julian Nicolas; Topol, Eric J.; Rajpurkar, Pranav				Topol, Eric J./JOZ-4372-2023; Adithan, Subathra/LJL-8675-2024						A Generalist Learner for Multifaceted Medical Image Interpretation								Arxiv											1	1;2024-05-13;https://www.arxiv.org/abs/2405.07988v1	arXiv:2405.07988			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 13 2024	2024	Current medical artificial intelligence systems are often limited to narrow applications, hindering their widespread adoption in clinical practice. To address this limitation, we propose MedVersa, a generalist learner that enables flexible learning and tasking for medical image interpretation. By leveraging a large language model as a learnable orchestrator, MedVersa can learn from both visual and linguistic supervision, support multimodal inputs, and perform real-time task specification. This versatility allows MedVersa to adapt to various clinical scenarios and perform multifaceted medical image analysis. We introduce MedInterp, the largest multimodal dataset to date for medical image interpretation, consisting of over 13 million annotated instances spanning 11 tasks across 3 modalities, to support the development of MedVersa. Our experiments demonstrate that MedVersa achieves state-of-the-art performance in 9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa is the first to showcase the viability of multimodal generative medical AI in implementing multimodal outputs, inputs, and dynamic task specification, highlighting its potential as a multifunctional system for comprehensive medical image analysis. This generalist approach to medical image interpretation paves the way for more adaptable and efficient AI -assisted clinical decision -making.																																	2024-06-08	PPRN:89035232		
J	Mercat, Jean; Vasiljevic, Igor; Keh, Sedrick; Arora, Kushal; Dave, Achal; Gaidon, Adrien; Kollar, Thomas										Linearizing Large Language Models								Arxiv											1	1;2024-05-10;https://www.arxiv.org/abs/2405.06640v1	arXiv:2405.06640			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 10 2024	2024	Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA).1 We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm. .																																	2024-06-06	PPRN:88998495		
J	Jiang, Dongfu; Li, Yishan; Zhang, Ge; Huang, Wenhao; Lin, Bill Yuchen; Chen, Wenhu				Huang, Wenhao/GWU-9337-2022; Jiang, Dongfu/JTV-4943-2023						TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks								Arxiv											4	4;2024-05-09;https://www.arxiv.org/abs/2310.00752v4| 3;2023-12-09;https://www.arxiv.org/abs/2310.00752v3| 2;2023-12-06;https://www.arxiv.org/abs/2310.00752v2| 1;2023-10-01;https://www.arxiv.org/abs/2310.00752v1	arXiv:2310.00752			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 09 2024	2024	We present TIGERScore1, a T rained metric that follows I nstruction G uidance to perform E xplainable, and R eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output → error analysis). We collected the ‘system outputs’ through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERScore can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8% accurate. Through these experimental results, we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task.																																	2024-05-29	PPRN:85348910		
J	Shoghi, Nima; Kolluru, Adeesh; Kitchin, John R.; Ulissi, Zachary W.; Zitnick, C. Lawrence; Wood, Brandon M.										From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction								Arxiv											3	3;2024-05-06;https://www.arxiv.org/abs/2310.16802v2| 2;2023-10-25;https://www.arxiv.org/abs/2310.16802v1| 1;2023-10-25;https://www.arxiv.org/abs/2310.16802v1	arXiv:2310.16802			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	Foundation models have been transformational in machine learning fields such as natural language processing and computer vision. Similar success in atomic property prediction has been limited due to the challenges of training effective models across multiple chemical domains. To address this, we introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy that simultaneously trains on multiple datasets from different chemical domains, treating each dataset as a unique pre-training task within a multi-task framework. Our combined training dataset consists of ∼ 120M systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and generalization by fine-tuning over a diverse set of downstream tasks and datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP demonstrates an average improvement of 59% over training from scratch and matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the potential of pre-training strategies that utilize diverse data to advance property prediction across chemical domains, especially for low-data tasks. 																																	2024-05-28	PPRN:85798958		
J	Diao, Shizhe; Pan, Rui; Dong, Hanze; Shum, KaShun; Zhang, Jipeng; Xiong, Wei; Zhang, Tong				Diao, Shizhe/JXY-7398-2024; Zhang, Tong/HGC-1090-2022						LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models								Arxiv											1	1;2024-05-05;https://www.arxiv.org/abs/2306.12420v2	arXiv:2306.12420			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 05 2024	2024	Foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, an increasing number of foundation models are becoming publicly accessible. However, a significant shortcoming of most of these models lies in their performance in specialized-domain and task-specific applications, necessitating domainand taskaware fine-tuning to develop effective scientific language models. As the number of available foundation models and specialized tasks keeps growing, the job of training scientific language models becomes highly nontrivial. In this paper, we initiate steps to tackle this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the domainand task-aware finetuning of general foundation models. LMFlow offers a complete finetuning workflow for a foundation model to support specialized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning, parameterefficient finetuning, alignment tuning, inference acceleration, long context generalization, model customization, and even multimodal finetuning, along with carefully designed and extensible APIs. This toolkit has been thoroughly tested and is available at https:// github.com/OptimalScale/LMFlow.1 																																	2024-05-28	PPRN:73458726		
J	Gu, Jindong; Jia, Xiaojun; de Jorge, Pau; Yu, Wenqain; Liu, Xinwei; Ma, Avery; Xun, Yuan; Hu, Anjun; Khakzar, Ashkan; Li, Zhijiang; Cao, Xiaochun; Torr, Philip				Jia, Xiaojun/IUM-2172-2023; liu, xinwei/IWD-6849-2023; Xun, Yuan/KIC-1143-2024						A Survey on Transferability of Adversarial Examples across Deep Neural Networks								Arxiv											2	2;2024-05-02;https://www.arxiv.org/abs/2310.17626v2| 1;2023-10-26;https://www.arxiv.org/abs/2310.17626v1	arXiv:2310.17626			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	The emergence of Deep Neural Networks (DNNs) has revolutionized various domains by enabling the resolution of complex tasks spanning image recognition, natural language processing, and scientific problem-solving. However, this progress has also brought to light a concerning vulnerability: adversarial examples. These crafted inputs, imperceptible to humans, can manipulate machine learning models into making erroneous predictions, raising concerns for safety-critical applications. An intriguing property of this phenomenon is the transferability of adversarial examples, where perturbations crafted for one model can deceive another, often with a different architecture. This intriguing property enables black-box attacks which circumvents the need for detailed knowledge of the target model. This survey explores the landscape of the adversarial transferability of adversarial examples. We categorize existing methodologies to enhance adversarial transferability and discuss the fundamental principles guiding each approach. While the predominant body of research primarily concentrates on image classification, we also extend our discussion to encompass other vision tasks and beyond. Challenges and opportunities are discussed, highlighting the importance of fortifying DNNs against adversarial vulnerabilities in an evolving landscape.																																	2024-05-20	PPRN:85822834		
J	Foutty, Benjamin A.; Kometter, Carlos R.; Devakul, Trithep; Reddy, Aidan P.; Watanabe, Kenji; Taniguchi, Takashi; Fu, Liang; Feldman, Benjamin E.				Fu, Liang/LTC-7906-2024; Devakul, Trithep/P-3390-2019; Watanabe, Kenji/H-2825-2011; TANIGUCHI, Takashi/H-2718-2011						Mapping twist-tuned multiband topology in bilayer WSe2								Arxiv											2	2;2024-04-29;https://www.arxiv.org/abs/2304.09808v3| 1;2023-04-19;https://www.arxiv.org/abs/2304.09808v1	arXiv:2304.09808			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 29 2024	2024	Semiconductor moire superlattices have been shown to host a wide array of interaction-driven ground states. However, twisted homobilayers have been difficult to study in the limit of large moire wavelength, where interactions are most dominant. Here, we conduct local electronic compressibility measurements of twisted bilayer WSe2 (tWSe2) at small twist angles. We demonstrate multiple topological bands which host a series of Chern insulators at zero magnetic field near a ‘magic angle’ around 1.23º. Using a locally applied electric field, we induce a topological quantum phase transition at one hole per moire unit cell. Our work establishes the topological phase diagram of a generalized Kane-Mele-Hubbard model in tWSe2, demonstrating a tunable platform for strongly correlated topological phases.																																	2024-05-15	PPRN:64393381		
J	Liu, Fangcheng; Tang, Yehui; Liu, Zhenhua; Ni, Yunsheng; Han, Kai; Wang, Yunhe				Han, Kai/AAC-7659-2019; Liu, Fangcheng/AAK-2631-2021						Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting								Arxiv											1	1;2024-04-29;https://www.arxiv.org/abs/2404.18911v1	arXiv:2404.18911			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 29 2024	2024	Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution. However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly. Drawing inspiration from early exiting, we propose a novel self -speculative decoding framework Kangaroo, , which uses a fixed shallow sub -network as a self -draft model, with the remaining layers serving as the larger target model. We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model’s representation ability. It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model. To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens. Specifically, we halt the small model’s subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold. Extensive experiments on the Spec -Bench demonstrate the effectiveness of Kangaroo. Under single -sequence verification, Kangaroo achieves speedups up to 1.68× . 68 × on Spec -Bench, outperforming Medusa -1 with 88.7% fewer additional parameters (67M compared to 591M).																																	2024-05-16	PPRN:88696898		
J	Li, Yongqi; Lin, Xinyu; Wang, Wenjie; Feng, Fuli; Pang, Liang; Li, Wenjie; Nie, Liqiang; He, Xiangnan; Chua, Tat-Seng				Li, Wenjie/F-9954-2010; Wang, Meng/AEZ-9059-2022; Pang, Liang/CAA-7686-2022						A Survey of Generative Search and Recommendation in the Era of Large Language Models								Arxiv											1	1;2024-04-25;https://www.arxiv.org/abs/2404.16924v1	arXiv:2404.16924			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 25 2024	2024	With the information explosion on the Web, search and recommendation are foundational infrastructures to satisfying users’ information needs. As the two sides of the same coin, both revolve around the same core research problem, matching queries with documents or users with items. In the recent few decades, search and recommendation have experienced synchronous technological paradigm shifts, including machine learning -based and deep learning -based paradigms. Recently, the superintelligent generative large language models have sparked a new paradigm in search and recommendation, i.e., , generative search (retrieval) and recommendation, which aims to address the matching problem in a generative manner. In this paper, we provide a comprehensive survey of the emerging paradigm in information systems and summarize the developments in generative search and recommendation from a unified perspective. Rather than simply categorizing existing works, we abstract a unified framework for the generative paradigm and break down the existing works into different stages within this framework to highlight the strengths and weaknesses. And then, we distinguish generative search and recommendation with their unique challenges, identify open problems and future directions, and envision the next information -seeking paradigm.																																	2024-06-05	PPRN:88965624		
J	Wang, Ruiyi; Yu, Haofei; Zhang, Wenxin; Qi, Zhengyang; Sap, Maarten; Neubig, Graham; Bisk, Yonatan; Zhu, Hao				王, 瑞义/I-8563-2018						SOTOPIA- π : Interactive Learning of Socially Intelligent Language Agents								Arxiv											2	2;2024-04-25;https://www.arxiv.org/abs/2403.08715v3| 1;2024-03-14;https://www.arxiv.org/abs/2403.08715v2	arXiv:2403.08715			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 25 2024	2024	Humans learn social skills through both imitation and social interaction . This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA- π , improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction.																																	2024-05-07	PPRN:88141190		
J	Mavromatis, Costas; Karypis, Petros; Karypis, George										Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization								Arxiv											1	1;2024-04-17;https://www.arxiv.org/abs/2404.11531v1	arXiv:2404.11531			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 17 2024	2024	Fusing knowledge from multiple Large Language Models (LLMs) can combine their diverse strengths to achieve improved performance on a given task. However, current fusion approaches either rely on learning-based fusers that do not generalize to new LLMs, or do not take into account how well each LLM understands the input. In this work, we study LLM fusion at test-time, which enables leveraging knowledge from arbitrary user-specified LLMs during inference. We introduce Pack of LLMs (PackLLM), an effective method for test-time fusion that leverages each LLM's expertise, given an input prompt. PackLLM performs model fusion by solving an optimization problem for determining each LLM's importance, so that perplexity over the input prompt is minimized. First, our simple PackLLM-sim variant validates that perplexity is a good indicator for measuring each LLM's expertise. Second, our PackLLM-opt variant approximately solves the perplexity minimization problem via a greedy algorithm. The derived importance weights are used to combine the LLMs during inference. We conduct experiments with over 100 total LLMs on a diverse set of tasks. Experimental results show that (i) perplexity is a reliable measure for LLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89% accuracy points, and (iii) PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92-11.94% accuracy points. [GRAPHICS]																																	2024-04-27	PPRN:88556753		
J	Wang, Yuxia; Reddy, Revanth Gangi; Mujahid, Zain Muhammad; Arora, Arnav; Rubashevskii, Aleksandr; Geng, Jiahui; Afzal, Osama Mohammed; Pan, Liangming; Borenstein, Nadav; Pillai, Aditya; Augenstein, Isabelle; Gurevych, Iryna; Nakov, Preslav				Pan, Liangming/LIF-2753-2024						Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers								Arxiv											2	2;2024-04-16;https://www.arxiv.org/abs/2311.09000v3| 1;2023-11-16;https://www.arxiv.org/abs/2311.09000v2	arXiv:2311.09000			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 16 2024	2024	The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document, aiming to facilitate the evaluation of automatic fact-checking systems. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims, with the best F1=0.63 by this annotation solution based on GPT-4. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.																																	2024-04-26	PPRN:86177187		
J	He, Yefei; Liu, Jing; Wu, Weijia; Zhou, Hong; Zhuang, Bohan				Zhuang, Bohan/AAL-5022-2021; Liu, Jing/LFU-9046-2024						EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models								Arxiv											3	3;2024-04-13;https://www.arxiv.org/abs/2310.03270v4| 2;2023-10-05;https://www.arxiv.org/abs/2310.03270v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03270v1	arXiv:2310.03270			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 13 2024	2024	Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and accelerate diffusion models, where post-training quantization (PTQ) and quantization-aware training (QAT) are two main approaches, each bearing its own properties. While PTQ exhibits efficiency in terms of both time and data usage, it may lead to diminished performance in low bit-width. On the other hand, QAT can alleviate performance degradation but comes with substantial demands on computational and data resources. In this paper, we introduce a data-free and parameter-efficient fine-tuning framework for low-bit diffusion models, dubbed EfficientDM, to achieve QAT-level performance with PTQ-like efficiency. Specifically, we propose a quantization-aware variant of the low-rank adapter (QALoRA) that can be merged with model weights and jointly quantized to low bit-width. The fine-tuning process distills the denoising capabilities of the full-precision model into its quantized counterpart, eliminating the requirement for training data. We also introduce scale-aware optimization and temporal learned step-size quantization to further enhance performance. Extensive experimental results demonstrate that our method significantly outperforms previous PTQ-based diffusion models while maintaining similar time and data efficiency. Specifically, there is only a 0.05 sFID increase when quantizing both weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization speed with comparable generation quality. 																																	2024-04-25	PPRN:85435507		
J	Zhou, Shijie; Chang, Haoran; Jiang, Sicheng; Fan, Zhiwen; Zhu, Zehao; Xu, Dejia; Chari, Pradyumna; You, Suya; Wang, Zhangyang; Kadambi, Achuta				Zhihua, Wang/AFO-5263-2022; Zhou, ShiJie/AAB-5295-2019						Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields								Arxiv											3	3;2024-04-08;https://www.arxiv.org/abs/2312.03203v3| 2;2024-03-29;https://www.arxiv.org/abs/2312.03203v2| 1;2023-12-06;https://www.arxiv.org/abs/2312.03203v1	arXiv:2312.03203			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 08 2024	2024	3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework encounters significant challenges, notably the disparities in spatial resolution and channel consistency between RGB images and feature maps. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model.																																	2024-04-21	PPRN:86422702		
J	Besta, Maciej; Memedi, Florim; Zhang, Zhenyu; Gerstenberger, Robert; Piao, Guangyuan; Blach, Nils; Nyczyk, Piotr; Copik, Marcin; Kwasniewski, Grzegorz; Mueller, Juergen; Gianinazzi, Lukas; Kubicek, Ales; Niewiadomski, Hubert; O'Mahony, Aidan; Mutlu, Onur; Hoefler, Torsten				Wang, Wenjin/HSE-1306-2023; Hoefler, Torsten/HKF-3023-2023; Kwasniewski, Grzegorz/HZJ-0461-2023						Demystifying Chains, Trees, and Graphs of Thoughts								Arxiv											4	4;2025-09-05;https://www.arxiv.org/abs/2401.14295v5| 3;2024-04-05;https://www.arxiv.org/abs/2401.14295v3| 2;2024-03-30;https://www.arxiv.org/abs/2401.14295v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.14295v1	arXiv:2401.14295			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 05 2024	2024	The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.																																	2024-04-20	PPRN:87336080		
J	Yan, Jun; Yadav, Vikas; Li, Shiyang; Chen, Lichang; Tang, Zheng; Wang, Hai; Srinivasan, Vijay; Ren, Xiang; Jin, Hongxia				YADAV, VIKAS/AGZ-4594-2022; ren, xiang/HLQ-5068-2023						Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection								Arxiv											3	3;2024-04-03;https://www.arxiv.org/abs/2307.16888v3| 2;2023-10-06;https://www.arxiv.org/abs/2307.16888v2| 1;2023-07-31;https://www.arxiv.org/abs/2307.16888v1	arXiv:2307.16888			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model's instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. 																																	2024-04-18	PPRN:74188945		
J	Hu, Yuchen; Wager, Stefan										Switchback Experiments under Geometric Mixing								Arxiv											3	3;2024-04-02;https://www.arxiv.org/abs/2209.00197v3| 2;2024-01-29;https://www.arxiv.org/abs/2209.00197v2| 1;2022-09-01;https://www.arxiv.org/abs/2209.00197v1	arXiv:2209.00197			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	The switchback is an experimental design that measures treatment effects by repeatedly turning an intervention on and off for a whole system. Switchback experiments are a robust way to overcome cross -unit spillover effects; however, they are vulnerable to bias from temporal carryovers. In this paper, we consider properties of switchback experiments in Markovian systems that mix at a geometric rate. We find that, in this setting, standard switchback designs suffer considerably from carryover bias: Their estimation error decays as T−1/3 in terms of the experiment horizon T, whereas in the absence of carryovers a faster rate of T−1/2 would have been possible. We also show, however, that judicious use of burn -in periods can considerably improve the situation, and enables errors that decay as log(T)1/2T−1/2. Our formal results are mirrored in an empirical evaluation.																																	2024-04-18	PPRN:12882160		
J	Tian, Junjiao; Aggarwal, Lavisha; Colaco, Andrea; Kira, Zsolt; Gonzalez-Franco, Mar				Tian, Junjiao/IYS-7424-2023; Gonzalez-Franco, Mar/L-4994-2014						Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion								Arxiv											3	3;2024-04-02;https://www.arxiv.org/abs/2308.12469v3| 2;2023-11-11;https://www.arxiv.org/abs/2308.12469v2| 1;2023-08-23;https://www.arxiv.org/abs/2308.12469v1	arXiv:2308.12469			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Producing quality segmentation masks for images is a fundamental problem in computer vision. Recent research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU.																																	2024-04-18	PPRN:83521804		
J	Zhao, Justin; Wang, Timothy; Abid, Wael; Angus, Geoffrey; Garg, Arnav; Kinnison, Jeffery; Sherstinsky, Alex; Molino, Piero; Addair, Travis; Rishi, Devvret										LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report								Arxiv											1	1;2024-04-01;	arXiv:2405.00732			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.																																	2024-11-16	PPRN:118600739		
J	Zhang, Yang; Lin, Ling-Fang; Moreo, Adriana; Maier, Thomas A.; Dagotto, Elbio				Maier, Thomas/F-6759-2012						Structural phase transition, s±-wave pairing, and magnetic stripe order in bilayered superconductor La3Ni2O7 under pressure								Arxiv											2	2;2024-03-26;https://www.arxiv.org/abs/2307.15276v4| 1;2023-07-31;https://www.arxiv.org/abs/2307.15276v2	arXiv:2307.15276			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Mar 26 2024	2024	Motivated by the recently discovered high-Tc superconductor La3Ni2O7, we comprehensively study this system using density functional theory and random phase approximation calculations. At low pressures, the Amam phase is stable, containing the Y2− mode distortion from the Fmmm phase, while the Fmmm phase is unstable. Because of small differences in enthalpy and a considerable Y2− mode amplitude, the two phases may coexist in the range between 10.6 and 14 GPa, beyond which the Fmmm phase dominates. In addition, the magnetic stripe-type spin order with wavevector (π, 0) was stable at the intermediate region. Pairing is induced in the s±-wave channel due to partial nesting between the M=(π, π) centered pockets and portions of the Fermi surface centered at the X=(π, 0) and Y=(0, π) points. This resembles results for iron-based superconductors but has a fundamental difference with iron pnictides and selenides. Moreover, our present efforts also suggest La3Ni2O7 is qualitatively different from infinite-layer nickelates and cuprate superconductors.																																	2024-04-14	PPRN:74188976		
J	Qin, Yihao; Wang, Shangwen; Lou, Yiling; Dong, Jinhao; Wang, Kaixin; Li, Xiaoling; Mao, Xiaoguang				liu, yiling/HNS-6339-2023; Wang, Shangwen/IQW-8844-2023						AgentFL: Scaling LLM-based Fault Localization to Project-Level Context								Arxiv											2	2;2025-02-24;https://www.arxiv.org/abs/2403.16362v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16362v1	arXiv:2403.16362			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	Fault Localization (FL) is an essential step during the debugging process. With the strong capabilities of code comprehension, the recent Large Language Models (LLMs) have demonstrated promising performance in diagnosing bugs in the code. Nevertheless, due to LLMs' limited performance in handling long contexts, existing LLM-based fault localization remains on localizing bugs within a small code scope (i.e., a method or a class), which struggles to diagnose bugs for a large code scope (i.e., an entire software system). To address the limitation, this paper presents AgentFL, a multi-agent system based on ChatGPT for automated fault localization. By simulating the behavior of a human developer, AgentFL models the FL task as a three-step process, which involves comprehension, navigation, and confirmation. Within each step, AgentFL hires agents with diversified expertise, each of which utilizes different tools to handle specific tasks. Particularly, we adopt a series of auxiliary strategies such as Test Behavior Tracking, Document-Guided Search, and Multi-Round Dialogue to overcome the challenges in each step. The evaluation on the widely used Defects4J-V1.2.0 benchmark shows that AgentFL can localize 157 out of 395 bugs within Top-1, which outperforms the other LLM-based approaches and exhibits complementarity to the state-of-the-art learning-based techniques. Additionally, we confirm the indispensability of the components in AgentFL with the ablation study and demonstrate the usability of AgentFL through a user study. Finally, the cost analysis shows that AgentFL spends an average of only 0.074 dollars and 97 seconds for a single bug.																																	2025-08-07	PPRN:88278124		
J	Hu, Liangxiao; Zhang, Hongwen; Zhang, Yuxiang; Zhou, Boyao; Liu, Boning; Zhang, Shengping; Nie, Liqiang				Liu, Boning/AAK-7643-2021; Hu, Liangxiao/NDS-9969-2025; yuxiang, zhang/LOS-0497-2024						GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians								Arxiv											1	1;2024-03-23;https://www.arxiv.org/abs/2312.02134v2	arXiv:2312.02134			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 23 2024	2024	We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.																																	2025-08-07	PPRN:123159563		
J	Zhang, Zheng; Hu, Wenbo; Lao, Yixing; He, Tong; Zhao, Hengshuang				Hu, Wenbo/JKH-5582-2023						Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting								Arxiv											1	1;2024-03-22;https://www.arxiv.org/abs/2403.15530v1	arXiv:2403.15530			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 22 2024	2024	3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.																																	2025-08-07	PPRN:123160347		
J	Zhou, Xuhui; Su, Zhe; Eisape, Tiwalayo; Kim, Hyunwoo; Sap, Maarten										Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs								Arxiv											1	1;2024-03-20;https://www.arxiv.org/abs/2403.05020v2	arXiv:2403.05020			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 20 2024	2024	Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.																																	2025-08-07	PPRN:123156103		
J	Dong, Zican; Tang, Tianyi; Li, Junyi; Zhao, Wayne Xin; Wen, Ji-Rong										BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models								Arxiv											2	2;2024-03-19;https://www.arxiv.org/abs/2309.13345v3| 1;2023-09-23;https://www.arxiv.org/abs/2309.13345v1	arXiv:2309.13345			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi -task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e., question answering, hallucination detection, text sorting, language modeling, and code completion, to cover various domains and core capacities of LLMs. We conduct experiments with five widely -used long -context models and further discuss five key questions for long text research. In the end, we discuss problems of current long -context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts																																	2024-04-12	PPRN:85186576		
J	Fu, Xiao; Yin, Wei; Hu, Mu; Ma, Yuexin; Wang, Kaixuan; Tan, Ping; Shen, Shaojie; Lin, Dahua; Long, Xiaoxiao				Lin, Dahua/W-6576-2019; zhu, xinge/LGZ-7330-2024						GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.12013v1	arXiv:2403.12013			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	We introduce GeoWizard, a new generative foundation model designed for estimating geometric attributes, e.g., depth and normals, from single images. While significant research has already been conducted in this area, the progress has been substantially limited by the low diversity and poor quality of publicly available datasets. As a result, the prior works either are constrained to limited scenarios or suffer from the inability to capture geometric details. In this paper, we demonstrate that generative models, as opposed to traditional discriminative models (e.g., CNNs and Transformers), can effectively address the inherently ill-posed problem. We further show that leveraging diffusion priors can markedly improve generalization, detail preservation, and efficiency in resource usage. Specifically, we extend the original stable diffusion model to jointly predict depth and normal, allowing mutual information exchange and high consistency between the two representations. More importantly, we propose a simple yet effective strategy to segregate the complex data distribution of various scenes into distinct sub-distributions. This strategy enables our model to recognize different scene layouts, capturing 3D geometry with remarkable fidelity. GeoWizard sets new benchmarks for zero-shot depth and normal prediction, significantly enhancing many downstream applications such as 3D reconstruction, 2D content creation, and novel viewpoint synthesis.																																	2024-04-11	PPRN:88196005		
J	Mikula, Maciej; Tworkowski, Szymon; Antoniak, Szymon; Piotrowski, Bartosz; Jiang, Albert Qiaochu; Zhou, Jin Peng; Szegedy, Christian; Kucinski, Lukasz; Milos, Piotr; Wu, Yuhuai				Milos, Piotr/AAQ-3048-2020						Magnushammer: A Transformer-Based Approach to Premise Selection								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2303.04488v3| 2;2024-01-29;https://www.arxiv.org/abs/2303.04488v2| 1;2023-03-08;https://www.arxiv.org/abs/2303.04488v1	arXiv:2303.04488			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	This paper presents a novel approach to premise selection, a crucial reasoning task in automated theorem proving. Traditionally, symbolic methods that rely on extensive domain knowledge and engineering effort are applied to this task. In contrast, this work demonstrates that contrastive training with the transformer architecture can achieve higher -quality retrieval of relevant premises, without the engineering overhead. Our method, Magnushammer, outperforms the most advanced and widely used automation tool in interactive theorem proving called Sledgehammer. On the PISA and miniF2F benchmarks Magnushammer achieves 59.5% (against 38.3%) and 34.0% (against 20.9%) success rates, respectively. By combining Magnushammer with a language -model -based automated theorem prover, we further improve the state-of-the-art proof success rate from 57.0% to 71.0% on the PISA benchmark using 4x fewer parameters. Moreover, we develop and open source a novel dataset for premise selection, containing textual representations of (proof state, relevant premise) pairs. To the best of our knowledge, this is the largest available premise selection dataset, and the first one for the Isabelle proof assistant.																																	2024-04-11	PPRN:44376330		
J	Lienen, Marten; Luedke, David; Hansen-Palmus, Jan; Guennemann, Stephan										From Zero to Turbulence: Generative Modeling for 3D Flow Simulation								Arxiv											3	3;2024-03-14;https://www.arxiv.org/abs/2306.01776v3| 2;2023-10-04;https://www.arxiv.org/abs/2306.01776v2| 1;2023-05-29;https://www.arxiv.org/abs/2306.01776v1	arXiv:2306.01776			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 14 2024	2024	Simulations of turbulent flows in 3D are one of the most expensive simulations in computational fluid dynamics (CFD). Many works have been written on surrogate models to replace numerical solvers for fluid flows with faster, learned, autoregressive models. However, the intricacies of turbulence in three dimensions necessitate training these models with very small time steps, while generating realistic flow states requires either long roll-outs with many steps and significant error accumulation or starting from a known, realistic flow state - something we aimed to avoid in the first place. Instead, we propose to approach turbulent flow simulation as a generative task directly learning the manifold of all possible turbulent flow states without relying on any initial flow state. For our experiments, we introduce a challenging 3D turbulence dataset of high-resolution flows and detailed vortex structures caused by various objects and derive two novel sample evaluation metrics for turbulent flows. On this dataset, we show that our generative model captures the distribution of turbulent flows caused by unseen objects and generates high-quality, realistic samples amenable for downstream applications without access to any initial state.																																	2024-04-11	PPRN:72853619		
J	Latif, Ehsan; Mai, Gengchen; Nyaaba, Matthew; Wu, Xuansheng; Liu, Ninghao; Lu, Guoyu; Li, Sheng; Liu, Tianming; Zhai, Xiaoming				Latif, Ehsan/AAH-8494-2019; Liu, Tianming/GLS-1211-2022; Mai, Gengchen/ABF-8620-2020; Wu, Xuansheng/JCE-2579-2023; Zhai, Xiaoming/AAB-7129-2021; Nyaaba, Matthew/GPW-8087-2022						AGI: Artificial General Intelligence for Education								Arxiv											4	4;2024-03-13;https://www.arxiv.org/abs/2304.12479v5| 3;2023-11-28;https://www.arxiv.org/abs/2304.12479v4| 2;2023-10-30;https://www.arxiv.org/abs/2304.12479v3| 1;2023-04-24;https://www.arxiv.org/abs/2304.12479v1	arXiv:2304.12479			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 13 2024	2024	Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This position paper reviews AGI's key concepts, capabilities, scope, and potential within future education, including achieving future educational goals, designing pedagogy and curriculum, and performing assessments. It highlights that AGI can significantly improve intelligent tutoring systems, educational assessment, and evaluation procedures. AGI systems can adapt to individual student needs, offering tailored learning experiences. They can also provide comprehensive feedback on student performance and dynamically adjust teaching methods based on student progress. The paper emphasizes that AGI's capabilities extend to understanding human emotions and social interactions, which are critical in educational settings. The paper discusses that ethical issues in education with AGI include data bias, fairness, and privacy and emphasizes the need for codes of conduct to ensure responsible AGI use in academic settings like homework, teaching, and recruitment. We also conclude that the development of AGI necessitates interdisciplinary collaborations between educators and AI engineers to advance research and application efforts.																																	2024-04-11	PPRN:65328649		
J	Ku, Max; Li, Tianle; Zhang, Kai; Lu, Yujie; Fu, Xingyu; Zhuang, Wenwen; Chen, Wenhu				Fu, Xingyu/GZM-3129-2022; Zhang, Kai/KOD-2592-2024; Zhuang, Wenwen/GXG-4417-2022						ImagenHub: Standardizing the evaluation of conditional image generation models								Arxiv											4	4;2024-03-10;https://www.arxiv.org/abs/2310.01596v4| 3;2023-12-03;https://www.arxiv.org/abs/2310.01596v3| 2;2023-10-17;https://www.arxiv.org/abs/2310.01596v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01596v1	arXiv:2310.01596			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 10 2024	2024	Recently, a myriad of conditional image generation and editing models have been developed to serve different downstream tasks, including text-to-image generation, text-guided image editing, subject-driven image generation, control-guided image generation, etc. However, we observe huge inconsistencies in experimental conditions: datasets, inference, and evaluation metrics - render fair comparisons difficult. This paper proposes ImagenHub, which is a one-stop library to standardize the inference and evaluation of all the conditional image generation models. Firstly, we define seven prominent tasks and curate high-quality evaluation datasets for them. Secondly, we built a unified inference pipeline to ensure fair comparison. Thirdly, we design two human evaluation scores, i.e. Semantic Consistency and Perceptual Quality, along with comprehensive guidelines to evaluate generated images. We train expert raters to evaluate the model outputs based on the proposed metrics. Our human evaluation achieves a high inter-worker agreement of Krippendorff's alpha on 76% models with a value higher than 0.4. We comprehensively evaluated a total of around 30 models and observed three key takeaways: (1) the existing models' performance is generally unsatisfying except for Text-guided Image Generation and Subject-driven Image Generation, with 74% models achieving an overall score lower than 0.5. (2) we examined the claims from published papers and found 83% of them hold with a few exceptions. (3) None of the existing automatic metrics has a Spearman's correlation higher than 0.2 except subject-driven image generation. Moving forward, we will continue our efforts to evaluate newly published models and update our leaderboard to keep track of the progress in conditional image generation.																																	2024-04-08	PPRN:85376691		
J	Schmid, David; Selby, John H.; Pusey, Matthew F.; Spekkens, Robert W.				Selby, John/AFK-9250-2022; Schmid, David/ORI-0290-2025						A structure theorem for generalized-noncontextual ontological models								Arxiv											2	2;2024-03-08;https://www.arxiv.org/abs/2005.07161v3| 1;2020-11-07;https://www.arxiv.org/abs/2005.07161v2	arXiv:2005.07161			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 08 2024	2024	It is useful to have a criterion for when the predictions of an operational theory should be considered classically explainable. Here we take the criterion to be that the theory admits of a generalized-noncontextual ontological model. Existing works on generalized noncontextuality have focused on experimental scenarios having a simple structure: typically, prepare-measure scenarios. Here, we formally extend the framework of ontological models as well as the principle of generalized noncontextuality to arbitrary compositional scenarios. We leverage a process-theoretic framework to prove that, under some reasonable assumptions, every generalized-noncontextual ontological model of a tomographically local operational theory has a surprisingly rigid and simple mathematical structure -- in short, it corresponds to a frame representation which is not overcomplete. One consequence of this theorem is that the largest number of ontic states possible in any such model is given by the dimension of the associated generalized probabilistic theory. This constraint is useful for generating noncontextuality no-go theorems as well as techniques for experimentally certifying contextuality. Along the way, we extend known results concerning the equivalence of different notions of classicality from prepare-measure scenarios to arbitrary compositional scenarios. Specifically, we prove a correspondence between the following three notions of classical explainability of an operational theory: (i) existence of a noncontextual ontological model for it, (ii) existence of a positive quasiprobability representation for the generalized probabilistic theory it defines, and (iii) existence of an ontological model for the generalized probabilistic theory it defines.																																	2024-04-07	PPRN:14737990		
J	Zhong, Nan; Xu, Yiran; Li, Sheng; Qian, Zhenxing; Zhang, Xinpeng				Qian, Zhenxing/AHC-9176-2022; Xu, Yiran/JCO-3216-2023						PatchCraft: Exploring Texture Patch for Efficient AI-generated Image Detection								Arxiv											3	3;2024-03-07;https://www.arxiv.org/abs/2311.12397v3| 2;2023-12-10;https://www.arxiv.org/abs/2311.12397v2| 1;2023-11-21;https://www.arxiv.org/abs/2311.12397v1	arXiv:2311.12397			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	Recent generative models show impressive performance in generating photographic images. Humans can hardly distinguish such incredibly realistic-looking AI-generated images from real ones. AI-generated images may lead to ubiquitous disinformation dissemination. Therefore, it is of utmost urgency to develop a detector to identify AI generated images. Most existing detectors suffer from sharp performance drops over unseen generative models. In this paper, we propose a novel AI-generated image detector capable of identifying fake images created by a wide range of generative models. We observe that the texture patches of images tend to reveal more traces left by generative models compared to the global semantic information of the images. A novel Smash&Reconstruction preprocessing is proposed to erase the global semantic information and enhance texture patches. Furthermore, pixels in rich texture regions exhibit more significant fluctuations than those in poor texture regions. Synthesizing realistic rich texture regions proves to be more challenging for existing generative models. Based on this principle, we leverage the inter-pixel correlation contrast between rich and poor texture regions within an image to further boost the detection performance. In addition, we build a comprehensive AI-generated image detection benchmark, which includes 17 kinds of prevalent generative models, to evaluate the effectiveness of existing baselines and our approach. Our benchmark provides a leaderboard for follow-up studies. Extensive experimental results show that our approach outperforms state-of-the-art baselines by a significant margin. Our project: https://fdmas.github.io/AIGCDetect																																	2024-04-05	PPRN:86224651		
J	Li, Gen; Huang, Yu; Efimov, Timofey; Wei, Yuting; Chi, Yuejie; Chen, Yuxin				Efimov, Timofey/ABB-5067-2020; Chi, Yuejie/G-6033-2012						Accelerating Convergence of Score-Based Diffusion Models, Provably								Arxiv											1	1;2024-03-06;https://www.arxiv.org/abs/2403.03852v1	arXiv:2403.03852			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 06 2024	2024	Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate O(1/T2) with T the number of steps, improving upon the O(1/T) rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate O(1/T), outperforming the rate O(1/√T) for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates ℓ2-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.																																	2024-04-03	PPRN:88043692		
J	Liu, Zixuan; Sun, Xiaolin; Zheng, Zizhan										Enhancing LLM Safety via Constrained Direct Preference Optimization								Arxiv											1	1;2024-03-04;https://www.arxiv.org/abs/2403.02475v1	arXiv:2403.02475			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 04 2024	2024	The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to LLMs that is missing in DPO while achieving significantly higher rewards under the same safety constraint compared to a recently proposed safe RLHF approach. 																																	2024-04-02	PPRN:88028476		
J	Abdallah, Abdelrahman; Jatowt, Adam				Jatowt, Adam/AAT-3213-2020; Elhadi, Abdelrhman/ABE-3643-2020						Generator-Retriever-Generator Approach for Open-Domain Question Answering								Arxiv											4	4;2024-02-11;https://www.arxiv.org/abs/2307.11278v2| 3;2023-07-21;https://www.arxiv.org/abs/2307.11278v1| 2;2023-07-21;https://www.arxiv.org/abs/2307.11278v1| 1;2024-03-01;	arXiv:2307.11278			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 01 2024	2024	Open-domain question answering (QA) tasks usually require the retrieval of relevant information from a large corpus to generate accurate answers. We propose a novel approach called Generator-Retriever-Generator (GRG) that combines document retrieval techniques with a large language model (LLM), by first prompting the model to generate contextual documents based on a given question. In parallel, a dual-encoder network retrieves documents that are relevant to the question from an external corpus. The generated and retrieved documents are then passed to the second LLM, which generates the final answer. By combining document retrieval and LLM generation, our approach addresses the challenges of open-domain QA, such as generating informative and contextually relevant answers. GRG outperforms the state-of-the-art generate-then-read and retrieve-then-read pipelines (GENREAD and RFiD) improving their performance by at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets, respectively. We provide code, datasets, and checkpoints at https://github.com/abdoelsayed2016/GRG.																																	2025-11-07	PPRN:74045457		
J	Liu, Yuhan; Li, Hanchen; Cheng, Yihua; Ray, Siddhant; Huang, Yuyang; Zhang, Qizheng; Du, Kuntai; Yao, Jiayi; Lu, Shan; Ananthanarayanan, Ganesh; Maire, Michael; Hoffmann, Henry; Holtzman, Ari; Jiang, Junchen				Liu, Yuhan/GZG-6903-2022; Yao, Jiayi/IXN-4828-2023; Cheng, Yihua/LDG-8022-2024						CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving								Arxiv											3	3;2024-03-14;https://www.arxiv.org/abs/2310.07240v3| 2;2023-10-11;https://www.arxiv.org/abs/2310.07240v1| 1;2024-03-01;	arXiv:2310.07240			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 01 2024	2024	As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause extra network delays. CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, which embraces KV cache's distributional properties, to encode a KV cache into more compact bitstream representations with negligible encoding/decoding overhead. This reduces the bandwidth demand to fetch the KV cache. Second, to maintain low context-loading delay and high generation quality, CacheGen adapts the streaming strategies to cope with changes in available bandwidth. When available bandwidth drops, CacheGen may raise the compression level for a part of the context or choose to recompute its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes and four datasets (662 contexts in total). Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3.2x while having negligible impact on the LLM response quality in accuracy or perplexity.																																	2025-11-07	PPRN:85562753		
J	Chen, Zhuo; Zhang, Yichi; Fang, Yin; Geng, Yuxia; Guo, Lingbing; Chen, Xiang; Li, Qian; Zhang, Wen; Chen, Jiaoyan; Zhu, Yushan; Li, Jiaqi; Liu, Xiaoze; Pan, Jeff Z.; Zhang, Ningyu; Chen, Huajun				Huajun, Chen/B-6340-2013; Zhuo, Cheng/HLV-8134-2023; Li, Jiaqi/D-5836-2011; Liu, Xiaoze/HGA-7395-2022; Geng, Yuxia/JWP-1719-2024						Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey								Arxiv											3	3;2024-02-26;https://www.arxiv.org/abs/2402.05391v4| 2;2024-02-09;https://www.arxiv.org/abs/2402.05391v2| 1;2024-02-08;https://www.arxiv.org/abs/2402.05391v1	arXiv:2402.05391			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work.																																	2024-03-28	PPRN:87573031		
J	Solatorio, Aivin V.				Solatorio, Aivin/LZI-3681-2025						GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2402.16829v1	arXiv:2402.16829			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 26 2024	2024	Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data. However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity. Traditional unsupervised triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB), GISTEmbed showcases consistent performance improvements across various model sizes and achieves state-of-the-art results in select categories. This framework enables significant enhancements for smaller models by leveraging the capabilities of powerful yet resource-intensive large models. GISTEmbed can potentially revolutionize the creation of highly efficient, smaller models, democratizing access to advanced AI technologies. Making these technologies more accessible and cost-effective, especially for applications constrained by resources, significantly expands the impact and accessibility of state-of-the-art AI solutions across diverse sectors.																																	2024-03-24	PPRN:87882893		
J	Thawakar, Omkar; Vayani, Ashmal; Khan, Salman; Cholakal, Hisham; Anwer, Rao M.; Felsberg, Michael; Baldwin, Tim; Xing, Eric P.; Khan, Fahad Shahbaz				Khan, Salman/M-4834-2016; Khan, Fahad Shahbaz/ABD-6646-2021						MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2402.16840v1	arXiv:2402.16840			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	"Bigger the better" has been the predominant trend in recent Large Language Models (LLMs) development. However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency. These requisites are crucial for privacy, security, and sustainable deployment. This paper explores the "less is more" paradigm by addressing the challenge of designing accurate yet efficient Small Language Models (SLMs) for resource constrained devices. Our primary contribution is the introduction of an accurate and fully transparent open-source 0.5 billion (0.5B) parameter SLM, named MobiLlama, catering to the specific needs of resource-constrained computing with an emphasis on enhanced performance with reduced resource demands. MobiLlama is a SLM design that initiates from a larger model and applies a careful parameter sharing scheme to reduce both the pre-training and the deployment cost. Our work strives to not only bridge the gap in open-source SLMs but also ensures full transparency, where complete training data pipeline, training code, model weights, and over 300 checkpoints along with evaluation codes is available at : https://github.com/mbzuai-oryx/MobiLlama.																																	2024-03-28	PPRN:87945043		
J	Li, Zuo-Xin; Yang, Lu; Zhou, Fu-Qiang				Zhou, Fuqiang/KZU-8061-2024; Li, Zuoxin/AGU-9407-2022						FSSD: Feature Fusion Single Shot Multibox Detector								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/1712.00960v4| 1;2018-05-17;https://www.arxiv.org/abs/1712.00960v3	arXiv:1712.00960			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 23 2024	2024	SSD (Single Shot Multibox Detetor) is one of the best object detection algorithms with both high accuracy and fast speed. However, SSD’s feature pyramid detection method makes it hard to fuse the features from different scales. In this paper, we proposed FSSD (Feature Fusion Single Shot Multibox Detector), an enhanced SSD with a novel and lightweight feature fusion module which can improve the performance significantly over SSD with just a little speed drop. In the feature fusion module, features from different layers with different scales are concatenated together, followed by some down-sampling blocks to generate new feature pyramid, which will be fed to multibox detectors to predict the final detection results. On the Pascal VOC 2007 test, our network can achieve 82.7 mAP (mean average precision) at the speed of 65.8 FPS (frame per second) with the input size 300×300 using a single Nvidia 1080Ti GPU. In addition, our result on COCO is also better than the conventional SSD with a large margin. Our FSSD outperforms a lot of state-of-the-art object detection algorithms in both aspects of accuracy and speed. Code is available at https://github.com/ lzx1413/CAFFE_SSD/tree/fssd.																																	2024-03-23	PPRN:12874571		
J	Shi, Zhengxiang; Lipani, Aldo				Lipani, Aldo/H-6258-2019						DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning								Arxiv											5	5;2024-02-18;https://www.arxiv.org/abs/2309.05173v5| 4;2024-01-21;https://www.arxiv.org/abs/2309.05173v4| 3;2023-12-18;https://www.arxiv.org/abs/2309.05173v3| 2;2023-10-12;https://www.arxiv.org/abs/2309.05173v2| 1;2023-09-11;https://www.arxiv.org/abs/2309.05173v1	arXiv:2309.05173			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 18 2024	2024	Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance while saving substantial memory and time costs compared to vanilla PT and its variants, without changing trainable parameter sizes. Through extensive experiments on 23 natural language processing (NLP) and vision-language (VL) tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline, in some scenarios. Additionally, we empirically show that DEPT grows more efficient as the model size increases. Our further study reveals that DePT integrates seamlessly with parameter-efficient transfer learning in the few-shot learning setting and highlights its adaptability to various model architectures and sizes.																																	2024-03-19	PPRN:84952813		
J	Luo, Haimin; Ouyang, Min; Zhao, Zijun; Jiang, Suyi; Zhang, Longwen; Zhang, Qixuan; Yang, Wei; Xu, Lan; Yu, Jingyi				Zhao, Zijun/JBS-6594-2023; Yang, Wei/KVY-4125-2024; Zhang, Long-Wen/JLL-8006-2023; Zhang, Qixuan/OHT-9917-2025						GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians								Arxiv											1	1;2024-02-16;https://www.arxiv.org/abs/2402.10483v1	arXiv:2402.10483			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 16 2024	2024	Hairstyle reflects culture and ethnicity at first glance. In the digital era, various realistic human hairstyles are also critical to high-fidelity digital human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time rendering for animation is a formidable challenge due to its sheer number of strands, complicated structures of geometry, and sophisticated interaction with light. This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities. At the heart of GaussianHair is the novel concept of representing each hair strand as a sequence of connected cylindrical 3D Gaussian primitives. This approach not only retains the hair's geometric structure and appearance but also allows for efficient rasterization onto a 2D image plane, facilitating differentiable volumetric rendering. We further enhance this model with the "GaussianHair Scattering Model", adept at recreating the slender structure of hair strands and accurately capturing their local diffuse color in uniform lighting. Through extensive experiments, we substantiate that GaussianHair achieves breakthroughs in both geometric and appearance fidelity, transcending the limitations encountered in state-of-the-art methods for hair reconstruction. Beyond representation, GaussianHair extends to support editing, relighting, and dynamic rendering of hair, offering seamless integration with conventional CG pipeline workflows. Complementing these advancements, we have compiled an extensive dataset of real human hair, each with meticulously detailed strand geometry, to propel further research in this field.																																	2024-05-01	PPRN:87798068		
J	Yin, Hongzhi; Qu, Liang; Chen, Tong; Yuan, Wei; Zheng, Ruiqi; Long, Jing; Xia, Xin; Shi, Yuhui; Zhang, Chengqi				Zheng, Ruiqi/LVR-9005-2024; Chen, Tong/GSJ-2114-2022; Shi, Yuhui/G-7472-2018						On-Device Recommender Systems: A Comprehensive Survey								Arxiv											2	2;2024-02-15;https://www.arxiv.org/abs/2401.11441v2| 1;2024-01-21;https://www.arxiv.org/abs/2401.11441v1	arXiv:2401.11441			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 15 2024	2024	Recommender systems have been widely deployed in various real -world applications to help users identify content of interest from massive amounts of information. Traditional recommender systems work by collecting user -item interaction data in a cloud -based data center and training a centralized model to perform the recommendation service. However, such cloud -based recommender systems (CloudRSs) inevitably suffer from excessive resource consumption, response latency, as well as privacy and security risks concerning both data and models. Recently, driven by the advances in storage, communication, and computation capabilities of edge devices, there has been a shift of focus from CloudRSs to on -device recommender systems (DeviceRSs), which leverage the capabilities of edge devices to minimize centralized data storage requirements, reduce the response latency caused by communication overheads, and enhance user privacy and security by localizing data processing and model training. Despite the rapid rise of DeviceRSs, there is a clear absence of timely literature reviews that systematically introduce, categorize and contrast these methods. To bridge this gap, we aim to provide a comprehensive survey of DeviceRSs, covering three main aspects: (1) the deployment and inference of DeviceRSs, exploring how large recommendation models can be compressed and utilized within resource -constrained on -device environments; (2) the training and update of DeviceRSs, discussing how local data can be leveraged for model optimization on the device side; (3) the security and privacy of DeviceRSs, unveiling their potential vulnerability to malicious attacks and defensive strategies to safeguard these systems. Furthermore, we provide a fine-grained and systematic taxonomy of the methods involved in each aspect, followed by a discussion regarding challenges and future research directions. This is the first comprehensive survey on DeviceRSs that covers a spectrum of tasks to fit various needs. We believe this survey will help readers effectively grasp the current research status in this field, equip them with relevant technical foundations, and stimulate new research ideas for developing DeviceRSs.																																	2024-03-13	PPRN:87277508		
J	Lu, Dong; Pang, Tianyu; Du, Chao; Liu, Qian; Yang, Xianjun; Lin, Min				Tianyu, Pang/AAW-2653-2020						Test-Time Backdoor Attacks on Multimodal Large Language Models								Arxiv											1	1;2024-02-13;https://www.arxiv.org/abs/2402.08577v1	arXiv:2402.08577			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 13 2024	2024	Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks. Our project page is available at https://sail-sg.github.io/AnyDoor/.																																	2024-05-25	PPRN:87669426		
J	Wang, Sifan; Li, Bowen; Chen, Yuhan; Perdikaris, Paris				Perdikaris, Paris/LNQ-5994-2024						PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks								Arxiv											3	3;2024-02-11;https://www.arxiv.org/abs/2402.00326v3| 2;2024-02-05;https://www.arxiv.org/abs/2402.00326v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.00326v1	arXiv:2402.00326			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 11 2024	2024	While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initialization scheme allows us to encode appropriate inductive biases corresponding to a given PDE system into the network architecture. We provide comprehensive empirical evidence showing that PirateNets are easier to optimize and can gain accuracy from considerably increased depth, ultimately achieving state-of-the-art results across various benchmarks. 																																	2024-02-27	PPRN:87456347		
J	Wu, Pengying; Mu, Yao; Wu, Bingxian; Hou, Yi; Ma, Ji; Zhang, Shanghang; Liu, Chang				Zhang, Lisa/AAW-9795-2021; yi, Hou/HRD-1375-2023; Wu, Bingxian/LXV-3797-2024						VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model								Arxiv											2	2;2024-02-06;https://www.arxiv.org/abs/2401.02695v2| 1;2024-01-05;https://www.arxiv.org/abs/2401.02695v1	arXiv:2401.02695			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning.																																	2024-05-25	PPRN:86998783		
J	Wu, Xinjian; Zeng, Fanhu; Wang, Xiudong; Chen, Xinghao				Chen, Xinghao/AAQ-3651-2021; Chen, Xinghao/AAA-6824-2019						PPT: Token Pruning and Pooling for Efficient Vision Transformers								Arxiv											3	3;2024-02-05;https://www.arxiv.org/abs/2310.01812v3| 2;2024-01-17;https://www.arxiv.org/abs/2310.01812v2| 1;2023-10-03;https://www.arxiv.org/abs/2310.01812v1	arXiv:2310.01812			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 05 2024	2024	Vision Transformers (ViTs) have emerged as powerful models in the field of computer vision, delivering superior performance across various vision tasks. However, the high computational complexity poses a significant barrier to their practical applications in real-world scenarios. Motivated by the fact that not all tokens contribute equally to the final predictions and fewer tokens bring less computational cost, reducing redundant tokens has become a prevailing paradigm for accelerating vision transformers. However, we argue that it is not optimal to either only reduce inattentive redundancy by token pruning, or only reduce duplicative redundancy by token merging. To this end, in this paper we propose a novel acceleration framework, namely token Pruning & Pooling Transformers (PPT), to adaptively tackle these two types of redundancy in different layers. By heuristically integrating both token pruning and token pooling techniques in ViTs without additional trainable parameters, PPT effectively reduces the model complexity while maintaining its predictive accuracy. For example, PPT reduces over 37% FLOPs and improves the throughput by over 45% for DeiT-S without any accuracy drop on the ImageNet dataset. The code is available at https://github.com/xjwu1024/PPT and https://github.com/mindspore-lab/models/																																	2024-05-25	PPRN:85381369		
J	Zhou, Pengyuan; Wang, Lin; Liu, Zhi; Hao, Yanbin; Hui, Pan; Tarkoma, Sasu; Kangasharju, Jussi				Kangasharju, Jussi/E-8922-2012; Zhou, Peng Yuan/AAJ-2139-2021; Hui, Pan/AAK-6660-2020; Hao, Yanbin/AAC-8050-2019; wang, Lin/GQO-7901-2022						A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2404.16038v1	arXiv:2404.16038			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 30 2024	2024	This paper offers an insightful examination of how currently top-trending AI technologies, i.e., generative artificial intelligence (Generative AI) and large language models (LLMs), are reshaping the field of video technology, including video generation, understanding, and streaming. It highlights the innovative use of these technologies in producing highly realistic videos, a significant leap in bridging the gap between real-world dynamics and digital creation. The study also delves into the advanced capabilities of LLMs in video understanding, demonstrating their effectiveness in extracting meaningful information from visual content, thereby enhancing our interaction with videos. In the realm of video streaming, the paper discusses how LLMs contribute to more efficient and user-centric streaming experiences, adapting content delivery to individual viewer preferences. This comprehensive review navigates through the current achievements, ongoing challenges, and future possibilities of applying Generative AI and LLMs to video-related tasks, underscoring the immense potential these technologies hold for advancing the field of video technology related to multimedia, networking, and AI communities.																																	2024-05-04	PPRN:88650717		
J	Zhu, Banghua; Jordan, Michael I.; Jiao, Jiantao										Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF								Arxiv											1	1;2024-01-29;https://www.arxiv.org/abs/2401.16335v1	arXiv:2401.16335			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human -centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed ’Iterative Data Smoothing’ (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.																																	2024-02-15	PPRN:87392940		
J	Katsch, Tobias										GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling								Arxiv											2	2;2024-01-27;https://www.arxiv.org/abs/2311.01927v2| 1;2023-11-03;https://www.arxiv.org/abs/2311.01927v1	arXiv:2311.01927			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 27 2024	2024	Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost O(l) recurrent mode and an efficient O(l log2l) parallel mode, where l is the sequence length, making use of highly optimized associative scan implementations. Furthermore, we derive an O(l2) surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on data-controlled cumulative sums for context aggregation, our findings suggest that incorporating data-controlled complex cumulative products may be a crucial step towards more powerful sequence models.																																	2024-02-15	PPRN:86035498		
J	Zhou, Yue; Guo, Chenlu; Wang, Xu; Chang, Yi; Wu, Yuan										A Survey on Data Augmentation in Large Model Era								Arxiv											1	1;2024-01-27;https://www.arxiv.org/abs/2401.15422v1	arXiv:2401.15422			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 27 2024	2024	Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data augmentation. Our discussion then expands to encompass the array of applications for these data augmentation methods within natural language processing, computer vision, and audio signal processing. We proceed to evaluate the successes and limitations of large model-based data augmentation across different scenarios. Concluding our review, we highlight prospective challenges and avenues for future exploration in the field of data augmentation. Our objective is to furnish researchers with critical insights, ultimately contributing to the advancement of more sophisticated large models. We consistently maintain the related open-source materials at: https://github.com/MLGroup-JLU/LLM-data-aug-survey.																																	2024-05-25	PPRN:87392786		
J	Bhardwaj, Lakshya; Bullimore, Mathew; Ferrari, Andrea E.V.; Schafer-Nameki, Sakura				Bhardwaj, Lakshya/ISU-5186-2023						Anomalies of Generalized Symmetries from Solitonic Defects								Arxiv											2	2;2024-01-26;https://www.arxiv.org/abs/2205.15330v3| 1;2024-01-24;https://www.arxiv.org/abs/2205.15330v2	arXiv:2205.15330			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 26 2024	2024	We propose the general idea that 't Hooft anomalies of generalized global symmetries can be understood in terms of the properties of solitonic defects, which generically are non-topological defects. The defining property of such defects is that they act as sources for background fields of generalized symmetries. 't Hooft anomalies arise when solitonic defects are charged under these generalized symmetries. We illustrate this idea for several kinds of anomalies in various spacetime dimensions. A systematic exploration is performed in 3d for 0-form, 1-form, and 2-group symmetries, whose 't Hooft anomalies are related to two special types of solitonic defects, namely vortex line defects and monopole operators. This analysis is supplemented with detailed computations of such anomalies in a large class of 3d gauge theories. Central to this computation is the determination of the gauge and 0-form charges of a variety of monopole operators: these involve standard gauge monopole operators, but also fractional gauge monopole operators, as well as monopole operators for 0-form symmetries. The charges of these monopole operators mainly receive contributions from Chern-Simons terms and fermions in the matter content. Along the way, we interpret the vanishing of the global gauge and ABJ anomalies, which are anomalies not captured by local anomaly polynomials, as the requirement that gauge monopole operators and mixed monopole operators for 0-form and gauge symmetries have non-fractional integer charges.																																	2024-05-25	PPRN:87316982		
J	Chan, Chunkit; Jiayang, Cheng; Wang, Weiqi; Jiang, Yuxin; Fang, Tianqing; Liu, Xin; Song, Yangqiu				Wang, Weiqi/KSL-8604-2024; Song, Yangqiu/OHU-0096-2025; JIANG, Yuxin/AAB-9309-2022; Fang, Tianqing/JJF-2802-2023; Cheng, Jiayang/LKO-0269-2024						ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations								Arxiv											2	2;2024-01-26;https://www.arxiv.org/abs/2304.14827v3| 1;2023-04-28;https://www.arxiv.org/abs/2304.14827v1	arXiv:2304.14827			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 26 2024	2024	This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we proceed to carry out thorough evaluations on the whole test sets of 11 datasets, including temporal and causal relations, PDTB2.0-based, and dialogue-based discourse relations. To ensure the reliability of our findings, we employ three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. Through our study, we discover that ChatGPT exhibits exceptional proficiency in detecting and reasoning about causal relations, albeit it may not possess the same level of expertise in identifying the temporal order between two events. While it is capable of identifying the majority of discourse relations with existing explicit discourse connectives, the implicit discourse relation remains a formidable challenge. Concurrently, ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.																																	2024-05-25	PPRN:66252317		
J	Lyons, Terry; Mcleod, Andrew D.										Signature Methods in Machine Learning								Arxiv											2	2;2024-01-26;https://www.arxiv.org/abs/2206.14674v5| 1;2022-06-29;https://www.arxiv.org/abs/2206.14674v1	arXiv:2206.14674			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 26 2024	2024	Signature-based techniques give mathematical insight into the interactions between complex streams of evolving data. These insights can be quite naturally translated into numerical approaches to understanding streamed data, and perhaps because of their mathematical precision, have proved useful in analysing streamed data in situations where the data is irregular, and not stationary, and the dimension of the data and the sample sizes are both moderate.   Understanding streamed multi-modal data is exponential: a word in n letters from an alphabet of size d can be any one of dn messages. Signatures provide a “lossy compression” of the information contained within such a stream by filtering out the parameterisation noise. More concretely, suppose we have a time series with 3 channels and N samples. There are 1+3N + 3N(3N +1)/2 = 1+ 9/2N(N +1) linearly independent quadratic polynomials defined on the time series. But the signature of this time series truncated to depth 2 only consists of 1 + 3 + 32 = 13 components which is, in particular, independent of the number of samples N. However, whilst the independence of the number of samples N has removed an exponential amount of noise, the dependence on the number of channels to the power of the depth ensures that an exponential amount of information remains.   This survey aims to stay in the domain where that exponential scaling can be managed directly. Scalability issues are an important challenge in many problems but would require another survey article and further ideas. This survey describes a range of contexts where the data sets are small and the existence of small sets of context free and principled features can be used effectively.   The mathematical nature of the tools can make their use intimidating to non-mathematicians. The examples presented in this article are intended to bridge this communication gap and provide tractable working examples drawn from the machine learning context. Notebooks are available online for several of these examples. This survey builds on the earlier paper of Ilya Chevryev and Andrey Kormilitzin which had broadly similar aims at an earlier point in the development of this machinery.   This article illustrates how the theoretical insights offered by signatures are simply realised in the analysis of application data in a way that is largely agnostic to the data type. Larger and more complex problems would expect to address scalability issues and draw on a wider range of data science techniques.   The article starts with a brief discussion of background material related to machine learning and signatures. This discussion fixes notation and terminology whilst simplifying the dependencies, but these background sections are not a substitute for the extensive literature they draw from.   Hopefully, by working some of the examples the reader will find access to useful and simple to deploy tools; tools that are moderately effective in analysing longitudinal data that is complex and irregular in contexts where massive machine learning is not a possibility.																																	2024-05-25	PPRN:12326299		
J	Luo, Sichun; Yao, Yuxuan; He, Bowei; Huang, Yinya; Zhou, Aojun; Zhang, Xinyi; Xiao, Yuanzhang; Zhan, Mingjie; Song, Linqi				Xiao, Yuanzhang/I-1678-2016; Zhan, Mingjie/MGW-5518-2025						Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation								Arxiv											1	1;2024-01-25;https://www.arxiv.org/abs/2401.13870v1	arXiv:2401.13870			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	Conventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their own strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance. In this paper, we introduce a general and model-agnostic framework known as Large Language model with mutual augmentation and adaptive aggregation for Recommendation (Llama4Rec). Llama4Rec synergistically integrates conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and the LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec, demonstrating its consistent and significant improvements in recommendation performance over baseline methods.																																	2024-05-25	PPRN:87336511		
J	Wu, Size; Zhang, Wenwei; Xu, Lumin; Jin, Sheng; Li, Xiangtai; Liu, Wentao; Loy, Chen Change				JIn, Sheng/JDM-6401-2023; Wu, Size/AGL-3354-2022; Zhang, Wenwei/HKO-4277-2023						CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction								Arxiv											2	2;2024-01-24;https://www.arxiv.org/abs/2310.01403v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01403v1	arXiv:2310.01403			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 24 2024	2024	Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in zero-shot image classification. However, when transferring the vision-language alignment of CLIP from global image representation to local region representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer from the domain shift from full images to local image regions. In this paper, we embark on an in-depth analysis of the region-language alignment in CLIP models, which is essential for downstream open-vocabulary dense prediction tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the image-level recognition ability of CLIP ViT to local image regions without needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by aligning a region representation extracted from its dense feature map with the image-level representation of the corresponding image crop. With the enhanced CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary object detection, semantic segmentation, and panoptic segmentation across various benchmarks.																																	2024-05-25	PPRN:85350310		
J	Qin, Jie; Wu, Jie; Chen, Weifeng; Ren, Yuxi; Li, Huixia; Wu, Hefeng; Xiao, Xuefeng; Wang, Rui; Wen, Shilei				li, huixia/IUP-7366-2023; Li, Shiyan/H-3445-2016						DiffusionGPT: LLM-Driven Text-to-Image Generation System								Arxiv											1	1;2024-01-18;https://www.arxiv.org/abs/2401.10061v1	arXiv:2401.10061			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 18 2024	2024	Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.																																	2024-05-25	PPRN:87223651		
J	Lv, Hui; Sun, Qianru				Sun, Qianru/HHN-0249-2022						Video Anomaly Detection and Explanation via Large Language Models								Arxiv											1	1;2024-01-11;https://www.arxiv.org/abs/2401.05702v1	arXiv:2401.05702			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jan 11 2024	2024	Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos. Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results. In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies. We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling. We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data. Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86% and +4.96%, respectively. More impressively, our approach can provide textual explanations for detected anomalies.																																	2024-01-26	PPRN:87124513		
J	Mo, Kangtong; Chu, Linyue; Zhang, Xingyu; Su, Xiran; Qian, Yang; Ou, Yining; Pretorius, Wian										DRAL: Deep Reinforcement Adaptive Learning for Multi-UAVs Navigation in Unknown Indoor Environment								Arxiv											3	3;2024-12-23;https://www.arxiv.org/abs/2409.03930v3| 2;2024-10-09;https://www.arxiv.org/abs/2409.03930v2| 1;2024-09-05;https://www.arxiv.org/abs/2409.03930v1	arXiv:2409.03930			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 23 2024	2024	Autonomous indoor navigation of UAVs presents numerous challenges, primarily due to the limited precision of GPS in enclosed environments. Additionally, UAVs' limited capacity to carry heavy or power-intensive sensors, such as overheight packages, exacerbates the difficulty of achieving autonomous navigation indoors. This paper introduces an advanced system in which a drone autonomously navigates indoor spaces to locate a specific target, such as an unknown Amazon package, using only a single camera. Employing a deep learning approach, a deep reinforcement adaptive learning algorithm is trained to develop a control strategy that emulates the decision-making process of an expert pilot. We demonstrate the efficacy of our system through real-time simulations conducted in various indoor settings. We apply multiple visualization techniques to gain deeper insights into our trained network. Furthermore, we extend our approach to include an adaptive control algorithm for coordinating multiple drones to lift an object in an indoor environment collaboratively. Integrating our DRAL algorithm enables multiple UAVs to learn optimal control strategies that adapt to dynamic conditions and uncertainties. This innovation enhances the robustness and flexibility of indoor navigation and opens new possibilities for complex multi-drone operations in confined spaces. The proposed framework highlights significant advancements in adaptive control and deep reinforcement learning, offering robust solutions for complex multi-agent systems in real-world applications.																																	2025-02-02	PPRN:91791315		
J	Yao, Yunzhi; Zhang, Ningyu; Xi, Zekun; Wang, Mengru; Xu, Ziwen; Deng, Shumin; Chen, Huajun				Huajun, Chen/B-6340-2013						Knowledge Circuits in Pretrained Transformers								Arxiv											3	3;2024-12-19;https://www.arxiv.org/abs/2405.17969v3| 2;2024-10-16;https://www.arxiv.org/abs/2405.17969v2| 1;2024-05-28;https://www.arxiv.org/abs/2405.17969v1	arXiv:2405.17969			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 19 2024	2024	The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, have allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuits hold potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing1.																																	2025-01-27	PPRN:89091130		
J	Nguyen, Dang; Chen, Jian; Wang, Yu; Wu, Gang; Park, Namyong; Hu, Zhengmian; Lyu, Hanjia; Wu, Junda; Aponte, Ryan; Xia, Yu; Li, Xintong; Shi, Jing; Chen, Hongjie; Lai, Viet Dac; Xie, Zhouhang; Kim, Sungchul; Zhang, Ruiyi; Yu, Tong; Tanjim, Mehrab; Ahmed, Nesreen K.; Mathur, Puneet; Yoon, Seunghyun; Yao, Lina; Kveton, Branislav; Nguyen, Thien Huu; Bui, Trung; Zhou, Tianyi; Rossi, Ryan A.; Dernoncourt, Franck				Zhang, Ruiyi/AAB-8402-2021; Wu, Junda/KIB-5820-2024; LYU, HANJIA/JCD-5591-2023; Hu, Zhengmian/KOD-5810-2024; li, xintong/OFM-6612-2025; Mathur, Puneet/IYJ-0089-2023; Xie, Zhouhang/JZK-0986-2024						GUI Agents: A Survey								Arxiv											1	1;2024-12-18;https://www.arxiv.org/abs/2412.13501v1	arXiv:2412.13501			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Dec 18 2024	2024	Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.																																	2025-01-24	PPRN:120039574		
J	Wu, Kun; Hou, Chengkai; Liu, Jiaming; Che, Zhengping; Ju, Xiaozhu; Yang, Zhuqin; Li, Meng; Zhao, Yinuo; Xu, Zhiyuan; Yang, Guang; Zhao, Zhen; Li, Guangyu; Jin, Zhao; Wang, Lecheng; Mao, Jilei; Wang, Xinhua; Fan, Shichao; Liu, Ning; Ren, Pei; Zhang, Qiang; Lyu, Yaoxu; Liu, Mengzhen; He, Jingyang; Luo, Yulin; Gao, Zeyu; Li, Chenxuan; Gu, Chenyang; Fu, Yankai; Wu, Di; Wang, Xingyu; Chen, Sixiang; Wang, Zhenyu; An, Pengju; Qian, Siyuan; Zhang, Shanghang; Tang, Jian				Che, Zhengping/U-2509-2019; Chen, Sixiang/KPY-4904-2024; Mao, Jilei/MTD-8508-2025; Shan, Caifeng/W-6178-2019; Zhang, Qizhe/LBI-4862-2024; Zhang, Qian/LKK-3896-2024; 刘, 梦真/GSN-5974-2022						RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation								Arxiv											1	1;2024-12-18;https://www.arxiv.org/abs/2412.13877v1	arXiv:2412.13877			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 18 2024	2024	Developing robust and general-purpose robotic manipulation policies is a key goal in the field of robotics. To achieve effective generalization, it is essential to construct comprehensive datasets that encompass a large number of demonstration trajectories and diverse tasks. Unlike vision or language data that can be collected from the Internet, robotic datasets require detailed observations and manipulation actions, necessitating significant investment in hardware-software infrastructure and human labor. While existing works have focused on assembling various individual robot datasets, there remains a lack of a unified data collection standard and insufficient diversity in tasks, scenarios, and robot types. In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot manipulation), featuring 55k real-world demonstration trajectories across 279 diverse tasks involving 61 different object classes. RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related information, including multi-view RGB-D images, proprioceptive robot state information, end effector details, and linguistic task descriptions. To ensure dataset consistency and reliability during policy learning, RoboMIND is built on a unified data collection platform and standardized protocol, covering four distinct robotic embodiments: the Franka Emika Panda, the UR-5e, the AgileX dual-arm robot, and the Tien Kung humanoid robot with dual dexterous hands. In addition, we create a digital twin environment in the Isaac Sim simulator, featuring the same tasks and assets as our real- world dataset. This simulation environment not only facilitates the low-cost collection of additional training data but also enables efficient evaluation. We provide a thorough quantitative and qualitative analysis of RoboMIND across multiple dimensions, offering detailed insights into the diversity of our datasets. In our experiments, we conduct extensive real-world testing with four state-of-the-art imitation learning methods, demonstrating that training with RoboMIND data results in a high manipulation success rate and strong generalization. In addition, investigations of failure reasons reveal promising directions for improvement. 																																	2025-01-24	PPRN:120039933		
J	Mu, Yao; Chen, Tianxing; Peng, Shijia; Chen, Zanxin; Gao, Zeyu; Zou, Yude; Lin, Lunkai; Xie, Zhiqiang; Luo, Ping				peng, shijia/LYP-1340-2024; Zhang, Xitian/AAX-5010-2021; Chen, Tianxing/AAI-3058-2021; pluo/GPG-2707-2022; Gao, Zeyu/KCY-9505-2024						RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)								Arxiv											2	2;2024-12-16;https://www.arxiv.org/abs/2409.02920v2| 1;2024-09-04;https://www.arxiv.org/abs/2409.02920v1	arXiv:2409.02920			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 16 2024	2024	In the rapidly advancing field of robotics, dual-arm coordination and complex object manipulation are essential capabilities for developing advanced autonomous systems. However, the scarcity of diverse, high-quality demonstration data and real-world-aligned evaluation benchmarks severely limits such development. To address this, we introduce RoboTwin, a generative digital twin framework that uses 3D generative foundation models and large language models to produce diverse expert datasets and provide a real-world-aligned evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates varied digital twins of objects from single 2D images, generating realistic and interactive scenarios. It also introduces a spatial relation-aware code generation framework that combines object annotations with large language models to break down tasks, determine spatial constraints, and generate precise robotic movement code. Our framework offers a comprehensive benchmark with both simulated and real-world data, enabling standardized evaluation and better alignment between simulated training and real-world performance. We validated our approach using the open- source COBOT Magic Robot platform. Policies pre-trained on RoboTwin-generated data and fine-tuned with limited real-world samples improve the success rate of over 70% for single-arm tasks and over 40% for dual-arm tasks compared to models trained solely on real-world data. This significant improvement demonstrates RoboTwin’s potential to enhance the development and evaluation of dual-arm robotic manipulation systems.																																	2025-01-23	PPRN:91736667		
J	Tavakoli, Armin; Pozas-Kerstjens, Alejandro; Brown, Peter; Araujo, Mateus				Araújo, Mateus/JVO-0622-2024; Tavakoli, Armin/AAB-6018-2022; Pozas-Kerstjens, Alejandro/AAB-5884-2022						Semidefinite programming relaxations for quantum correlations								Arxiv											3	3;2024-12-16;https://www.arxiv.org/abs/2307.02551v4| 2;2024-02-16;https://www.arxiv.org/abs/2307.02551v3| 1;2023-07-05;https://www.arxiv.org/abs/2307.02551v1	arXiv:2307.02551			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 16 2024	2024	Semidefinite programs are convex optimisation problems involving a linear objective function and a domain of positive semidefinite matrices. Over the last two decades, they have become an indispensable tool in quantum information science. Many otherwise intractable fundamental and applied problems can be successfully approached by means of relaxation to a semidefinite program. Here, we review such methodology in the context of quantum correlations. We discuss how the core idea of semidefinite relaxations can be adapted for a variety of research topics in quantum correlations, including nonlocality, quantum communication, quantum networks, entanglement, and quantum cryptography.																																	2025-01-24	PPRN:73805892		
J	Niu, Qian; Liu, Junyu; Bi, Ziqian; Feng, Pohsun; Peng, Benji; Chen, Keyu; Li, Ming; Yan, Lawrence KQ; Zhang, Yichao; Yin, Caitlyn Heqi; Fei, Cheng; Wang, Tianyang; Wang, Yunze; Chen, Silin; Liu, Ming				Wang, Tianyang/A-3442-2019; chen, keyu/KHZ-6419-2024						Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges								Arxiv											3	3;2024-12-11;https://www.arxiv.org/abs/2409.02387v6| 2;2024-11-18;https://www.arxiv.org/abs/2409.02387v5| 1;2024-10-26;https://www.arxiv.org/abs/2409.02387v4	arXiv:2409.02387			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Dec 11 2024	2024	This comprehensive review explores the intersection of Large Language Models (LLMs) and cognitive science, examining similarities and differences between LLMs and human cognitive processes. We analyze methods for evaluating LLMs cognitive abilities and discuss their potential as cognitive models. The review covers applications of LLMs in various cognitive fields, highlighting insights gained for cognitive science research. We assess cognitive biases and limitations of LLMs, along with proposed methods for improving their performance. The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities. Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition. This review provides a balanced perspective on the current state and future potential of LLMs in advancing our understanding of both artificial and human intelligence.																																	2025-01-19	PPRN:118942463		
J	Chen, Xi; Zhang, Zhifei; Zhang, He; Zhou, Yuqian; Kim, Soo Ye; Liu, Qing; Li, Yijun; Zhang, Jianming; Zhao, Nanxuan; Wang, Yilin; Ding, Hui; Lin, Zhe; Zhao, Hengshuang				Kim, Soo/AAB-3613-2020						UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics								Arxiv											1	1;2024-12-10;https://www.arxiv.org/abs/2412.07774v1	arXiv:2412.07774			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 10 2024	2024	We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation models that effectively balance consistency and variation across frames, we propose a unifying approach that treats image-level tasks as discontinuous video generation. Specifically, we treat varying numbers of input and output images as frames, enabling seamless support for tasks such as image generation, editing, customization, composition, etc. Although designed for image-level tasks, we leverage videos as a scalable source for universal supervision. UniReal learns world dynamics from large-scale videos, demonstrating advanced capability in handling shadows, reflections, pose variation, and object interaction, while also exhibiting emergent capability for novel applications.																																	2025-01-17	PPRN:119811417		
J	Picard, Cyril; Edwards, Kristen M.; Doris, Anna C.; Man, Brandon; Giannone, Giorgio; Alam, Md Ferdous; Ahmed, Faez				Alam, Md Ferdous/JZT-1569-2024; Ahmed, Faez/AAP-2924-2021						FROM CONCEPT TO MANUFACTURING : EVALUATING V ISION-LANGUAGE M ODELS FOR ENGINEERING DESIGN								Arxiv											2	2;2024-12-09;https://www.arxiv.org/abs/2311.12668v3| 1;2024-08-08;https://www.arxiv.org/abs/2311.12668v2	arXiv:2311.12668			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 09 2024	2024	Engineering design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision-language models (VLMs), such as GPT-4V, enabling AI to impact many more types of tasks. Our work presents a comprehensive evaluation of VLMs across a spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Specifically in this paper, we assess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design tasks such as sketch similarity analysis, CAD generation, topology optimization, manufacturability assessment, and engineering textbook problems. Through this structured evaluation, we not only explore VLMs' proficiency in handling complex design challenges but also identify their limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field.																																	2025-01-17	PPRN:86743786		
J	Zheng, Guangcong; Li, Teng; Jiang, Rui; Lu, Yehao; Wu, Tao; Li, Xi				Zheng, Guangcong/JFK-2840-2023; Lu, Yehao/HTS-7064-2023						CamI2V: Camera-Controlled Image-to-Video Diffusion Model								Arxiv											1	1;2024-12-04;https://www.arxiv.org/abs/2410.15957v3	arXiv:2410.15957			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 04 2024	2024	Recent advancements have integrated camera pose as a user-friendly and physics-informed condition in video diffusion models, enabling precise camera control. In this paper, we identify one of the key challenges as effectively modeling noisy cross-frame interactions to enhance geometry consistency and camera controllability. We innovatively associate the quality of a condition with its ability to reduce uncertainty and interpret noisy cross-frame features as a form of noisy condition. Recognizing that noisy conditions provide deterministic information while also introducing randomness and potential misguidance due to added noise, we propose applying epipolar attention to only aggregate features along corresponding epipolar lines, thereby accessing an optimal amount of noisy conditions. Additionally, we address scenarios where epipolar lines disappear, commonly caused by rapid camera movements, dynamic objects, or occlusions, ensuring robust performance in diverse environments. Furthermore, we develop a more robust and reproducible evaluation pipeline to address the inaccuracies and instabilities of existing camera control metrics. Our method achieves a 25.64% improvement in camera controllability on the RealEstate10K dataset without compromising dynamics or generation quality and demonstrates strong generalization to out-of-domain images. Training and inference require only 24GB and 12GB of memory, respectively, for 16-frame sequences at 256x256 resolution. We will release all checkpoints, along with training and evaluation code.  [GRAPHICS]																																	2025-01-15	PPRN:119699781		
J	Huang, Zhiyu; Zhang, Zixu; Vaidya, Ameya; Chen, Yuxiao; Lv, Chen; Fisac, Jaime Fernandez				Huang, Zhiyu/KXQ-7898-2024; Zhang, Zixu/F-8658-2010						Versatile Behavior Diffusion for Generalized Traffic Agent Simulation								Arxiv											2	2;2024-12-02;https://www.arxiv.org/abs/2404.02524v2| 1;2024-04-03;https://www.arxiv.org/abs/2404.02524v1	arXiv:2404.02524			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	Existing traffic simulation models often fail to capture the complexities of real-world scenarios, limiting the effective evaluation of autonomous driving systems. We introduce Versatile Behavior Diffusion (VBD), a novel traffic scenario generation framework that utilizes diffusion generative models to predict scene-consistent and controllable multi-agent interactions in closed-loop settings. VBD achieves state-of-the-art performance on the Waymo Sim Agents Benchmark and can effectively produce realistic and coherent traffic behaviors with complex agent interactions under diverse environmental conditions. Furthermore, VBD offers inference-time scenario editing through multi-step refinement guided by behavior priors and model-based optimization objectives. This capability allows for controllable multi-agent behavior generation, accommodating a wide range of user requirements across various traffic simulation applications. Despite being trained solely on publicly available datasets representing typical traffic conditions, we introduce conflict-prior and game-theoretic guidance approaches that enable the creation of interactive, long-tail safety-critical scenarios, which is essential for comprehensive testing and validation of autonomous vehicles. Lastly, we provide in-depth insights into effective training and inference strategies for diffusion-based traffic scenario generation models, highlighting best practices and common pitfalls. Our work significantly advances the ability to simulate complex traffic environments, offering a powerful tool for the development and assessment of autonomous driving technologies.																																	2025-01-11	PPRN:88395937		
J	Lai, Ming-Jun; Shen, Zhaiming										The Kolmogorov Superposition Theorem can Break the Curse of Dimensionality When Approximating High Dimensional Functions								Arxiv											3	3;2024-12-02;https://www.arxiv.org/abs/2112.09963v5| 2;2023-11-11;https://www.arxiv.org/abs/2112.09963v4| 1;2021-12-18;https://www.arxiv.org/abs/2112.09963v2	arXiv:2112.09963			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	We explain how to use Kolmogorov Superposition Theorem (KST) to break the curse of dimensionality when approximating a dense class of multivariate continuous functions. We first show that there is a class of functions called Kolmogorov-Lipschitz (KL) continuous in C ([0 , 1] d) which can be approximated by a special ReLU neural network of two hidden layers with a dimension independent approximation rate O (1/n) with approximation constant increasing quadratically in d . The number of parameters used in such neural network approximation equals to (6d + 2)n. Next we introduce KB-splines by using linear B-splines to replace the outer function and smooth the KB-splines to have the so-called LKB-splines as the basis for approximation. Our numerical evidence shows that the curse of dimensionality is broken in the following sense: When using the standard discrete least squares (DLS) method to approximate a continuous function, there exists a pivotal set of points in [0, 1]d with size at most O (nd) such that the rooted mean squares error (RMSE) from the DLS based on the pivotal set is similar to the RMSE of the DLS based on the original set with size O (nd). The pivotal point set is chosen by using matrix cross approximation technique and the number of LKB-splines used for approximation is the same as the size of the pivotal data set. Therefore, we do not need too many basis functions as well as too many function values to approximate a high dimensional continuous function f .																																	2025-01-11	PPRN:12373701		
J	Yu, Tian; Zhang, Shaolei; Feng, Yang				Zhang, Shengchen/IQT-8260-2023						Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models								Arxiv											1	1;2024-11-29;https://www.arxiv.org/abs/2411.19443v1	arXiv:2411.19443			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 29 2024	2024	Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM’s powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs. The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience1.																																	2025-01-10	PPRN:119580946		
J	Li, Cheng; Teney, Damien; Yang, Linyi; Wen, Qingsong; Xie, Xing; Wang, Jindong				wang, jindong/ACD-8485-2022; Wen, Qingsong/LTF-7625-2024						CulturePark: Boosting Cross-cultural Understanding in Large Language Models								Arxiv											3	3;2024-11-21;https://www.arxiv.org/abs/2405.15145v3| 2;2024-10-29;https://www.arxiv.org/abs/2405.15145v2| 1;2024-05-24;https://www.arxiv.org/abs/2405.15145v1	arXiv:2405.15145			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 21 2024	2024	Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. 																																	2025-01-03	PPRN:89009185		
J	Xu, Xiaohan; Tao, Chongyang; Shen, Tao; Xu, Can; Xu, Hongbo; Long, Guodong; Lou, Jian-guang; Ma, Shuai				ma, shuai/NAX-5505-2025; Tao, Chongyang/MBG-8179-2025; Long, Guodong/T-3441-2019						Re-Reading Improves Reasoning in Large Language Models								Arxiv											3	3;2024-11-19;https://www.arxiv.org/abs/2309.06275v4| 2;2024-02-29;https://www.arxiv.org/abs/2309.06275v2| 1;2023-09-12;https://www.arxiv.org/abs/2309.06275v1	arXiv:2309.06275			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 19 2024	2024	To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., Re-Reading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a "bidirectional" encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable "bidirectional" attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2’s adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies.1																																	2024-12-28	PPRN:84989017		
J	Berghaus, Kim V.; Kable, Joshua A.; Miranda, Vivian				Miranda, Vivian/LVD-3553-2024						Quantifying Scalar Field Dynamics with DESI 2024 Y1 BAO measurements								Arxiv											2	2;2024-11-18;https://www.arxiv.org/abs/2404.14341v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.14341v1	arXiv:2404.14341			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 18 2024	2024	Quintessence scalar fields are a natural candidate for evolving dark energy. Unlike the phenomenological w0wa parameterization of the dark energy equation of state, they cannot accommodate the phantom regime of dark energy w(z) < -1, or crossings into the phantom regime. Recent baryon acoustic oscillation (BAO) measurements by the Dark Energy Spectroscopic Instrument (DESI) indicate a preference for evolving dark energy over a cosmological constant, ranging from 2.6σ- 3.9σ- when fitting to w0wa, and combining the DESI BAO measurements with other cosmological probes. In this work, we directly fit three simple scalar field models to the DESI BAO data, combined with cosmic microwave background anisotropy measurements and supernova data sets. We find the best fit model to include a 2 - 4% kinetic scalar field energy Ωscf,k, for a canonical scalar field with a quadratic or linear potential. However, only the DESY-Y5 supernova data set combination shows a preference for quintessence over ΛCDM at the 95% confidence level. Fitting to the supernova data sets Pantheon, Pantheon+, DES-Y5, and Union3, we show that the mild tension (nσ < 3.4) under ΛCDM emerges from a BAO preference for smaller values of fractional mass-energy density Ωm < 0.29, while all supernova data sets, except for Pantheon, prefer larger values, Ωm > 0.3. The tension under ΛCDM remains noticeable (nσ < 2.8), when replacing two of the DESI BAO redshift bins with effective redshifts zeff = 0.51, and zeff = 0.706 with comparable BOSS DR 12 BAO measurements at zeff = 0.51, and zeff = 0.61. Canonical scalar fields as dark energy are successful in mitigating that tension.																																	2024-12-28	PPRN:88606670		
J	Ishak, M.; Pan, J.; Calderon, R.; Lodha, K.; Valogiannis, G.; Aviles, A.; Niz, G.; Yi, L.; Zheng, C.; Garcia-Quintero, C.; Mattia, A. de; Medina-Varela, L.; Cervantes-Cota, J. L.; Andrade, U.; Huterer, D.; Noriega, H. E.; Zhao, G.; Shafieloo, A.; Fang, W.; Ahlen, S.; Bianchi, D.; Brooks, D.; Burtin, E.; Chaussidon, E.; Claybaugh, T.; Cole, S.; Macorra, A. de la; Dey, Arjun; Fanning, K.; Ferraro, S.; Font-Ribera, A.; Forero-Romero, J. E.; Gaztanaga, E.; Gil-Marin, H.; Gutierrez, G.; Hahn, C.; Honscheid, K.; Howlett, C.; Juneau, S.; Kirkby, D.; Kisner, T.; Kremin, A.; Landriau, M.; Guillou, L. Le; Leauthaud, A.; Levi, M. E.; Meisner, A.; Miquel, R.; Moustakas, J.; Newman, J. A.; Palanque-Delabrouille, N.; Percival, W. J.; Poppett, C.; Prada, F.; Perez-Rafols, I.; Ross, A. J.; Rossi, G.; Sanchez, E.; Schlegel, D.; Schubnell, M.; Seo, H.; Sprayberry, D.; Tarle, G.; Vargas-Magana, M.; Weaver, B. A.; Wechsler, R. H.; Yeche, C.; Zarrouk, P.; Zhou, R.; Zou, H.				Wechsler, Risa/JOZ-3146-2023; Forero-Romero, Jaime E./GSO-0315-2022; Andrade, Uendert/GPF-3285-2022; Ross, Ashley/AAG-8865-2021; Percival, Will/HTM-1531-2023; Weijmans, Anne-Marie/KIL-5813-2024; Barrios Calderón, Romeo de Jesús/AGH-7322-2022; Gil Marin, Hector/B-2013-2017; Aviles, Alejandro/AAF-9583-2021; Cervantes-Cota, Jorge/O-1360-2018						Modified Gravity Constraints from the Full Shape Modeling of Clustering Measurements from DESI 2024								Arxiv											1	1;2024-11-18;https://www.arxiv.org/abs/2411.12026v1	arXiv:2411.12026			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 18 2024	2024	We present cosmological constraints on deviations from general relativity (GR) from the first-year of clustering observations from the Dark Energy Spectroscopic Instrument (DESI) in combination with other datasets. We first consider the μ(a,k)-Σ(a,k) modified gravity (MG) parametrization (as well as η(a,k)) in flat ΛCDM and w0waCDM backgrounds. Using a functional form for time-only evolution gives μ0=0.11−0.54+0.44 from DESI(FS+BAO)+BBN and a wide prior on ns. Using DESI(FS+BAO)+CMB+DESY3+DESY5-SN, we obtain μ0=0.05±0.22 and Σ0=0.009±0.045 in the ΛCDM background. In w0wa CDM, we obtain μ0=−0.24−0.28 +0.32 and Σ0=0.006±0.043, consistent with GR, and we still find a preference of the data for dynamical dark energy with w0>−1 and wa<0. We then use binned forms in the two backgrounds starting with two bins in redshift and then combining them with two bins in scale for a total of 4 and 8 MG parameters, respectively. All MG parameters are found consistent with GR. We also find that the tension reported for Σ0 with GR when using Planck PR3 goes away when we use the recent LoLLiPoP+HiLLiPoP likelihoods. As noted previously, this seems to indicate that the tension is related to the CMB lensing anomaly in PR3 which is also alleviated when using these likelihoods. We then constrain the class of Horndeski theory in the effective field theory of dark energy. We consider both EFT-basis and α-basis. Assuming a power law parametrization for the function Ω, which controls non-minimal coupling, we obtain Ω0=0.0120−0.013+0.0021 and s0=0.99−0.20+0.54 from DESI(FS+BAO)+DESY5SN+CMB in a ΛCDM background. Similar results are obtained when using the α-basis, where we constrain cM<1.24, and are all consistent with GR																																	2025-01-24	PPRN:119280623		
J	Wang, Zhenyi; Yang, Enneng; Shen, Li; Huang, Heng										A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning								Arxiv											2	2;2024-11-18;https://www.arxiv.org/abs/2307.09218v3| 1;2023-07-16;https://www.arxiv.org/abs/2307.09218v1	arXiv:2307.09218			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 18 2024	2024	Forgetting refers to the loss or deterioration of previously acquired knowledge. While existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new task, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we present a more nuanced understanding of this phenomenon and highlight its potential advantages. Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications. 																																	2024-12-28	PPRN:73990617		
J	Xiao, Zeqi; Wang, Tai; Wang, Jingbo; Cao, Jinkun; Zhang, Wenwei; Dai, Bo; Lin, Dahua; Pang, Jiangmiao				Wang, Tai/MVV-1100-2025; Lin, Dahua/W-6576-2019						Unified Human-Scene Interaction via Prompted Chain-of-Contacts								Arxiv											5	5;2024-11-05;https://www.arxiv.org/abs/2309.07918v5| 4;2024-09-03;https://www.arxiv.org/abs/2309.07918v4| 3;2024-04-19;https://www.arxiv.org/abs/2309.07918v3| 2;2023-09-17;https://www.arxiv.org/abs/2309.07918v2| 1;2023-09-14;https://www.arxiv.org/abs/2309.07918v1	arXiv:2309.07918			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 05 2024	2024	Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and user-friendly interfaces, require further exploration for the practical application of HSI. This paper presents a unified HSI framework, named UniHSI, that supports unified control of diverse interactions through language commands. The framework defines interaction as “Chain of Contacts (CoC)”, representing steps involving human joint-object part pairs. This concept is inspired by the strong correlation between interaction types and corresponding contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To support training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes.																																	2024-12-09	PPRN:85015307		
J	Mu, Tong; Helyar, Alec; Heidecke, Johannes; Achiam, Joshua; Vallone, Andrea; Kivlichan, Ian; Lin, Molly; Beutel, Alex; Schulman, John; Weng, Lilian				Mu, Tong/OOK-2179-2025						Rule Based Rewards for Language Model Safety								Arxiv											1	1;2024-11-02;https://www.arxiv.org/abs/2411.01111v1	arXiv:2411.01111			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 02 2024	2024	Reinforcement learning based fine-tuning of large language models (LLMs) on human preferences has been shown to enhance both their capabilities and safety behavior. However, in cases related to safety, without precise instructions to human annotators, the data collected may cause the model to become overly cautious, or to respond in an undesirable style, such as being judgmental. Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior. We propose a novel preference modeling approach that utilizes AI feedback and only requires a small amount of human data. Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g. refusals should not be judgmental) along with a LLM grader. In contrast to prior methods using AI feedback, our method uses fine-grained, composable, LLM-graded few-shot prompts as reward directly in RL training, resulting in greater control, accuracy and ease of updating. We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety.																																	2024-12-09	PPRN:119023868		
J	Rutherford, Alexander; Ellis, Benjamin; Gallici, Matteo; Cook, Jonathan; Lupu, Andrei; Ingvarsson, Gardar; Willi, Timon; Hammond, Ravi; Khan, Akbir; de Witt, Christian Schroeder; Souly, Alexandra; Bandyopadhyay, Saptarashmi; Samvelyan, Mikayel; Jiang, Minqi; Lange, Robert Tjarko; Whiteson, Shimon; Lacerda, Bruno; Hawes, Nick; Rocktaschel, Tim; Lu, Chris; Foerster, Jakob Nicolaus				Samvelyan, Mikayel/AAF-2149-2019						JaxMARL: Multi-Agent RL Environments and Algorithms in JAX								Arxiv											4	4;2024-11-02;https://www.arxiv.org/abs/2311.10090v5| 3;2023-12-19;https://www.arxiv.org/abs/2311.10090v4| 2;2023-11-20;https://www.arxiv.org/abs/2311.10090v3| 1;2023-11-16;https://www.arxiv.org/abs/2311.10090v1	arXiv:2311.10090			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 02 2024	2024	Benchmarks are crucial in the development of machine learning algorithms, with available environments significantly influencing reinforcement learning (RL) research. Traditionally, RL environments run on the CPU, which limits their scalability with typical academic compute. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, Python-based library that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is around 14 times faster than existing approaches, and up to 12500x when multiple training runs are vectorized. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a JAXbased approximate reimplementation of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL.																																	2024-12-16	PPRN:86176931		
J	Che, Xin; Hu, Jingdi; Zhou, Zirui; Zhang, Yong; Chu, Lingyang				Zhou, Zirui/Q-5046-2018						Training Fair Models in Federated Learning without Data Privacy Infringement								Arxiv											2	2;2024-11-01;https://www.arxiv.org/abs/2109.05662v2| 1;2021-09-13;https://www.arxiv.org/abs/2109.05662v1	arXiv:2109.05662			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 01 2024	2024	Training fair machine learning models becomes more and more important. As many powerful models are trained by collaboration among multiple parties, each holding some sensitive data, it is natural to explore the feasibility of training fair models in federated learning so that the fairness of trained models, the data privacy of clients, and the collaboration between clients can be fully respected simultaneously. However, the task of training fair models in federated learning is challenging, since it is far from trivial to estimate the fairness of a model without knowing the private data of the participating parties, which is often constrained by privacy requirements in federated learning. In this paper, we first propose a federated estimation method to accurately estimate the fairness of a model without infringing the data privacy of any party. Then, we use the fairness estimation to formulate a novel problem of training fair models in federated learning. We develop FedFair, a well-designed federated learning framework, which can successfully train a fair model with high performance without data privacy infringement. Our extensive experiments on three real-world data sets demonstrate the excellent fair model training performance of our method.																																	2024-12-16	PPRN:11927415		
J	Zhang, Qingyang; Wei, Yake; Han, Zongbo; Fu, Huazhu; Peng, Xi; Deng, Cheng; Hu, Qinghua; Xu, Cai; Wen, Jie; Hu, Di; Zhang, Changqing				清扬, 张/AAS-9152-2021; Fu, Huazhu/A-1411-2014; Yang, Linghui/R-4426-2018; Hu, Qinghua/GWM-6136-2022						Multimodal Fusion on Low-quality Data: A Comprehensive Survey								Arxiv											3	3;2024-11-01;https://www.arxiv.org/abs/2404.18947v3| 2;2024-05-05;https://www.arxiv.org/abs/2404.18947v2| 1;2024-04-27;https://www.arxiv.org/abs/2404.18947v1	arXiv:2404.18947			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	Multimodal fusion focuses on integrating information from multiple modalities with the goal of more accurate prediction, which has achieved remarkable progress in a wide range of scenarios, including autonomous driving and medical diagnosis. However, the reliability of multimodal fusion remains largely unexplored especially under low-quality data settings. This paper surveys the common challenges and recent advances of multimodal fusion in the wild and presents them in a comprehensive taxonomy. From a data-centric view, we identify four main challenges that are faced by multimodal fusion on low-quality data, namely (1) noisy multimodal data that are contaminated with heterogeneous noises, (2) incomplete multimodal data that some modalities are missing, (3) imbalanced multimodal data that the qualities or properties of different modalities are significantly different and (4) quality-varying multimodal data that the quality of each modality dynamically changes with respect to different samples. This new taxonomy will enable researchers to understand the state of the field and identify several potential directions. We also provide discussion for the open problems in this field together with interesting future research directions.																																	2024-12-09	PPRN:88700148		
J	Fang, Yunhao; Zhu, Ligeng; Lu, Yao; Wang, Yan; Molchanov, Pavlo; Kautz, Jan; Cho, Jang Hyun; Pavone, Marco; Han, Song; Yin, Hongxu				Yin, Hongxu/AAZ-3328-2020						VILA2 : VLM AUGMENTED VLM WITH SELF-IMPROVEMENT								Arxiv											2	2;2024-10-31;https://www.arxiv.org/abs/2407.17453v2| 1;2024-07-24;https://www.arxiv.org/abs/2407.17453v1	arXiv:2407.17453			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 31 2024	2024	While visual language model (VLM) architectures and training infrastructures advance rapidly, data curation remains under-explored where quantity and quality become a bottleneck. Existing work either crawls extra Internet data with a loose guarantee of quality or distills from black-box proprietary models ( e.g ., GPT-4V / Gemini) that are API frequency and performance bounded. This work enables a VLM to improve itself via data enhancement, exploiting its generative nature. We introduce a simple yet effective VLM augmentation scheme that includes a self-augment step and a specialist-augment step to iteratively improve data quality and hence, model performance. In the self-augment step, the instruction-finetuned VLM recaptions its pretraining caption datasets and then retrains from scratch leveraging refined data. Without any expensive human-in-the-loop annotation, we observe improvements in data quality and downstream accuracy boosts with three self-augmentation rounds – a viable free lunch to the current VLM training recipe. When self-augmentation saturates, we augment the caption diversity by leveraging specialty skills picked up from instruction finetuning. We finetune VLM specialists from the self-augmented VLM with domain-specific experts, including spatial, grounding, and OCR, to fuse task-aware synthetic data into the pretraining stage. Data quality improvements and hallucination reductions are cross-checked by VLM (GPT-4V, Gemini) and human judges. Combining self-augmentation and specialist-augmented training, VILA2 consistently improves the accuracy on a wide range of benchmarks over the prior art, producing a reusable pretraining dataset that is 300x more cost-efficient than human labeling.																																	2024-12-06	PPRN:91056513		
J	Cheng, Kanzhi; Li, Yantao; Xu, Fangzhi; Zhang, Jianbing; Zhou, Hao; Liu, Yang				Liu, Yuanchuan/AAN-9059-2020; Zhou, Hao/MVX-1646-2025						Vision-Language Models Can Self-Improve Reasoning via Reflection								Arxiv											1	1;2024-10-30;https://www.arxiv.org/abs/2411.00855v1	arXiv:2411.00855			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 30 2024	2024	Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, R3V, which iteratively enhances the model's Vision-language Reasoning by Reflecting on CoT Rationales. Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes. Specifically, we introduce the self-refine and self-select losses, enabling the model to refine flawed rationale and derive the correct answer by comparing rationale candidates. Experiments on a wide range of vision-language tasks show that R3V consistently improves multimodal LLM reasoning, achieving a relative improvement of 23 to 60 percent over GPT-distilled baselines. Additionally, our approach supports self-reflection on generated solutions, further boosting performance through test-time computation.																																	2024-12-09	PPRN:119022664		
J	Peng, Shengyun; Chen, Pin-Yu; Hull, Matthew; Chau, Duen Horng				Chen, Pin-Yu/AAA-1059-2020; Peng, Anthony/IWE-1371-2023						Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models								Arxiv											2	2;2024-10-30;https://www.arxiv.org/abs/2405.17374v3| 1;2024-05-28;https://www.arxiv.org/abs/2405.17374v2	arXiv:2405.17374			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 30 2024	2024	Safety alignment is crucial to ensure that large language models (LLMs) behave in ways that align with human preferences and prevent harmful actions during inference. However, recent studies show that the alignment can be easily compromised through finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as “safety basin”: random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop. This safety basin contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new V ISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. The LLM safety landscape also highlights the system prompt’s critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community. Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape .																																	2024-12-06	PPRN:89090822		
J	Sachdeva, Natasha; Hartnett, Gavin S.; Maity, Smarak; Marsh, Samuel; Wang, Yulun; Winick, Adam; Dougherty, Ryan; Canuto, Daniel; Chong, You Quan; Hush, Michael; Mundada, Pranav S.; Bentley, Christopher D.B.; Biercuk, Michael J.; Baum, Yuval				Bentley, Christopher/D-4530-2019; Hush, Michael/C-3396-2016; Biercuk, Michael/B-4768-2010						Quantum optimization using a 127-qubit gate-model IBM quantum computer can outperform quantum annealers for nontrivial binary optimization problems								Arxiv											3	3;2024-10-28;https://www.arxiv.org/abs/2406.01743v4| 2;2024-07-22;https://www.arxiv.org/abs/2406.01743v3| 1;2024-07-09;https://www.arxiv.org/abs/2406.01743v2	arXiv:2406.01743			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 28 2024	2024	We introduce a comprehensive quantum solver for binary combinatorial optimization problems on gate-model quantum computers that outperforms any published alternative and consistently delivers correct solutions for problems with up to 127 qubits. We provide an overview of the internal workflow, describing the integration of a customized ansatz and variational parameter update strategy, efficient error suppression in hardware execution, and QPU-overhead free and scalable post-processing to correct for bit-flip errors. We benchmark this solver on IBM quantum computers for several classically nontrivial unconstrained binary optimization problems—the entire optimization is conducted on hardware with no use of classical simulation or prior knowledge of the solution. First, we demonstrate the ability to correctly solve Max-Cut instances for random regular graphs with a variety of densities using up to 120 qubits, where the graph topologies are not matched to device connectivity. These demonstrations are at least 4× larger than previous successful implementations on a trapped-ion quantum computer and deliver up to 9× increased likelihood of success for identical problem instances at the scale of 32 qubits. Next, we apply the solver to higher-order binary optimization and successfully search for the ground state energy of a 127-qubit spin-glass model with linear, quadratic, and cubic interaction terms. Use of this new quantum solver increases the likelihood of finding the minimum energy by up to ∼ 1, 500× relative to published results using a D-Wave annealer, and it can find the correct solution when the annealer fails. Furthermore, for both problem types, the Q-CTRL solver outperforms a heuristic local solver used to indicate the relative difficulty of the problems pursued. Overall, these results represent the largest quantum optimizations successfully solved on hardware to date, and demonstrate the first time a gate-model quantum computer has been able to outperform an annealer for a class of binary optimization problems.																																	2024-12-03	PPRN:90751578		
J	Cho, Sukmin; Jeong, Soyeong; Seo, Jeongyeon; Hwang, Taeho; Park, Jong C.				Cho, Sukmin/JRW-4802-2023						Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations								Arxiv											2	2;2024-10-22;https://www.arxiv.org/abs/2404.13948v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.13948v1	arXiv:2404.13948			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 22 2024	2024	The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. RetrievalAugmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG ( GARAG ), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our GARAG to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world. 																																	2024-11-23	PPRN:88614216		
J	Chen, Pengcheng; Ye, Jin; Wang, Guoan; Li, Yanjun; Deng, Zhongying; Li, Wei; Li, Tianbin; Duan, Haodong; Huang, Ziyan; Su, Yanzhou; Wang, Benyou; Zhang, Shaoting; Fu, Bin; Cai, Jianfei; Zhuang, Bohan; Seibel, Eric J; He, Junjun; Qiao, Yu				Deng, Zhongying/JDW-1393-2023; huang, ziyan/KJM-6164-2024; Chen, Pengcheng/KHW-6551-2024; Wang, Benyou/Y-5146-2019; Qiao, Yu/ABD-5787-2021; duan, haodong/JEP-4396-2023						GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI								Arxiv											3	3;2024-10-21;https://www.arxiv.org/abs/2408.03361v7| 2;2024-10-07;https://www.arxiv.org/abs/2408.03361v6| 1;2024-08-08;https://www.arxiv.org/abs/2408.03361v2	arXiv:2408.03361			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 21 2024	2024	Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 284 datasets across 38 medical image modalities, 18 clinical-related tasks, 18 departments, and 4 perceptual granularities in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 53.96%, indicating significant room for improvement. Moreover, we identified five key insufficiencies in current cutting-edge LVLMs that need to be addressed to advance the development of better medical applications. We believe that GMAI-MMBench will stimulate the community to build the next generation of LVLMs toward GMAI.																																	2024-11-20	PPRN:91293324		
J	Li, Dawei; Yang, Shu; Tan, Zhen; Baik, Jae Young; Yun, Sukwon; Lee, Joseph; Chacko, Aaron; Hou, Bojian; Duong-Tran, Duy; Ding, Ying; Liu, Huan; Shen, Li; Chen, Tianlong				Duong-Tran, Duy/ISS-0656-2023; Yang, Shu/MVU-4847-2025; Li, Dawei/IZD-9687-2023; Shen, Li/AAB-6870-2021; Ding, Ying/X-3657-2019; Tan, Zhen/JCE-9258-2023						DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature								Arxiv											3	3;2024-10-17;https://www.arxiv.org/abs/2405.04819v4| 2;2024-10-04;https://www.arxiv.org/abs/2405.04819v3| 1;2024-05-12;https://www.arxiv.org/abs/2405.04819v2	arXiv:2405.04819			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. 																																	2024-11-16	PPRN:89029894		
J	Lin, Toru; Yin, Zhao-Heng; Qi, Haozhi; Abbeel, Pieter; Malik, Jitendra				Qi, Haozhi/ABD-9753-2021						Twisting Lids Off with Two Hands								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2403.02338v2| 1;1800-01-01;https://www.arxiv.org/abs/2403.02338v1	arXiv:2403.02338			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 14 2024	2024	Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, due to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we share novel insights into physical modeling, real-time perception, and reward design that enable policies trained in simulation using deep reinforcement learning (RL) to be effectively and efficiently transferred to the real world. Specifically, we consider the problem of twisting lids of various bottle-like objects with two hands, demonstrating policies with generalization capabilities across a diverse set of unseen objects as well as dynamic and dexterous behaviors. To the best of our knowledge, this is the first sim-to-real RL system that enables such capabilities on bimanual multi-fingered hands.																																	2025-01-11	PPRN:88023523		
J	Wang, Zihan; Li, Yunxuan; Wu, Yuexin; Luo, Liangchen; Hou, Le; Yu, Hongkun; Shang, Jingbo				Shang, Jingbo/T-4207-2019						Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2402.02658v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.02658v1	arXiv:2402.02658			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	Process supervision, using a trained verifier to evaluate the intermediate steps generated by a reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid the expensive effort of human annotation on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Inaccuracies of the reasoner would cause MiPS underestimating the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior observations on human curated data. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output supervision trained verifier). Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models.																																	2024-11-10	PPRN:87523924		
J	Deora, Puneesh; Ghaderi, Rouzbeh; Taheri, Hossein; Thrampoulidis, Christos				Tryfonopoulos, Christos/AAL-8960-2021						On the Optimization and Generalization of Multi-head Attention								Arxiv											2	2;2024-10-12;https://www.arxiv.org/abs/2310.12680v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.12680v1	arXiv:2310.12680			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 12 2024	2024	The training and generalization dynamics of the Transformer's core mechanism, namely the Attention mechanism, remain under-explored. Besides, existing analyses primarily focus on single-head attention. Inspired by the demonstrated benefits of overparameterization when training fully-connected networks, we investigate the potential optimization and generalization advantages of using multiple attention heads. Towards this goal, we derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, under a suitable realizability condition on the data. We then establish primitive conditions on the initialization that ensure realizability holds. Finally, we demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We expect the analysis can be extended to various data-model and architecture variations.																																	2024-11-06	PPRN:85719418		
J	Feng, Shangbin; Sorensen, Taylor; Liu, Yuhan; Fisher, Jillian; Park, Chan Young; Choi, Yejin; Tsvetkov, Yulia										Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration								Arxiv											2	2;2024-10-11;https://www.arxiv.org/abs/2406.15951v2| 1;2024-06-22;https://www.arxiv.org/abs/2406.15951v1	arXiv:2406.15951			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 11 2024	2024	While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it "plugs into" a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.1																																	2024-11-03	PPRN:89406696		
J	Jansen, Peter; Cote, Marc-Alexandre; Khot, Tushar; Bransom, Erin; Mishra, Bhavana Dalvi; Majumder, Bodhisattwa Prasad; Tafjord, Oyvind; Clark, Peter										DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents								Arxiv											2	2;2024-10-07;https://www.arxiv.org/abs/2406.06769v2| 1;2024-06-10;https://www.arxiv.org/abs/2406.06769v1	arXiv:2406.06769			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 07 2024	2024	Automated scientific discovery promises to accelerate progress across scientific domains. However, developing and evaluating an AI agent's capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent's ability to perform complete cycles of novel scientific discovery. DISCOVERYWORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay). It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations. Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. DISCOVERYWORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge. We find that strong baseline agents, that perform well in prior published environments, struggle on most DISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel challenges of discovery, and thus that DISCOVERYWORLD may help accelerate near-term development and assessment of scientific discovery competency in agents. 																																	2024-10-29	PPRN:89284828		
J	Son, Guijin; Ko, Hyunwoo; Lee, Hoyoung; Kim, Yewon; Hong, Seunghyeok										LLM-as-a-Judge & Reward Model: What They Can and Cannot Do								Arxiv											2	2;2024-10-02;https://www.arxiv.org/abs/2409.11239v2| 1;2024-09-17;https://www.arxiv.org/abs/2409.11239v1	arXiv:2409.11239			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 02 2024	2024	LLM-as-a-Judge and reward models are widely used alternatives of multiplechoice questions or human annotators for large language model (LLM) evaluation. Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning. However, despite their popularity, their effectiveness in diverse contexts, such as non-English prompts, factual verification, or challenging questions, remains unexplored. In this paper, we conduct a comprehensive analysis of automated evaluators, reporting several key findings on their behavior. First, we discover that English evaluation capabilities significantly influence language-specific evaluation capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages. Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language. Finally, we find that state-of-the-art evaluators struggle with challenging prompts in either English or Korean, underscoring their limitations in assessing or generating complex reasoning questions. We release the dataset and codes used in this research.																																	2024-10-16	PPRN:91939177		
J	Mele, Francesco Anna; Mele, Antonio Anna; Bittel, Lennart; Eisert, Jens; Giovannetti, Vittorio; Lami, Ludovico; Leone, Lorenzo; Oliviero, Salvatore F.E.				Eisert, Jens/D-9640-2017; Oliviero, SalvatoreFrancescoEmanuele/LMP-7997-2024; Leone, Lorenzo/OML-9300-2025						Learning quantum states of continuous variable systems								Arxiv											2	2;2024-09-26;https://www.arxiv.org/abs/2405.01431v2| 1;2024-05-02;https://www.arxiv.org/abs/2405.01431v1	arXiv:2405.01431			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 26 2024	2024	Quantum state tomography, aimed at deriving a classical description of an unknown state from measurement data, is a fundamental task in quantum physics. In this work, we analyse the ultimate achievable performance of tomography of continuous-variable systems, such as bosonic and quantum optical systems. We prove that tomography of these systems is extremely inefficient in terms of time resources, much more so than tomography of finite-dimensional systems: not only does the minimum number of state copies needed for tomography scale exponentially with the number of modes, but it also exhibits a dramatic scaling with the trace-distance error, even for low-energy states, in stark contrast with the finite-dimensional case. On a more positive note, we prove that tomography of Gaussian states is efficient. To accomplish this, we answer a fundamental question for the field of continuous-variable quantum information: if we know with a certain error the first and second moments of an unknown Gaussian state, what is the resulting trace-distance error that we make on the state? Lastly, we demonstrate that tomography of non-Gaussian states prepared through Gaussian unitaries and a few local non-Gaussian evolutions is efficient and experimentally feasible.																																	2024-10-09	PPRN:88728432		
J	Roy, Nandan				Roy, Nandan/AGE-9152-2022						Dynamical dark energy in the light of DESI 2024 data								Arxiv											2	2;2024-09-24;https://www.arxiv.org/abs/2406.00634v2| 1;2024-06-02;https://www.arxiv.org/abs/2406.00634v1	arXiv:2406.00634			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 24 2024	2024	The latest findings from the DESI (Dark Energy Spectroscopic Instrument) data release 1 (DR1) [3], combined with data from the cosmic microwave background and supernovae, suggest a preference for dynamical dark energy over the cosmological constant. This study has considered the Chevallier-Polarski-Linder (CPL) parameterization for the dark energy equation of state (EoS) and has indicated a possible phantom barrier crossing in the recent past. Despite CPL being the most commonly used parameterization, recent research has pointed out issues with its prior selection and parameter degeneracies. In this paper, we introduce an alternative two-parameter parameterization of the dark-energy EoS, at high redshifts, this new parameterization can be approximated to the CPL form. Our findings also indicate that the current value of the EoS of dark energy resembles quintessence, with evidence of a recent crossing of the phantom barrier, supporting the conclusions in [3]. Furthermore, our model significantly reduces the Hubble tension to about 2.8{sigma} when compared to Hubble Space Telescope and SH0ES data [51], and to 1.6{sigma} with standardized TRGB and Type Ia supernova data [56]. Bayesian model selection using Bayes factors shows a moderate preference for our parameterization over the {Lambda} CDM model, aligned with the DESI2024 results and favoring dynamical dark energy.																																	2024-10-08	PPRN:89163230		
J	Breuval, Louise; Riess, Adam G.; Casertano, Stefano; Yuan, Wenlong; Macri, Lucas M.; Romaniello, Martino; Murakami, Yukei S.; Scolnic, Daniel; Anand, Gagandeep S.; Soszynski, Igor				SCOLNIC, DANIEL/OHR-7390-2025; Riess, Adam/ABF-2480-2020						Small Magellanic Cloud Cepheids Observed with the Hubble Space Telescope Provide a New Anchor for the SH0ES Distance Ladder								Arxiv											2	2;2024-09-23;https://www.arxiv.org/abs/2404.08038v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.08038v1	arXiv:2404.08038			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 23 2024	2024	We present phase-corrected photometric measurements of 88 Cepheid variables in the core of the Small Magellanic Cloud (SMC), the first sample obtained with the Hubble Space Telescope’s ( HST ) Wide Field Camera 3, in the same homogeneous photometric system as past measurements of all Cepheids on the SH0ES distance ladder. We limit the sample to the inner core and model the geometry to reduce errors in prior studies due to the nontrivial depth of this cloud. Without crowding present in ground-based studies, we obtain an unprecedentedly low dispersion of 0.102 mag for a period-luminosity (P-L) relation in the SMC, approaching the width of the Cepheid instability strip. The new geometric distance to 15 late-type detached eclipsing binaries in the SMC offers a rare opportunity to improve the foundation of the distance ladder, increasing the number of calibrating galaxies from three to four. With the SMC as the only anchor, we find H0 = 74.1 ± 2.1 km s−1 Mpc−1 . Combining these four geometric distances with our HST photometry of SMC Cepheids, we obtain H0 = 73.17 ± 0.86 km s−1 Mpc−1 . By including the SMC in the distance ladder, we also double the range where the metallicity ([Fe/H]) dependence of the Cepheid P-L relation can be calibrated, and we find γ =-0.234 ± 0.052 mag dex−1 . Our local measurement of H0 based on Cepheids and Type Ia supernovae shows a 5.8σ tension with the value inferred from the cosmic microwave background assuming a Lambda cold dark matter (ΛCDM) cosmology, reinforcing the possibility of physics beyond ΛCDM.																																	2024-10-21	PPRN:88550958		
J	Xie, Yunfei; Wu, Juncheng; Tu, Haoqin; Yang, Siwei; Zhao, Bingchen; Zong, Yongshuo; Jin, Qiao; Xie, Cihang; Zhou, Yuyin				Zhao, Bingchen/NRB-6996-2025; Xie, Yunfei/MDT-6764-2025						A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?								Arxiv											1	1;2024-09-23;https://www.arxiv.org/abs/2409.15277v1	arXiv:2409.15277			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 23 2024	2024	Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. 																																	2024-10-07	PPRN:96380553		
J	Jungwirth, Tomas; Fernandes, Rafael M.; Sinova, Jairo; Smejkal, Libor				Šmejkal, Libor/G-8927-2014; Jungwirth, Tomas/G-8952-2014						Altermagnets and beyond: Nodal magnetically-ordered phases								Arxiv											1	1;2024-09-16;https://www.arxiv.org/abs/2409.10034v1	arXiv:2409.10034			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 16 2024	2024	The recent discovery of altermagnets has opened new perspectives in the field of ordered phases in condensed matter. In strongly-correlated superfluids, the nodal p-wave and d-wave ordered phases of 3 He and cuprates play a prominent role in physics for their rich phenomenology of the symmetry- breaking order parameters. While the p-wave and d-wave superfluids have been extensively studied over the past half a century, material realizations of their magnetic counterparts have remained elusive for many decades. This is resolved in altermagnets, whose recent discovery was driven by research in the field of spintronics towards highly scalable information technologies. Altermagnets feature d, g or i-wave magnetic ordering, with a characteristic alternation of spin polarization and spin-degenerate nodes. Here we review how altermagnetism can be identified from symmetries of collinear spin densities in crystal lattices, and can be realized at normal conditions in a broad family of insulating and conducting materials. We highlight salient electronic-structure signatures of the altermagnetic ordering, discuss extraordinary relativistic and topological phenomena that emerge in their band structures, and comment on strong-correlation effects. We then extend the discussion to non-collinear spin densities in crystals, including the prediction of p-wave magnets, and conclude with a brief summary of the reviewed physical properties of the nodal magnetically-ordered phases.																																	2024-12-24	PPRN:119223153		
J	Terry, Michael; Kulkarni, Chinmay; Wattenberg, Martin; Dixon, Lucas; Morris, Meredith Ringel				Dixon, Lucas/AFL-2608-2022						Interactive AI Alignment: Specification, Process, and Evaluation Alignment								Arxiv											2	2;2024-09-16;https://www.arxiv.org/abs/2311.00710v2| 1;2023-10-23;https://www.arxiv.org/abs/2311.00710v1	arXiv:2311.00710			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 16 2024	2024	Modern AI enables a high-level, declarative form of interaction: Users describe the intended outcome they wish an AI to produce, but do not actually create the outcome themselves. In contrast, in traditional user interfaces, users invoke specific operations to create the desired outcome. This paper revisits the basic input-output interaction cycle in light of this declarative style of interaction, and connects concepts in AI alignment to define three objectives for interactive alignment of AI: specification alignment (aligning on what to do), process alignment (aligning on how to do it), and evaluation alignment (assisting users in verifying and understanding what was produced). Using existing systems as examples, we show how these user-centered views of AI alignment can be used descriptively, prescriptively, and as an evaluative aid.																																	2024-10-03	PPRN:85976436		
J	Chen, Peng; Zhang, Yingying; Cheng, Yunyao; Shu, Yang; Wang, Yihang; Wen, Qingsong; Yang, Bin; Guo, Chenjuan				chen, peng/KMX-6316-2024; Wen, Qingsong/LTF-7625-2024; Cheng, Yunyao/LIG-1435-2024; Wang, Yihang/HJH-3237-2023						Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting								Arxiv											5	5;2024-09-15;https://www.arxiv.org/abs/2402.05956v5| 4;1800-01-01;https://www.arxiv.org/abs/2402.05956v4| 3;1800-01-01;https://www.arxiv.org/abs/2402.05956v3| 2;2024-02-20;https://www.arxiv.org/abs/2402.05956v2| 1;1800-01-01;https://www.arxiv.org/abs/2402.05956v1	arXiv:2402.05956			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 15 2024	2024	Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale Transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of Pathformer. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios. 																																	2024-12-24	PPRN:87603271		
J	Wang, Taige; Devakul, Trithep; Zaletel, Michael P.; Fu, Liang				Fu, Liang/LTC-7906-2024; Devakul, Trithep/L-1211-2015						Diverse magnetic orders and quantum anomalous Hall effect in twisted bilayer MoTe2 and WSe2								Arxiv											2	2;2024-09-11;https://www.arxiv.org/abs/2306.02501v3| 1;2023-06-04;https://www.arxiv.org/abs/2306.02501v1	arXiv:2306.02501			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 11 2024	2024	Twisted homobilayer transition metal dichalcogenide (TMD) offers a versatile platform for exploring band topology, interaction-driven phases, and magnetic orders. We study the interaction-driven phases in twisted TMD homobilayers and their low-energy collective excitations, focusing on the effect of band topology on magnetism and its thermal stability. From Hartree-Fock theory of the continuum model, we identify several magnetic and topological phases. By tuning the displacement field, we find two phase transitions involving a change in topology and magnetism respectively. We analyze the magnon spectrum, revealing the crucial role of band topology in stabilizing 2D ferromagnetism by amplifying easy-axis magnetic anisotropy, resulting in a large magnon gap of up to 7meV. As the magnon gap is directly tied to the stability of the magnetic phase to thermal fluctuations, our findings have several important experimental implications.																																	2024-09-27	PPRN:72854174		
J	Wang, Tianhao; Zhong, Xinyi; Fan, Zhou				Wang, Tianhao/KLY-4214-2024; Zhong, Xinyi/DLT-8847-2022						Universality of Approximate Message Passing algorithms and tensor networks								Arxiv											3	3;2024-09-08;https://www.arxiv.org/abs/2206.13037v5| 2;2024-06-12;https://www.arxiv.org/abs/2206.13037v4| 1;2022-06-27;https://www.arxiv.org/abs/2206.13037v3	arXiv:2206.13037			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 08 2024	2024	Approximate Message Passing (AMP) algorithms provide a valuable tool for studying mean-field approximations and dynamics in a variety of applications. Although these algorithms are often first derived for matrices having independent Gaussian entries or satisfying rotational invariance in law, their state evolution characterizations are expected to hold over larger universality classes of random matrix ensembles. We develop several new results on AMP universality. For AMP algorithms tailored to independent Gaussian entries, we show that their state evolutions hold over broadly defined generalized Wigner and white noise ensembles, including matrices with heavy-tailed entries and heterogeneous entrywise variances that may arise in data applications. For AMP algorithms tailored to rotational invariance in law, we show that their state evolutions hold over delocalized sign-and-permutation-invariant matrix ensembles that have a limit distribution over the diagonal, including sensing matrices composed of subsampled Hadamard or Fourier transforms and diagonal operators. We establish these results via a simplified moment-method proof, reducing AMP universality to the study of products of random matrices and diagonal tensors along a tensor network. As a by-product of our analyses, we show that the aforementioned matrix ensembles satisfy a notion of asymptotic freeness with respect to such tensor networks, which parallels usual definitions of freeness for traces of matrix products.																																	2024-09-24	PPRN:73300860		
J	Roshan, Rishav; White, Graham										Using gravitational waves to see the first second of the Universe								Arxiv											3	3;2024-09-06;https://www.arxiv.org/abs/2401.04388v3| 2;2024-08-23;https://www.arxiv.org/abs/2401.04388v2| 1;2024-01-09;https://www.arxiv.org/abs/2401.04388v1	arXiv:2401.04388			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 06 2024	2024	Gravitational waves are a unique probe of the early Universe, as the Universe is transparent to gravitational radiation right back to the end of inflation. In this article, we summarise detection prospects and the wide scope of primordial events that could lead to a detectable stochastic gravitational wave background. Any such background would shed light on what lies beyond the Standard Model, sometimes at remarkably high scales. We overview the range of strategies for detecting a stochastic gravitational wave background before delving deep into three major primordial events that can source such a background. Finally, we summarize the landscape of other sources of primordial backgrounds.																																	2024-09-26	PPRN:87078420		
J	Gao, Haoxiang; Wang, Zhongruo; Li, Yaqian; Long, Kaiwen; Yang, Ming; Shen, Yiqing										A Survey for Foundation Models in Autonomous Driving								Arxiv											4	4;2024-09-05;https://www.arxiv.org/abs/2402.01105v4| 3;2024-08-31;https://www.arxiv.org/abs/2402.01105v3| 2;2024-08-21;https://www.arxiv.org/abs/2402.01105v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01105v1	arXiv:2402.01105			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 05 2024	2024	The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.																																	2024-09-18	PPRN:87508310		
J	Wei, Julong; Yuan, Shanshuai; Li, Pengfei; Hu, Qingda; Gan, Zhongxue; Ding, Wenchao				Wei, Julong/NQS-2114-2025						OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving								Arxiv											1	1;2024-09-05;https://www.arxiv.org/abs/2409.03272v1	arXiv:2409.03272			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 05 2024	2024	The rise of multi-modal large language models(MLLMs) has spurred their applications in autonomous driving. Recent MLLM-based methods perform action by learning a direct mapping from perception to action, neglecting the dynamics of the world and the relations between action and world dynamics. In contrast, human beings possess world model that enables them to simulate the future states based on 3D internal visual representation and plan actions accordingly. To this end, we propose OccLLaMA, an occupancy-language-action generative world model, which uses semantic occupancy as a general visual representation and unifies vision-language-action(VLA) modalities through an autoregressive model. Specifically, we introduce a novel VQVAE-like scene tokenizer to efficiently discretize and reconstruct semantic occupancy scenes, considering its sparsity and classes imbalance. Then, we build a unified multi-modal vocabulary for vision, language and action. Furthermore, we enhance LLM, specifically LLaMA, to perform the next token/scene prediction on the unified vocabulary to complete multiple tasks in autonomous driving. Extensive experiments demonstrate that OccLLaMA achieves competitive performance across multiple tasks, including 4D occupancy forecasting, motion planning, and visual question answering, showcasing its potential as a foundation model in autonomous driving.																																	2024-09-18	PPRN:91751364		
J	Zhang, Jianguo; Lan, Tian; Zhu, Ming; Liu, Zuxin; Hoang, Thai; Kokane, Shirley; Yao, Weiran; Tan, Juntao; Prabhakar, Akshara; Chen, Haolin; Liu, Zhiwei; Feng, Yihao; Awalgaonkar, Tulika; Murthy, Rithesh; Hu, Eric; Chen, Zeyuan; Xu, Ran; Niebles, Juan Carlos; Heinecke, Shelby; Wang, Huan; Savarese, Silvio; Xiong, Caiming				Hu, Eric/AAM-5052-2021; Liu, Zhi-Wei/G-6187-2011; Niebles, Juan/AAT-5882-2021; Lan, Tian/JEP-4658-2023; Prabhakar, Akshara/LUA-0912-2024; Feng, Yihao/LVR-7524-2024; Liu, Zuxin/GQY-8303-2022						xLAM: A Family of Large Action Models to Empower AI Agent Systems								Arxiv											1	1;2024-09-05;https://www.arxiv.org/abs/2409.03215v1	arXiv:2409.03215			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 05 2024	2024	Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. We introduce and publicly release xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents' generalizability and performance across varied environments. Our experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, we aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks. 																																	2024-09-18	PPRN:91751294		
J	Mandi, Jayanta; Kotary, James; Berden, Senne; Mulamba, Maxime; Bucarey, Victor; Guns, Tias; Fioretto, Ferdinando				Mulamba Ke Tchomba, Maxime/GWN-1017-2022; Bucarey, Victor/E-1237-2019; Fioretto, Ferdinando/AAS-2096-2020; Mandi, Jayanta/KFQ-7360-2024						Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities								Arxiv											3	3;2024-09-04;https://www.arxiv.org/abs/2307.13565v4| 2;2024-05-23;https://www.arxiv.org/abs/2307.13565v3| 1;2023-07-25;https://www.arxiv.org/abs/2307.13565v1	arXiv:2307.13565			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 04 2024	2024	Decision-focused learning (DFL) is an emerging paradigm that integrates machine learning (ML) and constrained optimization to enhance decision quality by training ML models in an end-to-end system. This approach shows significant potential to revolutionize combinatorial decision-making in real-world applications that operate under uncertainty, where estimating unknown parameters within decision models is a major challenge. This paper presents a comprehensive review of DFL, providing an in-depth analysis of both gradient-based and gradient-free techniques used to combine ML and constrained optimization. It evaluates the strengths and limitations of these techniques and includes an extensive empirical evaluation of eleven methods across seven problems. The survey also offers insights into recent advancements and future research directions in DFL. 																																	2024-09-13	PPRN:74095479		
J	Gorbunov, Eduard; Danilova, Marina; Shibaev, Innokentiy; Dvurechensky, Pavel; Gasnikov, Alexander				Gorbunov, Eduard/U-1740-2019; Dvurechensky, Pavel/P-7295-2015; Gasnikov, Alexander/L-6371-2013						High Probability Complexity Bounds for Non-Smooth Stochastic Optimization with Heavy-Tailed Noise								Arxiv											2	2;2024-08-30;https://www.arxiv.org/abs/2106.05958v3| 1;2021-06-10;https://www.arxiv.org/abs/2106.05958v2	arXiv:2106.05958			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 30 2024	2024	Stochastic first-order methods are standard for training large-scale machine learning models. Random behavior may cause a particular run of an algorithm to result in a highly suboptimal objective value, whereas theoretical guarantees are usually proved for the expectation of the objective value. Thus, it is essential to theoretically guarantee that algorithms provide small objective residual with high probability. Existing methods for non-smooth stochastic convex optimization have complexity bounds with the dependence on the confidence level that is either negative-power or logarithmic but under an additional assumption of sub-Gaussian (light-tailed) noise distribution that may not hold in practice. In our paper, we resolve this issue and derive the first high-probability convergence results with logarithmic dependence on the confidence level for non-smooth convex stochastic optimization problems with non-sub-Gaussian (heavy-tailed) noise. To derive our results, we propose novel stepsize rules for two stochastic methods with gradient clipping. Moreover, our analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, we provide an extension for strongly convex problems. Finally, our results imply that the first (accelerated) method we consider also has optimal iteration and oracle complexity in all the regimes, and the second one is optimal in the non-smooth setting.																																	2024-09-07	PPRN:12200072		
J	Pang, Ye-Huang; Zhang, Xue; Huang, Qing-Guo				Zhang, Xue/KAM-3275-2024						Constraints on Redshift-Binned Dark Energy using DESI BAO Data								Arxiv											1	1;2024-08-27;https://www.arxiv.org/abs/2408.14787v1	arXiv:2408.14787			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 27 2024	2024	We parameterize the equation of state of late-time dark energy as w bin (z), with three redshift bins, characterized by a constant equation of state in each bin. Then, we constrain the parameters of the w bin CDM model using datasets from DESI BAO data, Planck CMB power spectrum, ACT DR6 lensing power spectrum, and type Ia supernova distance-redshift data of Pantheon Plus/DES Y5/Union3. The significances for w1 > −1 is 1.9σ, 2.6σ and 3.3σ, and w2 is consistent with −1 within 1σ level, while 1.6σ, 1.5σ and 1.5σ for w 3 < −1 in these three data combinations with different choices of type Ia supernova datasets, respectively. Additionally, to alleviate H 0 tension, we incorporate the early dark energy (EDE) model in the early-time universe and add the SH0ES absolute magnitude Mb prior (or H0 prior) to further constrain the w bin EDE model. In the w bin EDE model, we find a wbin (z) pattern similar to that in the w bin CDM model. The results of the three data combinations exhibit w 1 > −1 at 1.9σ, 1.7σ and 2.9σ level, meanwhile w3 < −1 at 1.3σ, 1.3σ and 1.3σ level, respectively. In all, our results indicate that the transition of dark energy from phantom at high redshifts to quintessence at low redshifts is not conclusive.																																	2024-09-06	PPRN:91564286		
J	Tang, Xiangru; Liu, Yuliang; Cai, Zefan; Shao, Yanjun; Lu, Junjie; Zhang, Yichi; Deng, Zexuan; Hu, Helan; An, Kaikai; Huang, Ruijun; Si, Shuzheng; Chen, Sheng; Zhao, Haozhe; Chen, Liang; Wang, Yan; Liu, Tianyu; Jiang, Zhiwei; Chang, Baobao; Fang, Yin; Qin, Yujia; Zhou, Wangchunshu; Zhao, Yilun; Cohan, Arman; Gerstein, Mark				ZHAO, YILUN/KHX-1731-2024; Liu, Yuliang/JAN-8179-2023; Jiang, Zhiwei/JLR-6369-2023; Shao, Yanjun/GQH-2952-2022; Gerstein, Mark/HSC-3904-2023; Liu, Tianyu/HJZ-0979-2023						ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code								Arxiv											4	4;2024-08-21;https://www.arxiv.org/abs/2311.09835v5| 3;2024-06-12;https://www.arxiv.org/abs/2311.09835v3| 2;2024-04-17;https://www.arxiv.org/abs/2311.09835v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09835v1	arXiv:2311.09835			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 21 2024	2024	Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution.																																	2024-08-31	PPRN:86177245		
J	Zhong, Chengzhi; Cheng, Fei; Liu, Qianying; Jiang, Junfeng; Wan, Zhen; Chu, Chenhui; Murawaki, Yugo; Kurohashi, Sadao										Beyond English-Centric LLMs: What Language Do Multilingual Language Models Think in?								Arxiv											1	1;2024-08-20;https://www.arxiv.org/abs/2408.10811v1	arXiv:2408.10811			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 20 2024	2024	In this study, we investigate whether nonEnglish-centric LLMs , despite their strong performance, ‘think’ in their respective dominant language: more precisely, ‘think’ refers to how the representations of intermediate layers, when un-embedded into the vocabulary space, exhibit higher probabilities for certain dominant languages during generation. We term such languages as internal latent languages . We examine the latent language of three typical categories of models for Japanese processing: Llama2, an English-centric model; Swallow, an English-centric model with continued pretraining in Japanese; and LLM-jp, a model pretrained on balanced English and Japanese corpora. Our empirical findings reveal that, unlike Llama2 which relies exclusively on English as the internal latent language, Japanese-specific Swallow and LLM-jp employ both Japanese and English, exhibiting dual internal latent languages. For any given target language, the model preferentially activates the latent language most closely related to it. In addition, we explore how intermediate layers respond to questions involving cultural conflicts between latent internal and target output languages. We further explore how the language identity shifts across layers while keeping consistent semantic meaning reflected in the intermediate layer representations. This study deepens the understanding of non-English-centric large language models, highlighting the intricate dynamics of language representation within their intermediate layers.																																	2024-08-30	PPRN:91499228		
J	Pisano, Matthew; Ly, Peter; Sanders, Abraham; Yao, Bingsheng; Wang, Dakuo; Strzalkowski, Tomek; Si, Mei										Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework								Arxiv											3	3;2024-08-18;https://www.arxiv.org/abs/2312.00029v3| 2;2024-03-15;https://www.arxiv.org/abs/2312.00029v2| 1;2023-11-16;https://www.arxiv.org/abs/2312.00029v1	arXiv:2312.00029			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 18 2024	2024	Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. Such vulnerabilities can lead to LLMs being manipulated into generating hazardous content: from instructions for creating dangerous materials to inciting violence or endorsing unethical behaviors. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM acting as a guardian to the primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis reviews that by using Bergeron to complement models with existing alignment training, we can significantly improve the robustness and safety of multiple, commonly used commercial and open-source LLMs. Specifically, we found that models integrated with Bergeron are, on average, nearly seven times more resistant to attacks compared to models without such support.																																	2024-08-30	PPRN:86359485		
J	Luo, Zhihui; Lv, Biao; Wang, Meng; Wu, Wei; Yao, Dao-Xin				luo, zhihui/HTQ-8904-2023; Wang, Meng/C-4888-2013						High-T<italic>C</italic>&nbsp;superconductivity in&nbsp;La3Ni2O7&nbsp;based on the bilayer two-orbital t-J model								Arxiv											4	4;2024-08-15;https://www.arxiv.org/abs/2308.16564v4| 3;2024-08-05;https://www.arxiv.org/abs/2308.16564v3| 2;2023-12-12;https://www.arxiv.org/abs/2308.16564v2| 1;2023-08-31;https://www.arxiv.org/abs/2308.16564v1	arXiv:2308.16564			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Aug 15 2024	2024	The recently discovered high-TC superconductor La3Ni2O7 has sparked renewed interest in the unconventional superconductivity. Here we study superconductivity in pressurized La3Ni2O7 based on a bilayer two-orbital t−J model, using the renormalized mean-field theory. Our results reveal a robust s±−wave pairing driven by the inter-layer dz2 magnetic coupling, which exhibits a transition temperature within the same order of magnitude as the experimentally observed Tc∼80 K. We establish a comprehensive superconducting phase diagram in the doping plane. Notably, the La3Ni2O7 under pressure is found situated roughly in the optimal doping regime of the phase diagram. When the dx2−y2 orbital becomes close to half-filling, d−wave and d + is pairing can emerge from the system. We discuss the interplay between Fermi surface topology and different pairing symmetries. The stability of the s±−wave pairing against Hund's coupling and other magnetic exchange couplings is discussed.																																	2024-08-23	PPRN:84623425		
J	Wang, Chengrui; Long, Qingqing; Xiao, Meng; Cai, Xunxin; Wu, Chengjun; Meng, Zhen; Wang, Xuezhi; Zhou, Yuanchun				liu, qingqing/HHD-0360-2022; Xiao, Meng/KCL-9504-2024						BioRAG: A RAG-LLM Framework for Biological Question Reasoning								Arxiv											2	2;2024-08-14;https://www.arxiv.org/abs/2408.01107v2| 1;2024-08-02;https://www.arxiv.org/abs/2408.01107v1	arXiv:2408.01107			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 14 2024	2024	The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BioRAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BioRAG deconstructs the question and employs an iterative retrieval process incorporated with the search engine for step-by-step reasoning. Rigorous experiments have demonstrated that our model outperforms fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks across multiple life science question-answering tasks.																																	2024-08-22	PPRN:91225322		
J	Chen, Hongzhan; Chen, Hehong; Yan, Ming; Xu, Wenshen; Gao, Xing; Shen, Weizhou; Quan, Xiaojun; Li, Chenliang; Zhang, Ji; Huang, Fei; Zhou, Jingren				Chen, Hui/AAE-6559-2019; Yan, Ming/LDT-2692-2024; Zhou, Mingyuan/AAE-8717-2021; Chen, Chih-Ming/I-2464-2015						SocialBench: Sociality Evaluation of Role-Playing Conversational Agents								Arxiv											3	3;2024-08-05;https://www.arxiv.org/abs/2403.13679v4| 2;2024-03-22;https://www.arxiv.org/abs/2403.13679v3| 1;2024-03-21;https://www.arxiv.org/abs/2403.13679v2	arXiv:2403.13679			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 05 2024	2024	Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce SocialBench, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group. Experimental results on SocialBench confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents. 																																	2024-08-11	PPRN:88257929		
J	Meng, Fanqing; Wang, Jin; Li, Chuanhao; Lu, Quanfeng; Tian, Hao; Liao, Jiaqi; Zhu, Xizhou; Dai, Jifeng; Qiao, Yu; Luo, Ping; Zhang, Kaipeng; Shao, Wenqi				Dai, Jifeng/HGU-8741-2022; pluo/GPG-2707-2022; Meng, fanqing/AAE-7775-2022; TIAN, Hao/GMU-6101-2022; Qiao, Yu/ABD-5787-2021						MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models								Arxiv											1	1;2024-08-05;https://www.arxiv.org/abs/2408.02718v1	arXiv:2408.02718			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 05 2024	2024	The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through multi-faceted analytical experiments, we identify key performance gaps and limitations, providing valuable insights for future model and data improvements. We aim for MMIU to advance the frontier of LVLM research and development, moving us toward achieving sophisticated multimodal multi-image user interactions.																																	2024-08-13	PPRN:91260079		
J	Zhang, Zeyu; Liu, Akide; Reid, Ian; Hartley, Richard; Zhuang, Bohan; Tang, Hao				Tang, Hao/LYP-1258-2024						Motion Mamba: Efficient and Long Sequence Motion Generation								Arxiv											3	3;2024-08-03;https://www.arxiv.org/abs/2403.07487v4| 2;2024-03-19;https://www.arxiv.org/abs/2403.07487v3| 1;2024-03-12;https://www.arxiv.org/abs/2403.07487v1	arXiv:2403.07487			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 03 2024	2024	Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation. 																																	2024-08-11	PPRN:88112832		
J	Wang, Jiaqi; Jiang, Hanqi; Liu, Yiheng; Ma, Chong; Zhang, Xu; Pan, Yi; Liu, Mengyuan; Gu, Peiran; Xia, Sichen; Li, Wenjun; Zhang, Yutong; Wu, Zihao; Liu, Zhengliang; Zhong, Tianyang; Ge, Bao; Zhang, Tuo; Qiang, Ning; Hu, Xintao; Jiang, Xi; Zhang, Xin; Zhang, Wei; Shen, Dinggang; Liu, Tianming; Zhang, Shu				Ma, Chong/MIT-9373-2025; yuan, yixuan/KLZ-6092-2024; Zhang, Tuo/NHP-8722-2025; Liu, Mengyuan/IUO-6172-2023; wu, zihao/R-8745-2019; zhang, yutong/GXV-2287-2022; wang, jiaqi/HHS-0123-2022; Liu, Tianming/GLS-1211-2022						A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks								Arxiv											1	1;2024-08-02;https://www.arxiv.org/abs/2408.01319v1	arXiv:2408.01319			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 02 2024	2024	In an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types—including text, images, videos, audio, and physiological sequences—MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.																																	2024-08-08	PPRN:91230090		
J	Liao, Qiayuan; Zhang, Bike; Huang, Xuanyu; Huang, Xiaoyu; Li, Zhongyu; Sreenath, Koushil				Huang, Xiaoyu/KGK-9842-2024						Berkeley Humanoid: A Research Platform for Learning-based Control								Arxiv											2	2;2024-07-31;https://www.arxiv.org/abs/2407.21781v1| 1;2024-07-31;https://www.arxiv.org/abs/2407.21781v1	arXiv:2407.21781			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 31 2024	2024	We introduce Berkeley Humanoid, a reliable and low-cost mid-scale humanoid research platform for learning-based control. Our lightweight, inhouse-built robot is designed specifically for learning algorithms with low simulation complexity, anthropomorphic motion, and high reliability against falls. The robot’s narrow sim-to-real gap enables agile and robust locomotion across various terrains in outdoor environments, achieved with a simple reinforcement learning controller using light domain randomization. Furthermore, we demonstrate the robot traversing for hundreds of meters, walking on a steep unpaved trail, and hopping with single and double legs as a testimony to its high performance in dynamical walking. Capable of omnidirectional locomotion and withstanding large perturbations with a compact setup, our system aims for scalable, sim-to-real deployment of learning-based humanoid systems. Please check our website for more details.																																	2024-12-16	PPRN:91239802		
J	Liu, Shi; Zheng, Kecheng; Chen, Wei										Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs								Arxiv											1	1;2024-07-31;https://www.arxiv.org/abs/2407.21771v1	arXiv:2407.21771			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 31 2024	2024	Existing Large Vision-Language Models (LVLMs) primarily align image features of vision encoder with Large Language Models (LLMs) to leverage their superior text generation capabilities. However, the scale disparity between vision encoder and language model may led to LLMs assuming a predominant role in multi-modal comprehension. This imbalance in LVLMs may result in the instances of hallucinatory. Concretely, LVLMs may generate consistent descriptions with or without visual input, indicating that certain outputs are influenced solely by context text. We refer to this phenomenon as "text inertia." To counteract this issue, we introduce a training-free algorithm to find an equilibrium point between image comprehension and language inference. Specifically, we adaptively involve adjusting and amplifying the attention weights assigned to image tokens, thereby granting greater prominence to visual elements. Meanwhile, we subtract the logits of multi-modal inputs from ones of pure text input, which can help LVLMs be not biased towards LLMs. By enhancing images tokens and reducing the stubborn output of LLM, we can let LVLM pay more attention to images, towards alleviating text inertia and reducing the hallucination in LVLMs. Our extensive experiments shows that this method substantially reduces the frequency of hallucinatory outputs in various LVLMs in terms of different metrics. 																																	2024-08-08	PPRN:91187572		
J	Abuelsaad, Tamer; Akkil, Deepak; Dey, Prasenjit; Jagmohan, Ashish; Vempaty, Aditya; Kokku, Ravi										Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems								Arxiv											1	1;2024-07-17;https://www.arxiv.org/abs/2407.13032v1	arXiv:2407.13032			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 17 2024	2024	AI Agents are changing the way work gets done, both in consumer and enterprise domains. However, the design patterns and architectures to build highly capable agents or multi-agent systems are still developing, and the understanding of the implication of various design choices and algorithms is still evolving. In this paper, we present our work on building a novel web agent, Agent-E 1 . Agent-E introduces numerous architectural improvements over prior state-of-the-art web agents such as hierarchical architecture, flexible DOM distillation and denoising method, and the concept of change observation to guide the agent towards more accurate performance. We first present the results of an evaluation of Agent-E on WebVoyager benchmark dataset and show that Agent-E beats other SOTA text and multi-mo dal web agents on this benchmark in most categories by 10-30%. We then synthesize our learnings from the development of Agent-E into general design principles for developing agentic systems. These include the use of domain-specific primitive skills, the importance of distillation and denoising of environmental observations, the advantages of a hierarchical architecture, and the role of agentic self-improvement to enhance agent efficiency and efficacy as the agent gathers experience.																																	2024-07-26	PPRN:90881216		
J	Haldar, Siddhant; Peng, Zhuoran; Pinto, Lerrel										BAKU: An Efficient Transformer for Multi-Task Policy Learning								Arxiv											2	2;2024-07-16;https://www.arxiv.org/abs/2406.07539v2| 1;2024-06-11;https://www.arxiv.org/abs/2406.07539v1	arXiv:2406.07539			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 16 2024	2024	Training generalist agents capable of solving diverse tasks is challenging, often requiring large datasets of expert demonstrations. This is particularly problematic in robotics, where each data point requires physical execution of actions in the real world. Thus, there is a pressing need for architectures that can effectively leverage the available training data. In this work, we present B AKU , a simple transformer architecture that enables efficient learning of multi-task robot policies. B AKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work. Our experiments on 129 simulated tasks across LIBERO, Meta-World suite, and the Deepmind Control suite exhibit an overall 18% absolute improvement over RT-1 and MT-ACT, with a 36% improvement on the harder LIBERO benchmark. On 30 real-world manipulation tasks, given an average of just 17 demonstrations per task, B AKU achieves a 91% success rate. Videos of the robot are best viewed at baku-robot.github.io.																																	2024-07-25	PPRN:89283826		
J	Liu, Wentao; Wu, Di; Wang, Jieci				wu, di/IYS-9217-2023; Wang, Jieci/A-8715-2018						Shadow of slowly rotating Kalb-Ramond black holes								Arxiv											2	2;2024-07-14;https://www.arxiv.org/abs/2407.07416v2| 1;2024-07-10;https://www.arxiv.org/abs/2407.07416v1	arXiv:2407.07416			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 14 2024	2024	Real astronomical objects possess spin, yet deriving exact solutions for rotating black holes within gravitational theories is a formidable challenge. To understand the shadow of rotating black holes in Lorentz-violating spacetimes induced by antisymmetric tensor fields, known as Kalb-Ramond (KR) fields, we have focused on the slow-rotation approximation framework. Using this approach, we have obtained first-order rotation series solutions, which describe slowly rotating KR black holes. For this solutions, we have plotted the black hole shadow contours under various parameters using the numerical backward ray-tracing method. As the Lorentzviolating parameter increases, not only the apparent size of the black hole shadow decreases, but also the effects of rotation, such as the D-shaped structure and frame-dragging, are amplified. Furthermore, the KR field also enhances gravitational lensing, causing the shadow to occupy a larger area within the photon ring. This distinctive feature can di fferentiate KR gravity from general relativity. Additionally, using the latest observational data from EHT on M87* and Sgr A*, we have provided constraints on the Lorentz-violating parameter of rotating KR black holes. We found that, compared to static black holes, rotating black holes allow for the presence of stronger Lorentz violation effects.																																	2024-07-23	PPRN:90760349		
J	Yin, Mingjia; Wu, Chuhan; Wang, Yufei; Wang, Hao; Guo, Wei; Wang, Yasheng; Liu, Yong; Tang, Ruiming; Lian, Defu; Chen, Enhong				Guo, Wei/OYD-7292-2025; Zheng, Yefeng/ABG-7053-2020; Lian, Defu/AFN-4573-2022						Entropy Law: The Story Behind Data Compression and LLM Performance								Arxiv											1	1;2024-07-11;https://www.arxiv.org/abs/2407.06645v3	arXiv:2407.06645			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 11 2024	2024	Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim to uncover the underlying relationships between LLM performance and data selection. Inspired by the information compression nature of LLMs, we uncover an ``entropy law'' that connects LLM performance with data compression ratio and first-epoch training loss, which reflect the information redundancy of a dataset and the mastery of inherent knowledge encoded in this dataset, respectively. Through both theoretical deduction and empirical evaluation, we find that model performance is negatively correlated to the compression ratio of training data, which usually yields a lower training loss. Based on the findings of the entropy law, we propose a quite efficient and universal data selection method named textbf{ZIP} for training LLMs, which aim to prioritize data subsets exhibiting a low compression ratio. Based on a multi-stage algorithm that selects diverse data in a greedy manner, we can obtain a good data subset with satisfactory diversity. Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages. We also present an interesting application of entropy law that can detect potential performance risks at the beginning of model training.																																	2024-07-22	PPRN:90769809		
J	Green, Daniel; Meyers, Joel				Meyers, Joel/OCL-3371-2025						The Cosmological Preference for Negative Neutrino Mass								Arxiv											1	1;2024-07-10;https://www.arxiv.org/abs/2407.07878v1	arXiv:2407.07878			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 10 2024	2024	The most precise determination of the sum of neutrino masses from cosmological data, derived from analysis of the cosmic microwave background (CMB) and baryon acoustic acoustic oscillations (BAO) from the Dark Energy Spectroscopic Instrument (DESI), favors a value below the minimum inferred from neutrino flavor oscillation experiments. We explore which data is most responsible of this puzzling aspect of the current constraints on neutrino mass and whether it is related to other anomalies in cosmology. We demonstrate conclusively that the preference for negative neutrino masses is a consequence of larger than expected lensing of the CMB in both the two- and four-point lensing statistics. Furthermore, we show that this preference is robust to changes in likelihoods of the BAO and CMB optical depth analyses given the available data. We then show that this excess clustering is not easily explained by changes to the expansion history and is likely distinct from the preference for for dynamical dark energy in DESI BAO data. Finally, we discuss how future data may impact these results, including an analysis of Planck CMB with mock DESI 5-year data. We conclude that the negative neutrino mass preference is likely to persist even as more cosmological data is collected in the near future.																																	2024-07-21	PPRN:90760339		
J	Cai, Yong; Zhu, Mian; Piao, Yun-Song				Cai, Yong/LTD-4750-2024; Zhu, Mian/HMP-6812-2023						Primordial black holes from null energy condition violation during inflation								Arxiv											2	2;2024-07-09;https://www.arxiv.org/abs/2305.10933v2| 1;2023-05-18;https://www.arxiv.org/abs/2305.10933v1	arXiv:2305.10933			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 09 2024	2024	Primordial black holes (PBHs) and the violation of the null energy condition (NEC) have significant implications for our understanding of the very early universe. We present a novel approach to generate PBHs via the NEC violation in a single-field inflationary scenario. In our scenario, the universe transitions from a first slow-roll inflation stage with a Hubble parameter H=Hinf1to a second slow-roll inflation stage with H=Hinf2≫Hinf1, passing through an intermediate stage of NEC violation. The NEC violation naturally enhances the primordial scalar power spectrum at a certain wavelength, leading to the production of PBHs with masses and abundances of observational interest. We also investigate the phenomenological signatures of scalar-induced gravitational waves(SIGWs) resulting from the enhanced density perturbations. Our work highlights the potential of utilizing a combination of PBHs, SIGWs, and primordial gravitational waves as a valuable probe for studying NEC violation during inflation, opening up new avenues for exploring the early universe.																																	2024-07-21	PPRN:70533969		
J	Dou, Shihan; Jia, Haoxiang; Wu, Shenxi; Zheng, Huiyuan; Zhou, Weikang; Wu, Muling; Chai, Mingxu; Fan, Jessica; Huang, Caishuang; Tao, Yunbo; Liu, Yan; Zhou, Enyu; Zhang, Ming; Zhou, Yuhao; Wu, Yueming; Zheng, Rui; Wen, Ming; Weng, Rongxiang; Wang, Jingang; Cai, Xunliang; Gui, Tao; Qiu, Xipeng; Zhang, Qi; Huang, Xuanjing				Wen, Ming/AEJ-8029-2022; Tao, Yunbo/NGS-4718-2025; Jia, Haoxiang/OEO-2861-2025; Gui, Tao/LWI-6783-2024; Wu, Yueming/KFS-2587-2024; Zhou, Yuhao/ABC-4280-2022						What's Wrong with Your Code Generated by Large Language Models? An Extensive Study								Arxiv											1	1;2024-07-08;https://www.arxiv.org/abs/2407.06153v1	arXiv:2407.06153			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 08 2024	2024	The increasing development of large language models (LLMs) in code generation has drawn significant attention among researchers. To enhance LLM-based code generation ability, current efforts are predominantly directed towards collecting high-quality datasets and leveraging diverse training technologies. However, there is a notable lack of comprehensive studies examining the limitations and boundaries of these existing methods. To bridge this gap, we conducted an extensive empirical study evaluating the performance of three leading closed-source LLMs and four popular open-source LLMs on three commonly used benchmarks. Our investigation, which evaluated the length, cyclomatic complexity and API number of the generated code, revealed that these LLMs face challenges in generating successful code for more complex problems, and tend to produce code that is shorter yet more complicated as compared to canonical solutions. Additionally, we developed a taxonomy of bugs for incorrect codes that includes three categories and 12 sub-categories, and analyze the root cause for common bug types. Furthermore, to better understand the performance of LLMs in real-world projects, we manually created a real-world benchmark comprising 140 code generation tasks. Our analysis highlights distinct differences in bug distributions between actual scenarios and existing benchmarks. Finally, we propose a novel training-free iterative method that introduces self-critique, enabling LLMs to critique and correct their generated code based on bug types and compiler feedback. Experimental results demonstrate that our approach can significantly mitigate bugs and increase the passing rate by 29.2% after two iterations, indicating substantial potential for LLMs to handle more complex problems.																																	2024-07-21	PPRN:90741290		
J	Lu, Weikai; Zeng, Ziqian; Wang, Jianwei; Lu, Zhengdong; Chen, Zelin; Zhuang, Huiping; Chen, Cen				Zeng, Ziqian/GXM-5147-2022						Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2404.05880v2| 1;2024-04-08;https://www.arxiv.org/abs/2404.05880v1	arXiv:2404.05880			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 03 2024	2024	Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within the model, leading to potential jailbreak risks for LLMs. In this paper, we propose a novel defense method called Eraser, which mainly includes three goals: unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment. The intuition is that if an LLM forgets the specific knowledge required to answer a harmful question, it will no longer have the ability to answer harmful questions. The training of Erase does not actually require the model's own harmful knowledge, and it can benefit from unlearning general answers related to harmful queries, which means it does not need assistance from the red team. The experimental results show that Eraser can significantly reduce the jailbreaking success rate for various attacks without compromising the general capabilities of the model. 																																	2024-07-19	PPRN:88466904		
J	Bensadoun, Raphael; Kleiman, Yanir; Azuri, Idan; Harosh, Omri; Vedaldi, Andrea; Neverova, Natalia; Gafni, Oran				Неверова, Наталья/A-8316-2014						Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects								Arxiv											1	1;2024-07-02;https://www.arxiv.org/abs/2407.02430v1	arXiv:2407.02430			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 02 2024	2024	The recent availability and adaptability of text-to-image models has sparked a new era in many related domains that benefit from the learned text priors as well as high-quality and fast generation capabilities, one of which is texture generation for 3D objects. Although recent texture generation methods achieve impressive results by using text-to-image networks, the combination of global consistency, quality, and speed, which is crucial for advancing texture generation to real-world applications, remains elusive. To that end, we introduce Meta 3D TextureGen: a new feedforward method comprised of two sequential networks aimed at generating high-quality and globally consistent textures for arbitrary geometries of any complexity degree in less than 20 seconds. Our method achieves state-of-the-art results in quality and speed by conditioning a text-to-image model on 3D semantics in 2D space and fusing them into a complete and high-resolution UV texture map, as demonstrated by extensive qualitative and quantitative evaluations. In addition, we introduce a texture enhancement network that is capable of up-scaling any texture by an arbitrary ratio, producing 4k pixel resolution textures.																																	2024-07-20	PPRN:90675472		
J	Zu, Lei; Zhang, Chi; Li, Yao-Yu; Gu, Yuchao; Tsai, Yue-Lin Sming; Fan, Yi-Zhong				Li, Yaoyu/GQH-2658-2022; Zhang, Binbin/AAA-4138-2019						Mirror QCD phase transition as the origin of the nanohertz Stochastic Gravitational-Wave Background								Arxiv											3	3;2024-07-02;https://www.arxiv.org/abs/2306.16769v4| 2;2024-02-02;https://www.arxiv.org/abs/2306.16769v3| 1;2023-07-31;https://www.arxiv.org/abs/2306.16769v2	arXiv:2306.16769			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 02 2024	2024	Several Pulsar Timing Array (PTA) collaborations have recently provided strong evidence for a nHz Stochastic Gravitational-Wave Background (SGWB). Here we investigate the implications of a first-order phase transition occurring within the early universe's dark quantum chromodynamics (dQCD) epoch, specifically within the framework of the mirror twin Higgs dark sector model. Our analysis indicates a distinguishable SGWB signal originating from this phase transition, which can explain the measurements obtained by PTAs. Remarkably, a significant portion of the parameter space for the SGWB signal also effectively resolves the existing tensions in both the H0 and S8 measurements in Cosmology. This intriguing correlation suggests a possible common origin of these three phenomena for 0.2<ΔNeff<0.5, where the mirror dark matter component constitutes about 30% of the total dark matter abundance. Next generation CMB experiment such as CMB-S4 is able to test this parameter region.																																	2024-07-19	PPRN:74181358		
J	Wang, Xiaohua; Wang, Zhenghua; Gao, Xuan; Zhang, Feiran; Wu, Yixin; Xu, Zhibo; Shi, Tianyuan; Wang, Zhengyuan; Li, Shizheng; Qian, Qi; Yin, Ruicheng; Lv, Changze; Zheng, Xiaoqing; Huang, Xuanjing				wang, zhengyuan/NTQ-1035-2025; Wu, Yixin/GVT-1788-2022						Searching for Best Practices in Retrieval-Augmented Generation								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.01219v1	arXiv:2407.01219			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a "retrieval as generation" strategy.																																	2024-07-18	PPRN:90652977		
J	Wu, Siyuan; Huang, Yue; Gao, Chujie; Chen, Dongping; Zhang, Qihui; Wan, Yao; Zhou, Tianyi; Zhang, Xiangliang; Gao, Jianfeng; Xiao, Chaowei; Sun, Lichao				ZHANG, QIHUI/OAJ-1060-2025; Chen, Dongping/LSL-8606-2024; Xiao, Chaowei/AAT-8772-2021; Zhang, Xiangliang/GXF-6961-2022; Gao, Jianfeng/AAP-8200-2021						UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models								Arxiv											2	2;2024-06-28;https://www.arxiv.org/abs/2406.18966v2| 1;2024-06-27;https://www.arxiv.org/abs/2406.18966v1	arXiv:2406.18966			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 28 2024	2024	Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents UniGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. UniGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, UniGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by UniGen, and each module within UniGen plays a critical role in this enhancement. Additionally, UniGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that UniGen effectively supports dynamic and evolving benchmarking, and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.																																	2024-07-17	PPRN:90145367		
J	Cummins, Chris; Seeker, Volker; Grubisic, Dejan; Roziere, Baptiste; Gehring, Jonas; Synnaeve, Gabriel; Leather, Hugh										Meta Large Language Model Compiler: Foundation Models of Compiler Optimization								Arxiv											1	1;2024-06-27;https://www.arxiv.org/abs/2407.02524v1	arXiv:2407.02524			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 27 2024	2024	Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.																																	2024-12-06	PPRN:90685071		
J	Shumailov, Ilia; Hayes, Jamie; Triantafillou, Eleni; Ortiz-Jimenez, Guillermo; Papernot, Nicolas; Jagielski, Matthew; Yona, Itay; Howard, Heidi; Bagdasaryan, Eugene										UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI								Arxiv											1	1;2024-06-27;https://www.arxiv.org/abs/2407.00106v1	arXiv:2407.00106			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 27 2024	2024	Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. In this paper we revisit the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in -context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. We introduce a concept of un unlearning , where unlearned knowledge gets reintroduced in -context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, we argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. We discuss feasibility of un unlearning for modern LLMs and examine broader implications.																																	2024-07-18	PPRN:90649278		
J	Lou, Chao; Jia, Zixia; Zheng, Zilong; Tu, Kewei										Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers								Arxiv											1	1;2024-06-24;https://www.arxiv.org/abs/2406.16747v1	arXiv:2406.16747			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 24 2024	2024	Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.																																	2024-07-12	PPRN:89407114		
J	Qiu, Haonan; Chen, Zhaoxi; Wang, Zhouxia; He, Yingqing; Xia, Menghan; Liu, Ziwei				Liu, Ziwei/AAG-6939-2021						FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models								Arxiv											1	1;2024-06-24;https://www.arxiv.org/abs/2406.16863v1	arXiv:2406.16863			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 24 2024	2024	Diffusion model has demonstrated remarkable capability in video generation, which further sparks interest in introducing trajectory control into the generation process. While existing works mainly focus on training-based methods (e.g., conditional adapter), we argue that diffusion model itself allows decent control over the generated content without requiring any training. In this study, we introduce a tuning-free framework to achieve trajectory-controllable video generation, by imposing guidance on both noise construction and attention computation. Specifically, 1) we first show several instructive phenomenons and analyze how initial noises influence the motion trajectory of generated content. 2) Subsequently, we propose FreeTraj, a tuning-free approach that enables trajectory control by modifying noise sampling and attention mechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger video generation with controllable trajectories. Equipped with these designs, users have the flexibility to provide trajectories manually or opt for trajectories automatically generated by the LLM trajectory planner. Extensive experiments validate the efficacy of our approach in enhancing the trajectory controllability of video diffusion models.																																	2024-07-15	PPRN:89416799		
J	Jin, Can; Peng, Hongwu; Zhao, Shiyu; Wang, Zhenting; Xu, Wujiang; Han, Ligong; Zhao, Jiahui; Zhong, Kai; Rajasekaran, Sanguthevar; Metaxas, Dimitris N.				Zhao, Jiahui/JBJ-5175-2023; Xu, Wujiang/KTI-1908-2024; Han, Ligong/HHD-2227-2022						APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking								Arxiv											1	1;2024-06-20;https://www.arxiv.org/abs/2406.14449v1	arXiv:2406.14449			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 20 2024	2024	Large Language Models (LLMs) have significantly enhanced Information Retrieval (IR) across various modules, such as reranking. Despite impressive performance, current zero-shot relevance ranking with LLMs heavily relies on human prompt engineering. Existing automatic prompt engineering algorithms primarily focus on language modeling and classification tasks, leaving the domain of IR, particularly reranking, underexplored. Directly applying current prompt engineering algorithms to relevance ranking is challenging due to the integration of query and long passage pairs in the input, where the ranking complexity surpasses classification tasks. To reduce human effort and unlock the potential of prompt optimization in reranking, we introduce a novel automatic prompt engineering algorithm named APEER. APEER iteratively generates refined prompts through feedback and preference optimization. Extensive experiments with four LLMs and ten datasets demonstrate the substantial performance improvement of APEER over existing state-of-the-art (SoTA) manual prompts. Furthermore, we find that the prompts generated by APEER exhibit better transferability across diverse tasks and LLMs. 																																	2024-07-06	PPRN:89375945		
J	Shuai, Xincheng; Ding, Henghui; Ma, Xingjun; Tu, Rongcheng; Jiang, Yu-Gang; Tao, Dacheng				Ma, Xingjun/W-4916-2019; Ding, Henghui/C-7486-2019; Tao, Dacheng/A-5449-2012						A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models								Arxiv											1	1;2024-06-20;https://www.arxiv.org/abs/2406.14555v1	arXiv:2406.14555			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	Image editing aims to edit the given synthetic or real image to meet the specific requirements from users. It is widely studied in recent years as a promising and challenging field of Artificial Intelligence Generative Content (AIGC). Recent significant advancement in this field is based on the development of text-to-image (T2I) diffusion models, which generate images according to text prompts. These models demonstrate remarkable generative capabilities and have become widely used tools for image editing. T2I-based image editing methods significantly enhance editing performance and offer a user-friendly interface for modifying content guided by multimodal inputs. In this survey, we provide a comprehensive review of multimodal-guided image editing techniques that leverage T2I diffusion models. First, we define the scope of image editing from a holistic perspective and detail various control signals and editing scenarios. We then propose a unified framework to formalize the editing process, categorizing it into two primary algorithm families. This framework offers a design space for users to achieve specific goals. Subsequently, we present an in-depth analysis of each component within this framework, examining the characteristics and applicable scenarios of different combinations. Given that training-based methods learn to directly map the source image to target one under user guidance, we discuss them separately, and introduce injection schemes of source image in different scenarios. Additionally, we review the application of 2D techniques to video editing, highlighting solutions for inter-frame inconsistency. Finally, we discuss open challenges in the field and suggest potential future research directions. 																																	2024-07-06	PPRN:89375250		
J	Wolanski, Stasiu; Barber, Ben										Ambiguity Clustering: an accurate and efficient decoder for qLDPC codes								Arxiv											1	1;2024-06-20;https://www.arxiv.org/abs/2406.14527v1	arXiv:2406.14527			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	Error correction allows a quantum computer to preserve a state long beyond the decoherence time of its physical qubits by encoding logical qubits in a larger number of physical qubits. The leading proposal for a scheme of quantum error correction is based on the surface code, but several recently proposed quantum low-density parity check (qLDPC) codes allow more logical information to be encoded in significantly fewer physical qubits. Key to any scheme of quantum error correction is the decoder, an algorithm that estimates the error state of the qubits from the results of syndrome measurements performed on them. The surface code has a variety of fast and accurate decoders, but the state -of -the -art decoder for general qLDPC codes, BP-OSD, has a high computational complexity. Here we introduce Ambiguity Clustering (AC), an algorithm which seeks to divide the measurement data into clusters which are decoded independently. We benchmark AC on the recently proposed bivariate bicycle codes and find that, at physically realistic error rates, AC is between one and three orders of magnitude faster than BP-OSD with no reduction in logical fidelity. Our CPU implementation of AC is already fast enough to decode the 144-qubit Gross code in real time for neutral atom and trapped ion systems.																																	2024-07-06	PPRN:89379458		
J	Zhang, Qimin; Qi, Weiwei; Zheng, Huili; Shen, Xinyu				Shen, Xinyu/IQT-5782-2023; Qi, Weiwei/IUM-5613-2023						CU-Net: a U-Net architecture for efficient brain-tumor segmentation on BraTS 2019 dataset								Arxiv											1	1;2024-06-19;https://www.arxiv.org/abs/2406.13113v1	arXiv:2406.13113			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 19 2024	2024	Accurately segmenting brain tumors from MRI scans is important for developing effective treatment plans and improving patient outcomes. This study introduces a new implementation of the Columbia-University-Net (CU-Net) architecture for brain tumor segmentation using the BraTS 2019 dataset. The CU-Net model has a symmetrical U-shaped structure and uses convolutional layers, max pooling, and upsampling operations to achieve high-resolution segmentation. Our CU-Net model achieved a Dice score of 82.41%, surpassing two other state-of-the-art models. This improvement in segmentation accuracy highlights the robustness and effectiveness of the model, which helps to accurately delineate tumor boundaries, which is crucial for surgical planning and radiation therapy, and ultimately has the potential to improve patient outcomes.																																	2024-07-06	PPRN:89378281		
J	Wang, Yidong; Guo, Qi; Yao, Wenjin; Zhang, Hongbo; Zhang, Xin; Wu, Zhen; Zhang, Meishan; Dai, Xinyu; Zhang, Min; Wen, Qingsong; Ye, Wei; Zhang, Shikun; Zhang, Yue				GUO, QI/HSE-6359-2023; Wen, Qingsong/LTF-7625-2024; zhang, min/IYI-9869-2023; Zhang, Shikun/ITU-3545-2023						AutoSurvey: Large Language Models Can Automatically Write Surveys								Arxiv											1	1;2024-06-18;https://www.arxiv.org/abs/2406.10252v2	arXiv:2406.10252			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 18 2024	2024	This paper introduces AutoSurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence. Traditional survey paper creation faces challenges due to the vast volume and complexity of information, prompting the need for efficient survey methods. While large language models (LLMs) offer promise in automating this process, challenges such as context window limitations, parametric knowledge constraints, and the lack of evaluation benchmarks remain. AutoSurvey addresses these challenges through a systematic approach that involves initial retrieval and outline generation, subsection drafting by specialized LLMs, integration and refinement, and rigorous evaluation and iteration. Our contributions include a comprehensive solution to the survey problem, a reliable evaluation method, and experimental validation demonstrating AutoSurvey's effectiveness.																																	2024-07-04	PPRN:89358226		
J	Ye, Xubing; Gan, Yukang; Huang, Xiaoke; Ge, Yixiao; Shan, Ying; Tang, Yansong				gan, yukang/NMK-7097-2025; 叶, 栩冰/JOJ-8652-2023; tang, yansong/ITV-8814-2023						VoCo-LLaMA: Towards Vision Compression with Large Language Models								Arxiv											2	2;2025-03-03;https://www.arxiv.org/abs/2406.12275v2| 1;2024-06-18;https://www.arxiv.org/abs/2406.12275v1	arXiv:2406.12275			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Vision-Language Models (VLMs) have achieved remarkable success in various multi-modal tasks, but they are often bottlenecked by the limited context window and high computational cost of processing high-resolution image inputs and videos. Vision compression can alleviate this problem by reducing the vision token count. Previous approaches compress vision tokens with external modules and force LLMs to understand the compressed ones, leading to visual information loss. However, the LLMs' understanding paradigm of vision tokens is not fully utilised in the compression learning process. We propose VoCo-LLaMA, the first approach to compress vision tokens using LLMs. By introducing Vision Compression tokens during the vision instruction tuning phase and leveraging attention distillation, our method distill how LLMs comprehend vision tokens into their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision compression and improves the computational efficiency during the inference stage. Specifically, our method achieves minimal performance loss with a compression ratio of 576$times$, resulting in up to 94.8$%$ fewer FLOPs and 69.6$%$ acceleration in inference time. Furthermore, through continuous training using time-series compressed token sequences of video frames, VoCo-LLaMA demonstrates the ability to understand temporal correlations, outperforming previous methods on popular video question-answering benchmarks. Our approach presents a promising way to unlock the full potential of VLMs' contextual window, enabling more scalable multi-modal applications. The project page, along with the associated code, can be accessed via $href{https://yxxxb.github.io/VoCo-LLaMA-page/}{text{this https URL}}$.																																	2025-08-07	PPRN:89359299		
J	Shi, Chufan; Yang, Cheng; Liu, Yaxin; Shui, Bo; Wang, Junjie; Jing, Mohan; Xu, Linran; Zhu, Xinyu; Li, Siheng; Zhang, Yuxiang; Liu, Gongye; Nie, Xiaomei; Cai, Deng; Yang, Yujiu				杨, 呈阳/LGZ-7175-2024; Wang, Junjie/JUF-3729-2023; Yang, Yujiu/JGM-0303-2023; Zhu, Xinyu/AAC-5551-2022; Liu, Gongye/LBA-5684-2024						ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation								Arxiv											1	1;2024-06-14;https://www.arxiv.org/abs/2406.09961v1	arXiv:2406.09961			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 14 2024	2024	We introduce a new benchmark, ChartMimic, aimed at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering. ChartMimic includes 1,000 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains(e.g., Physics, Computer Science, Economics, etc). These charts span 18 regular types and 4 advanced types, diversifying into 191 subcategories. Furthermore, we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts. Unlike existing code generation benchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to harmonize a blend of cognitive capabilities, encompassing visual understanding, code generation, and cross-modal reasoning. The evaluation of 3 proprietary models and 11 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an average score of 73.2 and 53.7, respectively, indicating significant room for improvement. We anticipate that ChartMimic will inspire the development of LMMs, advancing the pursuit of artificial general intelligence.																																	2024-07-04	PPRN:89332341		
J	Wang, Zijian; Li, Linhao				Wang, Zijian/IAQ-4374-2023						Anomaly in open quantum systems and its implications on mixed-state quantum phases								Arxiv											2	2;2024-06-13;https://www.arxiv.org/abs/2403.14533v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14533v1	arXiv:2403.14533			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 13 2024	2024	In this paper, we develop a systematic approach to characterize the ’t Hooft anomaly in open quantum systems. Owing to nontrivial couplings to the environment, symmetries in such systems manifest as either strong or weak type. By representing their symmetry transformation through superoperators, we incorporate them in a unified framework that enables a direct calculation of their anomalies. In the case where the full symmetre group is K × G , with K the strong symmetry and G the weak symmetry, we find that anomalies of bosonic systems are classified by Hd +2 ( K × G, U (1)) /H d +2 ( G, U (1)) in d spatial dimensions. To illustrate the power of anomalies in open quantum systems, we generally prove that anomaly must lead to nontrivial mixed-state quantum phases as long as the weak symmetry is imposed. Analogous to the “anomaly matching” condition ensuring nontrivial low-energy physics in closed systems, anomaly also guarantees nontrivial steady states and long-time dynamics for open quantum systems governed by Lindbladians. Notably, we identify a novel (1 + 1)-D mixed-state quantum phase that has no counterpart in closed systems, where the steady state shows no nontrivial correlation function in the bulk, but displays spontaneous symmetry breaking order on the boundary, which is enforced by anomalies. We further establish the general relations between mixed-state anomalies and such unconventional boundary correlation. Finally, we explore the generalization of the “anomaly inflow” mechanism in open quantum systems. We construct (1+ 1)-D and (2 + 1)-D Lindbladians whose steady states have mixed-state symmetryprotected-topological order in the bulk, with corresponding edge theories characterized by nontrivial anomalies.																																	2024-07-02	PPRN:88258732		
J	Liu, Yiqi; Moosavi, Nafise Sadat; Lin, Chenghua				LIU, yiqi/KSM-8236-2024						LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2311.09766v4| 2;2024-02-20;https://www.arxiv.org/abs/2311.09766v3| 1;2023-11-16;https://www.arxiv.org/abs/2311.09766v1	arXiv:2311.09766			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in a reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more reliable evaluation protocols in the future.																																	2024-06-22	PPRN:86176794		
J	Pan, Alexander; Jones, Erik; Jagadeesan, Meena; Steinhardt, Jacob										Feedback Loops With Language Models Drive In-Context Reward Hacking								Arxiv											3	3;2024-06-06;https://www.arxiv.org/abs/2402.06627v3| 2;2024-06-04;https://www.arxiv.org/abs/2402.06627v2| 1;2024-02-09;https://www.arxiv.org/abs/2402.06627v1	arXiv:2402.06627			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 04 2024	2024	Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior.																																	2024-06-22	PPRN:87603543		
J	Shen, Lingfeng; Mishra, Aayush; Khashabi, Daniel										Do pretrained Transformers Learn In-Context by Gradient Descent?								Arxiv											5	5;2024-06-03;https://www.arxiv.org/abs/2310.08540v5| 4;2024-02-29;https://www.arxiv.org/abs/2310.08540v4| 3;2023-11-30;https://www.arxiv.org/abs/2310.08540v3| 2;2023-11-24;https://www.arxiv.org/abs/2310.08540v2| 1;2023-10-12;https://www.arxiv.org/abs/2310.08540v1	arXiv:2310.08540			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 03 2024	2024	The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses emph{ICL objective} (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that emph{the equivalence between ICL and GD remains an open hypothesis} and calls for further studies.																																	2024-06-22	PPRN:85604286		
J	Chen, Shoufa; Xu, Mengmeng; Ren, Jiawei; Cong, Yuren; He, Sen; Xie, Yanping; Sinha, Animesh; Luo, Ping; Xiang, Tao; Perez-Rua, Juan-Manuel				pluo/GPG-2707-2022; Xie, Yanping/C-7950-2016; Xu, Mengmeng/OKS-2100-2025; CHEN, SHOUFA/HSH-2485-2023						GenTron: Diffusion Transformers for Image and Video Generation								Arxiv											2	2;2024-06-02;https://www.arxiv.org/abs/2312.04557v2| 1;2023-12-07;https://www.arxiv.org/abs/2312.04557v1	arXiv:2312.04557			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 02 2024	2024	In this study, we explore Transformer-based diffusion models for image and video generation. Despite the dominance of Transformer architectures in various fields due to their flexibility and scalability, the visual generative domain primarily utilizes CNN-based U-Net architectures, particularly in diffusion-based models. We introduce GenTron, a family of Generative models employing Transformer-based diffusion, to address this gap. Our initial step was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a process involving thorough empirical exploration of the conditioning mechanism. We then scale GenTron from approximately 900M to over 3B parameters, observing significant improvements in visual quality. Furthermore, we extend GenTron to text-to-video generation, incorporating novel motion-free guidance to enhance video quality. In human evaluations against SDXL, GenTron achieves a 51.1% win rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text alignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench, underscoring its strengths in compositional generation. We believe this work will provide meaningful insights and serve as a valuable reference for future research.																																	2024-11-09	PPRN:86443113		
J	Qian, Long; Li, Juncheng; Wu, Yu; Ye, Yaobo; Fei, Hao; Chua, Tat-Seng; Zhuang, Yueting; Tang, Siliang				Wang, Meng/AEZ-9059-2022; Fei, Hao/IZD-5292-2023						Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning								Arxiv											2	2;2024-06-02;https://www.arxiv.org/abs/2402.11435v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11435v1	arXiv:2402.11435			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 02 2024	2024	Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a large-scale video instruction dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level reasoning and localization. Zero-shot evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.																																	2024-06-22	PPRN:87763257		
J	Wang, Boxin; Ping, Wei; McAfee, Lawrence; Xu, Peng; Li, Bo; Shoeybi, Mohammad; Catanzaro, Bryan										InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining								Arxiv											3	3;2024-05-29;https://www.arxiv.org/abs/2310.07713v3| 2;2024-01-31;https://www.arxiv.org/abs/2310.07713v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07713v1	arXiv:2310.07713			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 29 2024	2024	Pretraining auto-regressive large language models~(LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA and reading comprehension tasks, 10% over GPT across 4 challenging long-form QA tasks, and 16% over GPT across 3 summarization tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. Our results highlight the promising direction to obtain a better GPT decoder through continued pretraining with retrieval before instruction tuning. 																																	2024-06-16	PPRN:85540119		
J	Guo, Pei-Fu; Chen, Ying-Hsuan; Tsai, Yun-Da; Lin, Shou-De				Tsai, YunDa/MVU-2214-2025						Towards Optimizing with Large Language Models								Arxiv											3	3;2024-05-27;https://www.arxiv.org/abs/2310.05204v3| 2;2023-11-28;https://www.arxiv.org/abs/2310.05204v2| 1;2023-10-08;https://www.arxiv.org/abs/2310.05204v1	arXiv:2310.05204			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 27 2024	2024	In this study, we evaluate the optimization capabilities of Large Language Models (LLMs) across diverse mathematical and combinatorial optimization tasks, where each task is described in natural language. These tasks require LLM to iteratively generate and evaluate solutions through interactive prompting, where each optimization step involves generating new solutions based on past results and then pass to subsequent iterations. We demonstrate that LLMs can perform various optimization algorithms and act as effective black-box optimizers, capable of intelligently optimizing unknown functions. We also introduce three simple yet informative metrics to evaluate optimization performance, applicable across diverse tasks and less sensitive to test sample variations. Our findings reveal that LLMs excel at optimizing small-scale problems with limited data and their performance is significantly affected by the dimension of problem and values, highlighting the need for further research in LLM optimization.																																	2024-06-11	PPRN:85572113		
J	Xiao, Jiancong; Li, Ziniu; Xie, Xingyu; Getzen, Emily; Fang, Cong; Long, Qi; Su, Weijie J.										On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization								Arxiv											2	2;2025-08-25;https://www.arxiv.org/abs/2405.16455v2| 1;2024-05-26;https://www.arxiv.org/abs/2405.16455v1	arXiv:2405.16455			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 26 2024	2024	Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that reinforcement learning from human feedback (RLHF)—the predominant approach for aligning LLMs with human preferences through a reward model—suffers from an inherent algorithmic bias due to its Kullback–Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley–Terry–Luce/Plackett–Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM’s policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT-1.3B and Llama-2-7B models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.																																	2024-06-08	PPRN:89056095		
J	Liu, Tianlin; Guo, Shangmin; Bianco, Leonardo; Calandriello, Daniele; Berthet, Quentin; Llinares, Felipe; Hoffmann, Jessica; Dixon, Lucas; Valko, Michal; Blondel, Mathieu				Guo, Shangmin/OXC-3050-2025; Dixon, Lucas/AFL-2608-2022						Decoding-time Realignment of Language Models								Arxiv											2	2;2024-05-24;https://www.arxiv.org/abs/2402.02992v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.02992v1	arXiv:2402.02992			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource -intensive, especially for large models. To address this challenge, we propose decoding -time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.																																	2024-06-08	PPRN:87517978		
J	Liu, Chenglong; Wei, Haoran; Chen, Jinyue; Kong, Lingyu; Ge, Zheng; Zhu, Zining; Zhao, Liang; Sun, Jianjian; Han, Chunrui; Zhang, Xiangyu				Chen, Jinyue/LIH-3814-2024; Sun, Jianjian/HNS-0565-2023						Focus Anywhere for Fine-grained Multi-page Document Understanding								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14295v1	arXiv:2405.14295			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Modern LVLMs still struggle to achieve fine-grained document understanding, such as OCR/translation/caption for regions of interest to the user, tasks that require the context of the entire page, or even multiple pages. Accordingly, this paper proposes Fox, an effective pipeline, hybrid data, and tuning strategy, that catalyzes LVLMs to focus anywhere on single/multi-page documents. We introduce a novel task to boost the document understanding by making LVLMs focus attention on the document-level region, such as redefining full-page OCR as foreground focus. We employ multiple vision vocabularies to extract visual hybrid knowledge for interleaved document pages (e.g., a page containing a photo). Meanwhile, we render cross-vocabulary vision data as the catalyzer to achieve a full reaction of multiple visual vocabularies and in-document figure understanding. Further, without modifying the weights of multiple vision vocabularies, the above catalyzed fine-grained understanding capabilities can be efficiently tuned to multi-page documents, enabling the model to focus anywhere in both format-free and page-free manners. Besides, we build a benchmark including 9 fine-grained sub-tasks (e.g., region-level OCR/summary, color-guided OCR) to promote document analysis in the community. The experimental results verify the superiority of our model.																																	2024-06-04	PPRN:88987823		
J	Liu, Zhao; Mera, Bruno; Fujimoto, Manato; Ozawa, Tomoki; Wang, Jie										Theory of Generalized Landau Levels and Implication for non-Abelian States								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14479v1	arXiv:2405.14479			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Quantum geometry is a fundamental concept to characterize the local properties of quantum states. It is recently demonstrated that saturating certain quantum geometric bounds allows a topological Chern band to share many essential features with the lowest Landau level, facilitating fractionalized phases in moiré flat bands. In this work, we systematically extend the consequence and universality of saturated geometric bounds to arbitrary Landau levels by introducing a set of single-particle states, which we term as ``generalized Landau levels''. These generalized Landau levels exhibit exactly quantized values of integrated trace of quantum metric determined by their corresponding Landau level indices, regardless of the nonuniformity of their quantum geometric quantities. We derive all geometric quantities for individual and multiple generalized Landau levels, discuss their relations, and understand them in light of the theory of holomorphic curves and moving frames. We further propose a model by superposing few generalized Landau levels which is supposed to capture a large portion of the single-particle Hilbert space of a generic Chern band analogous to the first Landau level. Using this model, we employ exact diagonalization to identify a single-particle geometric criterion for permitting the non-Abelian Moore-Read phase, which is potentially useful for future engineering of moiré materials and beyond. We use a double twisted bilayer graphene model with only adjacent layer hopping term to show the existence of first generalized Landau level type narrow band and zero-field Moore-Read state at the second magic angle which serves as a promising starting point for more detailed future studies. We expect that generalized Landau levels will serve as a systematic tool for analyzing topological Chern bands and fractionalized phases therein.																																	2025-10-15	PPRN:156842760		
J	Minderer, Matthias; Gritsenko, Alexey; Houlsby, Neil										Scaling Open-Vocabulary Object Detection								Arxiv											2	2;2024-05-22;https://www.arxiv.org/abs/2306.09683v3| 1;2023-06-16;https://www.arxiv.org/abs/2306.09683v1	arXiv:2306.09683			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 22 2024	2024	Open-vocabulary object detection has benefited greatly from pretrained visionlanguage models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to imagelevel pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space, pseudoannotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors already at comparable training scales ( ≈ 10M examples). However, with OWL-ST, we can scale to over 1B examples, yielding further large improvement: With an L/14 architecture, OWL-ST improves AP on LVIS rare classes, for which the model has seen no human box annotations , from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling.																																	2024-06-06	PPRN:73400971		
J	Sanmartin, Diego										KG-RAG: Bridging the Gap Between Knowledge and Creativity								Arxiv											1	1;2024-05-20;https://www.arxiv.org/abs/2405.12035v1	arXiv:2405.12035			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 20 2024	2024	Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems. LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially. Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks. 1																																	2024-06-15	PPRN:89094548		
J	Ostapenko, Oleksiy; Su, Zhan; Ponti, Edoardo Maria; Charlin, Laurent; Roux, Nicolas Le; Pereira, Matheus; Caccia, Lucas; Sordoni, Alessandro										Towards Modular LLMs by Building and Reusing a Library of LoRAs								Arxiv											1	1;2024-05-18;https://www.arxiv.org/abs/2405.11157v1	arXiv:2405.11157			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 18 2024	2024	The growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. We benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.																																	2024-06-15	PPRN:89097828		
J	Ma, Pingchuan; Wang, Tsun-Hsuan; Guo, Minghao; Sun, Zhiqing; Tenenbaum, Joshua B.; Rus, Daniela; Gan, Chuang; Matusik, Wojciech				Ma, Pingchuan/AFR-0634-2022; Wang, Tsun-Hsuan/JYP-5264-2024						LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery								Arxiv											1	1;2024-05-16;https://www.arxiv.org/abs/2405.09783v1	arXiv:2405.09783			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 16 2024	2024	Large Language Models have recently gained significant attention in scientific discovery for their extensive knowledge and advanced reasoning capabilities. However, they encounter challenges in effectively simulating observational feedback and grounding it with language to propel advancements in physical scientific discovery. Conversely, human scientists undertake scientific discovery by formulating hypotheses, conducting experiments, and revising theories through observational analysis. Inspired by this, we propose to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the computational strength of simulations. We introduce Scientific Generative Agent (SGA), a bilevel optimization framework: LLMs act as knowledgeable and versatile thinkers, proposing scientific hypotheses and reason about discrete components, such as physics equations or molecule structures; meanwhile, simulations function as experimental platforms, providing observational feedback and optimizing via differentiability for continuous parts, such as physical parameters. We conduct extensive experiments to demonstrate our framework’s efficacy in constitutive law discovery and molecular design, unveiling novel solutions that differ from conventional human expectations yet remain coherent upon analysis.																																	2024-06-12	PPRN:89076979		
J	Jeong, Seung Gyo; Choi, In Hyeok; Nair, Sreejith; Buiarelli, Luca; Pourbahari, Bita; Oh, Jin Young; Bassim, Nabil; Seo, Ambrose; Choi, Woo Seok; Fernandes, Rafael M.; Birol, Turan; Zhao, Liuyan; Lee, Jong Seok; Jalan, Bharat				Birol, Turan/D-1948-2012; Oh, Jin/ABG-7154-2020; Jalan, Bharat/H-9448-2015; Choi, Woo Seok/G-8783-2014; Fernandes, Rafael/E-9273-2010; Seo, Ambrose/B-6964-2008						Altermagnetic Polar Metallic phase in Ultra-Thin Epitaxially-Strained RuO2 Films								Arxiv											1	1;2024-05-09;https://www.arxiv.org/abs/2405.05838v1	arXiv:2405.05838			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 09 2024	2024	Altermagnetism refers to a wide class of compensated magnetic orders featuring magnetic sublattices with opposite spins related by rotational symmetry rather than inversion or translational operations, resulting in non-trivial spin splitting and high-order multipolar orders. Here, by combining theoretical analysis, electrical transport, X-ray and optical spectroscopies, and nonlinear optical measurements, we establish a phase diagram in hybrid molecular beam epitaxy-grown RuO2/TiO2 (110) films, mapping the broken symmetries along the altermagnetic/electronic/structural phase transitions as functions of film thickness and temperature. This phase diagram features a novel altermagnetic metallic polar phase in strained 2 nm samples, extending the concept of multiferroics to altermagnetic systems. These results provide a comprehensive understanding of altermagnetism upon epitaxial heterostructure design for emergent novel phases with multifunctionalities.																																	2024-08-02	PPRN:88980009		
J	Merullo, Jack; Eickhoff, Carsten; Pavlick, Ellie										Circuit Component Reuse Across Tasks in Transformer Language Models								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2310.08744v3| 1;2024-01-17;https://www.arxiv.org/abs/2310.08744v2	arXiv:2310.08744			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to 'repair' the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.																																	2024-05-24	PPRN:86283192		
J	Lin, Tianyi; Jin, Chi; Jordan, Michael I.										On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems								Arxiv											3	3;2023-09-14;https://www.arxiv.org/abs/1906.00331v9| 2;2021-11-16;https://www.arxiv.org/abs/1906.00331v8| 1;2024-05-03;https://www.arxiv.org/abs/1906.00331v10	arXiv:1906.00331			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 03 2024	2024	We consider nonconvex-concave minimax problems, minxmaxy∈Yf(x,y), where f is nonconvex in x but concave in y and Y is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function Φ(⋅):=maxy∈Yf(⋅,y) efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding light on its superior practical performance in training generative adversarial networks (GANs) and other real applications.																																	2024-05-28	PPRN:11958997		
J	Dohmatob, Elvis; Feng, Yunzhen; Kempe, Julia										Model Collapse Demystified: The Case of Regression								Arxiv											2	2;2024-04-30;https://www.arxiv.org/abs/2402.07712v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.07712v1	arXiv:2402.07712			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	In the era of proliferation of large language and image generation models, the phenomenon of ”model collapse” refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the setting of high -dimensional regression and obtain analytic formulae which quantitatively outline this phenomenon in a broad range of regimes. In the special case of polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.																																	2024-05-21	PPRN:87638502		
J	Hu, Yucheng; Lu, Yuxing				Lu, Yuxing/JMB-5818-2023						RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing								Arxiv											1	1;2024-04-30;https://www.arxiv.org/abs/2404.19543v1	arXiv:2404.19543			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.																																	2024-05-17	PPRN:88700183		
J	Berestycki, Nathanael; Powell, Ellen										Gaussian free field and Liouville quantum gravity								Arxiv											1	1;2024-04-25;https://www.arxiv.org/abs/2404.16642v1	arXiv:2404.16642			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 25 2024	2024	Over fourty years ago, the physicist Polyakov proposed a bold framework for string theory, in which the problem was reduced to the study of certain "random surfaces". He further made the tantalising suggestion that this theory could be explicitly solved. Recent breakthroughs from the last fifteen years have not only given a concrete mathematical basis for this theory but also verified some of its most striking predictions, as well as Polyakov's original vision. This theory, now known in the mathematics literature either as Liouville quantum gravity or Liouville conformal field theory, is based on a remarkable combination of ideas coming from different fields, above all probability and geometry. This book is intended to be an introduction to these developments assuming as few prerequisites as possible.																																	2024-08-03	PPRN:88651223		
J	Abramovich, Dan; Chen, Qile; Gross, Mark; Siebert, Bernd				Chen, Qile/NGS-5246-2025						Punctured logarithmic maps								Arxiv											1	1;2024-04-23;https://www.arxiv.org/abs/2009.07720v3	arXiv:2009.07720			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 23 2024	2024	We introduce a variant of stable logarithmic maps, which we call punctured logarithmic maps. They allow an extension of logarithmic Gromov-Witten theory in which marked points have a negative order of tangency with boundary divisors. As a main application we develop a gluing formalism which reconstructs stable logarithmic maps and their virtual cycles without expansions of the target, with tropical geometry providing the underlying combinatorics. Punctured Gromov-Witten invariants also play a pivotal role in the intrinsic construction of mirror partners by the last two authors in arXiv:1909.07649, conjecturally relating to symplectic cohomology, and in the logarithmic gauged linear sigma model in upcoming work of the second author with Felix Janda and Yongbin Ruan.																																	2024-05-16	PPRN:88622040		
J	He, Sunan; Nie, Yuxiang; Chen, Zhixuan; Cai, Zhiyuan; Wang, Hongmei; Yang, Shu; Chen, Hao				Chen, Hao/JHU-3470-2023; Yang, Shu/I-2051-2014						MedDr: Diagnosis-Guided Bootstrapping for Large-Scale Medical Vision-Language Learning								Arxiv											1	1;2024-04-23;https://www.arxiv.org/abs/2404.15127v1	arXiv:2404.15127			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 23 2024	2024	The rapid advancement of large-scale vision-language models has showcased remarkable capabilities across various tasks. However, the lack of extensive and high-quality image-text data in medicine has greatly hindered the development of large-scale medical vision-language models. In this work, we present a diagnosis-guided bootstrapping strategy that exploits both image and label information to construct vision-language datasets. Based on the constructed dataset, we developed MedDr, a generalist foundation model for healthcare capable of handling diverse medical data modalities, including radiology, pathology, dermatology, retinography, and endoscopy. Moreover, during inference, we propose a simple but effective retrieval-augmented medical diagnosis strategy, which enhances the model's generalization ability. Extensive experiments on visual question answering, medical report generation, and medical image diagnosis demonstrate the superiority of our method.																																	2024-05-02	PPRN:88626254		
J	Ni, Ansong; Allamanis, Miltiadis; Cohan, Arman; Deng, Yinlin; Shi, Kensen; Sutton, Charles; Yin, Pengcheng				Deng, Yinlin/KBQ-0846-2024						NExT: Teaching Large Language Models to Reason about Code Execution								Arxiv											1	1;2024-04-23;https://www.arxiv.org/abs/2404.14662v1	arXiv:2404.14662			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 23 2024	2024	A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka. rubber duck debugging). However, large language models (LLMs) of code are typically trained on the surface textual form of programs, thus may lack a semantic understanding of how programs execute at run-time. To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses self-training to bootstrap a synthetic training set of execution-aware rationales that lead to correct task solutions (e.g., fixed programs) without laborious manual annotation. Experiments on program repair tasks based on MBPP and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by 26.1% and 14.3% absolute, respectively, with significantly improved rationale quality as verified by automated metrics and human raters. Our model can also generalize to scenarios where program traces are absent at test-time.																																	2024-05-01	PPRN:88621543		
J	Raad, Maria Abi; Ahuja, Arun; Barros, Catarina; Besse, Frederic; Bolt, Andrew; Bolton, Adrian; Brownfield, Bethanie; Buttimore, Gavin; Cant, Max; Chakera, Sarah; Chan, Stephanie C.Y.; Clune, Jeff; Collister, Adrian; Copeman, Vikki; Cullum, Alex; Dasgupta, Ishita; Cesare, Dario de; Trapani, Julia Di; Donchev, Yani; Dunleavy, Emma; Engelcke, Martin; Faulkner, Ryan; Garcia, Frankie; Gbadamosi, Charles; Gong, Zhitao; Gonzales, Lucy; Gupta, Kshitij; Gregor, Karol; Hallingstad, Arne Olav; Harley, Tim; Haves, Sam; Hill, Felix; Hirst, Ed; Hudson, Drew A.; Hudson, Jony; Hughes-Fitt, Steph; Rezende, Danilo J.; Jasarevic, Mimi; Kampis, Laura; Ke, Rosemary; Keck, Thomas; Kim, Junkyung; Knagg, Oscar; Kopparapu, Kavya; Lampinen, Andrew; Legg, Shane; Lerchner, Alexander; Limont, Marjorie; Liu, Yulan; Loks-Thompson, Maria; Marino, Joseph; Cussons, Kathryn Martin; Matthey, Loic; Mcloughlin, Siobhan; Mendolicchio, Piermaria; Merzic, Hamza; Mitenkova, Anna; Moufarek, Alexandre; Oliveira, Valeria; Oliveira, Yanko; Openshaw, Hannah; Pan, Renke; Pappu, Aneesh; Platonov, Alex; Purkiss, Ollie; Reichert, David; Reid, John; Richemond, Pierre Harvey; Roberts, Tyson; Ruscoe, Giles; Elias, Jaume Sanchez; Sandars, Tasha; Sawyer, Daniel P.; Scholtes, Tim; Simmons, Guy; Slater, Daniel; Soyer, Hubert; Strathmann, Heiko; Stys, Peter; Tam, Allison C.; Teplyashin, Denis; Terzi, Tayfun; Vercelli, Davide; Vujatovic, Bojan; Wainwright, Marcus; Wang, Jane X.; Wang, Zhengdong; Wierstra, Daan; Williams, Duncan; Wong, Nathaniel; York, Sarah; Young, Nick		SIMA Team								Scaling Instructable Agents Across Many Simulated Worlds								Arxiv											1	1;2024-04-17;https://www.arxiv.org/abs/2404.10179v2	arXiv:2404.10179			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 17 2024	2024	Building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as open-ended, commercial video games. Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. Our approach focuses on language-driven generality while imposing minimal assumptions. Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments. In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.																																	2024-05-07	PPRN:88555574		
J	Ferrando, Javier; Voita, Elena										Information Flow Routes: Automatically Interpreting Language Models at Scale								Arxiv											2	2;2024-04-16;https://www.arxiv.org/abs/2403.00824v2| 1;2024-02-27;https://www.arxiv.org/abs/2403.00824v1	arXiv:2403.00824			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the role of some attention heads is overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.1																																	2024-04-27	PPRN:88024023		
J	Guo, Yanzhu; Shang, Guokan; Vazirgiannis, Michalis; Clavel, Chloe										The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text								Arxiv											2	2;2024-04-16;https://www.arxiv.org/abs/2311.09807v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09807v1	arXiv:2311.09807			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	This study investigates the consequences of training language models on synthetic data generated by their predecessors, an increasingly prevalent practice given the prominence of powerful generative models. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training method-ology on linguistic diversity, especially when conducted recursively over time. To assess this, we adapt and develop a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive finetuning experiments across various natural language generation tasks in English. Our findings re-veal a consistent decrease in the diversity of the model outputs through successive iterations, especially remarkable for tasks demanding high levels of creativity. This trend under scores the potential risks of training language models on synthetic text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of language models																																	2024-04-26	PPRN:86176603		
J	Galkin, Mikhail; Yuan, Xinyu; Mostafa, Hesham; Tang, Jian; Zhu, Zhaocheng				Yuan, Xinyu/KFS-4073-2024						Towards Foundation Models for Knowledge Graph Reasoning								Arxiv											2	2;2024-04-09;https://www.arxiv.org/abs/2310.04562v2| 1;2023-10-06;https://www.arxiv.org/abs/2310.04562v1	arXiv:2310.04562			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 09 2024	2024	Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance. [GRAPHICS]																																	2024-04-24	PPRN:85573444		
J	Deng, Yu; Hani, Zaher										LONG TIME JUSTIFICATION OF WAVE TURBULENCE THEORY								Arxiv											1	1;2024-04-07;https://www.arxiv.org/abs/2311.10082v2	arXiv:2311.10082			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 07 2024	2024	In a series of previous works [19, 20, 22], we gave a rigorous derivation of the homogeneous wave kinetic equation (WKE) up to small multiples of the kinetic timescale, which corresponds to short time solutions to the wave kinetic equation. In this work, we extend this justification to arbitrarily long times that cover the full lifespan of the WKE. This is the first long-time derivation ever obtained in any large data nonlinear (particle or wave) collisional kinetic limit.																																	2024-04-21	PPRN:88445734		
J	Yang, Diyi; Ziems, Caleb; Held, William; Shaikh, Omar; Bernstein, Michael S.; Mitchell, John										Social Skill Training with Large Language Models								Arxiv											1	1;2024-04-05;https://www.arxiv.org/abs/2404.04204v1	arXiv:2404.04204			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 05 2024	2024	People rely on social skills like conflict resolution to communicate effectively and to thrive in both work and personal life. However, practice environments for social skills are typically out of reach for most people. How can we make social skill training more available, accessible, and inviting? Drawing upon interdisciplinary research from communication and psychology, this perspective paper identifies social skill barriers to enter specialized fields. Then we present a solution that leverages large language models for social skill training via a generic framework. Our AI Partner, AI Mentor framework merges experiential learning with realistic practice and tailored feedback. This work ultimately calls for cross-disciplinary innovation to address the broader implications for workforce development and social equality.																																	2024-04-20	PPRN:88430092		
J	Kevian, Darioush; Syed, Usman; Guo, Xingang; Havens, Aaron; Dullerud, Geir; Seiler, Peter; Qin, Lianhui; Hu, Bin				Hu, Bin/AAJ-3459-2021; Guo, Xingang/OML-5380-2025						Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra								Arxiv											1	1;2024-04-04;https://www.arxiv.org/abs/2404.03647v1	arXiv:2404.03647			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 04 2024	2024	In this paper, we explore the capabilities of state-of-the-art large language models (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving undergraduate-level control problems. Controls provides an interesting case study for LLM reasoning due to its combination of mathematical theory and engineering design. We introduce ControlBench, a benchmark dataset tailored to reflect the breadth, depth, and complexity of classical control design. We use this dataset to study and evaluate the problem-solving abilities of these LLMs in the context of control engineering. We present evaluations conducted by a panel of human experts, providing insights into the accuracy, reasoning, and explanatory prowess of LLMs in control engineering. Our analysis reveals the strengths and limitations of each LLM in the context of classical control, and our results imply that Claude 3 Opus has become the state-of-the-art LLM for solving undergraduate control problems. Our study serves as an initial step towards the broader goal of employing artificial general intelligence in control engineering.																																	2024-04-19	PPRN:88413979		
J	Yu, Fanghua; Gu, Jinjin; Li, Zheyuan; Hu, Jinfan; Kong, Xiangtao; Wang, Xintao; He, Jingwen; Qiao, Yu; Dong, Chao				Qiao, Yu/ABD-5787-2021; Hu, Jin-Fan/ACM-8452-2022; He, Jingwen/HTM-5599-2023						Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild								Arxiv											2	2;2024-04-03;https://www.arxiv.org/abs/2401.13627v2| 1;2024-01-24;https://www.arxiv.org/abs/2401.13627v1	arXiv:2401.13627			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts.																																	2024-04-18	PPRN:87316834		
J	Ahuja, Sanchit; Aggarwal, Divyanshu; Gumma, Varun; Watts, Ishaan; Sathe, Ashutosh; Ochieng, Millicent; Hada, Rishav; Jain, Prachi; Axmed, Maxamed; Bali, Kalika; Sitaram, Sunayana				Jain, Prachi/KZU-2281-2024						MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks								Arxiv											2	2;2024-04-02;https://www.arxiv.org/abs/2311.07463v2| 1;2023-11-13;https://www.arxiv.org/abs/2311.07463v1	arXiv:2311.07463			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.																																	2024-05-03	PPRN:86134407		
J	Verwimp, Eli; Aljundi, Rahaf; Ben-David, Shai; Bethge, Matthias; Cossu, Andrea; Gepperth, Alexander; Hayes, Tyler L.; Huellermeier, Eyke; Kanan, Christopher; Kudithipudi, Dhireesha; Lampert, Christoph H.; Mundt, Martin; Pascanu, Razvan; Popescu, Adrian; Tolias, Andreas S.; van de Weijer, Joost; Liu, Bing; Lomonaco, Vincenzo; Tuytelaars, Tinne; van de Ven, Gido M.				van de Weijer, Joost/A-1643-2009; van de Ven, Gido/ABY-5626-2022; Mundt, Martin/AFL-6739-2022; Cossu, Andrea/IYJ-7422-2023; Kudithipudi, Dhireesha/KOC-8348-2024; Bethge, Matthias/B-1554-2008						Continual Learning: Applications and the Road Forward								Arxiv											2	2;2024-03-28;https://www.arxiv.org/abs/2311.11908v3| 1;2023-11-21;https://www.arxiv.org/abs/2311.11908v2	arXiv:2311.11908			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 28 2024	2024	Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: “Why should one care about continual learning in the first place?”. We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory -constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on -device learning, faster (re -)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptions in continual learning, we highlight and discuss four future directions for continual learning research. We hope that this work offers an interesting perspective on the future of continual learning, while displaying its potential value and the paths we have to pursue in order to make it successful. This work is the result of the many discussions the authors had at the Dagstuhl seminar on Deep Continual Learning, in March 2023.																																	2024-04-24	PPRN:86218692		
J	Yue, Minghao; Eilers, Anna-Christina; Simcoe, Robert A.; Mackenzie, Ruari; Matthee, Jorryt; Kashino, Daichi; Bordoloi, Rongmon; Lilly, Simon J.; Naidu, Rohan P.				Matthee, Jorryt/KHD-9384-2024; Kashino, Daichi/AAN-9011-2021						EIGER V. Characterizing the Host Galaxies of Luminous Quasars at z ≳ 6								Arxiv											2	2;2024-03-28;https://www.arxiv.org/abs/2309.04614v2| 1;2023-09-08;https://www.arxiv.org/abs/2309.04614v1	arXiv:2309.04614			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 28 2024	2024	We report JWST/NIRCam measurements of quasar host galaxy emissions and supermassive black hole (SMBH) masses for six quasars at 5.9 < z < 7.1 in the Emission-line galaxies and Intergalactic Gas in the Epoch of Reionization (EIGER) project. We obtain deep NIRCam imaging in the F115W, F200W, and F356W bands, as well as F356W grism spectroscopy of the quasars. We use bright unsaturated stars to construct models of the point spread function (PSF) and estimate the errors of these PSFs. We then measure or constrain the fluxes and morphology of the quasar host galaxies by fitting the quasar images as a point source plus an exponential disk. We successfully detect the host galaxy of three quasars, which have host-to-quasar flux ratios of ∼ 1% − 5%. Spectral Energy Distribution (SED) fitting suggests that these quasar host galaxies have stellar masses of M* > 1010M⊙. For quasars with host galaxy non-detections, we estimate the upper limits of their stellar masses. We use the grism spectra to measure the Hβ line profile and the continuum luminosity, then estimate the SMBH masses for the quasars. Our results indicate that the positive relation between SMBH masses and host galaxy stellar masses already exists at redshift z ≳ 6. The quasars in our sample show a high black hole to stellar mass ratio of MBH/M* ∼ 0.15, which is about ∼ 2 dex higher than local relations. We find that selection effects only contribute partially to the high MBH/M* ratios of high-redshift quasars. This result hints at a possible redshift evolution of the MBH − M* relation.																																	2024-04-17	PPRN:84950566		
J	Ma, Shuai; Chen, Qiaoyi; Wang, Xinru; Zheng, Chengbo; Peng, Zhenhui; Yin, Ming; Ma, Xiaojuan				chen, qiaoyi/ABG-1340-2021; Ma, Shuai/KXQ-8602-2024						Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making								Arxiv											2	2;2025-03-11;https://www.arxiv.org/abs/2403.16812v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16812v1	arXiv:2403.16812			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 25 2024	2024	In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans' appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.																																	2025-08-07	PPRN:88278388		
J	Peng, Qiwei; Chai, Yekun; Li, Xuhong				Chai, Yekun/HJG-7185-2022; Li, Xuhong/GYD-3485-2022						HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization								Arxiv											2	2;2024-03-24;https://www.arxiv.org/abs/2402.16694v2| 1;2024-02-26;https://www.arxiv.org/abs/2402.16694v1	arXiv:2402.16694			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 24 2024	2024	Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. Our work serves as a pioneering step towards filling the void in evaluating NL generalization in the area of multilingual code generation. We make our evaluation code and data publicly available at 																																	2024-04-14	PPRN:87882871		
J	Yu, Xiao; Qi, Yuang; Chen, Kejiang; Chen, Guoqiang; Yang, Xi; Zhu, Pengyuan; Zhang, Weiming; Yu, Nenghai				Chen, Kejiang/ABD-7057-2020; ZHANG, WEIMING/OJU-2642-2025; Chen, Guo-Qiang/HLG-1827-2023; Zhu, pengyuan/GPX-1461-2022						LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance								Arxiv											3	3;2024-06-12;https://www.arxiv.org/abs/2305.12519v3| 2;2024-03-23;https://www.arxiv.org/abs/2305.12519v2| 1;2023-05-21;https://www.arxiv.org/abs/2305.12519v1	arXiv:2305.12519			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 23 2024	2024	Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (textit{child}), LLM-Pat employs an intermediary LLM (textit{parent}) to reconstruct a textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encompassing four scenarios: student responses in educational settings, news creation, academic paper writing, and social media bots to assess the performance of LLM-Pat. The experiments show that LLM-Pat outperforms the existing detection methods and is more robust against paraphrasing attacks and re-translating attacks. Besides, LLM-Pat can also be used to trace which large language model the text was generated by. The constructed dataset and code will be released to benefit the community.																																	2025-08-07	PPRN:70969253		
J	Hu, Zhipeng; Zhao, Minda; Zhao, Chaoyi; Liang, Xinyue; Li, Lincheng; Zhao, Zeng; Fan, Changjie; Zhou, Xiaowei; Yu, Xin				Li, Lincheng/AAO-4355-2020; Liang, xinyue/CAH-7040-2022; Xu, Xiangyang/N-9292-2014						EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior								Arxiv											2	2;2024-03-21;https://www.arxiv.org/abs/2308.13223v2| 1;2023-08-25;https://www.arxiv.org/abs/2308.13223v1	arXiv:2308.13223			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	While image diffusion models have made significant progress in text-driven 3D content creation, they often fail to accurately capture the intended meaning of text prompts, especially for view information. This limitation leads to the Janus problem, where multi-faced 3D models are generated under the guidance of such diffusion models. In this paper, we propose a robust high-quality 3D content generation pipeline by exploiting orthogonal-view image guidance. First, we introduce a novel 2D diffusion model that generates an image consisting of four orthogonal-view sub-images based on the given text prompt. Then, the 3D content is created using this diffusion model. Notably, the generated orthogonal-view image provides strong geometric structure priors and thus improves 3D consistency. As a result, it effectively resolves the Janus problem and significantly enhances the quality of 3D content creation. Additionally, we present a 3D synthesis fusion network that can further improve the details of the generated 3D contents. Both quantitative and qualitative evaluations demonstrate that our method surpasses previous text-to-3D techniques. Project page: https://efficientdreamer.github.io.																																	2024-04-13	PPRN:83887193		
J	Liu, Zuyan; Dong, Yuhao; Rao, Yongming; Zhou, Jie; Lu, Jiwen				Rao, Yongming/U-8310-2019; Dong, Yuhao/JXM-8820-2024; Liu, Zuyan/NGS-3930-2025						Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models								Arxiv											2	2;2024-03-21;https://www.arxiv.org/abs/2403.12966v2| 1;2024-03-19;https://www.arxiv.org/abs/2403.12966v1	arXiv:2403.12966			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large Vision-Language Models (LVLMs) to extract useful features tailored to questions that aid the language model's response. Furthermore, a common practice among existing LVLMs is to utilize lower-resolution images, which restricts the ability for visual recognition. Our work introduces the Chain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel approach that enhances feature extraction by focusing on key regions of interest (ROI) within the image, corresponding to the posed questions or instructions. This technique allows LVLMs to access more detailed visual information without altering the original image resolution, thereby offering multi-granularity image features. By integrating Chain-of-Spot with instruct-following LLaVA-1.5 models, the process of image reasoning consistently improves performance across a wide range of multimodal datasets and benchmarks without bells and whistles and achieves new state-of-the-art results. Our empirical findings demonstrate a significant improvement in LVLMs' ability to understand and reason about visual content, paving the way for more sophisticated visual instruction-following applications. Code and models are available at https://github.com/dongyh20/Chain-of-Spot																																	2024-04-13	PPRN:88241432		
J	Chen, Yangyi; Sikka, Karan; Cogswell, Michael; Ji, Heng; Divakaran, Ajay				Chen, Yangyi/LSL-4051-2024						DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback								Arxiv											2	2;2024-03-19;https://www.arxiv.org/abs/2311.10081v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.10081v1	arXiv:2311.10081			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 19 2024	2024	We present DRESS  , a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multiturn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi -turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs– which focuses on LVLMs’ ability to refine responses by incorporating feedback in multi -turn interactions. To address the non -differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS ￼ can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi -turn interactions compared to SOTA LVLMs.																																	2024-04-12	PPRN:86177382		
J	Manukyan, Hayk; Sargsyan, Andranik; Atanyan, Barsegh; Wang, Zhangyang; Navasardyan, Shant; Shi, Humphrey										HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2312.14091v3| 2;2023-12-25;https://www.arxiv.org/abs/2312.14091v2| 1;2023-12-21;https://www.arxiv.org/abs/2312.14091v1	arXiv:2312.14091			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results. However, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts and performing high-resolution inpainting. Therefore, we introduce HD-Painter, a training free approach that accurately follows prompts and coherently scales to high resolution image inpainting. To this end, we design the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention scores by prompt information resulting in better text aligned generations. To further improve the prompt coherence we introduce the Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a post-hoc sampling strategy into the general form of DDIM to prevent out-of-distribution latent shifts. Moreover, HD-Painter allows extension to larger scales by introducing a specialized super-resolution technique customized for inpainting, enabling the completion of missing regions in images of up to 2K resolution. Our experiments demonstrate that HD-Painter surpasses existing state-of-the-art approaches quantitatively and qualitatively across multiple metrics and a user study. 																																	2024-04-11	PPRN:86787183		
J	Kacham, Praneeth; Mirrokni, Vahab; Zhong, Peilin										PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels								Arxiv											3	3;2024-03-17;https://www.arxiv.org/abs/2310.01655v3| 2;2024-02-07;https://www.arxiv.org/abs/2310.01655v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01655v1	arXiv:2310.01655			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 17 2024	2024	The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide emph{PolySketchFormer}, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in training compared to FlashAttention, with no observed degradation in quality across our experiments.																																	2024-04-11	PPRN:85375889		
J	Wang, Yueqian; Meng, Xiaojun; Liang, Jianxin; Wang, Yuxuan; Liu, Qun; Zhao, Dongyan										HawkEye: Training Video-Text LLMs for Grounding Text in Videos								Arxiv											1	1;2024-03-15;https://www.arxiv.org/abs/2403.10228v1	arXiv:2403.10228			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 15 2024	2024	Video -text Large Language Models (video -text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video -text LLMs that can perform temporal video grounding in a fully text -to -text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video -text corpus with segment -level captions and negative spans, with which we introduce two new timeaware training objectives to video -text LLMs. We also propose a coarsegrained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than other alternatives. Extensive experiments show that HawkEye is better at temporal video grounding and comparable on other video -text tasks with existing video -text LLMs, which verifies its superior video -text multi -modal understanding abilities.																																	2024-04-11	PPRN:88164271		
J	Wang, Ao; Chen, Hui; Lin, Zijia; Han, Jungong; Ding, Guiguang				Ding, Guiguang/KIL-3528-2024; Chen, Hui/AAU-9595-2021						RepViT: Revisiting Mobile CNN From ViT Perspective								Arxiv											4	4;2024-03-14;https://www.arxiv.org/abs/2307.09283v8| 3;2024-02-29;https://www.arxiv.org/abs/2307.09283v7| 2;2023-09-28;https://www.arxiv.org/abs/2307.09283v6| 1;2023-07-18;https://www.arxiv.org/abs/2307.09283v1	arXiv:2307.09283			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Recently, lightweight Vision Transformers (ViTs) demonstrate superior performance and lower latency, compared with lightweight Convolutional Neural Networks (CNNs), on resource -constrained mobile devices. Researchers have discovered many structural connections between lightweight ViTs and lightweight CNNs. However, the notable architectural disparities in the block structure, macro, and micro designs between them have not been adequately examined. In this study, we revisit the efficient design of lightweight CNNs from ViT perspective and emphasize their promising prospect for mobile devices. Specifically, we incrementally enhance the mobile -friendliness of a standard lightweight CNN, i.e., MobileNetV3, by integrating the efficient architectural designs of lightweight ViTs. This ends up with a new family of pure lightweight CNNs, namely RepViT. Extensive experiments show that RepViT outperforms existing state-of-the-art lightweight ViTs and exhibits favorable latency in various vision tasks. Notably, on ImageNet, RepViT achieves over 80% top -1 accuracy with 1.0 ms latency on an iPhone 12, which is the first time for a lightweight model, to the best of our knowledge. Besides, when RepViT meets SAM, our RepViT-SAM can achieve nearly 10× faster inference than the advanced MobileSAM. Codes and models are available at https : //github.com/THU-MIG/RepViT.																																	2024-04-11	PPRN:73992764		
J	Corona, Enric; Zanfir, Andrei; Bazavan, Eduard Gabriel; Kolotouros, Nikos; Alldieck, Thiemo; Sminchisescu, Cristian										VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis								Arxiv											1	1;2024-03-13;https://www.arxiv.org/abs/2403.08764v1	arXiv:2403.08764			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 13 2024	2024	We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions. VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization.																																	2024-04-11	PPRN:88133091		
J	Halawi, Danny; Denain, Jean-Stanislas; Steinhardt, Jacob										Overthinking the Truth: Understanding how Language Models Process False Demonstrations								Arxiv											2	2;2024-03-12;https://www.arxiv.org/abs/2307.09476v3| 1;2023-07-18;https://www.arxiv.org/abs/2307.09476v1	arXiv:2307.09476			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 12 2024	2024	Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model’s internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some “critical layer”, after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.1																																	2024-05-03	PPRN:73995022		
J	Ho, Anson; Besiroglu, Tamay; Erdil, Ege; Owen, David; Rahman, Robi; Guo, Zifan Carl; Atkinson, David; Thompson, Neil; Sevilla, Jaime				Erdil, Erkan/D-2031-2010; Owen, David/AAS-1605-2020; Thompson, Neil/T-9944-2019						Algorithmic progress in language models								Arxiv											1	1;2024-03-09;https://www.arxiv.org/abs/2403.05812v1	arXiv:2403.05812			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 09 2024	2024	We investigate the rate at which algorithms for pre -training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 95% confidence interval of around 5 to 14 months, substantially faster than hardware gains per Moore’s Law. We estimate augmented scaling laws, which enable us to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy benchmark data, our analysis quantifies the rapid progress in language modeling, shedding light on the relative contributions from compute and algorithms.																																	2024-04-04	PPRN:88099858		
J	Chen, Xuhai; Zhang, Jiangning; Tian, Guanzhong; He, Haoyang; Zhang, Wuhao; Wang, Yabiao; Wang, Chengjie; Liu, Yong				chengquan, Zhang/ACA-8100-2022; He, Haoyang/NEH-3017-2025; Wang, Yunxiao/AAQ-2686-2020; chen, xuhai/JJF-4740-2023; Tian, Guanzhong/JEP-8353-2023						CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection								Arxiv											2	2;2024-03-02;https://www.arxiv.org/abs/2311.00453v2| 1;2023-11-01;https://www.arxiv.org/abs/2311.00453v1	arXiv:2311.00453			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 02 2024	2024	This paper considers zero -shot Anomaly Detection (AD), performing AD without reference images of the test objects. We propose a framework called CLIP -AD to leverage the zero -shot capabilities of the large vision -language model CLIP. Firstly, we reinterpret the text prompts design from a distributional perspective and propose a Representative Vector Selection (RVS) paradigm to obtain improved text features. Secondly, we note opposite predictions and irrelevant highlights in the direct computation of the anomaly maps. To address these issues, we introduce a Staged DualPath model (SDP) that leverages features from various levels and applies architecture and feature surgery. Lastly, delving deeply into the two phenomena, we point out that the image and text features are not aligned in the joint embedding space. Thus, we introduce a fine-tuning strategy by adding linear layers and construct an extended model SDP+, further enhancing the performance. Abundant experiments demonstrate the effectiveness of our approach, e.g., on MVTec-AD, SDP outperforms the SOTA WinCLIP by +4.2↑/+10.7↑ in segmentation metrics F1-max/PRO, while SDP+ achieves +8.3↑/+20.5↑ improvements.																																	2024-04-02	PPRN:85930503		
J	Hong, Zhang-Wei; Shenfeld, Idan; Wang, Tsun-Hsuan; Chuang, Yung-Sung; Pareja, Aldo; Glass, James; Srivastava, Akash; Agrawal, Pulkit				Wang, Tsun-Hsuan/JYP-5264-2024; Chuang, Yung-Sung/HSH-6375-2023						Curiosity-driven Red-teaming for Large Language Models								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.19464v1	arXiv:2402.19464			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at url{https://github.com/Improbable-AI/curiosity_redteam}																																	2024-03-28	PPRN:88005518		
J	Lyle, Clare; Zheng, Zeyu; Khetarpal, Khimya; van Hasselt, Hado; Pascanu, Razvan; Martens, James; Dabney, Will				zheng, zeyu/A-3867-2013						Disentangling the Causes of Plasticity Loss in Neural Networks								Arxiv											1	1;2024-02-29;https://www.arxiv.org/abs/2402.18762v1	arXiv:2402.18762			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a stationary data distribution. In settings where this assumption is violated, e.g. deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.																																	2024-11-09	PPRN:87985664		
J	Halawi, Danny; Zhang, Fred; Yueh-Han, Chen; Steinhardt, Jacob										Approaching Human-Level Forecasting with Language Models								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18563v1	arXiv:2402.18563			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 28 2024	2024	Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.																																	2024-11-10	PPRN:88005068		
J	Nong, Yu; Aldeen, Mohammed; Cheng, Long; Hu, Hongxin; Chen, Feng; Cai, Haipeng				Cheng, Long/G-5027-2011; Hu, Hongxin/O-1328-2013; Nong, Yu/AEU-7962-2022						Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17230v1	arXiv:2402.17230			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	Security vulnerabilities are increasingly prevalent in modern software and they are widely consequential to our society. Various approaches to defending against these vulnerabilities have been proposed, among which those leveraging deep learning (DL) avoid major barriers with other techniques hence attracting more attention in recent years. However, DL-based approaches face critical challenges including the lack of sizable and quality-labeled task-specific datasets and their inability to generalize well to unseen, real-world scenarios. Lately, large language models (LLMs) have demonstrated impressive potential in various domains by overcoming those challenges, especially through chain-of-thought (CoT) prompting. In this paper, we explore how to leverage LLMs and CoT to address three key software vulnerability analysis tasks: identifying a given type of vulnerabilities, discovering vulnerabilities of any type, and patching detected vulnerabilities. We instantiate the general CoT methodology in the context of these tasks through VSP , our unified, vulnerability-semantics-guided prompting approach, and conduct extensive experiments assessing VSP versus five baselines for the three tasks against three LLMs and two datasets. Results show substantial superiority of our CoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy for vulnerability identification, discovery, and patching, respectively, on CVE datasets) over the baselines. Through in-depth case studies analyzing VSP failures, we also reveal current gaps in LLM/CoT for challenging vulnerability cases, while proposing and validating respective improvements.																																	2024-03-27	PPRN:87922044		
J	Gala, Jay; Jayakumar, Thanmay; Husain, Jaavid Aktar; Khan, Mohammed Safi Ur Rahman; Kanojia, Diptesh; Puduppully, Ratish; Khapra, Mitesh M.; Dabre, Raj; Murthy, Rudra; Kunchukuttan, Anoop				Kanojia, Diptesh/AAK-5838-2020						Airavata: Introducing Hindi Instruction-tuned LLM								Arxiv											2	2;2024-02-26;https://www.arxiv.org/abs/2401.15006v2| 1;2024-01-26;https://www.arxiv.org/abs/2401.15006v1	arXiv:2401.15006			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	We announce the initial release of "Airavata," an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.																																	2024-11-09	PPRN:87368415		
J	Guan, Jiaqi; Zhou, Xiangxin; Yang, Yuwei; Bao, Yu; Peng, Jian; Ma, Jianzhu; Liu, Qiang; Wang, Liang; Gu, Quanquan				Ma, Jianzhu/AAV-1904-2020						DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2403.07902v1	arXiv:2403.07902			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 26 2024	2024	Designing 3D ligands within a target binding site is a fundamental task in drug discovery. Existing structured-based drug design methods treat all ligand atoms equally, which ignores different roles of atoms in the ligand for drug design and can be less efficient for exploring the large drug-like molecule space. In this paper, inspired by the convention in pharmaceutical practice, we decompose the ligand molecule into two parts, namely arms and scaffold, and propose a new diffusion model, DecompDiff, with decomposed priors over arms and scaffold. In order to facilitate the decomposed generation and improve the properties of the generated molecules, we incorporate both bond diffusion in the model and additional validity guidance in the sampling phase. Extensive experiments on CrossDocked2020 show that our approach achieves state-of-the-art performance in generating high-affinity molecules while maintaining proper molecular properties and conformational stability, with up to -8.39 Avg. Vina Dock score and 24.5 Success Rate.																																	2024-04-08	PPRN:88126727		
J	Haberberger, Florian; Hainzl, Christian; Nam, Phan Thanh; Seiringer, Robert; Triay, Arnaud				Phan, Nam/C-7918-2012						The free energy of dilute Bose gases at low temperatures								Arxiv											2	2;2024-02-26;https://www.arxiv.org/abs/2304.02405v2| 1;2023-04-05;https://www.arxiv.org/abs/2304.02405v1	arXiv:2304.02405			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	We consider a low density Bose gas interacting through a repulsive potential in the thermodynamic limit. We justify the Lee–Huang–Yang conjecture of 1957 concerning the shape of the excitation spectrum. Rigorously we prove a lower bound for the free energy at suitably low temperatures, where the modified excitation spectrum leads to a second order correction to the ground state energy.																																	2024-03-24	PPRN:54474679		
J	Zhang, Yuxin; Zhao, Lirui; Lin, Mingbao; Sun, Yunyun; Yao, Yiwu; Han, Xingjia; Tanner, Jared; Liu, Shiwei; Ji, Rongrong				yao, yw/OTH-7724-2025						DYNAMIC SPARSE NO TRAINING ○: TRAINING-FREE FINE-TUNING FOR SPARSE LLMS								Arxiv											3	3;2024-02-26;https://www.arxiv.org/abs/2310.08915v3| 2;2023-10-17;https://www.arxiv.org/abs/2310.08915v2| 1;2023-10-13;https://www.arxiv.org/abs/2310.08915v1	arXiv:2310.08915			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on -device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry -academia gap, we introduce Dynamic Sparse No Training (DST∅T), a training -free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DS∅T minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning -and -growing on top of sparse LLMs. To accomplish this purpose, DS∅T particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DS∅T in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DS∅T is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training -free manner and open new venues to scale the great potential of sparsity to LLMs.																																	2024-03-25	PPRN:85614262		
J	Tornede, Alexander; Deng, Difan; Eimer, Theresa; Giovanelli, Joseph; Mohan, Aditya; Ruhkopf, Tim; Segel, Sarah; Theodorakopoulos, Daphne; Tornede, Tanja; Wachsmuth, Henning; Lindauer, Marius				Wachsmuth, Henning/AAH-7299-2021; Theodorakopoulos, Daphne/HCI-3849-2022; Giovanelli, Joseph/NUQ-5034-2025						AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2306.08107v3| 1;2023-06-13;https://www.arxiv.org/abs/2306.08107v1	arXiv:2306.08107			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 21 2024	2024	The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersection of AutoML and LLMs.																																	2024-03-20	PPRN:73361179		
J	Mohri, Christopher; Hashimoto, Tatsunori										Language Models with Conformal Factuality Guarantees								Arxiv											1	1;2024-02-15;https://www.arxiv.org/abs/2402.10978v1	arXiv:2402.10978			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 15 2024	2024	Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can provide 80-90% correctness guarantees while retaining the majority of the LM's original output.																																	2024-03-17	PPRN:87761344		
J	Li, Zhuan; Mong, Roger S.K.										Replica topological order in quantum mixed states and quantum error correction								Arxiv											1	1;2024-02-14;https://www.arxiv.org/abs/2402.09516v1	arXiv:2402.09516			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 14 2024	2024	Topological phases of matter offer a promising platform for quantum computation and quantum error correction. Nevertheless, unlike its counterpart in pure states, descriptions of topological order in mixed states remain relatively under-explored. Our work give two definitions for replica topological order in mixed states, which involve n copies of density matrices of the mixed state. Our framework categorizes topological orders in mixed states as either quantum, classical, or trivial, depending on the type of information that can be encoded. For the case of the toric code model in the presence of decoherence, we associate for each phase a quantum channel and describes the structure of the code space. We show that in the quantum-topological phase, there exists a postselection-based error correction protocol that recovers the quantum information, while in the classical-topological phase, the quantum information has decohere and cannot be fully recovered. We accomplish this by describing the mixed state as a projected entangled pairs state (PEPS) and identifying the symmetry-protected topological order of its boundary state to the bulk topology. We discuss the extent that our findings can be extrapolated to n → 1 limit.																																	2024-03-10	PPRN:87702858		
J	Schmidgall, Samuel; Harris, Carl; Essien, Ime; Olshvang, Daniel; Rahman, Tawsifur; Ji Woong, Kim; Ziaei, Rojin; Eshraghian, Jason; Abadir, Peter; Chellappa, Rama				Rahman, Tawsifur/GVT-7218-2022; Abadir, Peter/A-2067-2017						Addressing cognitive bias in medical language models								Arxiv											1	1;2024-02-14;https://www.arxiv.org/abs/2402.08113v2	arXiv:2402.08113			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 14 2024	2024	The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision -making settings. However, clinical decision -making is more complex than simulations because physicians’ decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases. In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B -chat, and the medically specialized PMC Llama 13B. We tested these models on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3, modified to replicate common clinically -relevant cognitive biases. Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B -chat and PMC Llama 13B, which were disproportionately affected by cognitive bias. Our findings highlight the critical need for bias mitigation in the development of medical LLMs, pointing towards safer and more reliable applications in healthcare.																																	2024-03-01	PPRN:87688297		
J	Kim, Kyungha; Lee, Sangyun; Huang, Kung-Hsiang; Chan, Hou Pong; Li, Manling; Ji, Heng										Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate								Arxiv											1	1;2024-02-12;https://www.arxiv.org/abs/2402.07401v1	arXiv:2402.07401			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Feb 12 2024	2024	Fact-checking research has extensively explored verification but less so the generation of natural-language explanations, crucial for user trust. While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence, advancing the credibility and trustworthiness of these explanations.																																	2024-05-25	PPRN:87636262		
J	Levy, Ryan; Luo, Di; Clark, Bryan K.				Levy, Ryan/IST-8014-2023						Classical Shadows for Quantum Process Tomography on Near-term Quantum Computers								Arxiv											3	3;2024-02-09;https://www.arxiv.org/abs/2110.02965v3| 2;2023-08-30;https://www.arxiv.org/abs/2110.02965v2| 1;2021-10-06;https://www.arxiv.org/abs/2110.02965v1	arXiv:2110.02965			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 09 2024	2024	Quantum process tomography is a powerful tool for understanding quantum channels and characterizing properties of quantum devices. Inspired by recent advances using classical shadows in quantum state tomography [H. -Y. Huang, R. Kueng, and J. Preskill, Nature Physics 16, 1050 (2020)], we have developed ShadowQPT, a classical shadow method for quantum process tomography. We introduce two related formulations with and without ancilla qubits. ShadowQPT stochastically reconstructs the Choi matrix of the device allowing for an a-posteri classical evaluation of the device on arbitrary inputs with respect to arbitrary outputs. Using shadows we then show how to compute overlaps, generate all k -weight reduced processes, and perform reconstruction via Hamiltonian learning. These latter two tasks are efficient for large systems as the number‘ of quantum measurements needed scales only logarithmically with the number of qubits. A number of additional approximations and improvements are developed including the use of a pair -factorized Clifford shadow and a series of post -processing techniques which significantly enhance the accuracy for recovering the quantum channel. We have implemented ShadowQPT using both Pauli and Clifford measurements on the IonQ trapped ion quantum computer for quantum processes up to n = 4 qubits and achieved good performance.																																	2024-05-25	PPRN:12833074		
J	Giacalone, Giuliano; Bally, Benjamin; Nijs, Govert; Shen, Shihang; Duguet, Thomas; Ebran, Jean-Paul; Elhatisari, Serdar; Frosini, Mikael; Laehde, Timo A.; Lee, Dean; Lu, Bing-Nan; Ma, Yuan-Zhuo; Meissner, Ulf-G.; Noronha-Hostler, Jacquelyn; Plumberg, Christopher; Rodriguez, Tomas R.; Roth, Robert; van der Schee, Wilke; Soma, Vittorio				Ma, Yuanzhuo/ABX-1003-2022; Rodríguez, Tomás/K-8282-2014; Noronha-Hostler, Jacquelyn/AAB-6282-2020; Shen, Shihang/HSG-3214-2023; Lu, Bing-Nan/S-9805-2017; Elhatisari, serdar/X-5059-2019						The unexpected uses of a bowling pin: exploiting 20Ne isotopes for precision characterizations of collectivity in small systems								Arxiv											1	1;2024-02-08;https://www.arxiv.org/abs/2402.05995v1	arXiv:2402.05995			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 08 2024	2024	Whether or not femto-scale droplets of quark -gluon plasma (QGP) are formed in so-called small systems at high-energy colliders is a pressing question in the phenomenology of the strong interaction. For proton -proton or proton -nucleus collisions the answer is inconclusive due to the large theoretical uncertainties plaguing the description of these processes. While upcoming data on collisions of 16O nuclei may mitigate these uncertainties in the near future, here we demonstrate the unique possibilities offered by complementing 16O16O data with collisions of 20Ne ions. We couple both NLEFT and PGCM ab initio descriptions of the structure of 20Ne and 16O to hydrodynamic simulations of 16O16O and 20Ne20Ne collisions at high energy. We isolate the imprints of the bowlingpin shape of 20Ne on the collective flow of hadrons, which can be used to perform quantitative tests of the hydrodynamic QGP paradigm. In particular, we predict that the elliptic flow of 20Ne20Ne collisions is enhanced by as much as 1.170(8)stat.(30)syst. for NLEFT and 1.139(6)stat.(39)syst. for PGCM relative to 16O16O collisions for the 1% most central events. At the same time, theoretical uncertainties largely cancel when studying relative variations of observables between two systems. This demonstrates a method based on experiments with two light -ion species for precision characterizations of the collective dynamics and its emergence in a small system.																																	2024-02-27	PPRN:87616178		
J	Lifshitz, Shalev; Paster, Keiran; Chan, Harris; Ba, Jimmy; Mcilraith, Sheila										STEVE-1: A Generative Model for Text-to-Behavior in Minecraft								Arxiv											2	2;2024-02-04;https://www.arxiv.org/abs/2306.00937v3| 1;2023-06-01;https://www.arxiv.org/abs/2306.00937v1	arXiv:2306.00937			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 04 2024	2024	Constructing AI models that respond to text instructions is challenging, especially for sequential decision -making tasks. This work introduces a methodology, inspired by unCLIP, for instruction -tuning generative models of behavior without relying on a large dataset of instruction -labeled trajectories. Using this methodology, we create an instruction -tuned Video Pretraining (VPT) model called STEVE -1, which can follow short -horizon open-ended text and visual instructions in MinecraftTM. STEVE -1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP’s latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self -supervised behavioral cloning and hindsight relabeling, reducing the need for costly human text annotations, and all for only $60 of compute. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text -conditioned image generation, STEVE -1 sets a new bar for open-ended instruction -following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early -game evaluation suite. We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier -free guidance, and data scaling. All resources, including our model weights, training scripts, and evaluation tools are made available for further research.																																	2024-05-25	PPRN:72812338		
J	Rolf, Esther; Klemmer, Konstantin; Robinson, Caleb; Kerner, Hannah				Klemmer, Konstantin/GZM-9026-2022						Mission Critical - Satellite Data is a Distinct Modality in Machine Learning								Arxiv											1	1;2024-02-02;https://www.arxiv.org/abs/2402.01444v1	arXiv:2402.01444			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	Satellite data has the potential to inspire a seismic shift for machine learning - one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society.																																	2024-05-01	PPRN:87507445		
J	Lu, Jianqiao; Zhong, Wanjun; Huang, Wenyong; Wang, Yufei; Zhu, Qi; Mi, Fei; Wang, Baojun; Wang, Weichao; Zeng, Xingshan; Shang, Lifeng; Jiang, Xin; Liu, Qun				SHANG, LIFENG/KIC-9695-2024; Huang, Yong/KFA-1191-2024; 江上鸥飞/AHA-6713-2022						SELF: Self-Evolution with Language Feedback								Arxiv											4	4;2024-02-01;https://www.arxiv.org/abs/2310.00533v4| 3;2023-11-30;https://www.arxiv.org/abs/2310.00533v3| 2;2023-10-07;https://www.arxiv.org/abs/2310.00533v2| 1;2023-10-01;https://www.arxiv.org/abs/2310.00533v1	arXiv:2310.00533			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 01 2024	2024	Large Language Models (LLMs) have demonstrated remarkable versatility across various domains. To further advance LLMs, we propose 'SELF' (Self-Evolution with Language Feedback), a novel approach that enables LLMs to self-improve through self-reflection, akin to human learning processes. SELF initiates with a meta-skill learning process that equips the LLMs with capabilities for self-feedback and self-refinement. Subsequently, the model undergoes an iterative process of self-evolution. In each iteration, it utilizes an unlabeled dataset of instructions to generate initial responses. These responses are enhanced through self-feedback and self-refinement. The model is then fine-tuned using this enhanced data. The model undergoes progressive improvement through this iterative self-evolution process. Moreover, the SELF framework enables the model to apply self-refinement during inference, which further improves response quality. Our experiments in mathematics and general tasks demonstrate that SELF can enhance the capabilities of LLMs without human intervention. The SELF framework indicates a promising direction for the autonomous evolution of LLMs, transitioning them from passive information receivers to active participants in their development.																																	2024-05-25	PPRN:85355403		
J	Zhang, Jingbo; Li, Xiaoyu; Wan, Ziyu; Wang, Can; Liao, Jing				Zhang, Jingbo/GRX-3761-2022; Li, Ruoyu/KEJ-3768-2024						Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields								Arxiv											2	2;2024-01-31;https://www.arxiv.org/abs/2305.11588v2| 1;2023-05-19;https://www.arxiv.org/abs/2305.11588v1	arXiv:2305.11588			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 31 2024	2024	Text-driven 3D scene generation is widely applicable to video gaming, film industry, and metaverse applications that have a large demand for 3D scenes. However, existing text-to-3D generation methods are limited to producing 3D objects with simple geometries and dreamlike styles that lack realism. In this work, we present Text2NeRF, which is able to generate a wide range of 3D scenes with complicated geometric structures and high-fidelity textures purely from a text prompt. To this end, we adopt NeRF as the 3D representation and leverage a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description. Specifically, we employ the diffusion model to infer the text-related image as the content prior and use a monocular depth estimation method to offer the geometric prior. Both content and geometric priors are utilized to update the NeRF model. To guarantee textured and geometric consistency between different views, we introduce a progressive scene inpainting and updating strategy for novel view synthesis of the scene. Our method requires no additional training data but only a natural language description of the scene as the input. Extensive experiments demonstrate that our Text2NeRF outperforms existing methods in producing photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of natural language prompts. Our code is available at https://github.com/eckertzhang/Text2NeRF.																																	2024-05-25	PPRN:70570910		
J	Pei, Qizhi; Zhang, Wei; Zhu, Jinhua; Wu, Kehan; Gao, Kaiyuan; Wu, Lijun; Xia, Yingce; Yan, Rui				Pei, Qizhi/NPI-8878-2025						BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations								Arxiv											3	3;2024-01-29;https://www.arxiv.org/abs/2310.07276v3| 2;2023-10-17;https://www.arxiv.org/abs/2310.07276v2| 1;2023-10-11;https://www.arxiv.org/abs/2310.07276v1	arXiv:2310.07276			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose BioT5, a comprehensive pretraining framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. BioT5 utilizes SELFIES for 100% robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, BioT5 distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. 																																	2024-05-25	PPRN:85540676		
J	Li, Linhao; Oshikawa, Masaki; Zheng, Yunqin				Oshikawa, Masaki/F-4992-2011						Non-Invertible Duality Transformation Between SPT and SSB Phases								Arxiv											2	2;2024-01-28;https://www.arxiv.org/abs/2301.07899v3| 1;2023-01-19;https://www.arxiv.org/abs/2301.07899v1	arXiv:2301.07899			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 28 2024	2024	In 1992, Kennedy and Tasaki constructed a non -local unitary transformation that maps between a Z2 × Z2 spontaneously symmetry breaking phase and the Haldane gap phase, which is a prototypical Symmetry -Protected Topological phase in modern framework, on an open spin chain. In this work, we propose a way to define it on a closed chain, by sacrificing unitarity. The operator realizing such a non -unitary transformation satisfies non -invertible fusion rule, and implements a generalized gauging of the Z2 × Z2 global symmetry. These findings connect the KennedyTasaki transformation to numerous other concepts developed for SPT phases, and opens a way to construct SPT phases systematically using the duality mapping.																																	2024-02-15	PPRN:35902204		
J	Luo, Linhao; Ju, Jiaxin; Xiong, Bo; Li, Yuan-Fang; Haffari, Gholamreza; Pan, Shirui				LUO, LINHAO/IAM-3162-2023; Ju, Jiaxin/HRB-0861-2023; Li, Yuan-Fang/T-7532-2019						ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning								Arxiv											3	3;2024-01-22;https://www.arxiv.org/abs/2309.01538v3| 2;2023-09-13;https://www.arxiv.org/abs/2309.01538v2| 1;2023-09-04;https://www.arxiv.org/abs/2309.01538v1	arXiv:2309.01538			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 22 2024	2024	Logical rules are essential for uncovering the logical connections between relations, which could improve reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for largescale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, the ranked rules can be used to conduct reasoning over KGs. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.																																	2024-05-25	PPRN:84733833		
J	Yang, Sen; Huang, Shujian; Dai, Xinyu; Chen, Jiajun				El-Ashram, Saeed/AAC-6060-2021						Multi-Candidate Speculative Decoding								Arxiv											1	1;2024-01-12;https://www.arxiv.org/abs/2401.06706v1	arXiv:2401.06706			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 12 2024	2024	Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.																																	2024-05-25	PPRN:87155172		
J	Bordenave, Charles; Benoit, Collins				Collins, Benoit/D-5568-2013						Norm of matrix-valued polynomials in random unitaries and permutations								Arxiv											2	2;2024-01-10;https://www.arxiv.org/abs/2304.05714v2| 1;2023-04-12;https://www.arxiv.org/abs/2304.05714v1	arXiv:2304.05714			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 10 2024	2024	We consider a non-commutative polynomial in several independent N-dimensional random unitary matrices, uniformly distributed over the unitary, orthogonal or symmetric groups, and assume that the coefficients are n-dimensional matrices. The main purpose of this paper is to study the operator norm of this random non-commutative polynomial. We compare it with its counterpart, where the random unitary matrices are replaced by the unitary generators of the free group von Neumann algebra. Our first result concerns the upper bound of the sequence: it is bounded above by an arbitrarily small inflation of its free counterpart with an overwhelming probability in the large N limit, and this estimate is uniform over all matrix coefficients as long as n < exp(Nα) for some explicit α > 0. Such results had been obtained by very different techniques for various regimes, all falling in the category n << N. Our result provides a new proof of the Peterson-Thom conjecture. Our second result is a universal quantitative lower bound for the operator norm of polynomials in independent N-dimensional random unitary and permutation matrices with coefficients in an arbitrary C*-algebra. A variant of this result for permutation matrices generalizes the Alon-Boppana lower bound in two directions: firstly, it applies to arbitrary polynomials and not only linear polynomials, and secondly, it applies to coefficients of an arbitrary C*-algebra with non-negative joint moments and not only for non-negative real numbers.																																	2024-01-31	PPRN:58922033		
J	Wei, Wei; Ren, Xubin; Tang, Jiabin; Wang, Qinyong; Su, Lixin; Cheng, Suqi; Wang, Junfeng; Yin, Dawei; Huang, Chao				Ren, Xubin/KOD-3622-2024; Wang, Qinyong/KPA-8187-2024; Yin, Dawei/JOR-9201-2023						LLMRec: Large Language Models with Graph Augmentation for Recommendation								Arxiv											5	5;2024-01-06;https://www.arxiv.org/abs/2311.00423v6| 4;2023-11-17;https://www.arxiv.org/abs/2311.00423v4| 3;2023-11-15;https://www.arxiv.org/abs/2311.00423v3| 2;2023-11-04;https://www.arxiv.org/abs/2311.00423v2| 1;2023-11-01;https://www.arxiv.org/abs/2311.00423v1	arXiv:2311.00423			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 06 2024	2024	The problem of data sparsity has long been a challenge in recom-mendation systems, and previous studies have attempted to address this issue by incorporating side information. However, this ap-proach often introduces side effects such as noise, availability issues, and low data quality, which in turn hinder the accurate modeling of user preferences and adversely impact recommendation perfor-mance. In light of the recent advancements in large language models (LLMs), which possess extensive knowledge bases and strong rea-soning capabilities, we propose a novel framework called LLMRec that enhances recommender systems by employing three simple yet effective LLM-based graph augmentation strategies. Our ap-proach leverages the rich content available within online platforms (e.g., Netflix, MovieLens) to augment the interaction graph in three ways: (i) reinforcing user-item interaction egde, (ii) enhancing the understanding of item node attributes, and (iii) conducting user node profiling, intuitively from the natural language perspective. By employing these strategies, we address the challenges posed by sparse implicit feedback and low-quality side information in recommenders. Besides, to ensure the quality of the augmentation, we develop a denoised data robustification mechanism that includes techniques of noisy implicit feedback pruning and MAE-based fea-ture enhancement that help refine the augmented data and improve its reliability. Furthermore, we provide theoretical analysis to sup-port the effectiveness of LLMRec and clarify the benefits of our method in facilitating model optimization. Experimental results on benchmark datasets demonstrate the superiority of our LLM-based augmentation approach over state-of-the-art techniques. To ensure reproducibility, we have made our code and augmented data publicly available at: https://github.com/HKUDS/LLMRec.git.																																	2024-01-25	PPRN:85919419		
J	Zhang, Daoan; Yang, Junming; Lyu, Hanjia; Jin, Zijian; Yao, Yuan; Chen, Mingkai; Luo, Jiebo				Chen, Mingkai/KFS-6989-2024; Luo, Jiebo/AAI-7549-2020; 杨, 峻明/GWC-0343-2022; LYU, HANJIA/JCD-5591-2023						CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs								Arxiv											1	1;2024-01-05;https://www.arxiv.org/abs/2401.02582v1	arXiv:2401.02582			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 05 2024	2024	When exploring the development of Artificial General Intelligence (AGI), a critical task for these models involves interpreting and processing information from multiple image inputs. However, Large Multimodal Models (LMMs) encounter two issues in such scenarios: (1) a lack of fine-grained perception, and (2) a tendency to blend information across multiple images. We first extensively investigate the capability of LMMs to perceive fine-grained visual details when dealing with multiple input images. The research focuses on two aspects: first, image-to-image matching (to evaluate whether LMMs can effectively reason and pair relevant images), and second, multi-image-to-text matching (to assess whether LMMs can accurately capture and summarize detailed image information). We conduct evaluations on a range of both open-source and closed-source large models, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model performance, we further develop a Contrastive Chain-of-Thought (CoCoT) prompting approach based on multi-input multimodal models. This method requires LMMs to compare the similarities and differences among multiple image inputs, and then guide the models to answer detailed questions about multi-image inputs based on the identified similarities and differences. Our experimental results showcase CoCoT's proficiency in enhancing the multi-image comprehension capabilities of large multimodal models.																																	2024-01-14	PPRN:86996973		
J	Wu, Tao; Zhang, Yong; Wang, Xintao; Zhou, Xianpan; Zheng, Guangcong; Qi, Zhongang; Shan, Ying; Li, Xi				Zheng, Guangcong/JFK-2840-2023; Zhou, Xianpan/JXM-4561-2024						CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities								Arxiv											2	2;2024-12-27;https://www.arxiv.org/abs/2408.13239v2| 1;2024-08-23;https://www.arxiv.org/abs/2408.13239v1	arXiv:2408.13239			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 27 2024	2024	Customized video generation aims to generate high-quality videos guided by text prompts and subject's reference images. However, since it is only trained on static images, the fine-tuning process of subject learning disrupts abilities of video diffusion models (VDMs) to combine concepts and generate motions. To restore these abilities, some methods use additional video similar to the prompt to fine-tune or guide the model. This requires frequent changes of guiding videos and even re-tuning of the model when generating different motions, which is very inconvenient for users. In this paper, we propose CustomCrafter, a novel framework that preserves the model's motion generation and conceptual combination abilities without additional video and fine-tuning to recovery. For preserving conceptual combination ability, we design a plug-and-play module to update few parameters in VDMs, enhancing the model's ability to capture the appearance details and the ability of concept combinations for new subjects. For motion generation, we observed that VDMs tend to restore the motion of video in the early stage of denoising, while focusing on the recovery of subject details in the later stage. Therefore, we propose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our subject learning modules, we reduce the impact of this module on motion generation in the early stage of denoising, preserving the ability to generate motion of VDMs. In the later stage of denoising, we restore this module to repair the appearance details of the specified subject, thereby ensuring the fidelity of the subject's appearance. Experimental results show that our method has a significant improvement compared to previous methods. 																																	2025-02-15	PPRN:91526147		
J	Shin, Juyeb; Jeong, Hyeonjun; Rameau, Francois; Kum, Dongsuk				Kim, Sanmin/OGP-8707-2025; Kum, Dongsuk/G-7362-2012; Rameau, Francois/ADD-5051-2022						InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning								Arxiv											2	2;2024-12-24;https://www.arxiv.org/abs/2301.04470v3| 1;2023-01-10;https://www.arxiv.org/abs/2301.04470v2	arXiv:2301.04470			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 24 2024	2024	For scalable autonomous driving, a robust map- based localization system, independent of GPS, is fundamental. To achieve such map-based localization, online high-definition (HD) map construction plays a significant role in accurate estimation of the pose. Although recent advancements in online HD map construction have predominantly investigated on vectorized representation due to its effectiveness, they suffer from computational cost and fixed parametric model, which limit scalability. To alleviate these limitations, we propose a novel HD map learning framework that leverages graph modeling. This framework is designed to learn the construction of diverse geometric shapes, thereby enhancing the scalability of HD map construction. Our approach involves representing the map elements as an instance- level graph by decomposing them into vertices and edges to facilitate accurate and efficient end-to-end vectorized HD map learning. Furthermore, we introduce an association strategy using a Graph Neural Network to efficiently handle the complex geometry of various map elements, while maintaining scalability. Comprehensive experiments on public open dataset show that our proposed network outperforms state-of-the-art model by 1.6 mAP. We further showcase the superior scalability of our approach compared to state-of-the-art methods, achieving a 4.8 mAP improvement in long range configuration. 																																	2025-02-02	PPRN:73493831		
J	Lai, Siqi; Xu, Zhao; Zhang, Weijia; Liu, Hao; Xiong, Hui				Xu, Zhao/JTV-3291-2023; LAI, SIQI/OKR-6456-2025; Zhang, Jian/HGE-5775-2022; Xiong, Hui/KIK-5457-2024						LLMLight: Large Language Models as Traffic Signal Control Agents								Arxiv											5	5;2024-12-17;https://www.arxiv.org/abs/2312.16044v5| 4;2024-03-05;https://www.arxiv.org/abs/2312.16044v4| 3;2024-02-13;https://www.arxiv.org/abs/2312.16044v3| 2;2024-02-09;https://www.arxiv.org/abs/2312.16044v2| 1;2023-12-26;https://www.arxiv.org/abs/2312.16044v1	arXiv:2312.16044			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 17 2024	2024	Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional TSC methods, primarily based on transportation engineering and reinforcement learning (RL), often struggle with generalization abilities across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments conducted on ten real-world and synthetic datasets, along with evaluations by fifteen human experts, demonstrate the exceptional effectiveness, generalization ability, and interpretability of LLMLight with LightGPT, outperforming nine baseline methods and ten advanced LLMs.																																	2025-01-24	PPRN:86828005		
J	Barrault, Loic; LCM team; Duquenne, Paul-Ambroise; Elbayad, Maha; Kozhevnikov, Artyom; Alastruey, Belen; Andrews, Pierre; Coria, Mariano; Couairon, Guillaume; Costa-jussa, Marta R.; Dale, David; Elsahar, Hady; Heffernan, Kevin; Janeiro, Joao Maria; Tran, Tuan; Ropers, Christophe; Sanchez, Eduardo; Roman, Robin San; Mourachko, Alexandre; Saleem, Safiyyah; Schwenk, Holger				Tran, Tuan/H-9439-2019; Heffernan, Kevin/KLC-4570-2024; Costa-jussà, Marta/M-7886-2013						Large Concept Models: Language Modeling in a Sentence Representation Space								Arxiv											1	1;2024-12-15;https://www.arxiv.org/abs/2412.08821v2	arXiv:2412.08821			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 15 2024	2024	LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for many tasks. The current established technology of LLMs is to process input and generate output at the token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well beyond single words, to analyze information and to generate creative content. In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept. Concepts are language- and modality-agnostic and represent a higher level idea or action in a flow. Hence, we build a "Large Concept Model". In this study, as proof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence embedding space, SONAR, which supports up to 200 languages in both text and speech modalities. The Large Concept Model is trained to perform autoregressive sentence prediction in an embedding space. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation, and models operating in a quantized SONAR space. These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens. We then scale one architecture to a model size of 7B parameters and training data of about 2.7T tokens. We perform an experimental evaluation on several generative tasks, namely summarization and a new task of summary expansion. Finally, we show that our model exhibits impressive zero-shot generalization performance to many languages, outperforming existing LLMs of the same size. The training code of our models is freely available.																																	2025-01-23	PPRN:119966146		
J	Hao, Zekun; Romero, David W.; Lin, Tsung-Yi; Liu, Ming-Yu				Hao, Zekun/ISU-6670-2023; Liu, Mingyu/ABC-4695-2020						Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale								Arxiv											1	1;2024-12-12;https://www.arxiv.org/abs/2412.09548v1	arXiv:2412.09548			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 12 2024	2024	Meshes are fundamental representations of 3D surfaces. However, creating high- quality meshes is a labor-intensive task that requires significant time and expertise in 3D modeling. While a delicate object often requires over 104 faces to be accurately modeled, recent attempts at generating artist-like meshes are limited to 1.6K faces and heavy discretization of vertex coordinates. Hence, scaling both the maximum face count and vertex coordinate resolution is crucial to producing high- quality meshes of realistic, complex 3D objects. We present M ESHTRON , a novel autoregressive mesh generation model able to generate meshes with up to 64K faces at 1024-level coordinate resolution –over an order of magnitude higher face count and 8× higher coordinate resolution than current state-of-the-art methods. M ESHTRON ’s scalability is driven by four key components: (i) an hourglass neural architecture, (ii) truncated sequence training, (iii) sliding window inference, and (iv) a robust sampling strategy that enforces the order of mesh sequences. This results in over 50% less training memory, 2.5× faster throughput, and better consistency than existing works. M ESHTRON generates meshes of detailed, complex 3D objects at unprecedented levels of resolution and fidelity, closely resembling those created by professional artists, and opening the door to more realistic generation of detailed 3D assets for animation, gaming, and virtual environments.																																	2025-01-20	PPRN:119903691		
J	Tan, Cheng; Gao, Zhangyang; Li, Siyuan; Li, Stan Z.				Li, SY/JPK-3839-2023; LI, Stan/OYF-6553-2025						SimVPv2: Towards Simple yet Powerful Spatiotemporal Predictive Learning								Arxiv											2	2;2024-12-12;https://www.arxiv.org/abs/2211.12509v4| 1;2022-11-22;https://www.arxiv.org/abs/2211.12509v1	arXiv:2211.12509			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 12 2024	2024	Recent years have witnessed remarkable advances in spatiotemporal predictive learning, with methods incorporating auxiliary inputs, complex neural architectures, and sophisticated training strategies. While SimVP has introduced a simpler, CNN-based baseline for this task, it still relies on heavy Unet-like architectures for spatial and temporal modeling, which still suffers from high complexity and computational overhead. In this paper, we propose SimVPv2, a streamlined model that eliminates the need for Unet architectures and demonstrates that plain stacks of convolutional layers, enhanced with an efficient Gated Spatiotemporal Attention mechanism, can deliver state-of-the-art performance. SimVPv2 not only simplifies the model architecture but also improves both performance and computational efficiency. On the standard Moving MNIST benchmark, SimVPv2 achieves superior performance compared to SimVP, with fewer FLOPs, about half the training time, and 60% faster inference efficiency. Extensive experiments across eight diverse datasets, including real-world tasks such as traffic forecasting and climate prediction, further demonstrate that SimVPv2 offers a powerful yet straightforward solution, achieving robust generalization across various spatiotemporal learning scenarios. We believe the proposed SimVPv2 can serve as a solid baseline to benefit the spatiotemporal predictive learning community.																																	2025-01-20	PPRN:23120464		
J	Wen, Kaiyue; Dang, Xingyu; Lyu, Kaifeng				Wen, Kaiyue/LRS-7555-2024						RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval								Arxiv											3	3;2024-12-06;https://www.arxiv.org/abs/2402.18510v4| 2;2024-05-10;https://www.arxiv.org/abs/2402.18510v3| 1;2024-02-29;https://www.arxiv.org/abs/2402.18510v2	arXiv:2402.18510			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 06 2024	2024	This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers.																																	2025-01-17	PPRN:87989762		
J	Shani, Lior; Rosenberg, Aviv; Cassel, Asaf; Lang, Oran; Calandriello, Daniele; Zipori, Avital; Noga, Hila; Keller, Orgad; Piot, Bilal; Szpektor, Idan; Hassidim, Avinatan; Matias, Yossi; Munos, Remi										Multi-turn Reinforcement Learning from Preference Human Feedback								Arxiv											2	2;2024-12-02;https://www.arxiv.org/abs/2405.14655v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14655v1	arXiv:2405.14655			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 02 2024	2024	Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular setting, we present a novel mirror-descent-based policy optimization algorithm for the general multi-turn preference- based RL problem, and prove its convergence to Nash equilibrium. To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.																																	2025-01-11	PPRN:88983298		
J	Wang, Lirui; Zhao, Jialiang; Du, Yilun; Adelson, Edward H.; Tedrake, Russ										PoCo: Policy Composition from and for Heterogeneous Robot Learning								Arxiv											3	3;2024-12-01;https://www.arxiv.org/abs/2402.02511v3| 2;2024-05-27;https://www.arxiv.org/abs/2402.02511v2| 1;2024-02-04;https://www.arxiv.org/abs/2402.02511v1	arXiv:2402.02511			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 01 2024	2024	Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real robot data and evaluate in tool-use tasks. The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments.																																	2025-01-11	PPRN:87523973		
J	Choi, Yichul; Rayhaun, Brandon C.; Zheng, Yunqin										Generalized Tube Algebras, Symmetry-Resolved Partition Functions, and Twisted Boundary States								Arxiv											2	2;2024-11-25;https://www.arxiv.org/abs/2409.02159v2| 1;2024-09-03;https://www.arxiv.org/abs/2409.02159v1	arXiv:2409.02159			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 25 2024	2024	We introduce a class of generalized tube algebras which describe how finite, non-invertible global symmetries of bosonic 1+1d QFTs act on operators which sit at the intersection point of a collection of boundaries and interfaces. We develop a 2+1d symmetry topological field theory (SymTFT) picture of boundaries and interfaces which, among other things, allows us to deduce the representation theory of these algebras. In particular, we initiate the study of a character theory, echoing that of finite groups, and demonstrate how many representation- theoretic quantities can be expressed as partition functions of the SymTFT on various backgrounds, which in turn can be evaluated explicitly in terms of generalized half-linking numbers. We use this technology to explain how the torus and annulus partition functions of a 1+1d QFT can be refined with information about its symmetries. We are led to a vast generalization of Ishibashi states in CFT: to any multiplet of conformal boundary conditions which transform into each other under the action of a symmetry, we associate a collection of generalized Ishibashi states, in terms of which the twisted sector boundary states of the theory and all of its orbifolds can be obtained as linear combinations. We derive a generalized Verlinde formula involving the characters of the boundary tube algebra which ensures that our formulas for the twisted sector boundary states respect open-closed duality. Our approach does not rely on rationality or the existence of an extended chiral algebra; however, in the special case of a diagonal RCFT with chiral algebra V and modular tensor category C, our formalism produces explicit closed-form expressions — in terms of the F-symbols and R-matrices of C, and the characters of V — for the twisted Cardy states, and the torus and annulus partition functions decorated by Verlinde lines.																																	2025-01-08	PPRN:91735949		
J	Li, Xiaotong; Zhang, Fan; Diao, Haiwen; Wang, Yueze; Wang, Xinlong; Duan, Ling-Yu				Diao, Haiwen/JJC-5475-2023; Xiaotong, Li/KDM-9760-2024						DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception								Arxiv											2	2;2024-11-24;https://www.arxiv.org/abs/2407.08303v2| 1;2024-07-11;https://www.arxiv.org/abs/2407.08303v1	arXiv:2407.08303			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 24 2024	2024	Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, and spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions. Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. 																																	2025-01-08	PPRN:90771215		
J	Wu, Jing; Le, Trung; Hayat, Munawar; Harandi, Mehrtash				Wu, Jing/AIF-2785-2022						Erasing Undesirable Influence in Diffusion Models								Arxiv											4	4;2024-11-20;https://www.arxiv.org/abs/2401.05779v4| 3;2024-07-28;https://www.arxiv.org/abs/2401.05779v3| 2;2024-02-05;https://www.arxiv.org/abs/2401.05779v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.05779v1	arXiv:2401.05779			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 20 2024	2024	Diffusion models are highly effective at generating high-quality images but pose risks, such as the unintentional generation of NSFW (not safe for work) content. Although various techniques have been proposed to mitigate unwanted influences in diffusion models while preserving overall performance, achieving a balance between these goals remains challenging. In this work, we introduce EraseDiff, an algorithm designed to preserve the utility of the diffusion model on retained data while removing the unwanted information associated with the data to be forgotten. Our approach formulates this task as a constrained optimization problem using the value function, resulting in a natural first-order algorithm for solving the optimization problem. By altering the generative process to deviate away from the ground-truth denoising trajectory, we update parameters for preservation while controlling constraint reduction to ensure effective erasure, striking an optimal trade-off. Extensive experiments and thorough comparisons with state-of-the-art algorithms demonstrate that EraseDiff effectively preserves the model's utility, efficacy, and efficiency.																																	2024-12-31	PPRN:87128180		
J	Zhang, Dan; Hu, Ziniu; Zhoubian, Sining; Du, Zhengxiao; Yang, Kaiyu; Wang, Zihan; Yue, Yisong; Dong, Yuxiao; Tang, Jie				Hu, Ziniu/HJI-4899-2023						SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models								Arxiv											3	3;2024-11-18;https://www.arxiv.org/abs/2401.07950v3| 2;2024-03-12;https://www.arxiv.org/abs/2401.07950v2| 1;2024-01-15;https://www.arxiv.org/abs/2401.07950v1	arXiv:2401.07950			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 18 2024	2024	Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. 																																	2024-12-27	PPRN:87187869		
J	Liu, Ting; Shi, Liangtao; Hong, Richang; Hu, Yue; Yin, Quanjun; Zhang, Linfeng				Zhang, Linfeng/JXM-1256-2024						Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model								Arxiv											1	1;2024-11-16;https://www.arxiv.org/abs/2411.10803v1	arXiv:2411.10803			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 16 2024	2024	The vision tokens in multimodal large language models usually exhibit significant spatial and temporal redundancy and take up most of the input tokens, which harms their inference efficiency. To solve this problem, some recent works were introduced to drop the unimportant tokens during inference where the importance of each token is decided only by the information in either the vision encoding stage or the prefilling stage. In this paper, we propose Multi-stage Token Dropping (MustDrop) to measure the importance of each token from the whole lifecycle, including the vision encoding stage, prefilling stage, and decoding stage. Concretely, in the visual encoding stage, MustDrop merges spatially adjacent tokens with high similarity, and establishes a key token set to retain the most vision-critical tokens, preventing them from being discarded in later stages. In the prefilling stage, MustDrop further compresses vision tokens by the guidance of text semantics, with a dual-attention filtering strategy. In the decoding stage, an output-aware cache policy is proposed to further reduce the size of the KV cache. By leveraging tailored strategies in the multi-stage process, MustDrop can more precisely recognize the important and redundant tokens, thus achieving an optimal balance between performance and efficiency. For instance, MustDrop reduces about 88.5% FLOPs on LLaVA with a compression ratio of 92.2% while maintaining comparable accuracy. 																																	2024-12-27	PPRN:119255371		
J	Hu, Siyuan; Ouyang, Mingyu; Gao, Difei; Shou, Mike Zheng				Shou, Mike Zheng/LXW-9197-2024						The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use								Arxiv											1	1;2024-11-15;https://www.arxiv.org/abs/2411.10323v1	arXiv:2411.10323			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 15 2024	2024	The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic, which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community.  [GRAPHICS]																																	2024-12-27	PPRN:119244253		
J	Chang, Chi-Chih; Lin, Wei-Cheng; Lin, Chien-Yu; Chen, Chong-Yan; Hu, Yu-Fang; Wang, Pei-Shuo; Huang, Ning-Chi; Ceze, Luis; Abdelfattah, Mohamed S.; Wu, Kai-Chiang				Chen, Chongyan/JXW-9579-2024; Abdelfattah, Mohamed/U-1337-2019						Palu: Compressing KV-Cache with Low-Rank Projection								Arxiv											2	2;2024-11-04;https://www.arxiv.org/abs/2407.21118v2| 1;2024-07-30;https://www.arxiv.org/abs/2407.21118v1	arXiv:2407.21118			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tensors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) optimized GPU kernels with operators fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89x on the RoPE-based attention module. When combined with quantization, Palu's inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91x speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu's superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. 																																	2024-12-09	PPRN:91174760		
J	Wang, Jiayu; Ming, Yifei; Shi, Zhenmei; Vineet, Vibhav; Wang, Xin; Li, Yixuan; Joshi, Neel				Shi, Zhenmei/KBA-9650-2024; Ming, Yifei/KIA-5069-2024; Li, Yixuan/GVU-6028-2022						Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models								Arxiv											2	2;2024-11-04;https://www.arxiv.org/abs/2406.14852v2| 1;2024-06-21;https://www.arxiv.org/abs/2406.14852v1	arXiv:2406.14852			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 04 2024	2024	Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.																																	2024-12-10	PPRN:89394655		
J	Udandarao, Vishaal; Prabhu, Ameya; Ghosh, Adhiraj; Sharma, Yash; Torr, Philip H.S.; Bibi, Adel; Albanie, Samuel; Bethge, Matthias				Bethge, Matthias/B-1554-2008; Albanie, Samuel/AAC-9729-2020						No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance								Arxiv											3	3;2024-10-29;https://www.arxiv.org/abs/2404.04125v3| 2;2024-04-08;https://www.arxiv.org/abs/2404.04125v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.04125v1	arXiv:2404.04125			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 29 2024	2024	Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found.																																	2024-12-09	PPRN:88430651		
J	Tan, Mingtian; Merrill, Mike A.; Gupta, Vinayak; Althoff, Tim; Hartvigsen, Thomas										Are Language Models Actually Useful for Time Series Forecasting?								Arxiv											2	2;2024-10-26;https://www.arxiv.org/abs/2406.16964v2| 1;2024-06-22;https://www.arxiv.org/abs/2406.16964v1	arXiv:2406.16964			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 26 2024	2024	Large language models (LLMs) are being applied to time series forecasting. But are language models actually useful for time series? In a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade forecasting performance—in most cases, the results even improve! We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and find that patching and attention structures perform similarly to LLM-based forecasters.1																																	2024-12-06	PPRN:89512747		
J	Wang, Feng; Mei, Jieru; Yuille, Alan										SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference								Arxiv											4	4;2024-10-26;https://www.arxiv.org/abs/2312.01597v4| 3;2024-01-03;https://www.arxiv.org/abs/2312.01597v3| 2;2023-12-17;https://www.arxiv.org/abs/2312.01597v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.01597v1	arXiv:2312.01597			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 26 2024	2024	Recent advances in contrastive language-image pretraining (CLIP) have demonstrated strong capabilities in zero-shot classification by aligning visual and textual features at an image level. However, in dense prediction tasks, CLIP often struggles to localize visual features within an image and fails to attain favorable pixel-level segmentation results. In this work, we investigate in CLIP’s spatial reasoning mechanism and identify that its failure of dense prediction is caused by a location misalignment issue in the self-attention process. Based on this observation, we propose a training-free adaptation approach for CLIP’s semantic segmentation, which only introduces a very simple modification to CLIP but can effectively address the issue of location misalignment. Specifically, we reform the self-attention mechanism with leveraging query-to-query and key-to-key similarity to determine attention scores. Remarkably, this minimal modification to CLIP significantly enhances its capability in dense prediction, improving the original CLIP’s 14.1% average zero-shot mIoU over eight semantic segmentation benchmarks to 38.2%, and outperforming the existing SoTA’s 33.9% by a large margin. 																																	2024-12-06	PPRN:86367196		
J	Li, Zhangheng; You, Keen; Zhang, Haotian; Feng, Di; Agrawal, Harsh; Li, Xiujun; Moorthy, Mohana Prasad Sathya; Nichols, Jeff; Yang, Yinfei; Gan, Zhe				Zhang, Haotian/CAH-0725-2022						Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms								Arxiv											1	1;2024-10-24;https://www.arxiv.org/abs/2410.18967v1	arXiv:2410.18967			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 24 2024	2024	Building a generalist model for user interface (UI) understanding is challenging due to various foundational issues, such as platform diversity, resolution variation, and data limitation. In this paper, we introduce Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI understanding across a wide range of platforms, including iPhone, Android, iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI 2 introduces three key innovations: support for multiple platform types, high-resolution perception through adaptive scaling, and advanced task training data generation powered by GPT4o with set-of-mark visual prompting. These advancements enable Ferret-UI 2 to perform complex, user-centered interactions, making it highly versatile and adaptable for the expanding diversity of platform ecosystems. Extensive empirical experiments on referring, grounding, user-centric advanced tasks (comprising 9 subtasks × 5 platforms), GUIDE next-action prediction dataset, and GUI-World multi-platform benchmark demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also shows strong cross-platform transfer capabilities.																																	2024-11-27	PPRN:118807911		
J	Huang, Hsiu-Yuan; Yang, Yutong; Zhang, Zhaoxi; Lee, Sanwoo; Wu, Yunfang				Zhang, Zhaoxi/GYR-2518-2022						A Survey of Uncertainty Estimation in LLMs: Theory Meets Practice								Arxiv											1	1;2024-10-20;https://www.arxiv.org/abs/2410.15326v1	arXiv:2410.15326			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 20 2024	2024	As large language models (LLMs) continue to evolve, understanding and quantifying the uncertainty in their predictions is critical for enhancing application credibility. However, the existing literature relevant to LLM uncertainty estimation often relies on heuristic approaches, lacking systematic classification of the methods. In this survey, we clarify the definitions of uncertainty and confidence, highlighting their distinctions and implications for model predictions. On this basis, we integrate theoretical perspectives, including Bayesian inference, information theory, and ensemble strategies, to categorize various classes of uncertainty estimation methods derived from heuristic approaches. Additionally, we address challenges that arise when applying these methods to LLMs. We also explore techniques for incorporating uncertainty into diverse applications, including out-of-distribution detection, data annotation, and question clarification. Our review provides insights into uncertainty estimation from both definitional and theoretical angles, contributing to a comprehensive understanding of this critical aspect in LLMs. We aim to inspire the development of more reliable and effective uncertainty estimation approaches for LLMs in real-world scenarios.																																	2024-11-20	PPRN:118755496		
J	Leone, Lorenzo; Bittel, Lennart				Leone, Lorenzo/OML-9300-2025						Stabilizer entropies are monotones for magic-state resource theory								Arxiv											3	3;2024-10-20;https://www.arxiv.org/abs/2404.11652v3| 2;2024-04-26;https://www.arxiv.org/abs/2404.11652v2| 1;2024-04-17;https://www.arxiv.org/abs/2404.11652v1	arXiv:2404.11652			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 20 2024	2024	Magic-state resource theory is a powerful tool with applications in quantum error correction, many-body physics, and classical simulation of quantum dynamics. Despite its broad scope, finding tractable resource monotones has been challenging. Stabilizer entropies have recently emerged as promising candidates (being easily computable and experimentally measurable detectors of nonstabilizerness) though their status as true resource monotones has been an open question ever since. In this Letter, we establish the monotonicity of stabilizer entropies for alpha geq 2 within the context of magic-state resource theory restricted to pure states. Additionally, we show that linear stabilizer entropies serve as strong monotones. Furthermore, we extend stabilizer entropies to mixed states as monotones via convex roof constructions, whose computational evaluation significantly outperforms optimization over stabilizer decompositions for low-rank density matrices. As a direct corollary, we provide improved conversion bounds between resource states, revealing a preferred direction of conversion between magic states. These results conclusively validate the use of stabilizer entropies within magic-state resource theory and establish them as the only known family of monotones that are experimentally measurable and computationally tractable.																																	2024-11-20	PPRN:88565155		
J	Hagemann, Paul; Mildenberger, Sophie; Ruthotto, Lars; Steidl, Gabriele; Yang, Nicole Tianjiao				Ruthotto, Lars/AFH-9326-2022						Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation								Arxiv											3	3;2024-10-19;https://www.arxiv.org/abs/2303.04772v4| 2;2023-11-04;https://www.arxiv.org/abs/2303.04772v3| 1;2023-03-08;https://www.arxiv.org/abs/2303.04772v1	arXiv:2303.04772			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 19 2024	2024	Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of finite size. This paper develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. In addition to the quest for generating images at ever-higher resolutions, our primary motivation is to create a well-posed infinite-dimensional learning problem that we can discretize consistently on multiple resolution levels. We thereby intend to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process using trace class operators to ensure that the latent distribution is well-defined in the infinite-dimensional setting and derive the reverse processes for finite-dimensional approximations. Second, we illustrate that approximating the score function with an operator network is beneficial for multilevel training. After deriving the convergence of the discretization and the approximation of multilevel training, we demonstrate some practical benefits of our infinite-dimensional SBDM approach on a synthetic Gaussian mixture example, the MNIST dataset, and a dataset generated from a nonlinear 2D reaction-diffusion equation.																																	2024-11-20	PPRN:44382169		
J	Lee, Sunjae; Choi, Junyoung; Lee, Jungjae; Wasi, Munim Hasan; Choi, Hojun; Ko, Steven Y.; Oh, Sangeun; Shin, Insik				Lee, Sunjae/LKJ-2642-2024						Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation								Arxiv											3	3;2024-10-16;https://www.arxiv.org/abs/2312.03003v3| 2;2024-03-16;https://www.arxiv.org/abs/2312.03003v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.03003v1	arXiv:2312.03003			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 16 2024	2024	The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MobileGPT, an innovative LLM-based mobile task automator equipped with a human-like app memory. MobileGPT emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular sub-tasks that can be re-used, re-arranged, and adapted for various objectives. We implement MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on a dataset of 185 tasks across 18 mobile apps. The results indicate that MobileGPT can automate and learn new tasks with 82.7% accuracy, and is able to adapt them to different contexts with near perfect (98.75%) accuracy while reducing both latency and cost by 62.5% and 68.8%, respectively, compared to the GPT-4 powered baseline.																																	2024-11-12	PPRN:86416633		
J	Lan, Yihuai; Hu, Zhiqiang; Wang, Lei; Wang, Yang; Zhao, Peilin; Lim, Ee-Peng; Ye, Deheng; Xiong, Hui; Wang, Hao				Hu, Zhiqiang/HIR-5043-2022; Wang, Hao/HLP-5008-2023						LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay								Arxiv											2	2;2024-10-13;https://www.arxiv.org/abs/2310.14985v4| 1;2024-03-07;https://www.arxiv.org/abs/2310.14985v3	arXiv:2310.14985			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 13 2024	2024	This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents’ social behaviors. Results affirm the framework’s effectiveness in creating adaptive agents and suggest LLM-based agents’ potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field’s research and applications. Our code is publicly available at https://github.com/ 3DAgentWorld/LLM-Game-Agent. .																																	2024-11-06	PPRN:88061189		
J	Volpe, M. Cristina										Neutrinos from dense environments : Flavor mechanisms, theoretical approaches, observations, and new directions								Arxiv											4	4;2024-10-10;https://www.arxiv.org/abs/2301.11814v4| 3;2024-02-20;https://www.arxiv.org/abs/2301.11814v3| 2;2024-01-23;https://www.arxiv.org/abs/2301.11814v2| 1;2023-01-27;https://www.arxiv.org/abs/2301.11814v1	arXiv:2301.11814			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 10 2024	2024	Neutrino masses and mixings produce vacuum oscillations, an established quantum mechanical phenomenon. In matter, the Mikheev-Smirnov-Wolfenstein effect, due to neutrino interactions with the background particles, triggers resonant flavor modification. In dense environments, such as core-collapse supernovae or compact mergers, sizable neutrino-neutrino interactions, shock waves and turbulence impact the neutrino flavor content under a variety of phenomena. Theoretical approaches of neutrino propagation range from the mean-field approximation to the full quantum kinetic equations. Intriguing connections have been uncovered between weakly interacting dense neutrino gases and other many-body systems and domains, from condensed matter and nuclear physics to quantum computing. Besides the intrinsic theoretical interest, establishing how neutrinos change flavor contributes to answer the longstanding open questions of how massive stars explode and of the r-process sites. It is also important for future observations of core-collapse supernova neutrinos and of the diffuse supernova neutrino background that should be discovered in the foreseeable future.																																	2024-11-03	PPRN:36025460		
J	Ma, Qianli; Liu, Zhen; Zheng, Zhenjing; Huang, Ziyang; Zhu, Siying; Yu, Zhongzhong; Kwok, James T.				Ma, Qianli/AAL-5191-2020; Zhu, Siying/MSY-0345-2025						A Survey on Time-Series Pre-Trained Models								Arxiv											2	2;2024-10-04;https://www.arxiv.org/abs/2305.10716v2| 1;2023-05-18;https://www.arxiv.org/abs/2305.10716v1	arXiv:2305.10716			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, pre-trained models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments involving 27 methods, 434 datasets, and 679 transfer learning scenarios are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work.																																	2024-10-25	PPRN:70532384		
J	Hua, Wenyue; Yang, Xianjun; Jin, Mingyu; Li, Zelong; Cheng, Wei; Tang, Ruixiang; Zhang, Yongfeng				Tang, Ruixiang/OEN-0104-2025						TrustAgent: Towards Safe and Trustworthy LLM-based Agents								Arxiv											3	3;2024-10-03;https://www.arxiv.org/abs/2402.01586v4| 2;2024-02-18;https://www.arxiv.org/abs/2402.01586v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01586v1	arXiv:2402.01586			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitutionbased agent framework, TrustAgent, , with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: preplanning strategy which injects safety knowledge to the model before plan generation, inplanning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent’s safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at https://github.com/ agiresearch/TrustAgent. .																																	2024-10-25	PPRN:87509781		
J	Saad-Falcon, Jon; Lafuente, Adrian Gamarra; Natarajan, Shlok; Maru, Nahum; Todorov, Hristo; Guha, Etash; Buchanan, E. Kelly; Chen, Mayee; Guha, Neel; Re, Christopher; Mirhoseini, Azalia										Archon: An Architecture Search Framework for Inference-Time Techniques								Arxiv											4	4;2024-10-03;https://www.arxiv.org/abs/2409.15254v5| 3;2024-09-27;https://www.arxiv.org/abs/2409.15254v4| 2;2024-09-26;https://www.arxiv.org/abs/2409.15254v3| 1;2024-09-24;https://www.arxiv.org/abs/2409.15254v2	arXiv:2409.15254			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 03 2024	2024	Inference-time techniques are emerging as highly effective tools to enhance large language model (LLM) capabilities. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of individual inference-time techniques and the interactions between them. Additionally, efficiently and automatically searching the space of model choices, inference-time techniques, and their compositions is challenging due to the large design space. To address these challenges, we introduce Archon, a modular framework for selecting, combining, and stacking layers of inference-time techniques to construct optimized LLM systems for target benchmarks. Rather than relying on a single LLM called once, we leverage a diverse set of LLMs and inference-time techniques, creating LLM systems greater than the sum of their parts. Archon defines an extensible design space, encompassing techniques such as generation ensembling, repeated sampling, ranking, fusion, critiquing, verification, and unit testing. It transforms the problem of building LLM systems into a hyperparameter optimization objective. Given the available LLMs, inference-time techniques, and compute budget, Archon utilizes hyperparameter search techniques to discover optimized architectures for target benchmark(s). We evaluate Archon architectures across a range of instruction-following, reasoning, and coding benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval, MixEval Hard, MATH, and CodeContests. Archon architectures outperform frontier models, such as GPT-4o and Claude 3.5 Sonnet, on these benchmarks, achieving an average accuracy increase of 15.1 percentage points by using all available LLMs. 																																	2025-01-25	PPRN:98862307		
J	Yang, Zhutian; Garrett, Caelan; Fox, Dieter; Lozano-Perez, Tomas; Kaelbling, Leslie Pack				Lozano-Perez, Tomas/J-9374-2012						Guiding Long-Horizon Task and Motion Planning with Vision Language Models								Arxiv											1	1;2024-10-03;https://www.arxiv.org/abs/2410.02193v1	arXiv:2410.02193			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 03 2024	2024	Vision-Language Models (VLM) can generate plausible high-level plans when prompted with a goal, the context, an image of the scene, and any planning constraints. However, there is no guarantee that the predicted actions are geometrically and kinematically feasible for a particular robot embodiment. As a result, many prerequisite steps such as opening drawers to access objects are often omitted in their plans. Robot task and motion planners can generate motion trajectories that respect the geometric feasibility of actions and insert physically necessary actions, but do not scale to everyday problems that require common-sense knowledge and involve large state spaces comprised of many variables. We propose VLM-TAMP, a hierarchical planning algorithm that leverages a VLM to generate goth semantically-meaningful and horizon-reducing intermediate subgoals that guide a task and motion planner. When a subgoal or action cannot be refined, the VLM is queried again for replanning. We evaluate VLM- TAMP on kitchen tasks where a robot must accomplish cooking goals that require performing 30-50 actions in sequence and interacting with up to 21 objects. VLM-TAMP substantially outperforms baselines that rigidly and independently execute VLM-generated action sequences, both in terms of success rates (50 to 100% versus 0%) and average task completion percentage (72 to 100% versus 15 to 45%).																																	2024-10-18	PPRN:102635683		
J	Chen, Mouxiang; Shen, Lefei; Li, Zhuo; Wang, Xiaoyun Joy; Sun, Jianling; Liu, Chenghao				Li, Zhuo/OUJ-6212-2025						VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters								Arxiv											1	1;2024-10-02;https://www.arxiv.org/abs/2408.17253v2	arXiv:2408.17253			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 02 2024	2024	Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either repurpose large language models (LLMs) or build large-scale time series datasets to develop TSF foundation models for universal forecasting. However, these methods face challenges due to the severe crossdomain gap or in-domain heterogeneity. This paper explores a new road to building a TSF foundation model from rich, high-quality natural images. Our key insight is that a visual masked autoencoder, pre-trained on the ImageNet dataset, can naturally be a numeric series forecaster. By reformulating TSF as an image reconstruction task, we bridge the gap between image pre-training and TSF downstream tasks. Surprisingly, without further adaptation in the time-series domain, the proposed V ISION TS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With fine-tuning for one epoch, V ISION TS could further improve the forecasting and achieve state-of-the-art performance in most cases. Extensive experiments reveal intrinsic similarities between images and real-world time series, suggesting visual models may offer a “free lunch” for TSF and highlight the potential for future cross-modality research. Our code is publicly available at https://github.com/Keytoyze/VisionTS. .																																	2024-10-19	PPRN:101092038		
J	Patel, Ajay; Hofmarcher, Markus; Leoveanu-Condrei, Claudiu; Dinu, Marius-Constantin; Callison-Burch, Chris; Hochreiter, Sepp				Callison-Burch, Chris/A-3393-2010						Large Language Models Can Self-Improve At Web Agent Tasks								Arxiv											2	2;2024-10-01;https://www.arxiv.org/abs/2405.20309v2| 1;2024-05-30;https://www.arxiv.org/abs/2405.20309v1	arXiv:2405.20309			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 01 2024	2024	Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.																																	2024-10-16	PPRN:89111137		
J	Jia, Baoxiong; Chen, Yixin; Yu, Huangyue; Wang, Yan; Niu, Xuesong; Liu, Tengyu; Li, Qing; Huang, Siyuan				Niu, Xuesong/MEO-3849-2025; Li, Qing/U-8995-2018						SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding								Arxiv											3	3;2024-09-24;https://www.arxiv.org/abs/2401.09340v3| 2;2024-03-06;https://www.arxiv.org/abs/2401.09340v2| 1;2024-01-17;https://www.arxiv.org/abs/2401.09340v1	arXiv:2401.09340			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Sep 24 2024	2024	3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. 																																	2024-10-08	PPRN:87209236		
J	Lee, Byeonghyeon; Lee, Howoong; Sun, Xiangyu; Ali, Usman; Park, Eunbyung				Ali, Usman/LZH-3835-2025; Park, Eunbyung/JUV-0796-2023; Sun, Xiangyu/IWN-7833-2023						Deblurring 3D Gaussian Splatting								Arxiv											3	3;2024-09-24;https://www.arxiv.org/abs/2401.00834v3| 2;2024-05-27;https://www.arxiv.org/abs/2401.00834v2| 1;2024-01-01;https://www.arxiv.org/abs/2401.00834v1	arXiv:2401.00834			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 24 2024	2024	Recent studies in Radiance Fields have paved the robust way for novel view synthesis with their photorealistic rendering quality. Nevertheless, they usually employ neural networks and volumetric rendering, which are costly to train and impede their broad use in various real-time applications due to the lengthy rendering time. Lately 3D Gaussians splatting-based approach has been proposed to model the 3D scene, and it achieves remarkable visual quality while rendering the images in real-time. However, it suffers from severe degradation in the rendering quality if the training images are blurry. Blurriness commonly occurs due to the lens defocusing, object motion, and camera shake, and it inevitably intervenes in clean image acquisition. Several previous studies have attempted to render clean and sharp images from blurry input images using neural fields. The majority of those works, however, are designed only for volumetric rendering-based neural radiance fields and are not straightforwardly applicable to rasterization-based 3D Gaussian splatting methods. Thus, we propose a novel real-time deblurring framework, Deblurring 3D Gaussian Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the covariance of each 3D Gaussian to model the scene blurriness. While Deblurring 3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct fine and sharp details from blurry images. A variety of experiments have been conducted on the benchmark, and the results have revealed the effectiveness of our approach for deblurring. 																																	2024-11-03	PPRN:86904427		
J	Wang, Hongru; Huang, Wenyu; Deng, Yang; Wang, Rui; Wang, Zezhong; Wang, Yufei; Mi, Fei; Pan, Jeff Z.; Wong, Kam-Fai				Huang, Wenyu/J-4257-2019; Deng, Yang/JAZ-0613-2023						UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems								Arxiv											2	2;2024-09-19;https://www.arxiv.org/abs/2401.13256v2| 1;2024-01-24;https://www.arxiv.org/abs/2401.13256v1	arXiv:2401.13256			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 19 2024	2024	Large Language Models (LLMs) have shown exceptional capabilities in many natural language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves better performance than previous strong baselines on the knowledge source selection and response generation task with itself as a retriever in a unified manner, and achieves new state-of-the-art when using more advanced external retriever. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.																																	2024-10-02	PPRN:87315393		
J	Qureshi, Mohammad Nomaan; Garg, Sparsh; Yandun, Francisco; Held, David; Kantor, George; Silwal, Abhishesh				Yandun Narvaez, Francisco/HNP-7210-2023						SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting								Arxiv											3	3;2024-10-07;https://www.arxiv.org/abs/2409.10161v3| 2;2024-09-28;https://www.arxiv.org/abs/2409.10161v2| 1;2024-09-16;https://www.arxiv.org/abs/2409.10161v1	arXiv:2409.10161			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Sep 16 2024	2024	Sim2Real transfer, particularly for manipulation policies relying on RGB images, remains a critical challenge in robotics due to the significant domain shift between synthetic and real-world visual data. In this paper, we propose SplatSim, a novel framework that leverages Gaussian Splatting as the primary rendering primitive to reduce the Sim2Real gap for RGB-based manipulation policies. By replacing traditional mesh representations with Gaussian Splats in simulators, SplatSim produces highly photorealistic synthetic data while maintaining the scalability and cost-efficiency of simulation. We demonstrate the effectiveness of our framework by training manipulation policies within SplatSim}and deploying them in the real world in a zero-shot manner, achieving an average success rate of 86.25%, compared to 97.5% for policies trained on real-world data.																																	2024-12-23	PPRN:100734467		
J	Zhou, Hao; Hu, Chengming; Yuan, Ye; Cui, Yufei; Jin, Yili; Chen, Can; Wu, Haolun; Yuan, Dun; Jiang, Li; Wu, Di; Liu, Xue; Zhang, Charlie; Wang, Xianbin; Liu, Jiangchuan				Cui, Yufei/LSK-5514-2024; Zhou, Hao/OIU-6502-2025; Liu, Wenyu/AAG-1426-2019						Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities								Arxiv											2	2;2024-09-16;https://www.arxiv.org/abs/2405.10825v2| 1;2024-05-17;https://www.arxiv.org/abs/2405.10825v1	arXiv:2405.10825			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Sep 16 2024	2024	Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.																																	2024-12-24	PPRN:89091783		
J	Wang, Xidong; Chen, Nuo; Chen, Junyin; Wang, Yidong; Zhen, Guorui; Zhang, Chunxian; Wu, Xiangbo; Hu, Yan; Gao, Anningzhe; Wan, Xiang; Li, Haizhou; Wang, Benyou				Hu, Yan/AAB-1755-2021; Wang, Yidong/I-4027-2014; Wang, Benyou/Y-5146-2019; Wang, Xidong/IZD-5718-2023						Apollo: A Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People								Arxiv											6	6;2024-10-12;https://www.arxiv.org/abs/2403.03640v6| 5;2024-09-14;https://www.arxiv.org/abs/2403.03640v5| 4;2024-08-16;https://www.arxiv.org/abs/2403.03640v4| 3;2024-06-28;https://www.arxiv.org/abs/2403.03640v3| 2;2024-03-09;https://www.arxiv.org/abs/2403.03640v2| 1;2024-03-06;https://www.arxiv.org/abs/2403.03640v1	arXiv:2403.03640			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 14 2024	2024	Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a proxy-tuning fashion. We will open-source training corpora, code, model weights and evaluation benchmark.																																	2024-12-24	PPRN:88048359		
J	He, Haorui; Shang, Zengqiang; Wang, Chaoren; Li, Xuyuan; Gu, Yicheng; Hua, Hua; Liu, Liwei; Yang, Chen; Li, Jiaqi; Shi, Peiyang; Wang, Yuancheng; Chen, Kai; Zhang, Pengyuan; Wu, Zhizheng				Wang, Yuancheng/GLR-2067-2022; LI, Jiaqi/KDM-5257-2024; Wang, Chaoren/KYP-8030-2024; Li, Xu-Yuan/ABF-3943-2021						Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation								Arxiv											3	3;2024-09-07;https://www.arxiv.org/abs/2407.05361v3| 2;2024-07-13;https://www.arxiv.org/abs/2407.05361v2| 1;2024-07-07;https://www.arxiv.org/abs/2407.05361v1	arXiv:2407.05361			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 07 2024	2024	Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. 																																	2024-09-26	PPRN:90741367		
J	He, Junjie; Geng, Yifeng; Bo, Liefeng										UniPortrait: A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalization								Arxiv											2	2;2024-09-06;https://www.arxiv.org/abs/2408.05939v2| 1;2024-08-12;https://www.arxiv.org/abs/2408.05939v1	arXiv:2408.05939			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 06 2024	2024	This paper presents UniPortrait, an innovative human image personalization framework that unifies single- and multi-ID customization with high face fidelity, extensive facial editability, free-form input description, and diverse layout generation. UniPortrait consists of only two plug-and-play modules: an ID embedding module and an ID routing module. The ID embedding module extracts versatile editable facial features with a decoupling strategy for each ID and embeds them into the context space of diffusion models. The ID routing module then combines and distributes these embeddings adaptively to their respective regions within the synthesized image, achieving the customization of single and multiple IDs. With a carefully designed two-stage training scheme, UniPortrait achieves superior performance in both single- and multi-ID customization. Quantitative and qualitative experiments demonstrate the advantages of our method over existing approaches as well as its good scalability, e.g., the universal compatibility with existing generative control tools. 																																	2024-09-18	PPRN:91353093		
J	Bi, Zhen; Zhang, Ningyu; Xue, Yida; Ou, Yixin; Ji, Daxiong; Zheng, Guozhou; Chen, Huajun				Ji, Daxiong/OIS-7288-2025; Bi, Zhen/KAM-7157-2024; Zhang, Ningyu/AAQ-7391-2021; Huajun, Chen/B-6340-2013						OceanGPT: A Large Language Model for Ocean Science Tasks								Arxiv											7	7;2024-09-03;https://www.arxiv.org/abs/2310.02031v8| 6;2024-05-23;https://www.arxiv.org/abs/2310.02031v7| 5;1800-01-01;https://www.arxiv.org/abs/2310.02031v6| 4;2024-02-06;https://www.arxiv.org/abs/2310.02031v5| 3;2023-10-25;https://www.arxiv.org/abs/2310.02031v4| 2;2023-10-19;https://www.arxiv.org/abs/2310.02031v3| 1;2023-10-04;https://www.arxiv.org/abs/2310.02031v2	arXiv:2310.02031			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 03 2024	2024	Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose OceanGPT, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.																																	2024-09-09	PPRN:85397962		
J	Ai, Wei; Shou, Yuntao; Meng, Tao; Yin, Nan; Li, Keqin				Shou, Yuntao/MFJ-1674-2025; Meng, Tao/LTD-6470-2024						DER-GCN: Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialogue Emotion Recognition								Arxiv											2	2;2024-08-31;https://www.arxiv.org/abs/2312.10579v2| 1;2023-12-17;https://www.arxiv.org/abs/2312.10579v1	arXiv:2312.10579			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 31 2024	2024	With the continuous development of deep learning (DL), the task of multimodal dialogue emotion recognition (MDER) has recently received extensive research attention, which is also an essential branch of DL. The MDER aims to identify the emotional information contained in different modalities, e.g., text, video, and audio, in different dialogue scenes. However, existing research has focused on modeling contextual semantic information and dialogue relations between speakers while ignoring the impact of event relations on emotion. To tackle the above issues, we propose a novel Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Emotion Recognition (DER-GCN) method. It models dialogue relations between speakers and captures latent event relations information. Specifically, we construct a weighted multi-relationship graph to simultaneously capture the dependencies between speakers and event relations in a dialogue. Moreover, we also introduce a Self-Supervised Masked Graph Autoencoder (SMGAE) to improve the fusion representation ability of features and structures. Next, we design a new Multiple Information Transformer (MIT) to capture the correlation between different relations, which can provide a better fuse of the multivariate information between relations. Finally, we propose a loss optimization strategy based on contrastive learning to enhance the representation learning ability of minority class features. We conduct extensive experiments on the IEMOCAP and MELD benchmark datasets, which verify the effectiveness of the DER-GCN model. The results demonstrate that our model significantly improves both the average accuracy and the f1 value of emotion recognition.																																	2024-09-11	PPRN:86688591		
J	Hatamizadeh, Ali; Song, Jiaming; Liu, Guilin; Kautz, Jan; Vahdat, Arash				Song, Jiaming/KLC-6750-2024						DiffiT: Diffusion Vision Transformers for Image Generation								Arxiv											3	3;2024-08-29;https://www.arxiv.org/abs/2312.02139v3| 2;2024-04-01;https://www.arxiv.org/abs/2312.02139v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02139v1	arXiv:2312.02139			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Aug 29 2024	2024	Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet256 dataset while having 19.85%, 16.88% less parameters than other Transformer-based diffusion models such as MDT and DiT,respectively. 																																	2024-09-23	PPRN:86379388		
J	Wills, Adam; Hsieh, Min-Hsiu; Yamasaki, Hayata										Constant-Overhead Magic State Distillation								Arxiv											1	1;2024-08-21;https://www.arxiv.org/abs/2408.07764v2	arXiv:2408.07764			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 21 2024	2024	Magic state distillation is a crucial yet resource-intensive process in fault-tolerant quantum computation. The protocol’s overhead, defined as the number of input magic states required per output magic state with an error rate below ε, typically grows as O (log γ (1/ε)). Achieving smaller overheads, i.e., smaller exponents γ, is highly desirable; however, all existing protocols require polylogarithmically growing overheads with some γ > 0, and identifying the smallest achievable exponent γ for distilling magic states of qubits has remained challenging. To address this issue, we develop magic state distillation protocols for qubits with efficient, polynomial-time decoding that achieve an O(1) overhead, meaning the optimal exponent γ = 0; this improves over the previous best of γ ≈ 0.678 due to Hastings and Haah. In our construction, we employ algebraic geometry codes to explicitly present asymptotically good quantum codes for 210-dimensional qudits that support transversally implementable logical gates in the third level of the Clifford hierarchy. The use of asymptotically good codes with non-vanishing rate and relative distance leads to the constant overhead. These codes can be realised by representing each 210-dimensional qudit as a set of 10 qubits, using stabiliser operations on qubits. The 10-qubit magic states distilled with these codes can be converted to and from conventional magic states for the controlled-controlled-Z (CCZ) and T gates on qubits with only a constant overhead loss, making it possible to achieve constant-overhead distillation of such standard magic states for qubits. These results resolve the fundamental open problem in quantum information theory concerning the construction of magic state distillation protocols with the optimal exponent.																																	2024-08-31	PPRN:91500060		
J	Aryabumi, Viraat; Su, Yixuan; Ma, Raymond; Morisot, Adrien; Zhang, Ivan; Locatelli, Acyr; Fadaee, Marzieh; Ustun, Ahmet; Hooker, Sara										To Code, or Not To Code? Exploring Impact of Code in Pre-training								Arxiv											1	1;2024-08-20;https://www.arxiv.org/abs/2408.10914v1	arXiv:2408.10914			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Aug 20 2024	2024	Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs’ performance, there is only limited work analyzing the precise impact of code on non-code tasks. In this work, we systematically investigate the impact of code data on general performance. We ask “ what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation ”. We conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, we find a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the addition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. Our work suggests investments in code quality and preserving code during pre-training have positive impacts.																																	2024-08-31	PPRN:91499450		
J	Nechaev, Dmitry; Pchelnikov, Alexey; Ivanova, Ekaterina				Ivanova, Ekaterina/AAE-1154-2019; Nechaev, Dmitry/IXN-7737-2023						Hibou: A Family of Foundational Vision Transformers for Pathology								Arxiv											2	2;2024-08-20;https://www.arxiv.org/abs/2406.05074v2| 1;1800-01-01;https://www.arxiv.org/abs/2406.05074v1	arXiv:2406.05074			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 20 2024	2024	Pathology, the microscopic examination of diseased tissue, is critical for diagnosing various medical conditions, particularly cancers. Traditional methods are labor-intensive and prone to human error. Digital pathology, which converts glass slides into high-resolution digital images for analysis by computer algorithms, revolutionizes the field by enhancing diagnostic accuracy, consistency, and efficiency through automated image analysis and large-scale data processing. Foundational transformer pretraining is crucial for developing robust, generalizable models as it enables learning from vast amounts of unannotated data. This paper introduces the Hibou family of foundational vision transformers for pathology, leveraging the DINOv2 framework to pretrain two model variants, Hibou-B and Hibou-L, on a proprietary dataset of over 1 million whole slide images (WSIs) representing diverse tissue types and staining techniques. Our pretrained models demonstrate superior performance on both patch-level and slide-level benchmarks, surpassing existing state-of-the-art methods. Notably, Hibou-L achieves the highest average accuracy across multiple benchmark datasets. To support further research and application in the field, we have open-sourced the Hibou models, which can be accessed at https://github.com/HistAI/hibou.																																	2024-08-30	PPRN:89232648		
J	Yu, Dianhai; Shen, Liang; Hao, Hongxiang; Gong, Weibao; Wu, Huachao; Bian, Jiang; Dai, Lirong; Xiong, Haoyi				Ma, Yanjun/ABF-9535-2020; Yu, Dianhai/LXV-2647-2024; XIONG, HAOYI/E-5079-2015						MoESys: A Distributed and Efficient Mixture-of-Experts Training and Inference System for Internet Services								Arxiv											2	2;2024-08-12;https://www.arxiv.org/abs/2205.10034v3| 1;2022-05-20;https://www.arxiv.org/abs/2205.10034v1	arXiv:2205.10034			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 12 2024	2024	While modern internet services, such as chatbots, search engines, and online advertising, demand the use of large-scale deep neural networks (DNNs), distributed training and inference over heterogeneous computing systems are desired to facilitate these DNN models. Mixture-of-Experts (MoE) is one the most common strategies to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present a novel MoESys that boosts efficiency in both large-scale training and inference. Specifically, in the training procedure, the proposed MoESys adopts an Elastic MoE training strategy with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms. For scalable inference in a single node, especially when the model size is larger than GPU memory, MoESys builds the CPU-GPU memory jointly into a ring of sections to load the model, and executes the computation tasks across the memory sections in a round-robin manner for efficient inference. We carried out extensive experiments to evaluate MoESys, where MoESys successfully trains a Unified Feature Optimization (UFO) model with a Sparsely-Gated Mixture-of-Experts model of 12B parameters in 8 days on 48 A100 GPU cards. The comparison against the state-of-the-art shows that MoESys outperformed DeepSpeed with 33% higher throughput (tokens per second) in training and 13% higher throughput in inference in general. Particularly, under unbalanced MoE Tasks, e.g., UFO, MoESys achieved 64% higher throughput with 18% lower memory footprints.																																	2024-08-21	PPRN:12922788		
J	Kumar, Shachi H; Sahay, Saurav; Mazumder, Sahisnu; Okur, Eda; Manuvinakurike, Ramesh; Beckage, Nicole; Su, Hsuan; Lee, Hung-yi; Nachman, Lama										Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models								Arxiv											1	1;2024-08-07;https://www.arxiv.org/abs/2408.03907v1	arXiv:2408.03907			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 07 2024	2024	Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLMbased bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short. We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.																																	2024-08-17	PPRN:91278503		
J	Hu, Hanxu; Lu, Hongyuan; Zhang, Huajian; Song, Yun-Ze; Lam, Wai; Zhang, Yue				Lam, Wai/GNW-3026-2022; Zhang, Huajian/GRJ-3291-2022						Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models								Arxiv											3	3;2024-08-05;https://www.arxiv.org/abs/2305.10276v7| 2;2023-10-04;https://www.arxiv.org/abs/2305.10276v6| 1;2023-05-17;https://www.arxiv.org/abs/2305.10276v1	arXiv:2305.10276			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 05 2024	2024	In this paper, we first investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act or reason correspondingly in text. By evaluating on classic spatial planning scenarios, we found that current LLMs still lack abilities to handle spatial relationships in texts. This arises a question: Is the natural language the best way to represent complex spatial environments for LLMs, or are other alternatives such as symbolic representations more efficient and effective for LLMs? To this end, we propose a novel method called COS  (Chain Chain-of o f-Symbol S ymbol Prompting) that represents the spatial relationships with condensed symbols during the chained intermediate thinking steps. COS  is easy to use and does not need additional training on LLMs. Extensive experiments indicate that COS  clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting described in natural language in all three spatial reasoning and planning tasks with even fewer tokens used in the inputs compared with CoT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for GPT-3.5-Turbo. COS also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World. Interestingly, we also observed emergent ability of abstract symbols understanding when the size of models scales up. 1																																	2024-08-09	PPRN:70015640		
J	Decoppet, Thibault D.										Drinfeld Centers and Morita Equivalence Classes of Fusion 2-Categories								Arxiv											2	2;2024-08-03;https://www.arxiv.org/abs/2211.04917v4| 1;2023-01-27;https://www.arxiv.org/abs/2211.04917v2	arXiv:2211.04917			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 03 2024	2024	We prove that the Drinfeld center of a fusion 2-category is invariant under Morita equivalence. We go on to show that the concept of Morita equivalence between connected fusion 2-categories corresponds to a notion of Witt equivalence between braided fusion 1-categories. A strongly fusion 2-category is a fusion 2-category whose braided fusion 1-category of endomorphisms of the monoidal unit is Vect or SVect. . We prove that every fusion 2-category is Morita equivalent to the 2-Deligne tensor product of a strongly fusion 2-category and an invertible fusion 2-category. We proceed to show that every fusion 2-category is Morita equivalent to a connected fusion 2-category. As a consequence, we find that every rigid algebra in a fusion 2-category is separable. This implies in particular that every fusion 2-category is separable. Conjecturally, separability ensures that a fusion 2-category is 4-dualizable. We define the dimension of a fusion 2-category, and prove that it is always non-zero. Finally, we show that the Drinfeld center of any fusion 2-category is a finite semisimple 2-category.																																	2024-08-11	PPRN:36024212		
J	Setty, Spurthi; Thakkar, Harsh; Lee, Alyssa; Chung, Eden; Vidra, Natan										Improving Retrieval for RAG based Question Answering Models on Financial Documents								Arxiv											2	2;2024-08-01;https://www.arxiv.org/abs/2404.07221v2| 1;2024-03-23;https://www.arxiv.org/abs/2404.07221v1	arXiv:2404.07221			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 01 2024	2024	The effectiveness of Large Language Models (LLMs) in generating accurate responses relies heavily on the quality of input provided, particularly when employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by sourcing the most relevant text chunk(s) to base queries upon. Despite the significant advancements in LLMs' response quality in recent years, users may still encounter inaccuracies or irrelevant answers; these issues often stem from suboptimal text chunk retrieval by RAG rather than the inherent capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine the RAG process. This paper explores the existing constraints of RAG pipelines and introduces methodologies for enhancing text retrieval. It delves into strategies such as sophisticated chunking techniques, query expansion, the incorporation of metadata annotations, the application of re-ranking algorithms, and the fine-tuning of embedding algorithms. Implementing these approaches can substantially improve the retrieval quality, thereby elevating the overall performance and reliability of LLMs in processing and responding to queries.																																	2024-08-08	PPRN:88501676		
J	He, Xuehai; Feng, Weixi; Zheng, Kaizhi; Lu, Yujie; Zhu, Wanrong; Li, Jiachen; Fan, Yue; Wang, Jianfeng; Li, Linjie; Yang, Zhengyuan; Lin, Kevin; Wang, William Yang; Wang, Lijuan; Wang, Xin Eric				Jiao, Licheng/JOZ-0842-2023; 李, 李林洁/JAD-1884-2023; Yang, Zhengyuan/AGQ-1232-2022; Lin, Kevin/JFS-1634-2023; Wang, Xin/ABD-3905-2020; He, Xuehai/AAB-1484-2022; Feng, Weixi/MGV-0350-2025						MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos								Arxiv											2	2;2024-07-30;https://www.arxiv.org/abs/2406.08407v3| 1;2024-06-13;https://www.arxiv.org/abs/2406.08407v2	arXiv:2406.08407			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 30 2024	2024	Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of "world models" -- interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities. To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding. MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc. MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.																																	2024-08-06	PPRN:89294234		
J	Liu, Zheyuan; Dou, Guangyao; Tan, Zhaoxuan; Tian, Yijun; Jiang, Meng										Machine Unlearning in Generative AI: A Survey								Arxiv											1	1;2024-07-30;https://www.arxiv.org/abs/2407.20516v1	arXiv:2407.20516			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jul 30 2024	2024	Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. 																																	2024-08-06	PPRN:91159563		
J	Li, Boyan; Luo, Yuyu; Chai, Chengliang; Li, Guoliang; Tang, Nan				LI, Boyan/KJM-0700-2024; luo, yuyu/IRY-8789-2023; Li, Guoliang/M-6614-2014						The Dawn of Natural Language to SQL: Are We Fully Ready?								Arxiv											3	3;2024-07-27;https://www.arxiv.org/abs/2406.01265v3| 2;2024-07-21;https://www.arxiv.org/abs/2406.01265v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01265v1	arXiv:2406.01265			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 27 2024	2024	Translating users' natural language questions into SQL queries (i.e., NL2SQL) significantly lowers the barriers to accessing relational databases. The emergence of Large Language Models has introduced a novel paradigm in NL2SQL tasks, enhancing capabilities dramatically. However, this raises a critical question: Are we fully prepared to deploy NL2SQL models in production? To address the posed questions, we present a multi-angle NL2SQL evaluation framework, NL2SQL360, to facilitate the design and test of new NL2SQL methods for researchers. Through NL2SQL360, we conduct a detailed comparison of leading NL2SQL methods across a range of application scenarios, such as different data domains and SQL characteristics, offering valuable insights for selecting the most appropriate NL2SQL methods for specific needs. Moreover, we explore the NL2SQL design space, leveraging NL2SQL360 to automate the identification of an optimal NL2SQL solution tailored to user-specific needs. Specifically, NL2SQL360 identifies an effective NL2SQL method, SuperSQL, distinguished under the Spdier dataset using the execution accuracy metric. Remarkably, SuperSQL achieves competitive performance with execution accuracy of 87% and 62.66% on the Spider and BIRD test sets, respectively.																																	2024-08-06	PPRN:89132232		
J	Lin, Xinyu; Wang, Wenjie; Li, Yongqi; Feng, Fuli; Ng, See-Kiong; Chua, Tat-Seng				Wang, Meng/AEZ-9059-2022						Bridging Items and Language: A Transition Paradigm for Large Language Model-Based Recommendation								Arxiv											2	2;2024-07-25;https://www.arxiv.org/abs/2310.06491v2| 1;2023-10-10;https://www.arxiv.org/abs/2310.06491v1	arXiv:2310.06491			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 25 2024	2024	Harnessing Large Language Models (LLMs) for recommendation is rapidly emerging, which relies on two fundamental steps to bridge the recommendation item space and the language space: 1) item indexing utilizes identifiers to represent items in the language space, and 2) generation grounding associates LLMs' generated token sequences to in-corpus items. However, previous methods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g., numeric IDs) and description-based identifiers (e.g., titles) either lose semantics or lack adequate distinctiveness. Moreover, prior generation grounding methods might generate invalid identifiers, thus misaligning with in-corpus items.    To address these issues, we propose a novel Transition paradigm for LLM-based Recommender (named TransRec) to bridge items and language. Specifically, TransRec presents multi-facet identifiers, which simultaneously incorporate ID, title, and attribute for item indexing to pursue both distinctiveness and semantics. Additionally, we introduce a specialized data structure for TransRec to ensure generating valid identifiers only and utilize substring indexing to encourage LLMs to generate from any position of identifiers. Lastly, TransRec presents an aggregated grounding module to leverage generated multi-facet identifiers to rank in-corpus items efficiently. We instantiate TransRec on two backbone models, BART-large and LLaMA-7B. Extensive results on three real-world datasets under diverse settings validate the superiority of TransRec.																																	2024-08-02	PPRN:85525472		
J	Ramos, Mayk Caldas; Collison, Christopher J.; White, Andrew D.				Caldas Ramos, Mayk/NAX-8075-2025						A Review of Large Language Models and Autonomous Agents in Chemistry								Arxiv											2	2;2024-07-25;https://www.arxiv.org/abs/2407.01603v2| 1;2024-06-26;https://www.arxiv.org/abs/2407.01603v1	arXiv:2407.01603			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 25 2024	2024	Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation. We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains. This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry. Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods.																																	2024-08-02	PPRN:90668887		
J	Vikram, Vasudev; Lemieux, Caroline; Sunshine, Joshua; Padhye, Rohan				Vikram, Vasudev/KHU-6860-2024						Can Large Language Models Write Good Property-Based Tests?								Arxiv											2	2;2024-07-22;https://www.arxiv.org/abs/2307.04346v2| 1;2023-07-10;https://www.arxiv.org/abs/2307.04346v1	arXiv:2307.04346			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 22 2024	2024	Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for PBTs. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we investigate using modern LLMs to automatically synthesize PBTs using two prompting techniques. A key challenge is to rigorously evaluate the LLM-synthesized PBTs. We propose a methodology to do so considering several properties of the generated tests: (1) validity, (2) soundness, and (3) property coverage, a novel metric that measures the ability of the PBT to detect property violations through generation of property mutants. In our evaluation on 40 Python library API methods across three models (GPT-4, Gemini-1.5-Pro, Claude-3-Opus), we find that with the best model and prompting approach, a valid and sound PBT can be synthesized in 2.4 samples on average. We additionally find that our metric for determining soundness of a PBT is aligned with human judgment of property assertions, achieving a precision of 100% and recall of 97%. Finally, we evaluate the property coverage of LLMs across all API methods and find that the best model (GPT-4) is able to automatically synthesize correct PBTs for 21% of properties extractable from API documentation.																																	2024-07-27	PPRN:73864940		
J	de Blas, Jorge; Du, Yong; Grojean, Christophe; Gu, Jiayin; Miralles, Victor; Peskin, Michael E.; Tian, Junping; Vos, Marcel; Vryonidou, Eleni				Du, Yong/HPF-7511-2023; Vos, Marcel/G-8123-2015; de Blas, Jorge/AAA-8681-2020; Howard, Michael/F-1587-2019; Vryonidou, Eleni/JVN-6700-2024						Global SMEFT Fits at Future Colliders								Arxiv											3	3;2024-07-19;https://www.arxiv.org/abs/2206.08326v5| 2;2024-07-18;https://www.arxiv.org/abs/2206.08326v4| 1;2022-06-16;https://www.arxiv.org/abs/2206.08326v1	arXiv:2206.08326			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 19 2024	2024	Based on the framework of Standard Model Effective Field Theory, we performed a few global fits, each containing a subset of dimension-6 operators, for the measurements that are expected at future colliders. The fit for the Higgs and electroweak sector improves what has been done for the European Strategy Update in 2020 on both EFT treatments and experimental inputs. A new comprehensive fit is performed focusing on 4-fermion interactions at future colliders. Top-quark sector is studied in a dedicated fit which restricts the operators and measurements to be directly related to top-quark. A small subset of CP-violating operators involving bosonic fields alone are also investigated. Various running scenarios for future e+ e- and Muon Colliders that are suggested in the Snowmass 2021 discussion are considered in the global fits. The outcomes from each fit are expressed in terms of either direct constraint on Wilson Coefficients or precision on Higgs and electroweak effective couplings.																																	2024-07-27	PPRN:12222253		
J	Fan, Yue; Ma, Xiaojian; Wu, Rujie; Du, Yuntao; Li, Jiaqi; Gao, Zhi; Li, Qing										<italic>VideoAgent</italic>: A Memory-augmented Multimodal Agent for Video Understanding								Arxiv											2	2;2024-07-15;https://www.arxiv.org/abs/2403.11481v2| 1;2024-03-18;https://www.arxiv.org/abs/2403.11481v1	arXiv:2403.11481			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 15 2024	2024	We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos. In particular, the proposed multimodal agent VideoAgent: 1) constructs a structured memory to store both the generic temporal event descriptions and object-centric tracking states of the video; 2) given an input task query, it employs tools including video segment localization and object memory querying along with other visual foundation models to interactively solve the task, utilizing the zero-shot tool-use ability of LLMs. VideoAgent demonstrates impressive performances on several long-horizon video understanding benchmarks, an average increase of 6.6% on NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between open-sourced models and private counterparts including Gemini 1.5 Pro.																																	2024-07-23	PPRN:88190147		
J	Mollenhauer, Mattes; Muecke, Nicole; Sullivan, T.J.										Learning linear operators: Infinite-dimensional regression as a well-behaved non-compact inverse problem								Arxiv											2	2;2024-07-10;https://www.arxiv.org/abs/2211.08875v3| 1;2022-11-16;https://www.arxiv.org/abs/2211.08875v2	arXiv:2211.08875			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 10 2024	2024	We consider the problem of learning a linear operator $theta$ between two Hilbert spaces from empirical observations, which we interpret as least squares regression in infinite dimensions. We show that this goal can be reformulated as an inverse problem for $theta$ with the feature that its forward operator is generally non-compact (even if $theta$ is assumed to be compact or of $p$-Schatten class). However, we prove that, in terms of spectral properties and regularisation theory, this inverse problem is equivalent to the known compact inverse problem associated with scalar response regression. Our framework allows for the elegant derivation of dimension-free rates for generic learning algorithms under Hölder-type source conditions. The proofs rely on the combination of techniques from kernel regression with recent results on concentration of measure for sub-exponential Hilbertian random variables. The obtained rates hold for a variety of practically-relevant scenarios in functional regression as well as nonlinear regression with operator-valued kernels and match those of classical kernel regression with scalar response.																																	2024-07-21	PPRN:43614181		
J	Zhu, Yinghao; Peng, Di; Zhang, Enkang; Pan, Bingying; Chen, Xu; Chen, Lixing; Ren, Huifen; Liu, Feiyang; Hao, Yiqing; Li, Nana; Xing, Zhenfang; Lan, Fujun; Han, Jiyuan; Wang, Junjie; Jia, Donghan; Wo, Hongliang; Gu, Yiqing; Gu, Yimeng; Ji, Li; Wang, Wenbin; Gou, Huiyang; Shen, Yao; Ying, Tianping; Chen, Xiaolong; Yang, Wenge; Cao, Huibo; Zheng, Changlin; Zeng, Qiaoshi; Guo, Jian-gang; Zhao, Jun				Chen, Lixing/ABB-7931-2020; zeng, qiaoshi/V-8250-2019; ZHANG, ENKANG/NXY-2079-2025; Shen, Yao/GPS-4936-2022; Guo, Jiangang/S-8572-2019; Han, Jiyuan/O-9282-2019; Wang, Wenbin/ABI-4704-2020; Cao, Huibo/A-6835-2016; Chen, Xiaolong/OKT-1351-2025; Xu, Yang/AAI-3566-2021; Wang, Junjie/JHT-6886-2023						Superconductivity in pressurized trilayer La$_4$Ni$_3$O$_{10-{delta}}$ single crystals								Arxiv											3	3;2024-07-09;https://www.arxiv.org/abs/2311.07353v3| 2;2024-01-02;https://www.arxiv.org/abs/2311.07353v2| 1;2023-11-13;https://www.arxiv.org/abs/2311.07353v1	arXiv:2311.07353			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jul 09 2024	2024	The pursuit of discovering new high-temperature superconductors that diverge from the copper-based paradigm1-3 carries profound implications for elucidating mechanisms behind superconductivity and may also enable new applications4-8. Here, our investigation reveals that application of pressure effectively suppresses the spin and charge order in trilayer nickelate La4Ni3O10-{delta} single crystals, leading to the emergence of superconductivity with a maximum critical temperature (Tc) of around 30 K at 69.0 GPa. The DC susceptibility measurements confirm a substantial diamagnetic response below Tc, indicating the presence of bulk superconductivity with a volume fraction exceeding 80%. In the normal state, we observe a "strange metal" behavior, characterized by a linear temperature-dependent resistance extending up to 300 K. Furthermore, the layer-dependent superconductivity observed hints at a unique interlayer coupling mechanism specific to nickelates, setting them apart from cuprates in this regard. Our findings provide crucial insights into the fundamental mechanisms underpinning superconductivity, while also introducing a new material platform to explore the intricate interplay between the spin/charge order, flat band structures, interlayer coupling, strange metal behavior and high-temperature superconductivity.																																	2024-07-21	PPRN:86130238		
J	Wang, Xinpeng; Ma, Bolei; Hu, Chengzhi; Weber-Genzel, Leon; Roettger, Paul; Kreuter, Frauke; Hovy, Dirk; Plank, Barbara				wang, xinpeng/KVB-4927-2024; Hovy, Dirk/MVX-7752-2025						"My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models								Arxiv											2	2;2024-07-04;https://www.arxiv.org/abs/2402.14499v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.14499v1	arXiv:2402.14499			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.																																	2024-07-20	PPRN:87806453		
J	Li, Zelong; Xu, Shuyuan; Mei, Kai; Hua, Wenyue; Rama, Balaji; Raheja, Om; Wang, Hao; Zhu, He; Zhang, Yongfeng				MEI, KAIYUAN/LFV-3617-2024; Xu, Shuyuan/KBP-9666-2024; Li, Zelong/JHV-1269-2023						AutoFlow: Automated Workflow Generation for Large Language Model Agents								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.12821v1	arXiv:2407.12821			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 01 2024	2024	Recent advancements in Large Language Models (LLMs) have shown significant progress in understanding complex natural language. One important application of LLM is LLM-based AI Agent, which leverages the ability of LLM as well as external tools for complex-task solving. To make sure LLM Agents follow an effective and reliable procedure to solve the given task, manually designed workflows are usually used to guide the working mechanism of agents. However, manually designing the workflows requires considerable efforts and domain knowledge, making it difficult to develop and deploy agents on massive scales. To address these issues, we propose AutoFlow, a framework designed to automatically generate workflows for agents to solve complex tasks. AutoFlow takes natural language program as the format of agent workflow and employs a workflow optimization procedure to iteratively optimize the workflow quality. Besides, this work offers two workflow generation methods: fine-tuning-based and in-context-based methods, making the AutoFlow framework applicable to both open-source and closed-source LLMs. Experimental results show that our framework can produce robust and reliable agent workflows. We believe that the automatic generation and interpretation of workflows in natural language represent a promising paradigm for solving complex tasks, particularly with the rapid development of LLMs.																																	2024-07-26	PPRN:90886117		
J	Phelps, Steve; Russell, Yvan I.				Russell, Yvan/LWJ-6673-2024						The Machine Psychology of Cooperation: Can GPT models operationalise prompts for altruism, cooperation, competitiveness and selfishness in economic games?								Arxiv											2	2;2024-06-29;https://www.arxiv.org/abs/2305.07970v2| 1;2023-05-13;https://www.arxiv.org/abs/2305.07970v1	arXiv:2305.07970			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 29 2024	2024	We investigated the capability of the GPT-3.5 large language model (LLM) to operationalize natural language descriptions of cooperative, competitive, altruistic, and self -interested behavior in two social dilemmas: the repeated Prisoners Dilemma and the one-shot Dictator Game. Using a withinsubject experimental design, we used a prompt to describe the task environment using a similar protocol to that used in experimental psychology studies with human subjects. We tested our research question by manipulating the part of our prompt which was used to create a simulated persona with different cooperative and competitive stances. We then assessed the resulting simulacras’ level of cooperation in each social dilemma, taking into account the effect of different partner conditions for the repeated game. Our results provide evidence that LLMs can, to some extent, translate natural language descriptions of different cooperative stances into corresponding descriptions of appropriate task behaviour, particularly in the one-shot game. There is some evidence of behaviour resembling conditional reciprocity for the cooperative simulacra in the repeated game, and for the later version of the model there is evidence of altruistic behaviour. Our study has potential implications for using LLM chatbots in task environments that involve cooperation, e.g. using chatbots as mediators and facilitators in public -goods negotiations.																																	2024-07-18	PPRN:69648372		
J	Kudler-Flam, Jonah; Leutheusser, Samuel; Satishchandran, Gautam				Satishchandran, Gautam/ABC-7014-2021						Generalized Black Hole Entropy is von Neumann Entropy								Arxiv											4	4;2024-06-27;https://www.arxiv.org/abs/2309.15897v4| 3;2024-01-05;https://www.arxiv.org/abs/2309.15897v3| 2;2023-10-24;https://www.arxiv.org/abs/2309.15897v2| 1;2023-09-27;https://www.arxiv.org/abs/2309.15897v1	arXiv:2309.15897			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 27 2024	2024	It has been argued that, while the individual terms in the generalized entropy, S gen. = A/4GN 4 G N + S ext. , are ill-defined in the semiclassical limit, their sum is well-defined if one takes into account perturbative quantum gravitational effects. The first term diverges as G N → 0 and the second diverges due to the infinite entanglement across the horizon which is characteristic of Type III von Neumann algebras. It was recently shown that the von Neumann algebra of observables “gravitationally dressed” to the mass of a Schwarzschild-AdS black hole or the energy of an observer in de Sitter spacetime admit a well-defined trace. The algebras are Type IIoc oc (which does not admit a maximum entropy state) and Type II1 1 (which admits a maximum entropy state) respectively and the von Neumann entropy of “semiclassical” states was found to be (up to an additive constant) the generalized entropy. However, these arguments rely on the existence of a stationary “equilibrium (KMS) state” and do not apply to, for example, black holes formed from gravitational collapse, Kerr black holes, or black holes in asymptotically de Sitter spacetime. These spacetimes are stationary but not in thermal equilibrium. In this paper, we present a general framework for obtaining the algebra of “gravitationally dressed” observables for a linear, Klein -Gordon field on any spacetime with a (bifurcate) Killing horizon. We prove, assuming the existence of a stationary state — which is not necessarily KMS — and suitable asymptotic decay of solutions, a “structure theorem” that the algebra of “gravitationally dressed” observables always contains a Type II factor of observables “localized” on the horizon. These assumptions have been rigorously proven in most cases of interest in this paper. Applying our general framework to the algebra of observables in the exterior of an asymptotically flat Kerr black hole where the fields are dressed to the black hole mass and angular momentum we find that the algebra is the product of a Type IIoc oc algebra on the horizon and a Type I oc algebra at past null infinity. The full algebra is Type IIoc oc and the von Neumann entropy of semiclassical states is the generalized entropy. In the case of Schwarzschild-de Sitter, despite the fact that we must introduce an observer, the algebra of observables dressed to the perturbed areas of the black hole and cosmological horizons is the product of Type IIoc oc algebras on each horizon. The entropy of semiclassical states is given by the sum of the areas of the two horizons as well as the entropy of quantum fields in between the horizons. Our results suggest that in all cases where there exists another “boundary structure” (e.g., an asymptotic boundary or another Killing horizon) the algebra of observables is Type IIoc oc and in the absence of such structures (e.g. de Sitter spacetime) the algebra is Type II1. 1 .																																	2024-07-15	PPRN:85321680		
J	Guo, Xun; Zheng, Mingwu; Hou, Liang; Gao, Yuan; Deng, Yufan; Wan, Pengfei; Zhang, Di; Liu, Yufan; Hu, Weiming; Zha, Zhengjun; Huang, Haibin; Ma, Chongyang				Zha, Zheng-Jun/AAE-8408-2020; Liu, Yufan/KHU-5787-2024; hu, weiming/IQW-3171-2023; Huang, Haibin/HHZ-1901-2022						I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models								Arxiv											4	4;2024-06-26;https://www.arxiv.org/abs/2312.16693v4| 3;2024-05-14;https://www.arxiv.org/abs/2312.16693v3| 2;2024-01-30;https://www.arxiv.org/abs/2312.16693v2| 1;2023-12-27;https://www.arxiv.org/abs/2312.16693v1	arXiv:2312.16693			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 26 2024	2024	Text-guided image-to-video (I2V) generation aims to generate a coherent video that preserves the identity of the input image and semantically aligns with the input prompt. Existing methods typically augment pretrained text-to-video (T2V) models by either concatenating the image with noised video frames channel-wise before being fed into the model or injecting the image embedding produced by pretrained image encoders in cross-attention modules. However, the former approach often necessitates altering the fundamental weights of pretrained T2V models, thus restricting the model's compatibility within the open-source communities and disrupting the model's prior knowledge. Meanwhile, the latter typically fails to preserve the identity of the input image. We present I2V-Adapter to overcome such limitations. I2V-Adapter adeptly propagates the unnoised input image to subsequent noised frames through a cross-frame attention mechanism, maintaining the identity of the input image without any changes to the pretrained T2V model. Notably, I2V-Adapter only introduces a few trainable parameters, significantly alleviating the training cost and also ensures compatibility with existing community-driven personalized models and control tools. Moreover, we propose a novel Frame Similarity Prior to balance the motion amplitude and the stability of generated videos through two adjustable control coefficients. Our experimental results demonstrate that I2V-Adapter is capable of producing high-quality videos. This performance, coupled with its agility and adaptability, represents a substantial advancement in the field of I2V, particularly for personalized and controllable applications.																																	2024-07-17	PPRN:86852184		
J	Dahl, Matthew; Magesh, Varun; Suzgun, Mirac; Ho, Daniel E.										Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models								Arxiv											2	2;2024-06-21;https://www.arxiv.org/abs/2401.01301v2| 1;2024-01-02;https://www.arxiv.org/abs/2401.01301v1	arXiv:2401.01301			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 21 2024	2024	Do large language models (LLMs) know the law? LLMs are increasingly being used to augment legal practice, education, and research, yet their revolutionary potential is threatened by the presence of “hallucinations”—textual output that is not consistent with legal facts. We present the first systematic evidence of these hallucinations in public -facing LLMs, documenting trends across jurisdictions, courts, time periods, and cases. Using OpenAI’s ChatGPT 4 and other public models, we show that LLMs hallucinate at least 58% of the time, struggle to predict their own hallucinations, and often uncritically accept users’ incorrect legal assumptions. We conclude by cautioning against the rapid and unsupervised integration of popular LLMs into legal tasks, and we develop a typology of legal hallucinations to guide future research in this area.																																	2024-07-11	PPRN:86913856		
J	Li, Yaowei; Wang, Xintao; Zhang, Zhaoyang; Wang, Zhouxia; Yuan, Ziyang; Xie, Liangbin; Zou, Yuexian; Shan, Ying				Zhang, Yang/I-4284-2014						Image Conductor: Precision Control for Interactive Video Synthesis								Arxiv											1	1;2024-06-21;https://www.arxiv.org/abs/2406.15339v1	arXiv:2406.15339			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 21 2024	2024	Filmmaking and animation production often require sophisticated techniques for coordinating camera transitions and object movements, typically involving labor-intensive real-world capturing. Despite advancements in generative AI for video creation, achieving precise control over motion for interactive video asset generation remains challenging. To this end, we propose Image Conductor, a method for precise control of camera transitions and object movements to generate video assets from a single image. An well-cultivated training strategy is proposed to separate distinct camera and object motion by camera LoRA weights and object LoRA weights. To further address cinematographic variations from ill-posed trajectories, we introduce a camera-free guidance technique during inference, enhancing object movements while eliminating camera transitions. Additionally, we develop a trajectory-oriented video motion data curation pipeline for training. Quantitative and qualitative experiments demonstrate our method's precision and fine-grained control in generating motion-controllable videos from images, advancing the practical application of interactive video synthesis.																																	2024-07-11	PPRN:89399693		
J	Chen, Sijin; Chen, Xin; Pang, Anqi; Zeng, Xianfang; Cheng, Wei; Fu, Yijun; Yin, Fukun; Wang, Yanru; Wang, Zhibin; Zhang, Chi; Yu, Jingyi; Yu, Gang; Fu, Bin; Chen, Tao				cheng, weihao/MUO-0523-2025; Chen, Xin/AAE-5265-2020; Yin, Fukun/KVB-6177-2024; Chen, Tao/IQV-1588-2023						MeshXL: Neural Coordinate Field for Generative 3D Foundation Models								Arxiv											2	2;2024-06-18;https://www.arxiv.org/abs/2405.20853v2| 1;2024-05-31;https://www.arxiv.org/abs/2405.20853v1	arXiv:2405.20853			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 18 2024	2024	The polygon mesh representation of 3D data exhibits great flexibility, fast rendering speed, and storage efficiency, which is widely preferred in various applications. However, given its unstructured graph representation, the direct generation of high-fidelity 3D meshes is challenging. Fortunately, with a pre-defined ordering strategy, 3D meshes can be represented as sequences, and the generation process can be seamlessly treated as an auto-regressive problem. In this paper, we validate the Neural Coordinate Field (NeurCF), an explicit coordinate representation with implicit neural embeddings, is a simple-yet-effective representation for large-scale sequential mesh modeling. After that, we present MeshXL, a family of generative pre-trained auto-regressive models, which addresses the process of 3D mesh generation with modern large language model approaches. Extensive experiments show that MeshXL is able to generate high-quality 3D meshes, and can also serve as foundation models for various down-stream applications.																																	2024-07-04	PPRN:89127237		
J	Li, Zelong; Hua, Wenyue; Wang, Hao; Zhu, He; Zhang, Yongfeng				Li, Zelong/HZI-3149-2023						Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents								Arxiv											1	1;2024-06-18;https://www.arxiv.org/abs/2402.00798v3	arXiv:2402.00798			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The work is open-sourced at https://github.com/agiresearch/Formal-LLM.																																	2025-08-07	PPRN:123163631		
J	Jin, Weizhao; Yao, Yuhang; Han, Shanshan; Gu, Jiajun; Joe-Wong, Carlee; Ravi, Srivatsan; Avestimehr, Salman; He, Chaoyang				Gu, Jiajun/KXR-0437-2024; Jin, Weizhao/HSI-4349-2023; Yao, Yuhang/GPF-7764-2022; han, shanshan/HNR-9453-2023; Avestimehr, Amir/O-7864-2019						FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System								Arxiv											3	3;2024-06-17;https://www.arxiv.org/abs/2303.10837v3| 2;2023-10-30;https://www.arxiv.org/abs/2303.10837v2| 1;2023-03-20;https://www.arxiv.org/abs/2303.10837v1	arXiv:2303.10837			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	Federated Learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as the aggregated local models on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present FedML-HE, the first practical federated learning system with efficient HE-based secure model aggregation. FedML-HE proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing customizable privacy preservation. Our optimized system demonstrates considerable overhead reduction, particularly for large foundation models (e.g., ~10x reduction for ResNet-50, and up to ~40x reduction for BERT), demonstrating the potential for scalable HE-based FL deployment.																																	2024-07-04	PPRN:46940555		
J	Kou, Siqi; Hu, Lanxiang; He, Zhezhi; Deng, Zhijie; Zhang, Hao										CLLMs: Consistency Large Language Models								Arxiv											3	3;2024-06-13;https://www.arxiv.org/abs/2403.00835v4| 2;2024-03-08;https://www.arxiv.org/abs/2403.00835v3| 1;2024-03-05;https://www.arxiv.org/abs/2403.00835v2	arXiv:2403.00835			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jun 13 2024	2024	Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4 × to 3.4 × improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.																																	2024-07-02	PPRN:88028119		
J	You, Zhiyuan; Gu, Jinjin; Li, Zheyuan; Cai, Xin; Zhu, Kaiwen; Dong, Chao; Xue, Tianfan				You, Zhiyuan/IUP-7379-2023; Zhu, Kaiwen/LRD-1522-2024; CAI, XIN/NIU-4026-2025						Descriptive Image Quality Assessment in the Wild								Arxiv											2	2;2024-06-12;https://www.arxiv.org/abs/2405.18842v2| 1;2024-05-29;https://www.arxiv.org/abs/2405.18842v1	arXiv:2405.18842			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 12 2024	2024	With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released in our project page.																																	2024-07-10	PPRN:89105037		
J	Song, Yixin; Xie, Haotong; Zhang, Zhengyan; Wen, Bo; Ma, Li; Mi, Zeyu; Chen, Haibo				Song, Yixin/IWV-3440-2023; Mi, Zeyu/NBY-3261-2025; Chen, Haibo/HCI-6124-2022; zhengyan, zhang/D-2029-2012						Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters								Arxiv											1	1;2024-06-11;https://www.arxiv.org/abs/2406.05955v2	arXiv:2406.05955			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixtureof -Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5× decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at https://huggingface.co/PowerInfer .																																	2024-06-28	PPRN:89279053		
J	Zhou, Zhi; Shi, Jiang-Xin; Song, Peng-Xiao; Yang, Xiao-Wen; Jin, Yi-Xuan; Guo, Lan-Zhe; Li, Yu-Feng				Shi, Jiang-Xin/OEN-1171-2025; li, yufeng/AAA-8596-2019						LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model								Arxiv											1	1;2024-06-07;https://www.arxiv.org/abs/2406.04614v1	arXiv:2406.04614			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	Large language models (LLMs), including both proprietary and open-source models, have showcased remarkable capabilities in addressing a wide range of downstream tasks. Nonetheless, when it comes to practical Chinese legal tasks, these models fail to meet the actual requirements. Proprietary models do not ensure data privacy for sensitive legal cases, while open-source models demonstrate unsatisfactory performance due to their lack of legal knowledge. To address this problem, we introduce LawGPT, the first open-source model specifically designed for Chinese legal applications. LawGPT comprises two key components: legal-oriented pre-training and legal supervised fine-tuning. Specifically, we employ large-scale Chinese legal documents for legal-oriented pre-training to incorporate legal domain knowledge. To further improve the model's performance on downstream legal tasks, we create a knowledge-driven instruction dataset for legal supervised fine-tuning. Our experimental results demonstrate that LawGPT outperforms the open-source LLaMA 7B model. 																																	2024-06-22	PPRN:89233101		
J	Cheng, Xiang; Chen, Yuxin; Sra, Suvrit				Chen, Yuxin/NBX-0051-2025						Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context								Arxiv											4	4;2024-06-04;https://www.arxiv.org/abs/2312.06528v6| 3;2024-04-19;https://www.arxiv.org/abs/2312.06528v5| 2;2023-12-26;https://www.arxiv.org/abs/2312.06528v3| 1;2023-12-11;https://www.arxiv.org/abs/2312.06528v1	arXiv:2312.06528			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 04 2024	2024	Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient -based learning algorithms under simple parameter configurations. . This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, , which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in -context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned.																																	2024-07-04	PPRN:86539696		
J	Wu, Yanmin; Meng, Jiarui; Li, Haijie; Wu, Chenming; Shi, Yahao; Cheng, Xinhua; Zhao, Chen; Feng, Haocheng; Ding, Errui; Wang, Jingdong; Zhang, Jian				Li, Haijie/HTQ-5093-2023						OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding								Arxiv											1	1;2024-06-04;https://www.arxiv.org/abs/2406.02058v1	arXiv:2406.02058			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 04 2024	2024	This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) capable of 3D point-level open vocabulary understanding. Our primary motivation stems from observing that existing 3DGS-based open vocabulary methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations. To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency. These features exhibit both intra-object consistency and inter-object distinction. Then, we propose a two-stage codebook to discretize these features from coarse to fine levels. At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level. Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features. Extensive experiments, including open vocabulary-based 3D object selection, 3D point cloud understanding, click-based 3D object selection, and ablation studies, demonstrate the effectiveness of our proposed method. 																																	2024-07-11	PPRN:89261203		
J	Cheng, Pengyu; Yang, Yifan; Li, Jian; Dai, Yong; Hu, Tianhao; Cao, Peixin; Du, Nan; Li, Xiaolong										Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game								Arxiv											4	4;2024-06-03;https://www.arxiv.org/abs/2311.08045v4| 3;2024-02-23;https://www.arxiv.org/abs/2311.08045v3| 2;2024-02-19;https://www.arxiv.org/abs/2311.08045v2| 1;2023-11-14;https://www.arxiv.org/abs/2311.08045v1	arXiv:2311.08045			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 03 2024	2024	Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing alignment methods depend on manually annotated preference data to guide the LLM optimization directions. However, continuously updating LLMs for alignment raises a distribution gap between model-generated samples and humanannotated responses, hindering training effectiveness. To mitigate this issue, previous methods require additional preference annotation on newly generated samples to adapt to the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an Adversarial Preference Optimization (APO) framework, in which the LLM and the reward model update alternatively via a minmax game. Through adversarial training, the reward model can adapt to the shifted generation distribution of the LLM without any additional annotation. With comprehensive experiments, we find the proposed adversarial training framework further enhances existing alignment baselines in terms of LLM helpfulness and harmlessness. The code is at https://github.com/Linear95/APO. .																																	2024-06-19	PPRN:86200889		
J	Ghosh, Tathagata; Ghoshal, Anish; Guo, Huai-Ke; Hajkarim, Fazlollah; King, Stephen F; Sinha, Kuver; Wang, Xin; White, Graham				Guo, Huaike/AAY-7856-2020						Did we hear the sound of the Universe boiling? Analysis using the full fluid velocity profiles and NANOGrav 15-year data								Arxiv											2	2;2024-06-01;https://www.arxiv.org/abs/2307.02259v2| 1;2023-07-03;https://www.arxiv.org/abs/2307.02259v1	arXiv:2307.02259			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 01 2024	2024	In this paper, we analyse sound waves arising from a cosmic phase transition where the full velocity profile is taken into account as an explanation for the gravitational wave spectrum observed by multiple pulsar timing array groups. Unlike the broken power law used in the literature, in this scenario the power law after the peak depends on the macroscopic properties of the phase transition, allowing for a better fit with pulsar timing array (PTA) data. We compare the best fit with that obtained using the usual broken power law and, unsurprisingly, find a better fit with the gravitational wave (GW) spectrum that utilizes the full velocity profile. We then discuss models that can produce the best-fit point and complementary probes using CMB experiments and searches for light particles in DUNE, IceCUBE-Gen2, neutrinoless double β-decay, and forward physics facilities at the LHC like FASERν, etc.																																	2024-06-22	PPRN:73800770		
J	Li, Zhaozhou; Dekel, Avishai; Sarkar, Kartick C.; Aung, Han; Giavalisco, Mauro; Mandelker, Nir; Tacchella, Sandro				Li, Zhenwei/AAL-1649-2021; Giavalisco, Mauro/AEV-5974-2022; Tacchella, Sandro/AAT-1602-2021; Dekel, Avishai/ABF-9516-2021						Feedback-Free Starbursts at Cosmic Dawn: Observable Predictions for JWST								Arxiv											2	2;2024-06-01;https://www.arxiv.org/abs/2311.14662v2| 1;2023-11-24;https://www.arxiv.org/abs/2311.14662v1	arXiv:2311.14662			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 01 2024	2024	Aims. We extend the analysis of a physical model within the standard cosmology that robustly predicts a high star-formation efficiency (SFE) in massive galaxies at cosmic dawn due to feedback-free starbursts (FFBs). This model implies an excess of bright galaxies at z ≳ 10 compared to the standard models based on the low SFE at later epochs, an excess that is indicated by JWST observations. Methods. Here we provide observable predictions of galaxy properties based on the analytic FFB scenario. These can be compared with simulations and JWST observations. We use the model to approximate the SFE as a function of redshift and mass, assuming a maximum SFE of ϵmax = 0.2 − 1 in the FFB regime. From this, we derive the evolution of the galaxy mass and luminosity functions as well as the cosmological evolution of stellar and star-formation densities. We then predict the star-formation history (SFH), galaxy sizes, outflows, gas fractions, metallicities, and dust attenuation, all as functions of mass and redshift in the FFB regime. Results. The major distinguishing feature of the model is the occurrence of FFBs above a mass threshold that declines with redshift. The luminosities and star formation rates in bright galaxies are predicted to be in excess of extrapolations of standard empirical models and standard cosmological simulations, an excess that grows from z ∼ 9 to higher redshifts. The FFB phase of ∼ 100 Myr is predicted to show a characteristic SFH that fluctuates on a timescale of ∼ 10 Myr. The stellar systems are compact (Re ∼ 0.3 kpc at z ∼ 10 and declining with z). The galactic gas consists of a steady wind driven by supernovae from earlier generations, with high outflow velocities (FWHM ∼ 1400 − 6700 km s−1), low gas fractions (< 0.1), low metallicities (≲ 0.1 Z⊙), and low dust attenuation (AUV ∼ 0.5 at z ∼ 10 and declining with z). We make tentative comparisons with current JWST observations for initial insights, anticipating more complete and reliable datasets for detailed quantitative comparisons in the future. The FFB predictions are also offered in digital form.																																	2024-06-19	PPRN:86279323		
J	Gundawar, Atharva; Verma, Mudit; Guan, Lin; Valmeekam, Karthik; Bhambri, Siddhant; Kambhampati, Subbarao										Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning								Arxiv											1	1;2024-05-31;https://www.arxiv.org/abs/2405.20625v1	arXiv:2405.20625			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 31 2024	2024	As the applicability of Large Language Models (LLMs) extends beyond traditional text processing tasks, there is a burgeoning interest in their potential to excel in planning and reasoning assignments, realms traditionally reserved for System 2 cognitive competencies. Despite their perceived versatility, the research community is still unraveling effective strategies to harness these models in such complex domains. The recent discourse introduced by the paper on LLM Modulo marks a significant stride, proposing a conceptual framework that enhances the integration of LLMs into diverse planning and reasoning activities. This workshop paper delves into the practical application of this framework within the domain of travel planning, presenting a specific instance of its implementation. We are using the Travel Planning benchmark by the OSU NLP group, a benchmark for evaluating the performance of LLMs in producing valid itineraries based on user queries presented in natural language. While popular methods of enhancing the reasoning abilities of LLMs such as Chain of Thought, ReAct, and Reflexion achieve a meager 0%, 0.6%, and 0% with GPT3.5-Turbo respectively, our operationalization of the LLM-Modulo framework for TravelPlanning domain provides a remarkable improvement, enhancing baseline performances by 4.6x for GPT4-Turbo and even more for older models like GPT3.5-Turbo from 0% to 5%. Furthermore, we highlight the other useful roles of LLMs in the planning pipeline, as suggested in LLM-Modulo, which can be reliably operationalized such as extraction of useful critics and reformulator for critics.																																	2024-06-19	PPRN:89131371		
J	Tewel, Yoad; Kaduri, Omri; Gal, Rinon; Yoni, Kasten; Wolf, Lior; Chechik, Gal; Atzmon, Yuval										Training-Free Consistent Text-to-Image Generation								Arxiv											3	3;2024-05-30;https://www.arxiv.org/abs/2402.03286v3| 2;2024-05-16;https://www.arxiv.org/abs/2402.03286v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.03286v1	arXiv:2402.03286			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, , a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi -subject scenarios, and even enable training -free personalization for common objects. Code will be available at our project page.																																	2024-06-16	PPRN:87523905		
J	Zhou, Qiji; Zhou, Ruochen; Hu, Zike; Lu, Panzhong; Gao, Siyang; Zhang, Yue				Lu, Panzhong/LBH-5752-2024						Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2405.13872v2| 1;2024-05-22;https://www.arxiv.org/abs/2405.13872v1	arXiv:2405.13872			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 29 2024	2024	Recent advancements in Chain-of-Thought (CoT) and related rationale-based works have significantly improved the performance of Large Language Models (LLMs) in complex reasoning tasks. With the evolution of Multimodal Large Language Models (MLLMs), enhancing their capability to tackle complex multimodal reasoning problems is a crucial frontier. However, incorporating multimodal rationales in CoT has yet to be thoroughly investigated. We propose the Image-of-Thought (IoT) prompting method, which helps MLLMs to extract visual rationales step-by-step. Specifically, IoT prompting can automatically design critical visual information extraction operations based on the input images and questions. Each step of visual information refinement identifies specific visual rationales that support answers to complex visual reasoning questions. Beyond the textual CoT, IoT simultaneously utilizes visual and textual rationales to help MLLMs understand complex multimodal information. IoT prompting has improved zero-shot visual reasoning performance across various visual understanding tasks in different MLLMs. Moreover, the step-by-step visual feature explanations generated by IoT prompting elucidate the visual reasoning process, aiding in analyzing the cognitive processes of large multimodal models.																																	2024-06-16	PPRN:88983236		
J	Huang, Yuanhui; Zheng, Wenzhao; Zhang, Yunpeng; Zhou, Jie; Lu, Jiwen				Zhang, Yunpeng/JKH-8458-2023						GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction								Arxiv											1	1;2024-05-27;https://www.arxiv.org/abs/2405.17429v1	arXiv:2405.17429			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 27 2024	2024	3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and semantics of the surrounding scene and is an important task for the robustness of vision-centric autonomous driving. Most existing methods employ dense grids such as voxels as scene representations, which ignore the sparsity of occupancy and the diversity of object scales and thus lead to unbalanced allocation of resources. To address this, we propose an object-centric representation to describe 3D scenes with sparse 3D semantic Gaussians where each Gaussian represents a flexible region of interest and its semantic features. We aggregate information from images through the attention mechanism and iteratively refine the properties of 3D Gaussians including position, covariance, and semantics. We then propose an efficient Gaussian-to-voxel splatting method to generate 3D occupancy predictions, which only aggregates the neighboring Gaussians for a certain position. We conduct extensive experiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental results demonstrate that GaussianFormer achieves comparable performance with state-of-the-art methods with only 17.8% - 24.8% of their memory consumption. 																																	2024-06-11	PPRN:89063201		
J	Khanna, Samar; Liu, Patrick; Zhou, Linqi; Meng, Chenlin; Rombach, Robin; Burke, Marshall; Lobell, David; Ermon, Stefano				Meng, Chenlin/HKF-5727-2023						DiffusionSat: A Generative Foundation Model for Satellite Imagery								Arxiv											2	2;2024-05-25;https://www.arxiv.org/abs/2312.03606v2| 1;2023-12-06;https://www.arxiv.org/abs/2312.03606v1	arXiv:2312.03606			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 25 2024	2024	Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop -yield prediction. Satellite images are significantly different from natural images – they can be multi -spectral, irregularly sampled across time – and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, highresolution remote sensing datasets. As text -based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi -spectral inputs and in -painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale generative foundation model for satellite imagery. The project website can be found here:																																	2024-06-11	PPRN:86422724		
J	Leong, Chak Tou; Cheng, Yi; Xu, Kaishuai; Wang, Jian; Wang, Hanlin; Li, Wenjie				Wang, Jian/LWJ-5908-2024; Li, Wenjie/F-9954-2010						No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks								Arxiv											1	1;2024-05-25;https://www.arxiv.org/abs/2405.16229v1	arXiv:2405.16229			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 25 2024	2024	The existing safety alignment of Large Language Models (LLMs) is found fragile and could be easily attacked through different strategies, such as through fine-tuning on a few harmful examples or manipulating the prefix of the generation results. However, the attack mechanisms of these strategies are still underexplored. In this paper, we ask the following question: while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities? To answer this question, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response. Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process. We utilize techniques such as logit lens and activation patching to identify model components that drive specific behavior, and we apply cross -model probing to examine representation shifts after an attack. In particular, we analyze the two most representative types of attack approaches: Explicit Harmful Attack (EHA) and Identity -Shifting Attack (ISA). Surprisingly, we find that their attack mechanisms diverge dramatically. Unlike ISA, EHA tends to aggressively target the harmful recognition stage. While both EHA and ISA disrupt the latter two stages, the extent and mechanisms of their attacks differ significantly. Our findings underscore the importance of understanding LLMs’ internal safeguarding process and suggest that diverse defense mechanisms are required to effectively cope with various types of attacks.																																	2024-06-11	PPRN:89071528		
J	Ge, Ce; Ma, Zhijian; Chen, Daoyuan; Li, Yaliang; Ding, Bolin				Ge, Ce/KGL-5172-2024						Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14908v1	arXiv:2405.14908			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Large language models exhibit exceptional generalization capabilities, primarily attributed to the utilization of diversely sourced data. However, conventional practices in integrating this diverse data heavily rely on heuristic schemes, lacking theoretical guidance. This research tackles these limitations by investigating strategies based on low-cost proxies for data mixtures, with the aim of streamlining data curation to enhance training efficiency. Specifically, we propose a unified scaling law, termed BIMIX , which accurately models the bivariate scaling behaviors of both data quantity and mixing proportions. We conduct systematic experiments and provide empirical evidence for the predictive power and fundamental principles of B I M IX . Notably, our findings reveal that entropy-driven training -free data mixtures can achieve comparable or even better performance than more resource -intensive methods. We hope that our quantitative insights can shed light on further judicious research and development in cost-effective language modeling.																																	2024-06-08	PPRN:89010623		
J	Shi, Yuheng; Dong, Minjing; Xu, Chang				Shi, Yuheng/IUO-7697-2023; Xu, Chang/AAG-9337-2019; Dong, Minjing/JBS-1969-2023						Multi-Scale VMamba: Hierarchy in Hierarchy Visual State Space Model								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14174v1	arXiv:2405.14174			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Despite the significant achievements of Vision Transformers (ViTs) in various vision tasks, they are constrained by the quadratic complexity. Recently, State Space Models (SSMs) have garnered widespread attention due to their global receptive field and linear complexity with respect to the input length, demonstrating substantial potential across fields including natural language processing and computer vision. To improve the performance of SSMs in vision tasks, a multi-scan strategy is widely adopted, which leads to significant redundancy of SSMs. For a better trade-off between efficiency and performance, we analyze the underlying reasons behind the success of the multi-scan strategy, where long-range dependency plays an important role. Based on the analysis, we introduce Multi-Scale Vision Mamba (MSVMamba) to preserve the superiority of SSMs in vision tasks with limited parameters. It employs a multi-scale 2D scanning technique on both original and downsampled feature maps, which not only benefits long-range dependency learning but also reduces computational costs. Additionally, we integrate a Convolutional Feed-Forward Network (ConvFFN) to address the lack of channel mixing. Our experiments demonstrate that MSVMamba is highly competitive, with the MSVMamba-Tiny model achieving 82.8% top-1 accuracy on ImageNet, 46.9% box mAP, and 42.2% instance mAP with the Mask R-CNN framework, 1x training schedule on COCO, and 47.6% mIoU with single-scale testing on ADE20K.																																	2024-06-05	PPRN:88988412		
J	Bi, Baolong; Liu, Shenghua; Mei, Lingrui; Wang, Yiwei; Ji, Pengliang; Cheng, Xueqi										Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts								Arxiv											1	1;2024-05-21;https://www.arxiv.org/abs/2405.11613v2	arXiv:2405.11613			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 21 2024	2024	The knowledge within large language models (LLMs) may become outdated quickly. While in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability. Our work aims to elucidate the superior performance of ICE in KE by analyzing the impacts of in-context new knowledge on token-wise distributions. We observe that despite a significant boost in logits of the new knowledge, the performance of ICE is still hindered by stubborn knowledge. Stubborn knowledge refers to facts that have gained excessive confidence during pretraining, making them hard to edit effectively. To address this issue and further enhance the performance of ICE, we propose a novel approach termed De coding by C ontrasting K nowledge (DeCK). DeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge. Our experiments consistently demonstrate that DeCK enhances the confidence of LLMs in edited facts. For instance, it improves the performance of LL A MA3-8B- INSTRUCT on MQ U AKE by up to 219%, demonstrating its capability to strengthen ICE in the editing of stubborn knowledge. DeCK can be easily integrated into any ICE method as a decoding component to enhance editing capabilities. Our work paves the way to develop both effective and accountable KE methods for LLMs. 																																	2024-08-24	PPRN:91460559		
J	Niklaus, Joel; Matoshi, Veton; Sturmer, Matthias; Chalkidis, Ilias; Ho, Daniel E.										MultiLegalPile: A 689GB Multilingual Legal Corpus								Arxiv											1	1;2024-05-19;https://www.arxiv.org/abs/2306.02069v3	arXiv:2306.02069			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 19 2024	2024	Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.																																	2024-06-01	PPRN:88963782		
J	Bhardwaj, Lakshya; Bottini, Lea E.; Schafer-Nameki, Sakura; Tiwari, Apoorv				Bhardwaj, Lakshya/ISU-5186-2023; Filho, Luciano/KDN-5303-2024						Illustrating the Categorical Landau Paradigm in Lattice Models								Arxiv											2	2;2024-05-16;https://www.arxiv.org/abs/2405.05302v2| 1;2024-05-08;https://www.arxiv.org/abs/2405.05302v1	arXiv:2405.05302			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 16 2024	2024	Recent years have seen the concept of global symmetry extended to non-invertible (or categorical) symmetries, for which composition of symmetry generators is not necessarily invertible. Such non-invertible symmetries lead to a generalization of the standard Landau paradigm. In this work we substantiate this framework by providing a (1+1)d lattice model, whose gapped phases and phase transitions can only be explained by symmetry breaking of non-invertible symmetries.																																	2024-06-12	PPRN:88822700		
J	Heckman, Jonathan J.; Hubner, Max; Torres, Ethan; Yu, Xingyang; Zhang, Hao Y.				Zhang, Hao/OEN-2482-2025						Top Down Approach to Topological Duality Defects								Arxiv											2	2;2024-05-16;https://www.arxiv.org/abs/2212.09743v4| 1;2022-12-19;https://www.arxiv.org/abs/2212.09743v2	arXiv:2212.09743			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 16 2024	2024	Topological duality defects arise as co dimension one generalized symmetry operators in quantum field theories (QFTs) with a duality symmetry. Recent investigations have shown that in the case of 4D N = 4 Super Yang-Mills (SYM) theory, an appropriate choice of (complexified) gauge coupling and global form of the gauge group can lead to a rather rich fusion algebra for the associated defects, leading to examples of non-invertible symmetries. In this work we present a top down construction of these duality defects which generalizes to QFTs with lower supersymmetry, where other 0-form symmetries are often present. We realize the QFTs of interest via D3-branes probing X a Calabi-Yau threefold cone with an isolated singularity at the tip of the cone. The IIB duality group descends to dualities of the 4D worldvolume theory. Non-trivial co dimension one topological interfaces arise from configurations of 7-branes “at infinity” which implement a suitable SL (2 , Z ) transformation when they are crossed. Reduction on the boundary topology ∂X results in a 5D symmetry TFT. Different realizations of duality defects, such as the gauging of 1-form symmetries with certain mixed anomalies and half-space gauging constructions, simply amount to distinct choices of where to place the branch cuts in the 5D bulk.																																	2024-12-16	PPRN:55178194		
J	Feng, Sidong; Chen, Chunyang				Chen, Chunyang/NMK-6150-2025						Prompting Is All You Need: Automated Android Bug Replay with Large Language Models								Arxiv											2	2;2024-05-08;https://www.arxiv.org/abs/2306.01987v3| 1;2023-06-03;https://www.arxiv.org/abs/2306.01987v1	arXiv:2306.01987			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 08 2024	2024	Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.																																	2024-05-28	PPRN:72853631		
J	Hao, Zhongkai; Su, Chang; Liu, Songming; Berner, Julius; Ying, Chengyang; Su, Hang; Anandkumar, Anima; Song, Jian; Zhu, Jun				su, hang/KEH-2976-2024						DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training								Arxiv											3	3;2024-05-07;https://www.arxiv.org/abs/2403.03542v4| 2;2024-03-08;https://www.arxiv.org/abs/2403.03542v3| 1;2024-03-07;https://www.arxiv.org/abs/2403.03542v2	arXiv:2403.03542			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 07 2024	2024	Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data.																																	2024-06-04	PPRN:88055648		
J	Li, Yunqi; Zhang, Lanjing; Zhang, Yongfeng				Zhang, Lanjing/GZB-0260-2022						Fairness of ChatGPT								Arxiv											2	2;2024-05-05;https://www.arxiv.org/abs/2305.18569v2| 1;2023-05-22;https://www.arxiv.org/abs/2305.18569v1	arXiv:2305.18569			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 05 2024	2024	Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited number of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high -stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT’s performance in high -takes fields including education, criminology, finance and healthcare. To conduct a thorough evaluation, we consider both group fairness and individual fairness metrics. We also observe the disparities in ChatGPT’s outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs’ fairness performance, facilitates bias mitigation and fosters the development of responsible AI systems. Code and data are open sourced on GitHub1.																																	2024-05-24	PPRN:72758659		
J	Yang, Jihan; Ding, Runyu; Deng, Weipeng; Wang, Zhe; Qi, Xiaojuan				Qi, Xiaojuan/MVV-7776-2025; Yang, Jihan/JQI-4498-2023; Wang, Zhe/ABG-6377-2021						RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding								Arxiv											3	3;2024-05-05;https://www.arxiv.org/abs/2304.00962v4| 2;2023-11-23;https://www.arxiv.org/abs/2304.00962v3| 1;2023-04-03;https://www.arxiv.org/abs/2304.00962v1	arXiv:2304.00962			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 05 2024	2024	We propose a lightweight and scalable Region al Point- Language Contrastive learning framework, namely RegionPLC , for open-world 3D scene understanding, aiming to identify and recognize open-set objects and categories. Specifically, based on our empirical studies, we introduce a 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations. Subsequently, we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3D learning from dense regional language supervision. We carry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets, and our model outperforms prior 3D open-world scene understanding approaches by an average of 17.2% and 9.1% for semantic and instance segmentation, respectively, while maintaining greater scalability and lower resource demands. Furthermore, our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3D reasoning without extra task-specific training. Code is available at github.																																	2024-05-28	PPRN:53743545		
J	Shaker, Abdelrahman; Maaz, Muhammad; Rasheed, Hanoona; Khan, Salman; Yang, Ming-Hsuan; Khan, Fahad Shahbaz				Shaker, Abdelrahman/AAA-8435-2021; Khan, Fahad Shahbaz/ABD-6646-2021; Maaz, Muhammad/GOK-1100-2022; Khan, Salman/M-4834-2016; Yang, Ming-Hsuan/T-9533-2019						UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation								Arxiv											2	2;2024-05-04;https://www.arxiv.org/abs/2212.04497v3| 1;2023-03-22;https://www.arxiv.org/abs/2212.04497v2	arXiv:2212.04497			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 04 2024	2024	Owing to the success of transformer models, recent works study their applicability in 3D medical segmentation tasks. Within the transformer models, the self-attention mechanism is one of the main building blocks that strives to capture long-range dependencies. However, the self-attention operation has quadratic complexity which proves to be a computational bottleneck, especially in volumetric medical imaging, where the inputs are 3D with numerous slices. In this paper, we propose a 3D medical image segmentation approach, named UNETR++, that offers both high-quality segmentation masks as well as efficiency in terms of parameters, compute cost, and inference speed. The core of our design is the introduction of a novel efficient paired attention (EPA) block that efficiently learns spatial and channel-wise discriminative features using a pair of inter-dependent branches based on spatial and channel attention. Our spatial attention formulation is efficient having linear complexity with respect to the input sequence length. To enable communication between spatial and channel-focused branches, we share the weights of query and key mapping functions that provide a complimentary benefit (paired attention), while also reducing the overall network parameters. Our extensive evaluations on five benchmarks, Synapse, BTCV, ACDC, BRaTs, and Decathlon-Lung, reveal the effectiveness of our contributions in terms of both efficiency and accuracy. On Synapse, our UNETR++ sets a new state-of-the-art with a Dice Score of 87.2%, while being significantly efficient with a reduction of over 71% in terms of both parameters and FLOPs, compared to the best method in the literature. 																																	2024-05-28	PPRN:47337105		
J	Zhang, Buyun; Luo, Liang; Chen, Yuxin; Nie, Jade; Liu, Xi; Guo, Daifeng; Zhao, Yanli; Li, Shen; Hao, Yuchen; Yao, Yantao; Lakshminarayanan, Guna; Wen, Ellie Dingqiao; Park, Jongsoo; Naumov, Maxim; Chen, Wenlin										Wukong: Towards a Scaling Law for Large-Scale Recommendation								Arxiv											3	3;2024-05-02;https://www.arxiv.org/abs/2403.02545v3| 2;2024-03-08;https://www.arxiv.org/abs/2403.02545v2| 1;2024-03-04;https://www.arxiv.org/abs/2403.02545v1	arXiv:2403.02545			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real -world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong’s unique design makes it possible to capture diverse, any -order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wukong’s scalability on an internal, large-scale dataset. The results show that Wukong retains its superiority in quality over state-of-the-art models, while holding the scaling law across two orders of magnitude in model complexity, extending beyond 100 GFLOP/example or equivalently up to Large Launguage Model (GPT-3) training compute scale, where prior arts fall short.																																	2024-05-21	PPRN:88028069		
J	Ho, Tsung-Yi; Khan, Sadaf; Liu, Jinwei; Li, Yu; Liu, Yi; Shi, Zhengyuan; Wang, Ziyi; Xu, Qiang; Young, Evangeline F.Y.; Yu, Bei; Zheng, Ziyang; Zhu, Binwu; Zhu, Keren; Chen, Yiqi; Huang, Ru; Liang, Yun; Lin, Yibo; Luo, Guojie; Sun, Guangyu; Wang, Runsheng; Wei, Xinming; Xue, Chenhao; Yang, Jun; Zhang, Haoyi; Zhang, Zuodong; Zhao, Yuxiang; Zou, Sunan; Chen, Lei; Huang, Yu; Li, Min; Tsaras, Dimitrios; Yuan, Mingxuan; Zhen, Hui-Ling; Chu, Zhufei; Fang, Wenji; Li, Xingquan; Yan, Junchi; Xie, Zhiyao; Zeng, Xuan				Chu, zhufei/D-5709-2013; Sun, Guangyu/GXF-4043-2022; Ho, Tsung-Yi/ABF-9929-2021; Liu, Jinwei/LUY-7954-2024; Li, Xingquan/JMC-9800-2023; zhang, zuodong/MIN-8508-2025; Fang, Wenji/OBO-7903-2025; Li, Min/U-7023-2019; Shi, Zhengyuan/JVY-7162-2024; Luo, Guojie/B-1559-2010; Wei, Xinming/KIA-9856-2024						The Dawn of AI-Native EDA: Opportunities and Challenges of Large Circuit Models								Arxiv											1	1;2024-05-01;https://www.arxiv.org/abs/2403.07257v2	arXiv:2403.07257			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 01 2024	2024	Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process. Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts.   We champion the creation of large circuit models (LCMs) that are inherently multimodal, crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies. Embracing this AI-native philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound shift-left in electronic design methodology. The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the benchmarks of electronic systems' capabilities.																																	2024-05-19	PPRN:88711002		
J	Ye, Jiasheng; Zheng, Zaixiang; Bao, Yu; Qian, Lihua; Wang, Mingxuan				qian, lihua/Q-2069-2016						DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2302.10025v2| 1;2023-02-20;https://www.arxiv.org/abs/2302.10025v1	arXiv:2302.10025			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 01 2024	2024	While diffusion models have achieved great success in generating continuous signals such as images and audio, it remains elusive for them to learn discrete sequence data like natural languages. Although recent advances circumvent this challenge of discreteness by embedding discrete tokens as continuous surrogates, they still fall short of satisfactory generation quality. To understand this, we first dive deep into the denoised training protocol of diffusion-based sequence generative models and determine their three severe problems: (1) failing to learn; (2) lack of scalability; and (3) neglecting source conditions. We argue that these problems can be boiled down to the pitfall of the not completely eliminated discreteness in the embedding space, and the scale of noises is decisive herein. In this paper, we introduce D I N OISER to facilitate diffusion models for sequence generation by manipulating noises. We propose to adaptively determine the range of sampled noise scales during training; and encourage the proposed diffused sequence learner to leverage source conditions with amplified noise scales during inference. Experiments show that D I N OISER enables consistent improvement over the baselines of previous diffusion sequence generative models on several conditional sequence modeling benchmarks thanks to both effective training and inference strategies. Analyses further verify that D I N OISER can make better use of source conditions to govern its generative process.																																	2024-05-19	PPRN:43718551		
J	Cai, Mu; Liu, Haotian; Park, Dennis; Mustikovela, Siva Karthik; Meyer, Gregory P.; Chai, Yuning; Lee, Yong Jae				Cai, Mu/AAD-8827-2022						ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts								Arxiv											2	2;2024-04-27;https://www.arxiv.org/abs/2312.00784v2| 1;2023-12-01;https://www.arxiv.org/abs/2312.00784v1	arXiv:2312.00784			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 27 2024	2024	While existing large vision -language multimodal models focus on whole image understanding, there is a prominent gap in achieving region -specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary (free -form) visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a “red bounding box” or “pointed arrow”. Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on regionunderstanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.																																	2024-05-12	PPRN:86360491		
J	Gao, Zhujin; Guo, Junliang; Tan, Xu; Zhu, Yongxin; Zhang, Fang; Bian, Jiang; Xu, Linli										Empowering Diffusion Models on the Embedding Space for Text Generation								Arxiv											2	2;2024-04-22;https://www.arxiv.org/abs/2212.09412v3| 1;2022-12-19;https://www.arxiv.org/abs/2212.09412v2	arXiv:2212.09412			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 22 2024	2024	Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies of the optimization challenges encountered with both the embedding space and the denoising model, which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the embedding space and unstable training. To alleviate this problem, we propose a new objective called the anchor loss which is more efficient than previous methods. Secondly, we find the noise levels of conventional schedules are insufficient for training a desirable denoising model while introducing varying degrees of degeneration in consequence. To address this challenge, we propose a novel framework called noise rescaling. Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.1																																	2024-05-01	PPRN:36057538		
J	Metger, Tony; Poremba, Alexander; Sinha, Makrand; Yuen, Henry										Simple constructions of linear-depth <italic>t</italic>-designs and pseudorandom unitaries								Arxiv											1	1;2024-04-19;https://www.arxiv.org/abs/2404.12647v1	arXiv:2404.12647			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 19 2024	2024	Uniformly random unitaries, i.e. unitaries drawn from the Haar measure, have many useful properties, but cannot be implemented efficiently. This has motivated a long line of research into random unitaries that “look” sufficiently Haar random while also being efficient to implement. Two different notions of derandomisation have emerged: t-designs are random unitaries that information-theoretically reproduce the first t moments of the Haar measure, and pseudorandom unitaries (PRUs) are random unitaries that are computationally indistinguishable from Haar random. In this work, we take a unified approach to constructing t-designs and PRUs. For this, we introduce and analyse the “PFC ensemble”, the product of a random computational basis permutation P, a random binary phase operator F, and a random Clifford unitary C. We show that this ensemble reproduces exponentially high moments of the Haar measure. We can then derandomise the PFC ensemble to show the following: Linear-depth t-designs. We give the first construction of a (diamond-error) approximate t-design with circuit depth linear in t. This follows from the PFC ensemble by replacing the random phase and permutation operators with their 2t-wise independent counterparts. Non-adaptive PRUs. We give the first construction of PRUs with non-adaptive security, i.e. we construct unitaries that are indistinguishable from Haar random to polynomial -time distinguishers that query the unitary in parallel on an arbitary state. This follows from the PFC ensemble by replacing the random phase and permutation operators with their pseudorandom counterparts. Adaptive pseudorandom isometries. We show that if one considers isometries (rather than unitaries) from n to n + ω (log n) qubits, a small modification of our PRU construction achieves adaptive security, i.e. even a distinguisher that can query the isometry adaptively in sequence cannot distinguish it from Haar random isometries. This gives the first construction of adaptive pseudorandom isometries. Under an additional conjecture, this proof also extends to adaptive PRUs.																																	2024-04-29	PPRN:88589540		
J	Ye, Qichen; Liu, Junling; Chong, Dading; Zhou, Peilin; Hua, Yining; Liu, Fenglin; Cao, Meng; Wang, Ziming; Cheng, Xuxin; Lei, Zhu; Guo, Zhenhua				Ye, Qichen/KHU-8286-2024; guo, zhenhua/AAD-1578-2020; Liu, Jianglai/P-2587-2015						Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model								Arxiv											2	2;2024-04-17;https://www.arxiv.org/abs/2310.09089v2| 1;2023-10-13;https://www.arxiv.org/abs/2310.09089v1	arXiv:2310.09089			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 17 2024	2024	Integrating large language models (LLMs) into healthcare holds great potential but faces challenges. Pre -training LLMs from scratch for domains like medicine is resource -heavy and often unfeasible. On the other hand, sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions. In response, we present a multi -stage training method combining domain -specific Continued Pre -training (CPT), SFT, and Direct Preference Optimization (DPO). In addition, we publish the Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set, respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by 7.5%. In the DPO phase, it scored 16.66 in BLEU -1 and 27.44 in ROUGE -1 on the Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in BLEU -1 and 24.21 in ROUGE -1). Additionally, our adoption of the Retrieval Augmented Generation (RAG) approach further enhanced the model performance. Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8% on CMExam. These results highlight the contribution of our novel training approach in building LLMs for medi																																	2024-04-27	PPRN:85632319		
J	Ma, Xuezhe; Yang, Xiaomeng; Xiong, Wenhan; Chen, Beidi; Yu, Lili; Zhang, Hao; May, Jonathan; Zettlemoyer, Luke; Levy, Omer; Zhou, Chunting				Liu, Qing/JMP-5564-2023; Ma, Xuezhe/GWN-1885-2022						Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length								Arxiv											1	1;2024-04-16;https://www.arxiv.org/abs/2404.08801v2	arXiv:2404.08801			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub -quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce MEGA- LODON, an neural architecture for efficient sequence modeling with unlimited context length. MEGALODON inherits the architecture of MEGA (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre -norm with two -hop residual configuration. In a controlled head -to -head comparison with LLAMA2, MEGALODON achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. MEGALODON reaches a training loss of 1.70, landing mid -way between LLAMA2- 7B (1.75) and 13B (1.67). The improvements of MEGALODON over Transformers are robust throughout a range of benchmarks across different tasks and modalities.																																	2024-04-26	PPRN:88538781		
J	Zhang, Yichi; Chen, Zhuo; Guo, Lingbing; Xu, Yajing; Zhang, Wen; Chen, Huajun				Huajun, Chen/B-6340-2013; Zhuo, Cheng/HLV-8134-2023						Making Large Language Models Perform Better in Knowledge Graph Completion								Arxiv											3	3;2024-04-14;https://www.arxiv.org/abs/2310.06671v2| 2;2023-10-10;https://www.arxiv.org/abs/2310.06671v1| 1;2023-10-10;https://www.arxiv.org/abs/2310.06671v1	arXiv:2310.06671			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 14 2024	2024	Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs. However, research about LLM-based KGC fails to sufficiently harness LLMs' inference proficiencies, overlooking critical structural information integral to KGs. In this paper, we explore methods to incorporate structural information into the LLMs, with the overarching goal of facilitating structure-aware reasoning. We first discuss on the existing LLM paradigms like in-context learning and instruction tuning, proposing basic structural information injection approaches. Then we propose a Knowledge Prefix Adapter (KoPA) to fulfill this stated goal. The KoPA uses a structural pre-training phase to comprehend the intricate entities and relations within KGs, representing them as structural embeddings. Then KoPA communicates such cross-modal structural information understanding to the LLMs through a knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens positioned as a prefix of the input prompt. We conduct comprehensive experiments and provide incisive analysis concerning how the introduction of cross-modal structural information would be better for LLM's factual knowledge reasoning ability. 																																	2024-04-25	PPRN:85525670		
J	Bechard, Patrice; Ayala, Orlando Marquez										Reducing hallucination in structured outputs via Retrieval-Augmented Generation								Arxiv											2	2;2024-04-12;https://www.arxiv.org/abs/2404.08189v1| 1;2024-04-12;https://www.arxiv.org/abs/2404.08189v1	arXiv:2404.08189			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 12 2024	2024	A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.																																	2025-01-11	PPRN:88550241		
J	Jiang, Ziyue; Liu, Jinglin; Ren, Yi; He, Jinzheng; Ye, Zhenhui; Ji, Shengpeng; Yang, Qian; Zhang, Chen; Wei, Pengfei; Wang, Chunfeng; Yin, Xiang; Ma, Zejun; Zhao, Zhou				jiang, ziyue/GSI-9122-2022; yang, qian/LRT-5966-2024; Wei, Pengfei/E-7924-2013; Liu, Jing-Lin/P-9835-2017						Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis								Arxiv											4	4;2024-04-10;https://www.arxiv.org/abs/2307.07218v4| 3;2024-03-18;https://www.arxiv.org/abs/2307.07218v3| 2;2023-09-28;https://www.arxiv.org/abs/2307.07218v2| 1;2023-07-14;https://www.arxiv.org/abs/2307.07218v1	arXiv:2307.07218			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 10 2024	2024	Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process. However, the prompting mechanisms of zero-shot TTS still face challenges in the following aspects: 1) previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage. 2) The prosodic information in prompts is highly coupled with timbre, making it untransferable to each other. This paper introduces Mega-TTS 2, a generic prompting mechanism for zero-shot TTS, to tackle the aforementioned challenges. Specifically, we design a powerful acoustic autoencoder that separately encodes the prosody and timbre information into the compressed latent space while providing high-quality reconstructions. Then, we propose a multi-reference timbre encoder and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts. We further leverage the probabilities derived from multiple P-LLM outputs to produce transferable and controllable prosody. Experimental results demonstrate that Mega-TTS 2 could not only synthesize identity-preserving speech with a short prompt of an unseen speaker from arbitrary sources but consistently outperform the fine-tuning method when the volume of data ranges from 10 seconds to 5 minutes. Furthermore, our method enables to transfer various speaking styles to the target timbre in a fine-grained and controlled manner. 																																	2024-04-24	PPRN:73933428		
J	Yao, Zengwei; Guo, Liyong; Yang, Xiaoyu; Kang, Wei; Kuang, Fangjun; Yang, Yifan; Jin, Zengrui; Lin, Long; Povey, Daniel				YANG, YIFAN/HPF-1451-2023						ZIPFORMER: A FASTER AND BETTER ENCODER FOR AUTOMATIC SPEECH RECOGNITION								Arxiv											4	4;2024-04-10;https://www.arxiv.org/abs/2310.11230v4| 3;2024-03-05;https://www.arxiv.org/abs/2310.11230v3| 2;2023-12-06;https://www.arxiv.org/abs/2310.11230v2| 1;2023-10-17;https://www.arxiv.org/abs/2310.11230v1	arXiv:2310.11230			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 10 2024	2024	The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a Transformer to learn both local and global dependencies. In this work we describe a faster, more memory efficient, and better -performing Transformer, called Zipformer. Modeling changes include: 1) a U -Net -like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re -use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor’s current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models.																																	2024-04-24	PPRN:85682046		
J	Qian, Zhiyin; Wang, Shaofei; Mihajlovic, Marko; Geiger, Andreas; Tang, Siyu				Wang, Shaofei/L-1017-2019						3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting								Arxiv											3	3;2024-04-04;https://www.arxiv.org/abs/2312.09228v3| 2;2023-12-15;https://www.arxiv.org/abs/2312.09228v2| 1;2023-12-14;https://www.arxiv.org/abs/2312.09228v1	arXiv:2312.09228			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 04 2024	2024	We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.																																	2024-04-19	PPRN:86587460		
J	Lyu, Chenyang; Du, Zefeng; Xu, Jitao; Duan, Yitao; Wu, Minghao; Lynn, Teresa; Aji, Alham Fikri; Wong, Derek F.; Liu, Siyou; Wang, Longyue				Wong, Derek F/CAI-7740-2022; Du, Zefeng/HHN-5029-2022; Xu, Jitao/OMM-6627-2025						A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models								Arxiv											3	3;2024-04-02;https://www.arxiv.org/abs/2305.01181v3| 2;2024-02-26;https://www.arxiv.org/abs/2305.01181v2| 1;2023-05-02;https://www.arxiv.org/abs/2305.01181v1	arXiv:2305.01181			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 02 2024	2024	Machine Translation (MT) has greatly advanced over the years due to the developments in deep neural networks. However, the emergence of Large Language Models (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT domain. In this context, we believe that the future of MT is intricately tied to the capabilities of LLMs. These models not only offer vast linguistic understandings but also bring innovative methodologies, such as prompt-based techniques, that have the potential to further elevate MT. In this paper, we provide an overview of the significant enhancements in MT that are influenced by LLMs and advocate for their pivotal role in upcoming MT research and implementations. We highlight several new MT directions, emphasizing the benefits of LLMs in scenarios such as Long-Document Translation, Stylized Translation, and Interactive Translation. Additionally, we address the important concern of privacy in LLM-driven MT and suggest essential privacy-preserving strategies. By showcasing practical instances, we aim to demonstrate the advantages that LLMs offer, particularly in tasks like translating extended documents. We conclude by emphasizing the critical role of LLMs in guiding the future evolution of MT and offer a roadmap for future exploration in the sector.																																	2024-04-18	PPRN:66804229		
J	Mitra, Chancharik; Huang, Brandon; Darrell, Trevor; Herzig, Roei				Herzig, Roei/JEP-6447-2023						Compositional Chain-of-Thought Prompting for Large Multimodal Models								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2311.17076v3	arXiv:2311.17076			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs. Code: https://github.com/chancharikmitra/CCoT																																	2024-05-03	PPRN:88367419		
J	Wu, Shuang; Zhu, Liwen; Yang, Tao; Xu, Shiwei; Fu, Qiang; Wei, Yang; Fu, Haobo				yang, tao/HJP-6478-2023						Enhance Reasoning for Large Language Models in the Game Werewolf								Arxiv											3	3;2024-03-29;https://www.arxiv.org/abs/2402.02330v2| 2;2024-02-04;https://www.arxiv.org/abs/2402.02330v1| 1;2024-02-04;https://www.arxiv.org/abs/2402.02330v1	arXiv:2402.02330			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System -1 tasks such as natural language processing, while the Thinker focuses on cognitive System -2 tasks that require complex logical analysis and domain -specific knowledge. Our framework is presented using a 9player Werewolf game that demands dual -system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18,800 human sessions and reinforcement learning. Experiments demonstrate the framework’s effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset 																																	2024-04-15	PPRN:87521977		
J	Cabezas-Escares, J.; Barrera, Nicolas F; Lavroff, Robert H; Alexandrova, Anastassia N.; Cardenas, C.; Munoz, F.				Munoz, Francisco/C-1140-2011						Electronic Structure and Vibrational Stability of Copper-substituted Lead Apatite (LK-99)								Arxiv											3	3;2024-03-27;https://www.arxiv.org/abs/2308.01135v4| 2;2024-01-28;https://www.arxiv.org/abs/2308.01135v3| 1;2023-08-02;https://www.arxiv.org/abs/2308.01135v1	arXiv:2308.01135			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 27 2024	2024	Two recent preprints in the physics archive (arXiv) have called attention as they claim experimental evidence that a Cu-substituted apatite material (dubbed LK-99) exhibits superconductivity at room temperature and pressure. If this proves to be true, LK-99 will be a "holy grail" of superconductors. In this work, we used Density Functional Theory (DFT+U) calculations to elucidate some key features of the electronic structure of LK-99. We find two different phases of this material: (i) a hexagonal lattice featuring metallic half-filled and spin-split bands, a nesting of the Fermi surface, a remarkably large electron-phonon coupling, but this lattice is vibrationally unstable. (ii) a triclinic lattice, with the Cu and surrounding O distorted. This lattice is vibrationally stable and its bands correspond to an insulator. In a crystal, the Cu atoms should oscillate between equivalent triclinic positions, with an average close to the hexagonal positions. We discuss the electronic structure expected from these fluctuations and if it is compatible with superconductivity.																																	2024-04-14	PPRN:74219115		
J	Hellstrom, Fredrik; Durisi, Giuseppe; Guedj, Benjamin; Raginsky, Maxim				Guedj, Benjamin/AAA-9872-2019; Durisi, Giuseppe/AAW-7013-2021						Generalization Bounds: Perspectives from Information Theory and PAC-Bayes								Arxiv											2	2;2024-03-27;https://www.arxiv.org/abs/2309.04381v2| 1;2023-09-08;https://www.arxiv.org/abs/2309.04381v1	arXiv:2309.04381			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 27 2024	2024	A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of PAC-Bayesian and information-theoretic generalization bounds. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demonstrate how many proofs in the area share a modular structure, through which the underlying ideas can be intuited. We pay special attention to the conditional mutual information (CMI) framework; analytical studies of the information complexity of learning algorithms; and the application of the proposed methods to deep learning. This monograph is intended to provide a comprehensive introduction to information-theoretic generalization bounds and their connection to PAC-Bayes, serving as a foundation from which the most recent developments are accessible. It is aimed broadly towards researchers with an interest in generalization and theoretical machine learning.																																	2024-04-15	PPRN:84910666		
J	Pascale, Massimo; Frye, Brenda L.; Pierel, Justin D.R.; Chen, Wenlei; Kelly, Patrick L.; Cohen, Seth H.; Windhorst, Rogier A.; Riess, Adam G.; Kamieneski, Patrick S.; Diego, Jose M.; Meena, Ashish K.; Cha, Sangjun; Oguri, Masamune; Zitrin, Adi; Jee, M.James; Foo, Nicholas; Leimbach, Reagen; Koekemoer, Anton M.; Conselice, C.J.; Dai, Liang; Goobar, Ariel; Siebert, Matthew R.; Strolger, Lou; Willner, S.P.				Zitrin, Adi/M-3402-2018; Diego, Jose/I-2511-2015; Chen, Wenlei/HJB-2881-2022; Meena, Ashish Kumar/HPG-4271-2023; Riess, Adam/ABF-2480-2020; Oguri, Masamune/C-6230-2011						SN H0pe: The First Measurement of <italic>H</italic>0 from a Multiply-Imaged Type Ia Supernova, Discovered by JWST								Arxiv											1	1;2024-03-27;https://www.arxiv.org/abs/2403.18902v1	arXiv:2403.18902			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 27 2024	2024	The first James Webb Space Telescope ({\it JWST}) Near InfraRed Camera (NIRCam) imaging in the field of the galaxy cluster PLCK G165.7+67.0 (z=0.35) uncovered a Type Ia supernova (SN Ia) at z = 1.78, called "SN H0pe." Three different images of this one SN were detected as a result of strong gravitational lensing, each one traversing a different path in spacetime, thereby inducing a relative delay in the arrival of each image. Follow-up {\it JWST} observations of all three SN images enabled photometric and rare spectroscopic measurements of the two relative time delays. Following strict blinding protocols which oversaw a live unblinding and regulated post-unblinding changes, these two measured time delays were compared to the predictions of seven independently constructed cluster lens models to measure a value for the Hubble constant, H0=71.8−7.6+9.8 km s−1 Mpc−1. The range of admissible H0 values predicted across the lens models limits further precision, reflecting the well-known degeneracies between lens model constraints and time delays. It has long been theorized that a way forward is to leverage a standard candle, however this has not been realized until now. For the first time, the lens models are evaluated by their agreement with the SN absolute magnification, breaking these degeneracies and producing our best estimate, H0=75.4−5.5+8.1 km s−1 Mpc−1. This is the first precision measurement of H0 from a multiply-imaged SN Ia, and provides a measurement in a rarely utilized redshift regime. This result agrees with other local universe measurements, yet exceeds the value of H0 derived from the early Universe with ≳90% confidence, increasing evidence of the Hubble tension. With the precision provided by only four more events, this approach could solidify this disagreement to >3σ.																																	2024-05-22	PPRN:88331857		
J	Cai, Zhenguang G.; Duan, Xufeng; Haslett, David A.; Wang, Shuqi; Pickering, Martin J.				Cai, Zhenguang/ABB-7200-2020						Do large language models resemble humans in language use?								Arxiv											2	2;2024-03-26;https://www.arxiv.org/abs/2303.08014v2| 1;2023-03-10;https://www.arxiv.org/abs/2303.08014v1	arXiv:2303.08014			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 26 2024	2024	Large language models (LLMs) such as ChatGPT and Vicuna have shown remarkable capacities in comprehending and producing language. However, their internal workings remain a black box, and it is unclear whether LLMs and chatbots can develop humanlike characteristics in language use. Cognitive scientists have devised many experiments that probe, and have made great progress in explaining, how people comprehend and produce language. We subjected ChatGPT and Vicuna to 12 of these experiments ranging from sounds to dialogue, preregistered and with 1000 runs (i.e., iterations) per experiment. ChatGPT and Vicuna replicated the human pattern of language use in 10 and 7 out of the 12 experiments, respectively. The models associated unfamiliar words with different meanings depending on their forms, continued to access recently encountered meanings of ambiguous words, reused recent sentence structures, attributed causality as a function of verb semantics, and accessed different meanings and retrieved different words depending on an interlocutor's identity. In addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible sentences that were likely to have been corrupted by noise, drew reasonable inferences, and overlooked semantic fallacies in a sentence. Finally, unlike humans, neither model preferred using shorter words to convey less informative content, nor did they use context to resolve syntactic ambiguities. We discuss how these convergences and divergences may result from the transformer architecture. Overall, these experiments demonstrate that LLMs such as ChatGPT (and Vicuna to a lesser extent) are humanlike in many aspects of human language processing.																																	2024-04-14	PPRN:46814641		
J	Liu, Haiyang; Zhu, Zihao; Becherini, Giorgio; Peng, Yichen; Su, Mingyang; Zhou, You; Zhe, Xuefei; Iwamoto, Naoya; Zheng, Bo; Black, Michael J.				Su, Mingyang/LFU-1660-2024; Zhu, Zihao/KCL-1889-2024; Liu, Haiyang/HOF-8380-2023; Xu, Zhe/HME-1698-2023						EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling								Arxiv											4	4;2024-03-30;https://www.arxiv.org/abs/2401.00374v5| 3;2024-03-27;https://www.arxiv.org/abs/2401.00374v4| 2;2024-03-25;https://www.arxiv.org/abs/2401.00374v3| 1;2024-01-02;https://www.arxiv.org/abs/2401.00374v2	arXiv:2401.00374			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Mar 25 2024	2024	We propose EMAGE, a framework to generate full-body human gestures from audio and masked gestures, encompassing facial, local body, hands, and global movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new mesh-level holistic co-speech dataset. BEAT2 combines MoShed SMPLX body with FLAME head parameters and further refines the modeling of head, neck, and finger movements, offering a community-standardized, high-quality 3D motion captured dataset. EMAGE leverages masked body gesture priors during training to boost inference performance. It involves a Masked Audio Gesture Transformer, facilitating joint training on audio-to-gesture generation and masked gesture reconstruction to effectively encode audio and body gesture hints. Encoded body hints from masked gestures are then separately employed to generate facial and body movements. Moreover, EMAGE adaptively merges speech features from the audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance the results' fidelity and diversity. Experiments demonstrate that EMAGE generates holistic gestures with state-of-the-art performance and is flexible in accepting predefined spatial-temporal gesture inputs, generating complete, audio-synchronized results. Our code and dataset are available at https://pantomatrix.github.io/EMAGE/																																	2025-08-07	PPRN:86914830		
J	Shinjo, Kazuya; Seki, Kazuhiro; Shirakawa, Tomonori; Sun, Rong-Yang; Yunoki, Seiji				Shinjo, Kazuya/AAV-1920-2021; Seki, Kazuhiro/C-3558-2016						Unveiling clean two-dimensional discrete time quasicrystals on a digital quantum computer								Arxiv											1	1;2024-03-25;https://www.arxiv.org/abs/2403.16718v1	arXiv:2403.16718			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	In periodically driven (Floquet) systems, evolution typically results in an infinite-temperature thermal state due to continuous energy absorption over time. However, before reaching thermal equilibrium, such systems may transiently pass through a meta-stable state known as a prethermal state. This prethermal state can exhibit phenomena not commonly observed in equilibrium, such as discrete time crystals (DTCs), making it an intriguing platform for exploring out-of-equilibrium dynamics. Here, we investigate the relaxation dynamics of initially prepared product states under periodic driving in a kicked Ising model using the IBM Quantum Heron processor, comprising 133 superconducting qubits arranged on a heavy-hexagonal lattice, over up to 100 time steps. We identify the presence of a prethermal regime characterised by magnetisation measurements oscillating at twice the period of the Floquet cycle and demonstrate its robustness against perturbations to the transverse field. Our results provide evidence supporting the realisation of a period-doubling DTC in a two-dimensional system. Moreover, we discover that the longitudinal field induces additional amplitude modulations in the magnetisation with a period incommensurate with the driving period, leading to the emergence of discrete time quasicrystals (DTQCs). These observations are further validated through comparison with tensor-network and state-vector simulations. Our findings not only enhance our understanding of clean DTCs in two dimensions but also highlight the utility of digital quantum computers for simulating the dynamics of quantum many-body systems, addressing challenges faced by state -of -the -art classical simulations.																																	2024-04-14	PPRN:88282471		
J	Xu, Dejia; Liang, Hanwen; Bhatt, Neel P.; Hu, Hezhen; Liang, Hanxue; Plataniotis, Konstantinos N.; Wang, Zhangyang				Liang, Hanwen/GWQ-5112-2022						Comp4D: LLM-Guided Compositional 4D Scene Generation								Arxiv											1	1;2024-03-25;https://www.arxiv.org/abs/2403.16993v1	arXiv:2403.16993			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	Recent advancements in diffusion models for 2D and 3D content creation have sparked a surge of interest in generating 4D content. However, the scarcity of 3D scene datasets constrains current methodologies to primarily object-centric generation. To overcome this limitation, we present Comp4D, a novel framework for Compositional 4D Generation. Unlike conventional methods that generate a singular 4D representation of the entire scene, Comp4D innovatively constructs each 4D object within the scene separately. Utilizing Large Language Models (LLMs), the framework begins by decomposing an input text prompt into distinct entities and maps out their trajectories. It then constructs the compositional 4D scene by accurately positioning these objects along their designated paths. To refine the scene, our method employs a compositional score distillation technique guided by the pre-defined trajectories, utilizing pre-trained diffusion models across text-to-image, text-to-video, and text-to-3D domains. Extensive experiments demonstrate our outstanding 4D content creation capability compared to prior arts, showcasing superior visual quality, motion fidelity, and enhanced object interactions.																																	2025-08-07	PPRN:123157045		
J	Ma, Wei; Wu, Daoyuan; Sun, Yuqiang; Wang, Tianwen; Liu, Shangqing; Zhang, Jian; Xue, Yue; Liu, Yang				Liu, Shangqing/LCD-8169-2024; Liu, Yang/D-2306-2013; Wu, Daoyuan/Y-3128-2018; Sun, Yuqiang/KBC-6335-2024; Ma, Wei/HJY-8389-2023						Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications								Arxiv											3	3;2024-09-14;https://www.arxiv.org/abs/2403.16073v3| 2;2024-08-17;https://www.arxiv.org/abs/2403.16073v2| 1;2024-03-24;https://www.arxiv.org/abs/2403.16073v1	arXiv:2403.16073			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 24 2024	2024	Smart contracts are decentralized applications built atop blockchains like Ethereum. Recent research has shown that large language models (LLMs) have potential in auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can achieve only 30% precision (when both decision and justification are correct). This is likely because off-the-shelf LLMs were primarily pre-trained on a general text/code corpus and not fine-tuned on the specific domain of Solidity smart contract auditing. In this paper, we propose TrustLLM, a general framework that combines fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications. Specifically, TrustLLM is inspired by the observation that expert human auditors first perceive what could be wrong and then perform a detailed analysis of the code to identify the cause. As such, TrustLLM employs a two-stage fine-tuning approach: it first tunes a Detector model to make decisions and then tunes a Reasoner model to generate causes of vulnerabilities. However, fine-tuning alone faces challenges in accurately identifying the optimal cause of a vulnerability. Therefore, we introduce two LLM-based agents, the Ranker and Critic, to iteratively select and debate the most suitable cause of vulnerability based on the output of the fine-tuned Reasoner model. To evaluate TrustLLM, we collected a balanced dataset with 1,734 positive and 1,810 negative samples to fine-tune TrustLLM. We then compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b). On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%. The causes generated by TrustLLM achieved a consistency of about 38% compared to the ground truth causes.																																	2025-08-07	PPRN:88280463		
J	Yang, Kai; Tao, Jian; Lyu, Jiafei; Ge, Chunjiang; Chen, Jiaxin; Li, Qimai; Shen, Weihan; Zhu, Xiaolong; Li, Xiu				lyu, jiafei/JFB-0806-2023; Yang, Kai/LVR-9531-2024; Ge, Chunjiang/LOS-5681-2024						Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model								Arxiv											3	3;2024-03-23;https://www.arxiv.org/abs/2311.13231v3| 2;2023-11-23;https://www.arxiv.org/abs/2311.13231v2| 1;2023-11-22;https://www.arxiv.org/abs/2311.13231v1	arXiv:2311.13231			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 23 2024	2024	Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. 																																	2024-04-13	PPRN:86241718		
J	Salvio, Alberto										Supercooling in Radiative Symmetry Breaking: Theory Extensions, Gravitational Wave Detection and Primordial Black Holes								Arxiv											3	3;2024-03-22;https://www.arxiv.org/abs/2307.04694v2| 2;2023-07-10;https://www.arxiv.org/abs/2307.04694v1| 1;2023-07-10;https://www.arxiv.org/abs/2307.04694v1	arXiv:2307.04694			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 22 2024	2024	First -order phase transitions, which take place when the symmetries are predominantly broken (and masses are then generated) through radiative corrections, produce observable gravitational waves and primordial black holes. We provide a model -independent approach that is valid for large -enough supercooling to quantitatively describe these phenomena in terms of few parameters, which are computable once the model is specified. The validity of a previously -proposed approach of this sort is extended here to a larger class of theories. Among other things, we identify regions of the parameter space that correspond to the background of gravitational waves recently detected by pulsar timing arrays (NANOGrav, CPTA, EPTA, PPTA) and others that are either excluded by the observing runs of LIGO and Virgo or within the reach of future gravitational wave detectors. Furthermore, we find regions of the parameter space where primordial black holes produced by large over -densities due to such phase transitions can account for dark matter. Finally, it is shown how this model -independent approach can be applied to specific cases, including a phenomenological completion of the Standard Model with right-handed neutrinos and gauged B − L undergoing radiative symmetry breaking.																																	2024-04-13	PPRN:73874267		
J	Lin, Shanchuan; Yang, Xiao										AnimateDiff-Lightning: Cross-Model Diffusion Distillation								Arxiv											1	1;2024-03-19;https://www.arxiv.org/abs/2403.12706v1	arXiv:2403.12706			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 19 2024	2024	We present AnimateDiff-Lightning for lightning-fast video generation. Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation. We discuss our modifications to adapt it for the video modality. Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility. We are pleased to release our distilled AnimateDiff-Lightning model for the community's use.																																	2024-04-12	PPRN:88241548		
J	Rabby, AKM Shahariar Azad; Zhang, Chengcui				Rabby, AKM Shahariar Azad/GLT-8073-2022						BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance Fields								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2306.03000v3| 2;2023-08-16;https://www.arxiv.org/abs/2306.03000v2| 1;2023-06-05;https://www.arxiv.org/abs/2306.03000v1	arXiv:2306.03000			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 18 2024	2024	Neural rendering combines ideas from classical computer graphics and machine learning to synthesize images from real-world observations. NeRF, short for Neural Radiance Fields, is a recent innovation that uses AI algorithms to create 3D objects from 2D images. By leveraging an interpolation approach, NeRF can produce new 3D reconstructed views of complicated scenes. Rather than directly restoring the whole 3D scene geometry, NeRF generates a volumetric representation called a "radiance field,'' which is capable of creating color and density for every point within the relevant 3D space. The broad appeal and notoriety of NeRF make it imperative to examine the existing research on the topic comprehensively. While previous surveys on 3D rendering have primarily focused on traditional computer vision-based or deep learning-based approaches, only a handful of them discuss the potential of NeRF. However, such surveys have predominantly focused on NeRF's early contributions and have not explored its full potential. NeRF is a relatively new technique continuously being investigated for its capabilities and limitations. This survey reviews recent advances in NeRF and categorizes them according to their architectural designs, especially in the field of novel view synthesis.																																	2024-04-12	PPRN:72852751		
J	Liu, Jie; Mozafari, Barzan				Liu, Jiechao/J-4795-2019						Query Rewriting via Large Language Models								Arxiv											1	1;2024-03-14;https://www.arxiv.org/abs/2403.09060v1	arXiv:2403.09060			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Query rewriting is one of the most effective techniques for coping with poorly written queries before passing them down to the query optimizer. Manual rewriting is not scalable, as it is error-prone and requires deep expertise. Similarly, traditional query rewriting algorithms can only handle a small subset of queries: rule-based techniques do not generalize to new query patterns and synthesis-based techniques cannot handle complex queries. Fortunately, the rise of Large Language Models (LLMs), equipped with broad general knowledge and advanced reasoning capabilities, has created hopes for solving some of these previously open problems. In this paper, we present GenRewrite, the first holistic system that leverages LLMs for query rewriting. We introduce the notion of Natural Language Rewrite Rules (NLR2s), and use them as hints to the LLM but also a means for transferring knowledge from rewriting one query to another, and thus becoming smarter and more effective over time. We present a novel counterexample-guided technique that iteratively corrects the syntactic and semantic errors in the rewritten query, significantly reducing the LLM costs and the manual effort required for verification. GenRewrite speeds up 22 out of 99 TPC queries (the most complex public benchmark) by more than 2x, which is 2.5x--3.2x higher coverage than state-of-the-art traditional query rewriting and 2.1x higher than the out-of-the-box LLM baseline.																																	2024-04-11	PPRN:88144766		
J	Inoue, Yuki; Ohashi, Hiroki										Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following								Arxiv											1	1;2024-03-12;https://www.arxiv.org/abs/2211.03267v2	arXiv:2211.03267			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 12 2024	2024	Embodied Instruction Following (EIF) studies how autonomous mobile manipulation robots should be controlled to accomplish long-horizon tasks described by natural language instructions. While much research on EIF is conducted in simulators, the ultimate goal of the field is to deploy the agents in real life. This is one of the reasons why recent methods have moved away from training models end-to-end and take modular approaches, which do not need the costly expert operation data. However, as it is still in the early days of importing modular ideas to EIF, a search for modules effective in the EIF task is still far from a conclusion. In this paper, we propose to extend the modular design using knowledge obtained from two external sources. First, we show that embedding the physical constraints of the deployed robots into the module design is highly effective. Our design also allows the same modular system to work across robots of different configurations with minimal modifications. Second, we show that the landmark-based object search, previously implemented by a trained model requiring a dedicated set of data, can be replaced by an implementation that prompts pretrained large language models for landmark-object relationships, eliminating the need for collecting dedicated training data. Our proposed Prompter achieves 41.53% and 45.32% on the ALFRED benchmark with high-level instructions only and step-by-step instructions, respectively, significantly outperforming the previous state of the art by 5.46% and 9.91%.																																	2024-04-08	PPRN:88120067		
J	Zhang, Bin; Ye, Yuxiao; Du, Guoqing; Hu, Xiaoru; Li, Zhishuai; Yang, Sun; Liu, Chi Harold; Zhao, Rui; Li, Ziyue; Mao, Hangyu				hu, xiaoru/LIH-0577-2024; Li, Ziyue/HZH-3369-2023; Zhang, Bin/U-9174-2019						Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation								Arxiv											1	1;2024-03-06;https://www.arxiv.org/abs/2403.02951v2	arXiv:2403.02951			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 06 2024	2024	Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.																																	2024-04-03	PPRN:88048365		
J	Zhao, Yingxiu; Yu, Bowen; Hui, Binyuan; Yu, Haiyang; Li, Minghao; Huang, Fei; Zhang, Nevin L.; Li, Yongbin				Bowen, Yu/MFH-7462-2025; Bin/B-6414-2019; li, Yongbin/MBG-2998-2025; XIAOJUAN, HU/GLQ-6536-2022						A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment								Arxiv											2	2;2024-02-29;https://www.arxiv.org/abs/2308.05696v2| 1;2023-08-10;https://www.arxiv.org/abs/2308.05696v1	arXiv:2308.05696			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 29 2024	2024	Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and human preferences. Extensive research has highlighted the importance of the quality and diversity of instruction data. However, the impact of data complexity, as a crucial metric, remains relatively unexplored from three aspects: (1)where the sustainability of performance improvements with increasing complexity is uncertain; (2)whether the improvement brought by complexity merely comes from introducing more training tokens; and (3)where the potential benefits of incorporating instructions from easy to difficult are not yet fully understood. In this paper, we propose Tree-Instruct to systematically enhance the instruction complexity in a controllable manner. By adding a specified number of nodes to instructions’ semantic trees, this approach not only yields new instruction data from the modified tree but also allows us to control the difficulty level of modified instructions. Our preliminary experiments reveal the following insights: (1)Increasing complexity consistently leads to sustained performance improvements of LLMs. (2)Under the same token budget, a few complex instructions outperform diverse yet simple instructions. (3)Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key.																																	2024-03-28	PPRN:75319852		
J	Yan, Yuling; Chen, Yuxin; Fan, Jianqing				Fan, Jianqing/B-2115-2008						Inference for Heteroskedastic PCA with Missing Data								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2107.12365v2	arXiv:2107.12365			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	This paper studies how to construct confidence regions for principal component analysis (PCA) in high dimension, a problem that has been vastly under-explored. While computing measures of uncertainty for nonlinear/nonconvex estimators is in general difficult in high dimension, the challenge is further compounded by the prevalent presence of missing data and heteroskedastic noise. We propose a novel approach to performing valid inference on the principal subspace under a spiked covariance model with missing data, on the basis of an estimator called HeteroPCA (Zhang et al., 2022). We develop non-asymptotic distributional guarantees for HeteroPCA, and demonstrate how these can be invoked to compute both confidence regions for the principal subspace and entrywise confidence intervals for the spiked covariance matrix. Our inference procedures are fully data-driven and adaptive to heteroskedastic random noise, without requiring prior knowledge about the noise levels.																																	2024-03-28	PPRN:87985886		
J	Peng, Binghui; Narayanan, Srini; Papadimitriou, Christos										On Limitations of the Transformer Architecture								Arxiv											2	2;2024-02-26;https://www.arxiv.org/abs/2402.08164v2| 1;2024-02-13;https://www.arxiv.org/abs/2402.08164v1	arXiv:2402.08164			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 26 2024	2024	What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.																																	2024-03-24	PPRN:87675425		
J	Jiang, Chaoya; Xu, Haiyang; Dong, Mengfan; Chen, Jiaxing; Ye, Wei; Yan, Ming; Ye, Qinghao; Zhang, Ji; Huang, Fei; Zhang, Shikun				Yan, Ming/LDT-2692-2024; Xu, Haiyang/AAC-2095-2021; Zhang, Shikun/ITU-3545-2023						Hallucination Augmented Contrastive Learning for Multimodal Large Language Model								Arxiv											3	3;2024-02-24;https://www.arxiv.org/abs/2312.06968v4| 2;2024-01-25;https://www.arxiv.org/abs/2312.06968v3| 1;2023-12-13;https://www.arxiv.org/abs/2312.06968v2	arXiv:2312.06968			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 24 2024	2024	Multi-modal large language models (MLLMs) have been shown to efficiently integrate natural language with visual information to handle multi -modal tasks. However, MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information. In this paper, we address hallucinations in MLLMs from a novel perspective of representation learning. We first analyzed the representation distribution of textual and visual tokens in MLLM, revealing two important findings: 1) there is a significant gap between textual and visual representations, indicating unsatisfactory cross -modal representation alignment; 2) representations of texts that contain and do not contain hallucinations are entangled, making it challenging to distinguish them. These two observations inspire us with a simple yet effective method to mitigate hallucinations. Specifically, we introduce contrastive learning into MLLMs and use text with hallucination as hard negative examples, naturally bringing representations of non-hallucinative text and visual samples closer while pushing way representations of non-hallucinating and hallucinative text. We evaluate our method quantitatively and qualitatively, showing its effectiveness in reducing hallucination occurrences and improving performance across multiple benchmarks. On the MMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the baseline MiniGPT-4/LLaVA.																																	2024-03-25	PPRN:86572776		
J	Lu, Qingyu; Qiu, Baopu; Ding, Liang; Zhang, Kanjian; Kocmi, Tom; Tao, Dacheng				Tao, Dacheng/A-5449-2012; Shen, Li/AEZ-9528-2022; Ding, Liang/IXD-6099-2023						Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models								Arxiv											3	3;2024-02-21;https://www.arxiv.org/abs/2303.13809v3| 2;2023-10-08;https://www.arxiv.org/abs/2303.13809v2| 1;2023-03-24;https://www.arxiv.org/abs/2303.13809v1	arXiv:2303.13809			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Feb 21 2024	2024	Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called textbf{texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.																																	2024-03-21	PPRN:49973032		
J	Ma, Yubo; Gou, Zhibin; Hao, Junheng; Xu, Ruochen; Wang, Shuohang; Pan, Liangming; Yang, Yujiu; Cao, Yixin; Sun, Aixin; Awadalla, Hany; Chen, Weizhu				Sun, Aixin/A-9852-2008; Pan, Liangming/LIF-2753-2024; Yang, Yujiu/JGM-0303-2023; cao, yixin/ABV-6408-2022; Xu, Ruochen/KDM-6820-2024; Ma, Yubo/AAK-2005-2021; Hao, Junheng/AAJ-8770-2020						SciAgent: Tool-augmented Language Models for Scientific Reasoning								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2402.11451v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11451v1	arXiv:2402.11451			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 21 2024	2024	Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.																																	2024-11-09	PPRN:87761390		
J	Qian, Yusu; Zhang, Haotian; Yang, Yinfei; Gan, Zhe				Qian, Yu/GQZ-5697-2022; Zhang, Haotian/CAH-0725-2022						How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts								Arxiv											1	1;2024-02-20;https://www.arxiv.org/abs/2402.13220v1	arXiv:2402.13220			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 20 2024	2024	The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench,1 a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini -Pro, to open -sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction -tuned models, such as LRVInstruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD -Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MADBench can serve as a valuable benchmark to stimulate further research to enhance models’ resilience against deceptive prompts.																																	2024-11-09	PPRN:87779029		
J	Mamajek, Eric; Stapelfeldt, Karl										NASA Exoplanet Exploration Program (ExEP) Mission Star List for the Habitable Worlds Observatory (2023)								Arxiv											1	1;2024-02-19;https://www.arxiv.org/abs/2402.12414v1	arXiv:2402.12414			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 19 2024	2024	The Astro 2020 Decadal Survey "Pathways to Discovery in Astronomy and Astrophysics for the 2020s" has recommended that "after a successful mission and technology maturation program, NASA should embark on a program to realize a mission to search for biosignatures from a robust number of about ~25 habitable zone planets and to be a transformative facility for general astrophysics," and prescribing that the high-contrast direct imaging mission would have "a target off-axis inscribed diameter of approximately 6 meters." The Decadal Survey assumed an exo-Earth frequency of ~25%, requiring that approximately 100 cumulative habitable zones of nearby stars should be surveyed. Surveying the nearby bright stars, and taking into account inputs from the LUVOIR and HabEx mission studies (but without being overly prescriptive in the required starlight suppression technology or requirements), we compile a list of ~160 stars whose exo-Earths would be the most accessible for a systematic imaging survey of habitable zones with a 6-m-class space telescope in terms of angular separation, planet brightness in reflected light, and planet-star brightness ratio. We compile this star list to motivate observations and analysis to help inform observatory design (mission-enabling "precursor science") and enhance the science return of the Habitable Worlds Observatory (HWO) survey for exo-Earths (mission-enhancing "preparatory science"). It is anticipated that this list of target stars and their properties will be updated periodically by the NASA Exoplanet Exploration Program.																																	2024-03-19	PPRN:87777007		
J	Shao, Minghao; Chen, Boyuan; Jancheska, Sofija; Dolan-Gavitt, Brendan; Garg, Siddharth; Karri, Ramesh; Shafique, Muhammad				Chen, Bo/B-7817-2018; Sadiq, Muhammad/HMP-3877-2023						An Empirical Evaluation of LLMs for Solving Offensive Security Challenges								Arxiv											1	1;2024-02-19;https://www.arxiv.org/abs/2402.11814v1	arXiv:2402.11814			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 19 2024	2024	Capture The Flag (CTF) challenges are puzzles related to computer security scenarios. With the advent of large language models (LLMs), more and more CTF participants are using LLMs to understand and solve the challenges. However, so far no work has evaluated the effectiveness of LLMs in solving CTF challenges with a fully automated workflow. We develop two CTF-solving workflows, human-in-the-loop (HITL) and fully-automated, to examine the LLMs' ability to solve a selected set of CTF challenges, prompted with information about the question. We collect human contestants' results on the same set of questions, and find that LLMs achieve higher success rate than an average human participant. This work provides a comprehensive evaluation of the capability of LLMs in solving real world CTF challenges, from real competition to fully automated workflow. Our results provide references for applying LLMs in cybersecurity education and pave the way for systematic evaluation of offensive cybersecurity capabilities in LLMs.																																	2024-03-15	PPRN:87759161		
J	Zhang, Lei; Zhang, Yuge; Ren, Kan; Li, Dongsheng; Yang, Yuqing				Yuqing, Yang/ADJ-2720-2022; zhang, yufei/ABV-5938-2022						MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks								Arxiv											2	2;2024-02-18;https://www.arxiv.org/abs/2304.14979v2| 1;2023-04-28;https://www.arxiv.org/abs/2304.14979v1	arXiv:2304.14979			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 18 2024	2024	The field of machine learning (ML) has gained widespread adoption, leading to significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time-consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework, which leverages the state-of-the-art large language models to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness.																																	2024-03-15	PPRN:66252267		
J	Bhendawade, Nikhil; Belousova, Irina; Fu, Qichen; Mason, Henry; Rastegari, Mohammad; Najibi, Mahyar										Speculative Streaming: Fast LLM Inference without Auxiliary Models								Arxiv											1	1;2024-02-16;https://www.arxiv.org/abs/2402.11131v1	arXiv:2402.11131			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 16 2024	2024	Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while using ~10000X fewer extra parameters, making it well-suited for resource-constrained devices.																																	2024-03-21	PPRN:87798551		
J	Olausson, Theo X.; Gu, Alex; Lipkin, Benjamin; Zhang, Cedegao E.; Solar-Lezama, Armando; Tenenbaum, Joshua B.; Levy, Roger										LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers								Arxiv											2	2;2024-02-14;https://www.arxiv.org/abs/2310.15164v2| 1;2023-10-23;https://www.arxiv.org/abs/2310.15164v1	arXiv:2310.15164			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 14 2024	2024	Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting -based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first -order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open -source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain -of -Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.1																																	2024-05-25	PPRN:85753097		
J	Fomichev, Stepan; Hejazi, Kasra; Zini, Modjtaba Shokrian; Kiser, Matthew; Fraxanet, Joana; Casares, Pablo Antonio Moreno; Delgado, Alain; Huh, Joonsuk; Voigt, Arne-Christian; Mueller, Jonathan E.; Arrazola, Juan Miguel				Shokrian Zini, Modjtaba/AAD-3501-2022; Moreno Casares, Pablo Antonio/HSI-1599-2023; Delgado Gran, Alain/AGQ-8617-2022						Initial state preparation for quantum chemistry on quantum computers								Arxiv											2	2;2024-02-09;https://www.arxiv.org/abs/2310.18410v2| 1;2023-10-27;https://www.arxiv.org/abs/2310.18410v1	arXiv:2310.18410			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 09 2024	2024	Quantum algorithms for ground-state energy estimation of chemical systems require a high-quality initial state. However, initial state preparation is commonly either neglected entirely, or assumed to be solved by a simple product state like Hartree-Fock. Even if a nontrivial state is prepared, strong correlations render ground state overlap inadequate for quality assessment. In this work, we address the initial state preparation problem with an end-to-end algorithm that prepares and quantifies the quality of initial states, accomplishing the latter with a new metric – the energy distribution. To be able to prepare more complicated initial states, we introduce an implementation technique for states in the form of a sum of Slater determinants that exhibits significantly better scaling than all prior approaches. We also propose low-precision quantum phase estimation (QPE) for further state quality refinement. The complete algorithm is capable of generating high-quality states for energy estimation, and is shown in select cases to lower the overall estimation cost by several orders of magnitude when compared with the best single product state ansatz. More broadly, the energy distribution picture suggests that the goal of QPE should be reinterpreted as generating improvements compared to the energy of the initial state and other classical estimates, which can still be achieved even if QPE does not project directly onto the ground state. Finally, we show how the energy distribution can help in identifying potential quantum advantage.																																	2024-05-25	PPRN:85889472		
J	Beurer-Kellner, Luca; Fischer, Marc; Vechev, Martin										Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation								Arxiv											1	1;2024-02-07;https://www.arxiv.org/abs/2403.06988v1	arXiv:2403.06988			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 07 2024	2024	To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub -word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre -computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2× speedup over unconstrained decoding – thereby outperforming existing approaches by a wide margin.																																	2024-04-08	PPRN:88119677		
J	Liu, Tennison; Astorga, Nicolas; Seedat, Nabeel; van der Schaar, Mihaela										Large Language Models to Enhance Bayesian Optimization								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2402.03921v1	arXiv:2402.03921			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 06 2024	2024	Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive -to -evaluate black -box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present LLAMBO, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few -shot learning proficiency, and domain knowledge of LLMs can enhance various components of model -based BO. Our findings illustrate that LLAMBO is effective at zero -shot warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end -to -end method. We empirically validate LLAMBO’s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.																																	2024-02-21	PPRN:87533541		
J	Zeng, Zhongshen; Chen, Pengguang; Liu, Shu; Jiang, Haiyun; Jia, Jiaya				Jiang, Haiyun/JKI-8101-2023; Jia, Jiaya/I-3251-2012						MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation								Arxiv											3	3;2024-02-06;https://www.arxiv.org/abs/2312.17080v3| 2;2024-01-20;https://www.arxiv.org/abs/2312.17080v2| 1;2023-12-28;https://www.arxiv.org/abs/2312.17080v1	arXiv:2312.17080			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta -reasoning. This approach addresses critical shortcomings in existing math problem -solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result -oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance five times better than GPT3.5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open -source and closedsource communities, uncovering fundamental deficiencies in their training and evaluation approaches.																																	2024-05-25	PPRN:86850859		
J	Zhang, Michael; Bhatia, Kush; Kumbong, Hermann; Re, Christopher										The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry								Arxiv											1	1;2024-02-06;https://www.arxiv.org/abs/2402.04347v1	arXiv:2402.04347			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 06 2024	2024	Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) "finetuned-conversion" of task-specific Transformers into linear versions that recover task performance, and (3) "pretrained-conversion" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or "spiky") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.																																	2024-05-25	PPRN:87553231		
J	Perlitz, Yotam; Bandel, Elron; Gera, Ariel; Arviv, Ofir; Ein-Dor, Liat; Shnarch, Eyal; Slonim, Noam; Shmueli-Scheuer, Michal; Choshen, Leshem										Efficient Benchmarking (of Language Models)								Arxiv											4	4;2024-01-30;https://www.arxiv.org/abs/2308.11696v4| 3;2023-09-18;https://www.arxiv.org/abs/2308.11696v3| 2;2023-08-31;https://www.arxiv.org/abs/2308.11696v2| 1;2023-08-22;https://www.arxiv.org/abs/2308.11696v1	arXiv:2308.11696			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 30 2024	2024	The increasing versatility of language models (LMs) has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs, extending to thousands of GPU hours per model. However, the efficiency aspect of these evaluation efforts had raised little discussion in the literature.   In this work, we present the problem of Efficient Benchmarking, namely, intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case, we investigate how different benchmark design choices affect the computation-reliability trade-off. We propose to evaluate the reliability of such decisions, by using a new measure – Decision Impact on Reliability, DIoR for short. We find, for example, that a benchmark leader may change by merely removing a low-ranked model from the benchmark, and observe that a correct benchmark ranking can be obtained by considering only a fraction of the evaluation examples. Based on our findings, we outline a set of concrete recommendations for efficient benchmark design and utilization practices. To take a step further, we use our finding to propose an evaluation algorithm, that, when applied to the HELM benchmark, leads to dramatic cost savings with minimal loss of benchmark reliability, often reducing computation by x100 or more.1																																	2024-02-16	PPRN:82994703		
J	Zhong, Li; Wang, Zilong				Wang, Zilong/C-3784-2012						Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation								Arxiv											4	4;2024-01-27;https://www.arxiv.org/abs/2308.10335v5| 3;2023-10-17;https://www.arxiv.org/abs/2308.10335v4| 2;2023-08-27;https://www.arxiv.org/abs/2308.10335v2| 1;2023-08-20;https://www.arxiv.org/abs/2308.10335v1	arXiv:2308.10335			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 27 2024	2024	Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right - They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in the code generated by LLMs, which further facilitates the incorrect code applied in real-world software. Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from StackOverflow on 24 representative Java APIs. We summarize thecommon misuse patterns of these APIs and evaluate them oncurrent popular LLMs. The evaluation results show that evenfor GPT-4, 62% of the generated code contains API misuses,which would cause unexpected consequences if the code isintroduced into real-world software.																																	2024-02-15	PPRN:82038778		
J	Cheng, Shu-Lin; Lee, Da-Shin; Ng, Kin-Wang				Fu, Chenglai/H-5330-2018; Ng, Kin-Wang/AAP-8348-2021						Primordial perturbations from ultra-slow-roll single-field inflation with quantum loop effects								Arxiv											4	4;2024-01-25;https://www.arxiv.org/abs/2305.16810v5| 3;2023-12-07;https://www.arxiv.org/abs/2305.16810v4| 2;2023-09-09;https://www.arxiv.org/abs/2305.16810v3| 1;2023-05-26;https://www.arxiv.org/abs/2305.16810v1	arXiv:2305.16810			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jan 25 2024	2024	It is known that the single-field inflation with a transient ultra-slow-roll phase can produce a large curvature perturbation at small scales for the formation of primordial black holes. In our previous work, we have considered quantum loop corrections to the curvature perturbation and found that the growth of these small-scale modes would affect the curvature perturbation at large scales probed by cosmic microwave background observation. In this work, we will further derive the constraints on the growing modes in the transition between the slow-roll and the ultra-slow-roll phases under the effect of the loop corrections. Our results would help clarify the recent controversy on whether or not the primordial-black-hole formation from the single-field inflation is ruled out at one-loop level.																																	2024-02-12	PPRN:72727908		
J	Jayasumana, Sadeep; Ramalingam, Srikumar; Veit, Andreas; Glasner, Daniel; Chakrabarti, Ayan; Kumar, Sanjiv				Jayasumana, Sadeep/W-1590-2019						Rethinking FID: Towards a Better Evaluation Metric for Image Generation								Arxiv											2	2;2024-01-25;https://www.arxiv.org/abs/2401.09603v2| 1;2023-11-30;https://www.arxiv.org/abs/2401.09603v1	arXiv:2401.09603			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 25 2024	2024	As with many machine learning problems, the progress of image generation methods hinges on good evaluation metrics. One of the most popular is the Fre´chet Inception Distance (FID). FID estimates the distance between a distribution of Inception-v3 features of real images, and those of images generated by the algorithm. We highlight important drawbacks of FID: Inception’s poor representation of the rich and varied content generated by modern text-to image models, incorrect normality assumptions, and poor sample complexity. We call for a reevaluation of FID’s use as the primary quality metric for generated images. We empirically demonstrate that FID contradicts human raters, it does not reflect gradual improvement of iterative text-to image models, it does not capture distortion levels, and that it produces inconsistent results when varying the sample size. We also propose an alternative new metric, CMMD, based on richer CLIP embeddings and the maximum mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased estimator that does not make any assumptions on the probability distribution of the embeddings and is sample efficient. Through extensive experiments and analysis, we demonstrate that FID-based evaluations of text to-image models may be unreliable, and that CMMD offers a more robust and reliable assessment of image quality. A reference implementation of CMMD is available at: https://github.com/google-research/googleresearch/tree/master/cmmd.																																	2024-05-25	PPRN:87221260		
J	Zhang, Dong; Zhang, Xin; Zhan, Jun; Li, Shimin; Zhou, Yaqian; Qiu, Xipeng										SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation								Arxiv											1	1;2024-01-25;https://www.arxiv.org/abs/2401.13527v2	arXiv:2401.13527			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jan 25 2024	2024	Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. 																																	2024-05-25	PPRN:87331202		
J	Law, Casey J; Sharma, Kritti; Ravi, Vikram; Chen, Ge; Catha, Morgan; Connor, Liam; Faber, Jakob T; Hallinan, Gregg; Harnach, Charlie; Hellbourg, Greg; Hobbs, Rick; Hodge, David; Hodges, Mark; Lamb, James W; Rasmussen, Paul; Sherman, Myles B; Shi, Jun; Simard, Dana; Squillace, Reynier; Weinreb, Sander; Woody, David P; Yadlapalli, Nitika				Law, Casey/AAW-3133-2021; Ravi, Vikram/AAR-6952-2020; Jun, Shi/KVZ-1457-2024						Deep Synoptic Array Science: First FRB and Host Galaxy Catalog								Arxiv											2	2;2024-01-24;https://www.arxiv.org/abs/2307.03344v2| 1;2023-07-07;https://www.arxiv.org/abs/2307.03344v1	arXiv:2307.03344			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 24 2024	2024	Fast Radio Bursts (FRBs) are a powerful and mysterious new class of transient that are luminous enough to be detected at cosmological distances. By associating FRBs to host galaxies, we can measure intrinsic and environmental properties that test FRB origin models, in addition to using them as precise probes of distant cosmic gas. The Deep Synoptic Array (DSA-110) is a radio interferometer built to maximize the rate at which it can simultaneously detect and localize FRBs. Here, we present the first sample of FRBs and host galaxies discovered by the DSA-110. This sample of 11 FRBs is the largest, most uniform sample of localized FRBs to date , as it is selected based on association to host galaxies identified in optical imaging by Pan-STARRS1 . These FRBs have not been observed to repeat and their radio properties (dispersion, temporal scattering, energy) are similar to that of the known non-repeating FRB population. Most host galaxies have ongoing star formation, as has been identified before for FRB hosts. Two hosts of the new sample are massive, quiescent galaxies. The distribution of star-formation history across this host-galaxy sample shows that the delay-time distribution is wide, with a powerlaw model that spans from ∼ 100 Myr to ≳ 2Gyr. This requires the existence of one or more progenitor formation channels associated with old stellar populations, such as the binary evolution of compact objects.																																	2024-03-13	PPRN:73840167		
J	Zhang, Yinan; Tzeng, Eric; Du, Yilun; Kislyuk, Dmitry										Large-scale Reinforcement Learning for Diffusion Models								Arxiv											1	1;2024-01-20;https://www.arxiv.org/abs/2401.12244v1	arXiv:2401.12244			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 20 2024	2024	Text-to-image diffusion models are a class of deep generative models that have demonstrated an impressive capacity for high-quality image generation. However, these models are susceptible to implicit biases that arise from web-scale text-image training pairs and may inaccurately model aspects of images we care about. This can result in suboptimal samples, model bias, and images that do not align with human ethics and preferences. In this paper, we present an effective scalable algorithm to improve diffusion models using Reinforcement Learning (RL) across a diverse set of reward functions, such as human preference, compositionality, and fairness over millions of images. We illustrate how our approach substantially outperforms existing methods for aligning diffusion models with human preferences. We further illustrate how this substantially improves pretrained Stable Diffusion (SD) models, generating samples that are preferred by humans 80.3% of the time over those from the base SD model while simultaneously improving both the composition and diversity of generated samples.																																	2024-05-25	PPRN:87301213		
J	Nakaji, Kouhei; Kristensen, Lasse Bjorn; Campos-Gonzalez-Angulo, Jorge A.; Vakili, Mohammad Ghazi; Huang, Haozhe; Bagherimehrab, Mohsen; Gorgulla, Christoph; Wong, Fute; Mccaskey, Alex; Kim, Jin-Sung; Nguyen, Thien; Rao, Pooja; Aspuru-Guzik, Alan				Vakili, Mohammad/AAQ-6200-2020; Kristensen, Lasse/OZF-6146-2025; Gorgulla, Christoph/HGU-9277-2022; Bagherimehrab, Mohsen/AFX-7645-2022; Aspuru-Guzik, Alan/A-4984-2008						The generative quantum eigensolver (GQE) and its application for ground state search								Arxiv											1	1;2024-01-17;https://www.arxiv.org/abs/2401.09253v1	arXiv:2401.09253			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 17 2024	2024	We introduce the generative quantum eigensolver (GQE), a novel method for applying classical generative models for quantum simulation. The GQE algorithm optimizes a classical generative model to produce quantum circuits with desired properties. Here, we develop a transformer-based implementation, which we name the generative pre-trained transformer-based (GPT) quantum eigensolver (GPT-QE), leveraging both pre-training on existing datasets and training without any prior knowledge. We demonstrate the effectiveness of training and pre-training GPT-QE in the search for ground states of electronic structure Hamiltonians. GQE strategies can extend beyond the problem of Hamiltonian simulation into other application areas of quantum computing.																																	2024-05-25	PPRN:87201081		
J	Walke, Homer; Black, Kevin; Lee, Abraham; Kim, Moo Jin; Du, Max; Zheng, Chongyi; Zhao, Tony; Hansen-Estruch, Philippe; Vuong, Quan; He, Andre; Myers, Vivek; Fang, Kuan; Finn, Chelsea; Levine, Sergey				Kim, Moo-Jin/KCK-0674-2024; Vuong, Quan-Hoang/F-2115-2010; Lee, Abraham/E-7401-2011						BridgeData V2: A Dataset for Robot Learning at Scale								Arxiv											3	3;2024-01-17;https://www.arxiv.org/abs/2308.12952v3| 2;2023-09-21;https://www.arxiv.org/abs/2308.12952v2| 1;2023-08-24;https://www.arxiv.org/abs/2308.12952v1	arXiv:2308.12952			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 17 2024	2024	We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that training on a greater variety of skills leads to improved generalization. By publicly sharing BridgeData V2 and our pre-trained models, we aim to accelerate research in scalable robot learning methods. [GRAPHICS]																																	2024-05-25	PPRN:83525251		
J	Zhang, Yazhou; Wang, Mengyao; Wu, Youxi; Tiwari, Prayag; Li, Qiuchi; Wang, Benyou; Qin, Jing				Qin, Jing/J-9807-2016; Tiwari, Prayag/N-6261-2017; Wang, Benyou/Y-5146-2019; Li, Qiuchi/MDT-3578-2025						DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations								Arxiv											4	4;2024-01-17;https://www.arxiv.org/abs/2310.11374v4| 3;2023-12-18;https://www.arxiv.org/abs/2310.11374v3| 2;2023-12-08;https://www.arxiv.org/abs/2310.11374v2| 1;2023-10-17;https://www.arxiv.org/abs/2310.11374v1	arXiv:2310.11374			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 17 2024	2024	Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the SOTA baselines and other SOTA LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.																																	2024-05-25	PPRN:85671283		
J	Bandara, Wele Gedara Chaminda; Nair, Nithin Gopalakrishnan; Patel, Vishal M.				Bandara, Wele Gedara Chaminda/ADX-2372-2022; Patel, Vishal/ABD-2012-2021						DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Change Detection								Arxiv											2	2;2024-01-12;https://www.arxiv.org/abs/2206.11892v3| 1;2022-06-23;https://www.arxiv.org/abs/2206.11892v2	arXiv:2206.11892			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 12 2024	2024	Remote sensing change detection is crucial for understanding the dynamics of our planet's surface, facilitating the monitoring of environmental changes, evaluating human impact, predicting future trends, and supporting decision-making. In this work, we introduce a novel approach for change detection that can leverage off-the-shelf, unlabeled remote sensing images in the training process by pre-training a Denoising Diffusion Probabilistic Model (DDPM) - a class of generative models used in image synthesis. DDPMs learn the training data distribution by gradually converting training images into a Gaussian distribution using a Markov chain. During inference (i.e., sampling), they can generate a diverse set of samples closer to the training distribution, starting from Gaussian noise, achieving state-of-the-art image synthesis results. However, in this work, our focus is not on image synthesis but on utilizing it as a pre-trained feature extractor for the downstream application of change detection. Specifically, we fine-tune a lightweight change classifier utilizing the feature representations produced by the pre-trained DDPM alongside change labels. Experiments conducted on the LEVIR-CD, WHU-CD, DSIFN-CD, and CDD datasets demonstrate that the proposed DDPM-CD method significantly outperforms the existing state-of-the-art change detection methods in terms of F1 score, IoU, and overall accuracy, highlighting the pivotal role of pre-trained DDPM as a feature extractor for downstream applications. 																																	2024-01-31	PPRN:12235465		
J	Poplawski, Nikodem										Classical Physics: Spacetime and Fields								Arxiv											1	1;2024-01-11;https://www.arxiv.org/abs/0911.0334v3	arXiv:0911.0334			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 11 2024	2024	We present a self-contained introduction to the classical theory of spacetime and fields. This exposition is based on the most general principles: the principle of general covariance (relativity) and the principle of least action. The order of the exposition is: 1. Spacetime (principle of general covariance and tensors, affine connection, curvature, metric, space and time, tetrad and spin connection, Lorentz group, spinors); 2. Fields (principle of least action, gravitational field, matter, symmetries and conservation laws, particle limit of field, gravitational field equations, spinor fields, electromagnetic field). In this order, a particle is a special case of a field existing in spacetime, and classical mechanics can be derived from field theory.																																	2024-05-25	PPRN:87156706		
J	Vincenzi, M.; Brout, D.; Armstrong, P.; Popovic, B.; Taylor, G.; Acevedo, M.; Camilleri, R.; Chen, R.; Davis, T.M.; Lee, J.; Lidman, C.; Hinton, S.R.; Kelsey, L.; Kessler, R.; Moeller, A.; Qu, H.; Sako, M.; Sanchez, B.; Scolnic, D.; Smith, M.; Sullivan, M.; Wiseman, P.; Asorey, J.; Bassett, B.A.; Carollo, D.; Carr, A.; Foley, R.J.; Frohmaier, C.; Galbany, L.; Glazebrook, K.; Kovacs, E.; Kuehn, K.; Malik, U.; Nichol, R.C.; Rose, B.; Tucker, B.E.; Toy, M.; Tucker, D.L.; Yuan, F.; Abbott, T.M.C.; Aguena, M.; Alves, O.; Andrade-Oliveira, F.; Annis, J.; Bacon, D.; Bechtol, K.; Bernstein, G.M.; Brooks, D.; Burke, D.L.; Carnero Rosell, A.; Carretero, J.; Castander, F.J.; Conselice, C.; da Costa, L.N.; Pereira, M.E.S.; Desai, S.; Diehl, H.T.; Doel, P.; Ferrero, I.; Flaugher, B.; Friedel, D.; Frieman, J.; Garcia-Bellido, J.; Gatti, M.; Giannini, G.; Gruen, D.; Gruendl, R.A.; Hollowood, D.L.; Honscheid, K.; Huterer, D.; James, D.J.; Kuropatkin, N.; Lahav, O.; Lee, S.; Lin, H.; Marshall, J.L.; Mena-Fernandez, J.; Menanteau, F.; Miquel, R.; Palmese, A.; Pieres, A.; Plazas Malagon, A.A.; Porredon, A.; Romer, A.K.; Roodman, A.; Sanchez, E.; Sanchez Cid, D.; Schubnell, M.; Sevilla-Noarbe, I.; Suchyta, E.; Swanson, M.E.C.; Tarle, G.; To, C.; Walker, A.R.; Weaverdyck, N.				Asorey, Jacobo/AAV-8707-2020; Garcia-Bellido, Juan/C-2920-2017; Galbany, Lluís/A-8963-2017; Sullivan, Mark/KJM-3800-2024; Porredon, Anna/S-4780-2016; Carnero, Aurelio/AAH-3411-2019; Sanchez, Eusebio/H-5228-2015; SCOLNIC, DANIEL/OHR-7390-2025; CHEN, REBECCA/KIX-2591-2024; Armstrong, Patrick/MIK-4482-2025; Bassett, David/B-2489-2009; Glazebrook, Karl/N-3488-2015; Carr, Anthony/HPD-1155-2023; Wiseman, Philip/HTQ-8020-2023; Castander, Francisco/E-8021-2017; Frieman, Josh/AED-8202-2022; Hinton, Samuel/AAU-2376-2020; Kuropatkin, Nikita/ODM-7732-2025; Tucker, Brad/KIJ-7488-2024; Taylor, Georgina/IWM-3368-2023						The Dark Energy Survey Supernova Program: Cosmological Analysis and Systematic Uncertainties								Arxiv											1	1;2024-01-05;https://www.arxiv.org/abs/2401.02945v1	arXiv:2401.02945			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 05 2024	2024	We present the full Hubble diagram of photometrically-classified Type Ia supernovae (SNe Ia) from the Dark Energy Survey supernova program (DES-SN). DES-SN discovered more than 20,000 SN candidates and obtained spectroscopic redshifts of 7,000 host galaxies. Based on the light-curve quality, we select 1635 photometrically-identified SNe Ia with spectroscopic redshift 0.10 < z <1.13, which is the largest sample of supernovae from any single survey and increases the number of known z > 0.5 supernovae by a factor of five. In a companion paper we present cosmological results of the DES-SN sample combined with 194 spectroscopically-classified SNe Ia at low redshift as an anchor for cosmological fits. Here we present extensive modeling of this combined sample and validate the entire analysis pipeline used to derive distances. We show that the statistical and systematic uncertainties on cosmological parameters are σΩM,stat+sys ΛCDM=0.017 in a flat ΛCDM model, and (σΩM, σw)stat+syswCDM = (0.082, 0.152) in a flat wCDM model. Combining the DES SN data to the highly complementary CMB measurements by Planck Collaboration (2020) reduces by a factor of 4 uncertainties on cosmological parameters. In all cases, statistical uncertainties dominate over systematics. We show that uncertainties due to photometric classification make up less than 10% of the total systematic uncertainty budget. This result sets the stage for the next generation of SN cosmology surveys such as the Vera C. Rubin Observatory’s Legacy Survey of Space and Time.																																	2025-01-24	PPRN:86997340		
J	Bansal, Rachit; Samanta, Bidisha; Dalmia, Siddharth; Gupta, Nitish; Vashishth, Shikhar; Ganapathy, Sriram; Bapna, Abhishek; Jain, Prateek; Talukdar, Partha				Samanta, Bidisha/NPI-7619-2025; Vashishth, Shikhar/E-7030-2019						LLM Augmented LLMs: Expanding Capabilities through Composition								Arxiv											1	1;2024-01-04;https://www.arxiv.org/abs/2401.02412v1	arXiv:2401.02412			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 04 2024	2024	Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.																																	2024-05-25	PPRN:86964731		
J	Bocquet, S.; Grandis, S.; Bleem, L.E.; Klein, M.; Mohr, J.J.; Schrabback, T.; Abbott, T.M.C.; Ade, P.A.R.; Aguena, M.; Alarcon, A.; Allam, S.; Allen, S.W.; Alves, O.; Amon, A.; Anderson, A.J.; Annis, J.; Ansarinejad, B.; Austermann, J.E.; Avila, S.; Bacon, D.; Bayliss, M.; Beall, J.A.; Bechtol, K.; Becker, M.R.; Bender, A.N.; Benson, B.A.; Bernstein, G.M.; Bhargava, S.; Bianchini, F.; Brodwin, M.; Brooks, D.; Bryant, L.; Campos, A.; Canning, R.E.A.; Carlstrom, J.E.; Carnero Rosell, A.; Carrasco Kind, M.; Carretero, J.; Castander, F.J.; Cawthon, R.; Chang, C.L.; Chang, C.; Chaubal, P.; Chen, R.; Chiang, H.C.; Choi, A.; Chou, T-l.; Citron, R.; Corbett Moran, C.; Cordero, J.; Costanzi, M.; Crawford, T.M.; Crites, A.T.; da Costa, L.N.; Pereira, M.E.S.; Davis, C.; Davis, T.M.; Derose, J.; Desai, S.; de Haan, T.; Diehl, H.T.; Dobbs, M.A.; Dodelson, S.; Doux, C.; Drlica-Wagner, A.; Eckert, K.; Elvin-Poole, J.; Everett, S.; Everett, W.; Ferrero, I.; Ferte, A.; Flores, A.M.; Frieman, J.; Gallicchio, J.; Garcia-Bellido, J.; Gatti, M.; George, E.M.; Giannini, G.; Gladders, M.D.; Gruen, D.; Gruendl, R.A.; Gupta, N.; Gutierrez, G.; Halverson, N.W.; Harrison, I.; Hartley, W.G.; Herner, K.; Hinton, S.R.; Holder, G.P.; Hollowood, D.L.; Holzapfel, W.L.; Honscheid, K.; Hrubes, J.D.; Huang, N.; Hubmayr, J.; Huff, E.M.; Huterer, D.; Irwin, K.D.; James, D.J.; Jarvis, M.; Khullar, G.; Kim, K.; Knox, L.; Kraft, R.; Krause, E.; Kuehn, K.; Kuropatkin, N.; Keruzore, F.; Lahav, O.; Lee, A.T.; Leget, P.-f.; Li, D.; Lin, H.; Lowitz, A.; Maccrann, N.; Mahler, G.; Mantz, A.; Marshall, J.L.; Mccullough, J.; Mcdonald, M.; Mcmahon, J.J.; Mena-Fernandez, J.; Menanteau, F.; Meyer, S.S.; Miquel, R.; Montgomery, J.; Myles, J.; Natoli, T.; Navarro-Alsina, A.; Nibarger, J.P.; Noble, G.I.; Novosad, V.; Ogando, R.L.C.; Omori, Y.; Padin, S.; Pandey, S.; Paschos, P.; Patil, S.; Pieres, A.; Plazas Malagon, A.A.; Porredon, A.; Prat, J.; Pryke, C.; Raveri, M.; Reichardt, C.L.; Roberson, J.; Rollins, R.P.; Romero, C.; Roodman, A.; Ruhl, J.E.; Rykoff, E.S.; Saliwanchik, B.R.; Salvati, L.; Sanchez, C.; Sanchez, E.; Sanchez Cid, D.; Saro, A.; Schaffer, K.K.; Secco, L.F.; Sevilla-Noarbe, I.; Sharon, K.; Sheldon, E.; Shin, T.; Sievers, C.; Smecher, G.; Smith, M.; Somboonpanyakul, T.; Sommer, M.; Stalder, B.; Stark, A.A.; Stephen, J.; Strazzullo, V.; Suchyta, E.; Tarle, G.; To, C.; Troxel, M.A.; Tucker, C.; Tutusaus, I.; Varga, T.N.; Veach, T.; Vieira, J.D.; Vikhlinin, A.; von der Linden, A.; Wang, G.; Weaverdyck, N.; Weller, J.; Whitehorn, N.; Wu, W.L.K.; Yanny, B.; Yefremenko, V.; Yin, B.; Young, M.; Zebrowski, J.A.; Zhang, Y.; Zohren, H.; Zuntz, J.				Sanchez, Eusebio/H-5228-2015; Krause, Elisabeth/KJL-5339-2024; Avila, Santiago/GPF-5137-2022; Ogando, Ricardo/A-1747-2010; Reichardt, Christian/IST-5017-2023; Carretero, Jorge/L-8237-2014; Alves, Otavio/KIE-9976-2024; Grandis, Sebastian/KCY-7025-2024; Gatti, Marco/AAP-7559-2021; Frieman, Josh/AED-8202-2022; Garcia-Bellido, Juan/C-2920-2017; Raveri, Marco/AAR-5633-2020; Hinton, Samuel/AAU-2376-2020; Somboonpanyakul, Taweewat/JQN-1946-2023; Ansarinejad, Behzad/HHZ-1473-2022; Porredon, Anna/S-4780-2016; Pereira, Moisés/JCE-2886-2023; Ruhl, John/HHS-2212-2022						SPT Clusters with DES and HST Weak Lensing. II. Cosmological Constraints from the Abundance of Massive Halos								Arxiv											1	1;2024-01-04;https://www.arxiv.org/abs/2401.02075v1	arXiv:2401.02075			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 04 2024	2024	We present cosmological constraints from the abundance of galaxy clusters selected via the thermal Sunyaev-Zel’dovich (SZ) effect in South Pole Telescope (SPT) data with a simultaneous mass calibration using weak gravitational lensing data from the Dark Energy Survey (DES) and the Hubble Space Telescope (HST). The cluster sample is constructed from the combined SPT-SZ, SPTpol ECS, and SPTpol 500d surveys, and comprises 1,005 confirmed clusters in the redshift range 0.25 − 1.78 over a total sky area of 5,200 deg2. We use DES Year 3 weak-lensing data for 688 clusters with redshifts z < 0.95 and HST weak-lensing data for 39 clusters with 0.6 < z < 1.7. The weak-lensing measurements enable robust mass measurements of sample clusters and allow us to empirically con-strain the SZ observable–mass relation without having to make strong assumptions about, e.g., the hydrodynamical state of the clusters. For a flat ΛCDM cosmology, and marginalizing over the sum of massive neutrinos, we measure Ωm = 0.286 ± 0.032, σ8 = 0.817 ± 0.026, and the parameter combination σ8 (Ωm/0.3)0.25 = 0.805 ± 0.016. Our measurement of S8 ≡σ8 √Ωm/0.3 = 0.795 ± 0.029 and the constraint from Planck CMB anisotropies (2018 TT,TE,EE+lowE) differ by 1.1σ. In com-bination with that Planck dataset, we place a 95% upper limit on the sum of neutrino masses Pmν < 0.18 eV. When additionally allowing the dark energy equation of state parameter w to vary, we obtain w = −1.45 ± 0.31 from our cluster-based analysis. In combination with Planck data, we measure w = −1.34−0.15+0.22, or a 2.2σ difference with a cosmological constant. We use the cluster abundance to measure σ8 in five redshift bins between 0.25 and 1.8, and we find the results to be consistent with structure growth as predicted by the ΛCDM model fit to Planck primary CMB data.																																	2025-01-24	PPRN:86971218		
J	Zhou, Zhaokun; Che, Kaiwei; Fang, Wei; Tian, Keyu; Zhu, Yuesheng; Yan, Shuicheng; Tian, Yonghong; Yuan, Li				Yuan, Li/AET-1324-2022; Yan, Shuicheng/HCI-1431-2022; TIAN, Yonghong/M-4937-2013; Zhou, zhaokun/JFA-9413-2023						Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket								Arxiv											1	1;2024-01-04;https://www.arxiv.org/abs/2401.02020v1	arXiv:2401.02020			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 04 2024	2024	Spiking Neural Networks (SNNs), known for their biologically plausible architecture, face the challenge of limited performance. The self-attention mechanism, which is the cornerstone of the high-performance Transformer and also a biologically inspired structure, is absent in existing SNNs. To this end, we explore the potential of leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA) and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for softmax and captures the sparse visual feature employing spike-based Query, Key, and Value. This sparse computation without multiplication makes SSA efficient and energy-saving. Further, we develop a Spiking Convolutional Stem (SCS) with supplementary convolutional layers to enhance the architecture of Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer V2. To train larger and deeper Spikformer V2, we introduce a pioneering exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we pre-train Spikformer V2 with masking and reconstruction style inspired by the mainstream self-supervised Transformer, and then finetune the Spikformer V2 on the image classification on ImageNet. Extensive experiments show that Spikformer V2 outperforms other previous surrogate training and ANN2SNN methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of 81.10% with just 1 time step. To the best of our knowledge, this is the first time that the SNN achieves 80+% accuracy on ImageNet. The code will be available at Spikformer V2.																																	2024-05-25	PPRN:86970686		
J	Xue, Jinlong; Deng, Yayue; Gao, Yingming; Li, Ya				deng, yayue/JJE-8661-2023; Gao, Yingming/AIC-0100-2022; Xue, Jinlong/GWC-3919-2022						Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation								Arxiv											1	1;2024-01-02;https://www.arxiv.org/abs/2401.01044v1	arXiv:2401.01044			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 02 2024	2024	Recent advancements in diffusion models and large language models (LLMs) have significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning AIGC application designed to generate audio from natural language prompts, is attracting increasing attention. However, existing TTA studies often struggle with generation quality and text-audio alignment, especially for complex textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I) diffusion models, we introduce Auffusion, a TTA system adapting T2I model frameworks to TTA task, by effectively leveraging their inherent generative strengths and precise cross-modal alignment. Our objective and subjective evaluations demonstrate that Auffusion surpasses previous TTA approaches using limited data and computational resource. Furthermore, previous studies in T2I recognizes the significant impact of encoder choice on cross-modal alignment, like fine-grained details and object bindings, while similar evaluation is lacking in prior TTA works. Through comprehensive ablation studies and innovative cross-attention map visualizations, we provide insightful assessments of text-audio alignment in TTA. Our findings reveal Auffusion's superior capability in generating audios that accurately match textual descriptions, which further demonstrated in several related tasks, such as audio style transfer, inpainting and other manipulations. Our implementation and demos are available at https://auffusion.github.io.																																	2024-01-09	PPRN:86914538		
J	Li, Zijie; Li, Henry; Shi, Yichun; Farimani, Amir Barati; Kluger, Yuval; Yang, Linjie; Wang, Peng				linjie, yang/OQK-6993-2025; Farimani, Amir/K-4601-2019						Dual Diffusion for Unified Image Generation and Understanding								Arxiv											1	1;2024-12-31;https://www.arxiv.org/abs/2501.00289v1	arXiv:2501.00289			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 31 2024	2024	Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.																																	2025-01-24	PPRN:120261093		
J	Luo, Run; Zhang, Haonan; Chen, Longze; Lin, Ting-En; Liu, Xiong; Wu, Yuchuan; Yang, Min; Wang, Minzheng; Zeng, Pengpeng; Gao, Lianli; Shen, Heng Tao; Li, Yunshui; Xia, Xiaobo; Huang, Fei; Song, Jingkuan; Li, Yongbin				zeng, pengpeng/AAF-8882-2021; Li, Yunshui/JNS-3522-2023; wu, yuchuan/NTR-2009-2025; li, Yongbin/MBG-2998-2025						MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct								Arxiv											2	2;2024-12-31;https://www.arxiv.org/abs/2409.05840v5| 1;2024-09-19;https://www.arxiv.org/abs/2409.05840v3	arXiv:2409.05840			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 31 2024	2024	The development of Multimodal Large Language Models (MLLMs) has seen significant advancements with increasing demands in various fields (e.g., multimodal agents, embodied intelligence). While model-driven approaches attempt to enhance MLLMs capabilities through diverse architectures, the gains have become increasingly marginal. Conversely, data-driven methods, which scale up image-text instruction data, are more effective but face limited data diversity and complexity challenges. The absence of high-quality data constitutes a significant development barrier for MLLMs. To address the data quality bottleneck, we propose MMEvol, a novel multimodal instruction data evolution framework. This framework iteratively improve data quality through a refined combination of fine-grained perception, cognitive reasoning, and interaction evolution, generating a more complex and diverse image-text instruction dataset that empowers MLLMs with enhanced capabilities. Beginning with an initial set of instructions, SEED-163K, we utilize MMEvol to systematically broaden the diversity of instruction types, extend visual reasoning steps to improve cognitive reasoning abilities, and thoroughly explore fine-grained information within images to enhance visual understanding and robustness. To comprehensively evaluate the effectiveness of our approach, we conduct extensive qualitative analysis and quantitative experiments across 13 vision-language tasks. Compared to baseline models trained with the initial seed data, the results demonstrate that our method achieves an average accuracy improvement of 3.1 percentage points. Furthermore, our approach reaches state-of-the-art (SOTA) performance in nine tasks using significantly less data compared to state-of-the-art models.																																	2025-03-15	PPRN:92377982		
J	Xie, Rui; Zhao, Chen; Zhang, Kai; Zhang, Zhenyu; Zhou, Jun; Yang, Jian; Tai, Ying				Zhang, Kai/ABD-5145-2021						AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation								Arxiv											3	3;2024-12-27;https://www.arxiv.org/abs/2404.01717v4| 2;2024-05-23;https://www.arxiv.org/abs/2404.01717v3| 1;2024-04-03;https://www.arxiv.org/abs/2404.01717v2	arXiv:2404.01717			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 27 2024	2024	Blind super-resolution methods based on Stable Diffusion (SD) demonstrate impressive generative capabilities in reconstructing clear, high-resolution (HR) images with intricate details from low- resolution (LR) inputs. However, their practical applicability is often limited by poor efficiency, as they require hundreds to thousands of sampling steps. Inspired by Adversarial Diffusion Distillation (ADD), we incorporate this approach to design a highly effective and efficient blind super-resolution method. Nonetheless, two challenges arise: First, the original ADD significantly reduces result fidelity, leading to a perception-distortion imbalance. Second, SD-based methods are sensitive to the quality of the conditioning input, while LR images often have complex degradation, which further hinders effectiveness. To address these issues, we introduce a Timestep-Adaptive ADD (TA-ADD) to mitigate the perception-distortion imbalance caused by the original ADD. Furthermore, we propose a prediction-based self-refinement strategy to estimate HR, which allows for the provision of more high- frequency information without the need for additional modules. Extensive experiments show that our method, AddSR, generates superior restoration results while being significantly faster than previous SD-based state-of-the-art models (e.g., 7 × faster than SeeSR).																																	2025-02-15	PPRN:88390505		
J	Chen, Zhuoming; Sadhukhan, Ranajoy; Ye, Zihao; Zhou, Yang; Zhang, Jianyu; Nolte, Niklas; Tian, Yuandong; Douze, Matthijs; Bottou, Leon; Jia, Zhihao; Chen, Beidi				Ye, Zihao/HKM-8264-2023; Jia, Zhihao/JMB-4418-2023						MagicPIG: LSH Sampling for Efficient LLM Generation								Arxiv											2	2;2024-12-18;https://www.arxiv.org/abs/2410.16179v4| 1;2024-10-28;https://www.arxiv.org/abs/2410.16179v2	arXiv:2410.16179			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 18 2024	2024	Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MAGIcPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MAGIcPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MAGIcPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MAGIcPIG can improve decoding throughput by up to 5× across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens.																																	2025-01-25	PPRN:118941910		
J	Pila, Jonathan; Shankar, Ananth N.; Tsimerman, Jacob; Esnault, Helene; Groechenig, Michael										Canonical Heights on Shimura Varieties and the Andre-Oort Conjecture								Arxiv											1	1;2024-12-17;https://www.arxiv.org/abs/2109.08788v4	arXiv:2109.08788			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 17 2024	2024	The main purpose of this work is to prove the André-Oort conjecture in full generality.																																	2025-01-24	PPRN:120003061		
J	Zhao, Weixiang; Hu, Yulin; Li, Zhuojun; Deng, Yang; Guo, Jiahe; Sui, Xingyu; Zhao, Yanyan; Qin, Bing; Chua, Tat-Seng; Liu, Ting				liu, ting/GZM-3326-2022; Wang, Meng/AEZ-9059-2022; Deng, Yang/JAZ-0613-2023						Towards Comprehensive Post Safety Alignment of Large Language Models via Safety Patching								Arxiv											2	2;2024-12-17;https://www.arxiv.org/abs/2405.13820v2| 1;2024-05-22;https://www.arxiv.org/abs/2405.13820v1	arXiv:2405.13820			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 17 2024	2024	Safety alignment of large language models (LLMs) has been gaining increasing attention. However, current safety-aligned LLMs suffer from the fragile and imbalanced safety mechanisms, which can still be induced to generate unsafe responses, exhibit over-safety by rejecting safe user inputs, and fail to preserve general utility after safety alignment. To this end, we propose a novel post safety alignment (PSA) method to address these inherent and emerging safety challenges, including safety enhancement, over-safety mitigation, and utility preservation. In specific, we introduce SAFE PATCHING, a novel framework for comprehensive PSA, where two distinct safety patches are developed on the harmful data to enhance safety and mitigate over-safety concerns, and then seamlessly integrated into the target LLM backbone without compromising its utility. Extensive experiments on four representative aligned LLMs, including LLaMA-2/3, Gemma and Mistral, show that S AFE P ATCHING achieves a more comprehensive PSA than baseline methods, further optimizing the balance between being helpful and harmless in current aligned LLMs. Also, SAFE PATCHING demonstrates its superiority in continual PSA scenarios. 																																	2025-01-25	PPRN:88989980		
J	Zheng, Jiawei; Hong, Hanghai; Liu, Feiyan; Wang, Xiaoli; Su, Jingsong; Liang, Yonggui; Wu, Shikai				Wu, Shikai/HPD-8824-2023; Zheng, Jiawei/ABB-9023-2020						DragFT: Adapting Large Language Models with Dictionary and Retrieval Augmented Fine-tuning for Domain-specific Machine Translation								Arxiv											2	2;2024-12-17;https://www.arxiv.org/abs/2402.15061v2| 1;2024-02-23;https://www.arxiv.org/abs/2402.15061v1	arXiv:2402.15061			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 17 2024	2024	Large language models (LLMs) have shown great potential in domain-specific machine translation (MT). However, one major issue is that LLMs pre-trained on general domain corpus might not generalize well to specific domains due to the lack of domain-specific knowledge. To address this issue, this paper focuses on enhancing the domain-specific MT capability of LLMs, by providing high-quality training datasets and proposing a novel fine-tuning framework denoted by DragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced prompting integrates dictionary information into prompts to improve the translation of domain-specific terminology.; (ii) RAG-based few-shot example selection provides high-quality examples that simulate both the domain and style characteristics; (iii) Fine-tuning with few-shot examples further enhances performance when using in-domain examples. We deploy DragFT on three well-known LLM backbones with 13B training parameters to validate its effectiveness. The results on three domain-specific datasets show that DragFT achieves a significant performance boost and shows superior performance compared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance improvement of DragFT over existing LLMs can be attributed to incorporating relevant knowledge while mitigating noise.																																	2025-01-25	PPRN:87870105		
J	Kulikov, Vladimir; Kleiner, Matan; Huberman-Spiegelglas, Inbar; Michaeli, Tomer										FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models								Arxiv											1	1;2024-12-11;https://www.arxiv.org/abs/2412.08629v1	arXiv:2412.08629			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 11 2024	2024	Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX. 																																	2025-01-22	PPRN:119841624		
J	Wang, Chunwei; Lu, Guansong; Yang, Junwei; Huang, Runhui; Han, Jianhua; Hou, Lu; Zhang, Wei; Xu, Hang				Han, Jianhua/OLR-8671-2025						ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance								Arxiv											1	1;2024-12-09;https://www.arxiv.org/abs/2412.06673v1	arXiv:2412.06673			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 09 2024	2024	In this paper, we introduce ILLUME, a unified multi- modal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining – over four times fewer than what is typically needed – while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.																																	2025-01-17	PPRN:119798510		
J	Smejkal, Libor				Šmejkal, Libor/G-8927-2014						Altermagnetic multiferroics and altermagnetoelectric effect								Arxiv											1	1;2024-11-29;https://www.arxiv.org/abs/2411.19928v1	arXiv:2411.19928			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 29 2024	2024	Magneto electric multiferroics are highly sought after for applications in low-power electronics and for advancing fundamental research, including axion insulators and dark matter detection. However, achieving a combination of ferroic spin and electric orders, along with their controllable switching, remains a significant challenge in conventional ferromagnets and antiferromagnets. Here, we present first-principles evidence that time-reversal symmetry-breaking altermagnetic spin polarization with relatively high critical temperatures can emerge in ferroelectrics BaCuF4(TN∼275K) and Ca3Mn2O7(TN∼110K). Furthermore, we classify all possible altermagnetic polar spin groups, revealing altermagnetism in a collinear phase of BiFeO3. We also propose an altermagneto electric effect, a nonrelativistic cross-coupling between altermagnetic spin polarization and ferro electric polarization, mediated by a rotation of nonmagnetic polyhedra in the lattice structure. Our findings suggest an alternative pathway towards high-temperature magneto electric multiferroicity and the electric field control of altermagnetic order parameters.																																	2025-01-10	PPRN:119581127		
J	Fish, Sara; Gonczarowski, Yannai A.; Shorrer, Ran										Algorithmic Collusion by Large Language Models								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/2404.00806v2	arXiv:2404.00806			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs). We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions ("prompts") may increase collusion. Novel off-path analysis techniques uncover price-war concerns as contributing to these phenomena. Our results extend to auction settings. Our findings uncover unique challenges to any future regulation of LLM-based pricing agents, and black-box pricing agents more broadly.																																	2025-01-08	PPRN:119465392		
J	Yang, Jian; Liu, Zheng-Xin; Fang, Chen				yang, jian/JEO-6281-2023						Symmetry invariants and classes of quasiparticles in magnetically ordered systems having weak spin-orbit coupling								Arxiv											3	3;2024-11-22;https://www.arxiv.org/abs/2105.12738v5| 2;2023-09-03;https://www.arxiv.org/abs/2105.12738v4| 1;2021-05-26;https://www.arxiv.org/abs/2105.12738v2	arXiv:2105.12738			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 22 2024	2024	Symmetry invariants of a group specify the classes of quasiparticles, namely the classes of projective irreducible co-representations in systems having that symmetry. More symmetry invariants exist in discrete point groups than the full rotation group $mathrm{O(3)}$, leading to new quasiparticles restricted to lattices that do not have any counterpart in a vacuum. We focus on the fermionic quasiparticle excitations under ``spin-space group'' symmetries, applicable to materials where long-range magnetic order and itinerant electrons coexist. We provide a list of 218 classes of new quasiparticles that can only be realized in the spin-space groups. These quasiparticles have at least one of the following properties that are qualitatively distinct from those discovered in magnetic space group(MSG)s, and distinct from each other:(i) degree of degeneracy,(ii) dispersion as function of momentum, and(iii) rules of coupling to external probe fields. We rigorously prove this result as a theorem that directly relates these properties to the symmetry invariants, and then illustrate this theorem with a concrete example, by comparing three 12-fold fermions having different sets of symmetry invariants including one discovered in MSG. Our approach can be generalized to realize more quasiparticles whose little co-groups are beyond those considered in our work.																																	2025-01-03	PPRN:12068414		
J	Chalnev, Sviatoslav; Siu, Matthew; Conmy, Arthur										Improving Steering Vectors by Targeting Sparse Autoencoder Features								Arxiv											2	2;2024-11-21;https://www.arxiv.org/abs/2411.02193v2| 1;2024-11-04;https://www.arxiv.org/abs/2411.02193v1	arXiv:2411.02193			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 21 2024	2024	To control the behavior of language models, steering methods attempt to ensure that outputs of the model satisfy specific pre-defined properties. Adding steering vectors to the model is a promising method of model control that is easier than finetuning, and may be more robust than prompting.However, it can be difficult to anticipate the effects of steering vectors produced by methods such as CAA [Panickssery et al., 2024] or the direct use of SAE latents [Templeton et al., 2024]. In our work, we address this issue by using SAEs to measure the effects of steering vectors, giving us a method that can be used to understand the causal effect of any steering vector intervention. We use this method for measuring causal effects to develop an improved steering method, SAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific SAE features while minimizing unintended side effects. We show that overall, SAE-TS balances steering effects with coherence better than CAA and SAE feature steering, when evaluated on a range of tasks.																																	2024-12-31	PPRN:119088934		
J	Chen, Lei; Meng, Yuan; Tang, Chen; Ma, Xinzhu; Jiang, Jingyan; Wang, Xin; Wang, Zhi; Zhu, Wenwu				Zhu, Wenwu/C-5025-2018; Ma, Xinzhu/MGA-5961-2025; Wang, Zhi/H-2753-2013						Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers								Arxiv											2	2;2024-11-19;https://www.arxiv.org/abs/2406.17343v2| 1;2024-06-25;https://www.arxiv.org/abs/2406.17343v1	arXiv:2406.17343			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Nov 19 2024	2024	Recent advancements in diffusion models, particularly the architectural transformation from UNet-based models to Diffusion Transformers (DiTs), significantly improve the quality and scalability of image and video generation. However, despite their impressive capabilities, the substantial computational costs of these large-scale models pose significant challenges for real-world deployment. Post-Training Quantization (PTQ) emerges as a promising solution, enabling model compression and accelerated inference for pretrained models, without the costly retraining. However, research on DiT quantization remains sparse, and existing PTQ frameworks, primarily designed for traditional diffusion models, tend to suffer from biased quantization, leading to notable performance degradation. In this work, we identify that DiTs typically exhibit significant spatial variance in both weights and activations, along with temporal variance in activations. To address these issues, we propose Q-DiT, a novel approach that seamlessly integrates two key techniques: automatic quantization granularity allocation to handle the significant variance of weights and activations across input channels, and sample-wise dynamic activation quantization to adaptively capture activation changes across both timesteps and samples. Extensive experiments conducted on ImageNet and VBench demonstrate the effectiveness of the proposed QDiT. Specifically, when quantizing DiT-XL/2 to W6A8 on ImageNet (256× 256), Q-DiT achieves a remarkable reduction in FID by 1.09 compared to the baseline. Under the more challenging W4A8 setting, it maintains high fidelity in image and video generation, establishing a new benchmark for efficient, high-quality quantization in DiTs. 																																	2024-12-31	PPRN:89503724		
J	Ma, Xinyin; Fang, Gongfan; Mi, Michael Bi; Wang, Xinchao				Ma, Xinyin/MIO-6425-2025; Wang, Xinchao/L-7655-2018						Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching								Arxiv											2	2;2024-11-16;https://www.arxiv.org/abs/2406.01733v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01733v1	arXiv:2406.01733			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 16 2024	2024	Diffusion Transformers have recently demonstrated unprecedented generative capabilities for various tasks. The encouraging results, however, come with the cost of slow inference, since each denoising step requires inference on a transformer model with a large scale of parameters. In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68% of the computation in the cache steps (46.84% for all steps), with less than 0.01 drop in FID. To achieve this, we introduce a novel scheme, named Learning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for diffusion transformers. Specifically, by leveraging the identical structure of layers in transformers and the sequential nature of diffusion, we explore redundant computations between timesteps by treating each layer as the fundamental unit for caching. To address the challenge of the exponential search space in deep models for identifying layers to cache and remove, we propose a novel differentiable optimization objective. An input-invariant yet timestep-variant router is then optimized, which can finally produce a static computation graph. Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed. 																																	2024-12-28	PPRN:89262089		
J	Rosato, Romeo Felice; Destounis, Kyriakos; Pani, Paolo				Pani, Paolo/AAG-3902-2021						Ringdown stability: greybody factors as stable gravitational-wave observables								Arxiv											3	3;2024-11-14;https://www.arxiv.org/abs/2406.01692v3| 2;2024-06-10;https://www.arxiv.org/abs/2406.01692v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01692v1	arXiv:2406.01692			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 14 2024	2024	The quasinormal mode spectrum of black holes plays a crucial role in the modelling of post- merger ringdown signals. However, the spectrum is extremely sensitive to small deformations of the system and describes the linear response only in a certain (not precisely defined) timeframe after the merger. We argue here that the greybody factors, recently shown to describe the ringdown spectral amplitude at relatively high frequencies, are instead stable under small perturbations of the system and free of certain ambiguities that plague the quasinormal mode spectrum. Our analysis also unveils a nontrivial interplay: while certain ringdown quantities are dominated by the contribution of spectrally unstable quasinormal modes, these modes conspire to produce stable observables. Thus, we propose a complementary approach to ringdown studies, which circumvents some limitations of the standard quasinormal mode description.																																	2024-12-21	PPRN:89248709		
J	He, Yancheng; Li, Shilong; Liu, Jiaheng; Tan, Yingshui; Wang, Weixun; Huang, Hui; Bu, Xingyuan; Guo, Hangyu; Hu, Chengwei; Zheng, Boren; Lin, Zhuoran; Liu, Xuepeng; Sun, Dekai; Lin, Shirong; Zheng, Zhicheng; Zhu, Xiaoyong; Su, Wenbo; Zheng, Bo				Zheng, boren/GQQ-7819-2022; Lin, shirong/KCZ-2294-2024; Zheng, Zhicheng/AAT-3645-2021; Bu, Xingyuan/HKE-2520-2023; Zheng, Bo/JDW-6453-2023; Li, Shilong/KHW-4252-2024						Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models								Arxiv											1	1;2024-11-13;https://www.arxiv.org/abs/2411.07140v2	arXiv:2411.07140			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 13 2024	2024	New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.																																	2024-12-21	PPRN:119218481		
J	Bukharin, Alexander; Li, Shiyang; Wang, Zhengyang; Yang, Jingfeng; Yin, Bing; Li, Xian; Zhang, Chao; Zhao, Tuo; Jiang, Haoming				Wang, Zhengyang/KYM-3585-2024; Yang, Jingfeng/HLP-5419-2023						Data Diversity Matters for Robust Instruction Tuning								Arxiv											3	3;2024-11-11;https://www.arxiv.org/abs/2311.14736v3| 2;2024-02-05;https://www.arxiv.org/abs/2311.14736v2| 1;2023-11-21;https://www.arxiv.org/abs/2311.14736v1	arXiv:2311.14736			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 11 2024	2024	Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following performance, therefore improving robustness. We validate the performance of QDIT on several large scale instruction tuning datasets, where we find it can substantially improve worst and average case performance compared to quality-driven data selection.																																	2024-12-19	PPRN:86296858		
J	Luo, Jianlan; Xu, Charles; Wu, Jeffrey; Levine, Sergey										Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning								Arxiv											1	1;2024-11-06;https://www.arxiv.org/abs/2410.21845v2	arXiv:2410.21845			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 06 2024	2024	Reinforcement learning (RL) holds great promise for enabling autonomous acquisition of complex robotic manipulation skills, but realizing this potential in real-world settings has been challenging. We present a human-in-the-loop vision-based RL system that demonstrates impressive performance on a diverse set of dexterous manipulation tasks, including dynamic manipulation, precision assembly, and dual-arm coordination. Our approach integrates demonstrations and human corrections, efficient RL algorithms, and other system-level design choices to learn policies that achieve near-perfect success rates and fast cycle times within just 1 to 2.5 hours of training. We show that our method significantly outperforms imitation learning baselines and prior RL approaches, with an average 2x improvement in success rate and 1.8x faster execution. Through extensive experiments and analysis, we provide insights into the effectiveness of our approach, demonstrating how it learns robust, adaptive policies for both reactive and predictive control strategies. Our results suggest that RL can indeed learn a wide range of complex vision-based manipulation policies directly in the real world within practical training times. We hope this work will inspire a new generation of learned robotic manipulation techniques, benefiting both industrial applications and research advancements. 																																	2024-12-16	PPRN:119060619		
J	Setton, David J.; Greene, Jenny E.; Graaff, Anna de; Ma, Yilun; Leja, Joel; Matthee, Jorryt; Bezanson, Rachel; Boogaard, Leindert A.; Cleri, Nikko J.; Katz, Harley; Labbe, Ivo; Maseda, Michael V.; Mcconachie, Ian; Miller, Tim B.; Price, Sedona H.; Suess, Katherine A.; Dokkum, Pieter van; Wang, Bingjie; Weibel, Andrea; Whitaker, Katherine E.; Williams, Christina C.				Matthee, Jorryt/KHD-9384-2024; Wang, Bingjie/GRR-9044-2022; Leja, Joel/JPL-7942-2023; Labbe, Ivo/B-1408-2016						Little Red Dots at an Inflection Point: Ubiquitous "V-Shaped" Turnover Consistently Occurs at the Balmer Limit								Arxiv											1	1;2024-11-05;https://www.arxiv.org/abs/2411.03424v1	arXiv:2411.03424			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 05 2024	2024	Among the most puzzling early discoveries of JWST are “Little Red Dots” – compact red sources that host broad Balmer emission lines and, in many cases, exhibit a “V shaped” change in slope in the rest- optical. The physical properties of Little Red Dots currently have order-of-magnitude uncertainties, because models to explain the continuum of these sources differ immensely. Here, we leverage the complete selection of red sources in the RUBIES program, supplemented with public PRISM spectra, to study the origin of this “V shape”. By fitting a broken power law with a flexible inflection point, we find that a large fraction (20/44, nearly all spatially unresolved) of extremely red Hα emitters at 2 < z < 6 exhibit a strong change in slope, and that all strong inflections appear associated with the Balmer limit (0.3645 µm). Using a simple model of a reddened AGN with an unobscured scattered light component, we demonstrate that the observed “V shape” in Little Red Dots is unlikely to occur at any specific wavelength if the entire continuum is dominated by light from a power law AGN continuum. In contrast, models with an intrinsic feature at the Balmer limit, such as those that are dominated by evolved stellar populations in the rest-UV-to-optical, can produce the observed spectral shapes, provided that a reddened component picks up sufficiently redward of the break. While no model can comfortably explain the full Little Red Dot spectral energy distribution, the common inflection location suggests that it is most likely a single component that consistently dominates the rest-UV-to-optical in Little Red Dots, and that this component is associated with T ∼ 104 K hydrogen due to the clear preference for a break at H∞.																																	2025-01-24	PPRN:119055964		
J	Bhalla, Usha; Oesterling, Alex; Srinivas, Suraj; Calmon, Flavio P.; Lakkaraju, Himabindu										Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)								Arxiv											2	2;2024-11-04;https://www.arxiv.org/abs/2402.10376v2| 1;2024-02-16;https://www.arxiv.org/abs/2402.10376v1	arXiv:2402.10376			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 04 2024	2024	CLIP embeddings have demonstrated remarkable performance across a wide range of multimodal applications. However, these high-dimensional, dense vector representations are not easily interpretable, limiting our understanding of the rich structure of CLIP and its use in downstream applications that require transparency. In this work, we show that the semantic structure of CLIP's latent space can be leveraged to provide interpretability, allowing for the decomposition of representations into semantic concepts. We formulate this problem as one of sparse recovery and propose a novel method, Sparse Linear Concept Embeddings, for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE is task-agnostic and can be used, without training, to explain and even replace traditional dense CLIP representations, maintaining high downstream performance while significantly improving their interpretability. We also demonstrate significant use cases of SpLiCE representations including detecting spurious correlations and model editing.																																	2025-01-08	PPRN:87730016		
J	Zhang, Ge; Du, Xinrun; Chen, Bei; Liang, Yiming; Luo, Tongxu; Zheng, Tianyu; Zhu, Kang; Cheng, Yuyang; Xu, Chunpu; Guo, Shuyue; Zhang, Haoran; Qu, Xingwei; Wang, Junjie; Yuan, Ruibin; Li, Yizhi; Wang, Zekun; Liu, Yudong; Tsai, Yu-Hsuan; Zhang, Fengji; Lin, Chenghua; Huang, Wenhao; Fu, Jie				Wang, Junjie/JUF-3729-2023; Huang, Wenhao/GWU-9337-2022; Zhang, Fengji/JXM-1406-2024; Wang, ZK/GXV-0814-2022; Zhang, Haoran/KEJ-6929-2024; Zheng, Tianyu/JXM-4664-2024; Tsai, Yu-Hsuan/GOH-2105-2022; liang, yiming/LXV-0676-2024						CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark								Arxiv											4	4;2024-11-04;https://www.arxiv.org/abs/2401.11944v4| 3;2024-09-09;https://www.arxiv.org/abs/2401.11944v3| 2;2024-03-18;https://www.arxiv.org/abs/2401.11944v2| 1;2024-01-22;https://www.arxiv.org/abs/2401.11944v1	arXiv:2401.11944			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	As the capabilities of large multimodal models ( LMMs ) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU (Yue et al., 2023). CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines. These questions span 30 subjects and comprise 39 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. CMMMU focuses on complex perception and reasoning with domain-specific knowledge in the Chinese context. We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracy of 43%, indicating a large space for improvement. CMMMU aims to enhance the development of next-generation LMMs for expert AI and support LMM democratization through offering varied language contexts.																																	2025-01-15	PPRN:87277954		
J	Zhu, Yongxin; Li, Bocheng; Xin, Yifei; Xu, Linli										Addressing Representation Collapse in Vector Quantized Models with One Linear Layer								Arxiv											1	1;2024-11-04;https://www.arxiv.org/abs/2411.02038v1	arXiv:2411.02038			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	Vector Quantization (VQ) is a widely used method for converting continuous representations into discrete codes, which has become fundamental in unsupervised representation learning and latent generative models. However, VQ models are often hindered by the problem of representation collapse in the latent space, which leads to low codebook utilization and limits the scalability of the codebook for large-scale training. Existing methods designed to mitigate representation collapse typically reduce the dimensionality of latent space at the expense of model capacity, which do not fully resolve the core issue. In this study, we conduct a theoretical analysis of representation collapse in VQ models and identify its primary cause as the disjoint optimization of the codebook, where only a small subset of code vectors are updated through gradient descent. To address this issue, we propose SimVQ , a novel method which reparameterizes the code vectors through a linear transformation layer based on a learnable latent basis. This transformation optimizes the entire linear space spanned by the codebook, rather than merely updating the code vector selected by the nearest-neighbor search in vanilla VQ models. Although it is commonly understood that the multiplication of two linear matrices is equivalent to applying a single linear layer, our approach works surprisingly well in resolving the collapse issue in VQ models with just one linear layer. We validate the efficacy of SimVQ through extensive experiments across various modalities, including image and audio data with different model architectures. The results show that SimVQ not only effectively addresses the problem of representation collapse but also proves highly adaptable and easy to implement, suggesting its broad applicability in diverse machine learning contexts. 																																	2024-12-09	PPRN:119022133		
J	Devoto, Alessio; Zhao, Yu; Scardapane, Simone; Minervini, Pasquale				Scardapane, Simone/AAA-2267-2020						A Simple and Effective<italic> L2</italic> Norm-Based Strategy for KV Cache Compression								Arxiv											3	3;2024-11-03;https://www.arxiv.org/abs/2406.11430v4| 2;2024-10-29;https://www.arxiv.org/abs/2406.11430v3| 1;2024-06-17;https://www.arxiv.org/abs/2406.11430v1	arXiv:2406.11430			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 03 2024	2024	The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV Cache size involve either finetuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder- only Transformers-based models and observe that attention allocation patterns stay consistent across most layers. Surprisingly, we find a clear correlation between the L2 norm and the attention scores over cached KV pairs, where a low L2 norm of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV Cache based on the L2 norm of key embeddings. Our experimental results show that this simple strategy can reduce the KV Cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy. Moreover, without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability.																																	2024-12-16	PPRN:89347813		
J	Jiang, Zhenyu; Xie, Yuqi; Lin, Kevin; Xu, Zhenjia; Wan, Weikang; Mandlekar, Ajay; Fan, Linxi “Jim”; Zhu, Yuke										DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning								Arxiv											1	1;2024-10-31;https://www.arxiv.org/abs/2410.24185v1	arXiv:2410.24185			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 31 2024	2024	Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the amount of cost and human effort involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the challenges of simultaneously controlling multiple arms and multifingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-tosim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Additional results at dexmimicgen.github.io.																																	2024-12-06	PPRN:118937324		
J	Liu, Chris Yuhao; Wang, Yaxuan; Flanigan, Jeffrey; Liu, Yang				Wang, YaXuan/HQY-9283-2023						Large Language Model Unlearning via Embedding-Corrupted Prompts								Arxiv											1	1;2024-10-31;https://www.arxiv.org/abs/2406.07933v2	arXiv:2406.07933			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 31 2024	2024	Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at textit{nearly zero side effects} in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases.																																	2025-01-03	PPRN:119337789		
J	Pramanik, Malabika; Yang, Tongou; Zahl, Joshua										A FURSTENBERG-TYPE PROBLEM FOR CIRCLES, AND A KAUFMAN-TYPE RESTRICTED PROJECTION THEOREM IN R3								Arxiv											2	2;2024-10-25;https://www.arxiv.org/abs/2207.02259v3| 1;2022-07-05;https://www.arxiv.org/abs/2207.02259v1	arXiv:2207.02259			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 25 2024	2024	We resolve a conjecture of Fassler and Orponen on the dimension of exceptional projections to one-dimensional subspaces indexed by a space curve in R3 . We do this by obtaining sharp LP bounds for a variant of the Wolff circular maximal function over fractal sets for a class of C2 curves related to Sogge’s cinematic curvature condition. A key new tool is the use of lens cutting techniques from discrete geometry.																																	2024-12-06	PPRN:12212688		
J	Zhuang, Yuchen; Sun, Haotian; Yu, Yue; Qiang, Rushi; Wang, Qifan; Zhang, Chao; Dai, Bo				Qi, Wang/HTL-8233-2023; yuchen, zhuang/JXX-9721-2024						HYDRA: Model Factorization Framework for Black-Box LLM Personalization								Arxiv											2	2;2024-10-25;https://www.arxiv.org/abs/2406.02888v3| 1;2024-06-11;https://www.arxiv.org/abs/2406.02888v2	arXiv:2406.02888			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 25 2024	2024	Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark. 																																	2024-12-06	PPRN:89278788		
J	Fu, Xiaohan; Li, Shuheng; Wang, Zihan; Liu, Yihao; Gupta, Rajesh K.; Berg-Kirkpatrick, Taylor; Fernandes, Earlence										Imprompter: Tricking LLM Agents into Improper Tool Use								Arxiv											1	1;2024-10-22;https://www.arxiv.org/abs/2410.14923v2	arXiv:2410.14923			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 22 2024	2024	Large Language Model (LLM) Agents are an emerging computing paradigm that blends generative machine learning with tools such as code interpreters, web browsing, email, and more generally, external resources. These agent-based systems represent an emerging shift in personal computing. We contribute to the security foundations of agent-based systems and surface a new class of automatically computed obfuscated adversarial prompt attacks that violate the confidentiality and integrity of user resources connected to an LLM agent. We show how prompt optimization techniques can find such prompts automatically given the weights of a model. We demonstrate that such attacks transfer to production-level agents. For example, we show an information exfiltration attack on Mistral’s LeChat agent that analyzes a user’s conversation, picks out personally identifiable information, and formats it into a valid markdown command that results in leaking that data to the attacker’s server. This attack shows a nearly 80% success rate in an end-toend evaluation. We conduct a range of experiments to characterize the efficacy of these attacks and find that they reliably work on emerging agent-based systems like Mistral’s LeChat, ChatGLM, and Meta’s Llama. These attacks are multimodal, and we show variants in the text-only and image domains. 																																	2024-11-22	PPRN:118767264		
J	Qiang, Yao; Zhou, Xiangyu; Zade, Saleh Zare; Roshani, Mohammad Amin; Khanduri, Prashant; Zytko, Douglas; Zhu, Dongxiao				Zhu, Dongxiao/G-4049-2010; Khanduri, Prashant/NGR-6579-2025						Learning to Poison Large Language Models During Instruction Tuning								Arxiv											2	2;2024-10-22;https://www.arxiv.org/abs/2402.13459v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.13459v1	arXiv:2402.13459			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 22 2024	2024	The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning (GBTL) algorithm to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various tasks, including sentiment analysis, domain generation, and question answering, our poisoning strategy demonstrates a high success rate in compromising various LLMs' outputs. We further propose two defense strategies against data poisoning attacks, including in-context learning (ICL) and continuous learning (CL), which effectively rectify the behavior of LLMs and significantly reduce the decline in performance. Our work highlights the significant security risks present during the instruction tuning of LLMs and emphasizes the necessity of safeguarding LLMs against data poisoning attacks.																																	2024-11-26	PPRN:87788061		
J	Liu, Yuyan; Ding, Sirui; Zhou, Sheng; Fan, Wenqi; Tan, Qiaoyu										MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction								Arxiv											2	2;2024-10-18;https://www.arxiv.org/abs/2406.12950v2| 1;2024-06-18;https://www.arxiv.org/abs/2406.12950v1	arXiv:2406.12950			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 18 2024	2024	Molecular property prediction (MPP) is a fundamental and crucial task in drug discovery. However, prior methods are limited by the requirement for a large number of labeled molecules and their restricted ability to generalize for unseen and new tasks, both of which are essential for real-world applications. To address these challenges, we present MolecularGPT for few-shot MPP. From a perspective on instruction tuning, we fine-tune large language models (LLMs) based on curated molecular instructions spanning over 1000 property prediction tasks. This enables building a versatile and specialized LLM that can be adapted to novel MPP tasks without any fine-tuning through zero- and few-shot in-context learning (ICL). MolecularGPT exhibits competitive in-context reasoning capabilities across 10 downstream evaluation datasets, setting new benchmarks for few-shot molecular prediction tasks. More importantly, with just two-shot examples, MolecularGPT can outperform standard supervised graph neural network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM baselines by up to 15.7% increase on classification accuracy and decrease of 17.9 on regression metrics (e.g., RMSE) under zero-shot. This study demonstrates the potential of LLMs as effective few-shot molecular property predictors. 																																	2024-11-16	PPRN:89379077		
J	Yang, Ke; Liu, Yao; Chaudhary, Sapana; Fakoor, Rasool; Chaudhari, Pratik; Karypis, George; Rangwala, Huzefa										AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents								Arxiv											1	1;2024-10-17;https://www.arxiv.org/abs/2410.13825v1	arXiv:2410.13825			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 17 2024	2024	Autonomy via agents using large language models (LLMs) for personalized, standardized tasks boosts human efficiency. Automating web tasks (like booking hotels within a budget) is increasingly sought after. Fulfilling practical needs, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Prior research often handcrafts web agent strategies (e.g., prompting templates, multi-agent systems, search methods, etc.) and the corresponding in-context examples, which may not generalize well across all real-world scenarios. On the other hand, there has been limited study on the misalignment between a web agent's observation/action representation and the pre-training data of the LLM it's based on. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. Our study enhances an LLM-based web agent by simply refining its observation and action space to better align with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.																																	2024-11-12	PPRN:115361836		
J	Li, Senmao; Hu, Taihang; van de Weijer, Joost; Khan, Fahad Shahbaz; Liu, Tao; Li, Linxuan; Yang, Shiqi; Wang, Yaxing; Cheng, Ming-Ming; Yang, Jian				Khan, Fahad Shahbaz/ABD-6646-2021; Yang, Shiqi/KCJ-8733-2024; Cheng, Ming-Ming/A-2527-2009						Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference								Arxiv											2	2;2024-10-15;https://www.arxiv.org/abs/2312.09608v2| 1;2023-12-15;https://www.arxiv.org/abs/2312.09608v1	arXiv:2312.09608			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 15 2024	2024	One of the main drawback of diffusion models is the slow inference time for image generation. Among the most successful approaches to addressing this problem are distillation methods. However, these methods require considerable computational resources. In this paper, we take another approach to diffusion model acceleration. We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features. This provides insights regarding their changes during the inference process. In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different timesteps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process. Additionally, we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41% and 24% respectively, and DiT model sampling by 34%, while maintaining high-quality generation performance.																																	2024-11-10	PPRN:86650721		
J	Liang, Yingyu; Sha, Zhizhou; Shi, Zhenmei; Song, Zhao; Zhou, Yufa				Shi, Zhenmei/KBA-9650-2024; Zhou, Yufa/MIP-8150-2025						Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time								Arxiv											2	2;2024-10-15;https://www.arxiv.org/abs/2408.13233v2| 1;2024-08-23;https://www.arxiv.org/abs/2408.13233v1	arXiv:2408.13233			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 15 2024	2024	The computational complexity of the self-attention mechanism in popular transformer architectures poses significant challenges for training and inference, and becomes the bottleneck for long inputs. Is it possible to significantly reduce the quadratic time complexity of computing the gradients in multi-layer transformer models? This paper proves that a novel fast approximation method can calculate the gradients in almost linear time n1+ o(1) where n is the input sequence length, while it maintains a polynomially small approximation error 1/ / poly(n) across the entire model. Our theory holds for general loss functions and when the multi-layer transformer model contains many practical sub-modules, such as residual connection, casual mask, and multi-head attention. By improving the efficiency of gradient computation, we hope that this work will facilitate more effective training and deployment of long-context language models based on our theoretical results.																																	2024-11-10	PPRN:91525557		
J	Liu, Xu; Liu, Juncheng; Woo, Gerald; Aksu, Taha; Liang, Yuxuan; Zimmermann, Roger; Liu, Chenghao; Savarese, Silvio; Xiong, Caiming; Sahoo, Doyen				Liang, Yuxuan/KXR-3882-2024						Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts								Arxiv											1	1;2024-10-14;https://www.arxiv.org/abs/2410.10469v1	arXiv:2410.10469			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	Time series foundation models have demonstrated impressive performance as zeroshot forecasters, i.e., they can tackle a wide variety of downstream forecasting tasks without explicit task-specific training. However, achieving effectively unified training on time series remains an open challenge. Existing approaches introduce some level of model specialization to account for the highly heterogeneous nature of time series data. For instance, M OIRAI pursues unified training by employing multiple input/output projection layers, each tailored to handle time series at a specific frequency. Similarly, TimesFM maintains a frequency embedding dictionary for this purpose. We identify two major drawbacks to this human-imposed frequency-level model specialization: (1) Frequency is not a reliable indicator of the underlying patterns in time series. For example, time series with different frequencies can display similar patterns, while those with the same frequency may exhibit varied patterns. (2) Non-stationarity is an inherent property of real-world time series, leading to varied distributions even within a short context window of a single time series. Frequency-level specialization is too coarse-grained to capture this level of diversity. To address these limitations, this paper introduces M OIRAI-M O E, using a single input/output projection layer while delegating the modeling of diverse time series patterns to the sparse mixture of experts (MoE) within Transformers. With these designs, M OIRAI-M O E reduces reliance on human-defined heuristics and enables automatic token-level specialization. Extensive experiments on 39 datasets demonstrate the superiority of M OIRAI-M O E over existing foundation models in both in-distribution and zero-shot scenarios. Furthermore, this study conducts comprehensive model analyses to explore the inner workings of time series MoE foundation models and provides valuable insights for future research.																																	2024-11-06	PPRN:112583439		
J	Lu, Zhenyi; Fan, Chenghao; Wei, Wei; Qu, Xiaoye; Chen, Dangyang; Cheng, Yu										Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2406.15479v2| 1;2024-06-17;https://www.arxiv.org/abs/2406.15479v1	arXiv:2406.15479			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on 20 datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of 28.34% .34% in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. 1																																	2024-11-05	PPRN:89414707		
J	Zhou, Yukai; Huang, Zhijie; Lu, Feiyang; Qin, Zhan; Wang, Wenjie				黄, 志杰/AAA-2256-2020						Don't Say No: Jailbreaking LLM by Suppressing Refusal								Arxiv											2	2;2024-10-12;https://www.arxiv.org/abs/2404.16369v2| 1;2024-04-25;https://www.arxiv.org/abs/2404.16369v1	arXiv:2404.16369			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 12 2024	2024	Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to jailbreaking attacks, where carefully crafted prompts seduce them to produce toxic content. One category of jailbreak attacks is reformulating the task as an optimization by eliciting the LLM to generate affirmative responses. However, such optimization objective has its own limitations, such as the restriction on the predefined objectionable behaviors, leading to suboptimal attack performance. In this study, we first uncover the reason why vanilla target loss is not optimal, then we explore and enhance the loss objective and introduce the DSN (Don't Say No) attack, which achieves successful attack by suppressing refusal. Another challenge in studying jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the responses. The existing evaluation such as refusal keyword matching reveals numerous false positive and false negative instances. To overcome this challenge, we propose an Ensemble Evaluation pipeline that novelly incorporates Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potential of the DSN and effectiveness of Ensemble Evaluation compared to baseline methods.																																	2024-11-06	PPRN:88651163		
J	Zhao, Shuai; Jia, Meihuizi; Tuan, Luu Anh; Pan, Fengjun; Wen, Jinming				Luu, Anh Tuan/AAG-3582-2021						Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning								Arxiv											6	6;2024-10-09;https://www.arxiv.org/abs/2401.05949v6| 5;2024-10-01;https://www.arxiv.org/abs/2401.05949v5| 4;2024-02-16;https://www.arxiv.org/abs/2401.05949v4| 3;2024-01-20;https://www.arxiv.org/abs/2401.05949v3| 2;2024-01-12;https://www.arxiv.org/abs/2401.05949v2| 1;2024-01-11;https://www.arxiv.org/abs/2401.05949v1	arXiv:2401.05949			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 09 2024	2024	In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack , to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model’s generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models 1 .																																	2024-10-29	PPRN:87128166		
J	Zhou, Zihao; Liu, Shudong; Ning, Maizhen; Liu, Wei; Wang, Jindong; Wong, Derek F.; Huang, Xiaowei; Wang, Qiufeng; Huang, Kaizhu				wang, jindong/ACD-8485-2022; Wong, Derek F/CAI-7740-2022; Liu, Shudong/ORI-2267-2025; Huang, Kaizhu/LKL-1952-2024; Huang, Xiaowei/GYA-3151-2022; Wang, Qiufeng/ACA-9839-2022						Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist								Arxiv											2	2;2024-10-08;https://www.arxiv.org/abs/2407.08733v2| 1;2024-07-11;https://www.arxiv.org/abs/2407.08733v1	arXiv:2407.08733			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 08 2024	2024	Exceptional mathematical reasoning ability is one of the key features that demonstrate the power of large language models (LLMs). How to comprehensively define and evaluate the mathematical abilities of LLMs, and even reflect the user experience in real-world scenarios, has emerged as a critical issue. Current benchmarks predominantly concentrate on problem-solving capabilities, presenting a substantial risk of model overfitting and fails to accurately measure the genuine mathematical reasoning abilities. In this paper, we argue that if a model really understands a problem, it should be robustly and readily applied across a diverse array of tasks. To this end, we introduce MATHCHECK, a well-designed checklist for testing task generalization and reasoning robustness, as well as an automatic tool to generate checklists efficiently. MATHCHECK includes multiple mathematical reasoning tasks and robustness tests to facilitate a comprehensive evaluation of both mathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we develop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual reasoning and multi-modal reasoning capabilities, respectively, serving as upgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K. We adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 26 LLMs and 17 multi-modal LLMs, assessing their comprehensive mathematical reasoning abilities. Our results demonstrate that while frontier LLMs like GPT-4o continue to excel in various abilities on the checklist, many other model families exhibit a significant decline. Further experiments indicate that, compared to traditional math benchmarks, MATHCHECK better reflects true mathematical abilities and represents mathematical intelligence more linearly, thereby supporting our design. Using MATHCHECK, we can also efficiently conduct informative behavior analysis to deeply investigate models. Finally, we show that our proposed checklist paradigm can easily extend to other reasoning tasks for their comprehensive evaluation.1																																	2024-10-29	PPRN:90769771		
J	Joel, Sathvik; Wu, Jie JW; Fard, Fatemeh				Fard, Fatemeh/LVS-2042-2024; Sam, Joel/JDW-3472-2023						Survey on Code Generation for Low resource and Domain Specific Programming Languages								Arxiv											1	1;2024-10-04;https://www.arxiv.org/abs/2410.03981v1	arXiv:2410.03981			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	Large Language Models (LLMs) have shown remarkable capabilities in code generation for popular programming languages. However, their performance on Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs) remains a critical challenge. This gap affects millions of developers- with Rust alone having 3.5 million users- who are currently unable to fully leverage LLM capabilities. LRPLs and DSLs face unique challenges including severe data scarcity and, for DSLs, highly specialized syntax and semantics that are poorly represented in general-purpose datasets. Addressing these challenges is crucial as LRPLs and DSLs significantly enhance development efficiency in specialized domains and applications including financial and scientific works. While several surveys on LLMs for software engineering and code exist, none comprehensively address the challenges and opportunities specific to LRPLs and DSLs. Our survey fills this gap by providing a systematic review of the current state, methodologies, and challenges in leveraging LLMs for code generation in LRPL and DSL. We filtered 111 papers from over 27 , 000 published studies from 2020 − 2024 to understand the capabilities and limitations of LLMs in these specialized domains. We report LLMs used, benchmarks, and metrics to evaluate code generation in LRPLs and DSLs, as well as strategies used to enhance LLM performance, and the collected datasets and curations methods in this context. We identified four main evaluation techniques used in the literature, along with several metrics to assess code generation in LRPL and DSL. We categorized the methods used for LLM improvement into six main groups and summarized the novel methods and architectures proposed by the researchers. While different techniques, metrics, and datasets are used, there is a lack of a standard approach and a benchmark dataset to evaluate code generation in several LRPLs and DSLs. The unique requirements of LRPLs and DSLs emphasize the need for developing new techniques and exploring combined approaches to address the code generation challenges. As the domains are specialized, lack of datasets is one of the main barriers, which requires more attention by exploring alternate sources or synthesized data. This survey serves as a comprehensive resource for researchers and practitioners working at the intersection of LLMs, software engineering, and specialized programming languages, providing a foundation for future advancements in code generation for LRPLs and DSLs.																																	2024-10-27	PPRN:104314756		
J	Chuang, Yung-Sung; Qiu, Linlu; Hsieh, Cheng-Yu; Krishna, Ranjay; Kim, Yoon; Glass, James				Chuang, Yung-Sung/HSH-6375-2023						Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using <italic>Only</italic> Attention Maps								Arxiv											2	2;2024-10-03;https://www.arxiv.org/abs/2407.07071v2| 1;2024-07-09;https://www.arxiv.org/abs/2407.07071v1	arXiv:2407.07071			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.1																																	2024-10-18	PPRN:90750137		
J	Manvi, Rohin; Singh, Anikait; Ermon, Stefano										Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation								Arxiv											1	1;2024-10-03;https://www.arxiv.org/abs/2410.02725v1	arXiv:2410.02725			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 03 2024	2024	Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval increases from 21% to 34% with 16 samples and math performance on GSM8K improves from 84% to 91%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50-75% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs.																																	2024-10-18	PPRN:102574851		
J	Zhang, Qingru; Singh, Chandan; Liu, Liyuan; Liu, Xiaodong; Yu, Bin; Gao, Jianfeng; Zhao, Tuo				Liu, Liyuan/AGI-9812-2022						Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs								Arxiv											2	2;2024-10-01;https://www.arxiv.org/abs/2311.02262v2| 1;2023-11-03;https://www.arxiv.org/abs/2311.02262v1	arXiv:2311.02262			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 01 2024	2024	In human-written articles, we often leverage the subtleties of text style, such as bold and italics, to guide the attention of readers. These textual emphases are vital for the readers to grasp the conveyed information. When interacting with large language models (LLMs), we have a similar need -- steering the model to pay closer attention to user-specified information, e.g., an instruction. Existing methods, however, are constrained to process plain text and do not support such a mechanism. This motivates us to introduce PASTA -- Post-hoc Attention STeering Approach, a method that allows LLMs to read text with user-specified emphasis marks. To this end, PASTA identifies a small subset of attention heads and applies precise attention reweighting on them, directing the model attention to user-specified parts. Like prompting, PASTA is applied at inference time and does not require changing any model parameters. Experiments demonstrate that PASTA can substantially enhance an LLM's ability to follow user instructions or integrate new knowledge from user inputs, leading to a significant performance improvement on a variety of tasks, e.g., an average accuracy improvement of 22% for LLAMA-7B.  																																	2024-10-13	PPRN:86054395		
J	Li, Yiming; Li, Sihang; Liu, Xinhao; Gong, Moonjun; Li, Kenan; Chen, Nuo; Wang, Zijun; Li, Zhiheng; Jiang, Tao; Yu, Fisher; Wang, Yue; Zhao, Hang; Yu, Zhiding; Feng, Chen				li, yiming/JTS-9207-2023; feng, chen/JLM-8296-2023; Jiang, Tao/AGX-8391-2022; Wang, Yuepeng/GPS-9328-2022; Wang, zijun/JNS-5435-2023						SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving								Arxiv											3	3;2024-09-30;https://www.arxiv.org/abs/2306.09001v3| 2;2023-09-30;https://www.arxiv.org/abs/2306.09001v2| 1;2023-06-15;https://www.arxiv.org/abs/2306.09001v1	arXiv:2306.09001			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 30 2024	2024	Monocular scene understanding is a foundational component of autonomous systems. Within the spectrum of monocular perception topics, one crucial and useful task for holistic 3D scene understanding is semantic scene completion (SSC), which jointly completes semantic information and geometric details from RGB input. However, progress in SSC, particularly in large-scale street views, is hindered by the scarcity of high-quality datasets. To address this issue, we introduce SSCBench, a comprehensive benchmark that integrates scenes from widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo). SSCBench follows an established setup and format in the community, facilitating the easy exploration of SSC methods in various street views. We benchmark models using monocular, trinocular, and point cloud input to assess the performance gap resulting from sensor coverage and modality. Moreover, we have unified semantic labels across diverse datasets to simplify cross-domain generalization testing. We commit to including more datasets and SSC models to drive further advancements in this field.																																	2024-10-10	PPRN:73362964		
J	Meng, Fanqing; Shao, Wenqi; Luo, Lixin; Wang, Yahong; Chen, Yiran; Lu, Quanfeng; Yang, Yue; Yang, Tianshuo; Zhang, Kaipeng; Qiao, Yu; Luo, Ping				Meng, fanqing/AAE-7775-2022; chen, yiran/KGK-7727-2024; Qiao, Yu/ABD-5787-2021; Yang, Tianshuo/AAI-2989-2021; Luo, Ping/HGE-7623-2022						PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image Models								Arxiv											2	2;2024-09-21;https://www.arxiv.org/abs/2406.11802v3| 1;2024-06-17;https://www.arxiv.org/abs/2406.11802v1	arXiv:2406.11802			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 21 2024	2024	Text-to-image (T2I) models have made substantial progress in generating images from textual prompts. However, they frequently fail to produce images consistent with physical commonsense, a vital capability for applications in world simulation and everyday tasks. Current T2I evaluation benchmarks focus on metrics such as accuracy, bias, and safety, neglecting the evaluation of models' internal knowledge, particularly physical commonsense. To address this issue, we introduce PhyBench, a comprehensive T2I evaluation dataset comprising 700 prompts across 4 primary categories: mechanics, optics, thermodynamics, and material properties, encompassing 31 distinct physical scenarios. We assess 6 prominent T2I models, including proprietary models DALLE3 and Gemini, and demonstrate that incorporating physical principles into prompts enhances the models' ability to generate physically accurate images. Our findings reveal that: (1) even advanced models frequently err in various physical scenarios, except for optics; (2) GPT-4o, with item-specific scoring instructions, effectively evaluates the models' understanding of physical commonsense, closely aligning with human assessments; and (3) current T2I models are primarily focused on text-to-image translation, lacking profound reasoning regarding physical commonsense. We advocate for increased attention to the inherent knowledge within T2I models, beyond their utility as mere image generation tools. The data will be available soon.																																	2024-10-07	PPRN:89349383		
J	Fontana, Nicolo; Pierri, Francesco; Aiello, Luca Maria				Fontana, Nicolo'/NFT-6638-2025; Aiello, Luca Maria/ABB-2507-2021; Pierri, Francesco/JXX-2667-2024						Nicer Than Humans: How do Large Language Models Behave in the Prisoner's Dilemma?								Arxiv											2	2;2024-09-19;https://www.arxiv.org/abs/2406.13605v2| 1;2024-06-19;https://www.arxiv.org/abs/2406.13605v1	arXiv:2406.13605			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 19 2024	2024	The behavior of Large Language Models (LLMs) as artificial social agents is largely unexplored, and we still lack extensive evidence of how these agents react to simple social stimuli. Testing the behavior of AI agents in classic Game Theory experiments provides a promising theoretical framework for evaluating the norms and values of these agents in archetypal social situations. In this work, we investigate the cooperative behavior of three LLMs (Llama2, Llama3, and GPT3.5) when playing the Iterated Prisoner's Dilemma against random adversaries displaying various levels of hostility. We introduce a systematic methodology to evaluate an LLM's comprehension of the game rules and its capability to parse historical gameplay logs for decision-making. We conducted simulations of games lasting for 100 rounds and analyzed the LLMs' decisions in terms of dimensions defined in the behavioral economics literature. We find that all models tend not to initiate defection but act cautiously, favoring cooperation over defection only when the opponent's defection rate is low. Overall, LLMs behave at least as cooperatively as the typical human player, although our results indicate some substantial differences among models. In particular, Llama2 and GPT3.5 are more cooperative than humans, and especially forgiving and non-retaliatory for opponent defection rates below 30%. More similar to humans, Llama3 exhibits consistently uncooperative and exploitative behavior unless the opponent always cooperates. Our systematic approach to the study of LLMs in game theoretical scenarios is a step towards using these simulations to inform practices of LLM auditing and alignment.																																	2024-10-03	PPRN:89379136		
J	Long, Fuchen; Qiu, Zhaofan; Yao, Ting; Mei, Tao				mei, tao/AAG-9455-2019						VideoStudio: Generating Consistent-Content and Multi-Scene Videos								Arxiv											2	2;2024-09-16;https://www.arxiv.org/abs/2401.01256v2| 1;2024-01-02;https://www.arxiv.org/abs/2401.01256v1	arXiv:2401.01256			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 16 2024	2024	The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoStudio, for consistent- content and multi-scene video generation. Technically, VideoStudio leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoStudio identifies the common entities throughout the script and asks LLM to detail each entity. The resultant entity description is then fed into a text-to-image model to generate a reference image for each entity. Finally, VideoStudio outputs a multi-scene video by generating each scene video via a diffusion process that takes the reference images, the descriptive prompt of the event and camera movement into account. The diffusion model incorporates the reference images as the condition and alignment to strengthen the content consistency of multi-scene videos. Extensive experiments demonstrate that VideoStudio outperforms the SOTA video generation models in terms of visual quality, content consistency, and user preference. Source code is available at https://github.com/FuchenUSTC/VideoStudio.																																	2024-12-24	PPRN:86915092		
J	Filiot, Alexandre; Jacob, Paul; Kain, Alice Mac; Saillard, Charlie										Phikon-v2, A large and public feature extractor for biomarker prediction								Arxiv											1	1;2024-09-13;https://www.arxiv.org/abs/2409.09173v1	arXiv:2409.09173			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Sep 13 2024	2024	Gathering histopathology slides from over 100 publicly available cohorts, we compile a diverse dataset of 460 million pathology tiles covering more than 30 cancer sites. Using this dataset, we train a large self-supervised vision transformer using DINOv2 [1] and publicly release one iteration of this model for further experimentation, coined Phikon-v2. While trained on publicly available histology slides, Phikon-v2 surpasses our previously released model (Phikon) and performs on par with other histopathology foundation models (FM) trained on proprietary data. Our benchmarks include eight slide-level tasks with results reported on external validation cohorts avoiding any data contamination between pre-training and evaluation datasets. Our downstream training procedure follows a simple yet robust ensembling strategy yielding a +1.75 AUC increase across tasks and models compared to one-shot retraining (p<0.001). We compare Phikon (ViT-B) and Phikon-v2 (ViT-L) against 14 different histology feature extractors, making our evaluation the most comprehensive to date. Our result support evidences that DINOv2 handles joint model and data scaling better than iBOT. Also, we show that recent scaling efforts are overall beneficial to downstream performance in the context of biomarker prediction with GigaPath [2] and H-Optimus-0 [3] (two ViT-g with 1.1B parameters each) standing out. However, the statistical margins between the latest top-performing FMs remain mostly non-significant; some even underperform on specific indications or tasks such as MSI prediction- deposed by a 13x smaller model developed internally. While latest foundation models may exhibit limitations for clinical deployment, they nonetheless offer excellent grounds for the development of more specialized and cost-efficient histology encoders fueling AI-guided diagnostic tools.																																	2024-12-24	PPRN:119224258		
J	Conforti, Giovanni; Durmus, Alain; Silveri, Marta Gentiloni										KL Convergence Guarantees for Score diffusion models under minimal data assumptions								Arxiv											2	2;2024-09-12;https://www.arxiv.org/abs/2308.12240v2| 1;2023-08-23;https://www.arxiv.org/abs/2308.12240v1	arXiv:2308.12240			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 12 2024	2024	Diffusion models are a new class of generative models that revolve around the estimation of the score function associated with a stochastic differential equation. Subsequent to its acquisition, the approximated score function is then harnessed to simulate the corresponding time-reversal process, ultimately enabling the generation of approximate data samples. Despite their evident practical significance these models carry, a notable challenge persists in the form of a lack of comprehensive quantitative results, especially in scenarios involving non-regular scores and estimators. In almost all reported bounds in Kullback Leibler (KL) divergence, it is assumed that either the score function or its approximation is Lipschitz uniformly in time. However, this condition is very restrictive in practice or appears to be difficult to establish. To circumvent this issue, previous works mainly focused on establishing convergence bounds in KL for an early stopped version of the diffusion model and a smoothed version of the data distribution, or assuming that the data distribution is supported on a compact manifold. These explorations have led to interesting bounds in either Wasserstein or Fortet-Mourier metrics. However, the question remains about the relevance of such early-stopping procedure or compactness conditions. In particular, if there exist a natural and mild condition ensuring explicit and sharp convergence bounds in KL. In this article, we tackle the aforementioned limitations by focusing on score diffusion models with fixed step size stemming from the Ornstein-Uhlenbeck semigroup and its kinetic counterpart. Our study provides a rigorous analysis, yielding simple, improved and sharp convergence bounds in KL applicable to any data distribution with finite Fisher information with respect to the standard Gaussian distribution.																																	2024-09-27	PPRN:82915907		
J	Xia, Renqiu; Mao, Song; Yan, Xiangchao; Zhou, Hongbin; Zhang, Bo; Peng, Haoyang; Pi, Jiahao; Fu, Daocheng; Wu, Wenjie; Ye, Hancheng; Feng, Shiyang; Wang, Bin; Xu, Chao; He, Conghui; Cai, Pinlong; Dou, Min; Shi, Botian; Zhou, Sheng; Wang, Yongwei; Wang, Bin; Yan, Junchi; Wu, Fei; Qiao, Yu				Fu, Daocheng/LCE-6917-2024; Wang, Bin/MVU-8917-2025; Wu, Fan/AID-2699-2022; Xu, Chao/JPX-2933-2023; Zhang, Bo/ABF-8476-2021; WU, WENJIE/K-5750-2018; He, Conghui/AAZ-3323-2021; Cai, Pinlong/P-6490-2017; Wang, Bin/NWH-2233-2025; pi, jihao/GYJ-7625-2022; Shi, Botian/HTT-0363-2023						DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models								Arxiv											2	2;2024-09-11;https://www.arxiv.org/abs/2406.11633v2| 1;2024-06-17;https://www.arxiv.org/abs/2406.11633v1	arXiv:2406.11633			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 11 2024	2024	Scientific documents record research findings and valuable human knowledge, comprising a vast corpus of high-quality data. Leveraging multi-modality data extracted from these documents and assessing large models' abilities to handle scientific document-oriented tasks is therefore meaningful. Despite promising advancements, large models still perform poorly on multi-page scientific document extraction and understanding tasks, and their capacity to process within-document data formats such as charts and equations remains under-explored. To address these issues, we present DocGenome, a structured document benchmark constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access community, using our custom auto-labeling pipeline. DocGenome features four key characteristics: 1) Completeness: It is the first dataset to structure data from all modalities including 13 layout attributes along with their LaTeX source codes. 2) Logicality: It provides 6 logical relationships between different entities within each scientific document. 3) Diversity: It covers various document-oriented tasks, including document classification, visual grounding, document layout detection, document transformation, open-ended single-page QA and multi-page QA. 4) Correctness: It undergoes rigorous quality control checks conducted by a specialized team. We conduct extensive experiments to demonstrate the advantages of DocGenome and objectively evaluate the performance of large models on our benchmark.																																	2024-09-27	PPRN:89352840		
J	Cao, Boxi; Lu, Keming; Lu, Xinyu; Chen, Jiawei; Ren, Mengjie; Xiang, Hao; Liu, Peilin; Lu, Yaojie; He, Ben; Han, Xianpei; Sun, Le; Lin, Hongyu; Yu, Bowen				Bowen, Yu/MFH-7462-2025; Lin, Hongyu/LXA-3658-2024; Han, Xianpei/MTC-8266-2025; Lu, Xinyu/KIB-5798-2024						Towards Scalable Automated Alignment of LLMs: A Survey								Arxiv											3	3;2024-09-03;https://www.arxiv.org/abs/2406.01252v3| 2;2024-07-17;https://www.arxiv.org/abs/2406.01252v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01252v1	arXiv:2406.01252			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 03 2024	2024	Alignment is the most critical step in building large language models (LLMs) that meet human needs. With the rapid development of LLMs gradually surpassing human capabilities, traditional alignment methods based on human-annotation are increasingly unable to meet the scalability demands. Therefore, there is an urgent need to explore new sources of automated alignment signals and technical approaches. In this paper, we systematically review the recently emerging methods of automated alignment, attempting to explore how to achieve effective, scalable, automated alignment once the capabilities of LLMs exceed those of humans. Specifically, we categorize existing automated alignment methods into 4 major categories based on the sources of alignment signals and discuss the current status and potential development of each category. Additionally, we explore the underlying mechanisms that enable automated alignment and discuss the essential factors that make automated alignment technologies feasible and effective from the fundamental role of alignment.																																	2024-09-11	PPRN:89163061		
J	Huang, Jiaxing; Zhang, Jingyi				Zhang, Jingyi/KPS-8963-2024; Huang, Jiaxing/HZH-3850-2023						A Survey on Evaluation of Multimodal Large Language Models								Arxiv											1	1;2024-08-28;https://www.arxiv.org/abs/2408.15769v1	arXiv:2408.15769			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 28 2024	2024	Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) "what to evaluate" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) "how to evaluate" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.																																	2024-09-19	PPRN:91781838		
J	Peng, Yifan; Tian, Jinchuan; Chen, William; Arora, Siddhant; Yan, Brian; Sudo, Yui; Shakeel, Muhammad; Choi, Kwanghee; Shi, Jiatong; Chang, Xuankai; Jung, Jee-weon; Watanabe, Shinji				Shi, Jiatong/AAW-6040-2021; Peng, Yifan/M-1605-2016; Watanabe, Shinji/AEH-1279-2022; Muhammad, Shakeel/AAZ-5510-2021						OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer								Arxiv											3	3;2024-08-27;https://www.arxiv.org/abs/2401.16658v3| 2;2024-06-16;https://www.arxiv.org/abs/2401.16658v2| 1;2024-01-30;https://www.arxiv.org/abs/2401.16658v1	arXiv:2401.16658			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 27 2024	2024	Recent studies have highlighted the importance of fully open foundation models. The Open Whisper-style Speech Model (OWSM) is an initial step towards reproducing OpenAI Whisper using public data and open-source toolkits. However, previous versions of OWSM (v1 to v3) are still based on standard Transformer, which might lead to inferior performance compared to state-of-the-art speech encoder architectures. This work aims to improve the performance and efficiency of OWSM without additional data. We present a series of E-Branchformer-based models named OWSM v3.1, ranging from 100M to 1B parameters. OWSM v3.1 outperforms its predecessor, OWSM v3, in most evaluation benchmarks, while showing an improved inference speed of up to 25%. We further reveal the emergent ability of OWSM v3.1 in zero-shot contextual biasing speech recognition. We also provide a model trained on a subset of data with low license restrictions. We will publicly release the code, pre-trained models, and training logs.																																	2024-09-06	PPRN:87417466		
J	Huang, Jing; Wu, Zhengxuan; Potts, Christopher; Geva, Mor; Geiger, Atticus				Geva, Mor/JJF-9095-2023						RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations								Arxiv											2	2;2024-08-26;https://www.arxiv.org/abs/2402.17700v2| 1;2024-02-27;https://www.arxiv.org/abs/2402.17700v1	arXiv:2402.17700			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 26 2024	2024	Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. 																																	2024-09-06	PPRN:87921231		
J	Chen, Haoxing; Hong, Yan; Huang, Zizheng; Xu, Zhuoer; Gu, Zhangxuan; Li, Yaohui; Lan, Jun; Zhu, Huijia; Zhang, Jianfu; Wang, Weiqiang; Li, Huaxiong				Weiqiang, Wang/AAX-1219-2020; Li, Huaxiong/AAR-8881-2020						DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark								Arxiv											3	3;2024-08-22;https://www.arxiv.org/abs/2405.19707v3| 2;2024-07-16;https://www.arxiv.org/abs/2405.19707v2| 1;2024-05-30;https://www.arxiv.org/abs/2405.19707v1	arXiv:2405.19707			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 22 2024	2024	Recently, video generation techniques have advanced rapidly. Given the popularity of video content on social media platforms, these models intensify concerns about the spread of fake information. Therefore, there is a growing demand for detectors capable of distinguishing between fake AI-generated videos and mitigating the potential harm caused by fake information. However, the lack of large-scale datasets from the most advanced video generators poses a barrier to the development of such detectors. To address this gap, we introduce the first AI-generated video detection dataset, GenVideo. It features the following characteristics: (1) a large volume of videos, including over one million AI-generated and real videos collected; (2) a rich diversity of generated content and methodologies, covering a broad spectrum of video categories and generation techniques. We conducted extensive studies of the dataset and proposed two evaluation methods tailored for real-world-like scenarios to assess the detectors' performance: the cross-generator video classification task assesses the generalizability of trained detectors on generators; the degraded video classification task evaluates the robustness of detectors to handle videos that have degraded in quality during dissemination. Moreover, we introduced a plug-and-play module, named Detail Mamba (DeMamba), designed to enhance the detectors by identifying AI-generated videos through the analysis of inconsistencies in temporal and spatial dimensions. Our extensive experiments demonstrate DeMamba's superior generalizability and robustness on GenVideo compared to existing detectors. We believe that the GenVideo dataset and the DeMamba module will significantly advance the field of AI-generated video detection. 																																	2024-08-31	PPRN:89113649		
J	Tyser, Keith; Segev, Ben; Longhitano, Gaston; Zhang, Xin-Yu; Meeks, Zachary; Lee, Jason; Garg, Uday; Belsten, Nicholas; Shporer, Avi; Udell, Madeleine; Te'eni, Dov; Drori, Iddo				Te'eni, Dov/LCE-1217-2024; Belsten, Nick/KAM-5556-2024						AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews								Arxiv											1	1;2024-08-19;https://www.arxiv.org/abs/2408.10365v1	arXiv:2408.10365			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 19 2024	2024	Automatic reviewing helps handle a large volume of papers, provides early feedback and quality control, reduces bias, and allows the analysis of trends. Paper reviews are used by researchers and academics, students, lecturers, innovators and entrepreneurs, policymakers and funding agencies, science journalists, and the general public to navigate research, analyze trends, find educational purposes, and find collaborators. We evaluate the alignment of automatic paper reviews with human reviews using an arena of human preferences by pairwise comparisons. Gathering human preference may be timeconsuming; therefore, we also use an LLM to automatically evaluate reviews to increase sample efficiency while reducing bias. In addition to evaluating human and LLM preferences among LLM reviews, we fine-tune an LLM to predict human preferences, predicting which reviews humans will prefer in a head-to-head battle between LLMs. We artificially introduce errors into papers and analyze the LLM’s responses to identify limitations, use adaptive review questions, meta prompting, role-playing, integrate visual and textual analysis, use venue-specific reviewing materials, and predict human preferences, improving upon the limitations of the traditional review processes. We make the reviews of publicly available arXiv and open-access Nature journal papers available online, along with a free service which helps authors review and revise their research papers and improve their quality. This work develops proof-of-concept LLM reviewing systems that quickly deliver consistent, high-quality reviews and evaluate their quality. We mitigate the risks of misuse, inflated review scores, overconfident ratings, and skewed score distributions by augmenting the LLM with multiple documents, including the review form, reviewer guide, code of ethics and conduct, area chair guidelines, and previous year statistics, by finding which errors and shortcomings of the paper may be detected by automated reviews, and evaluating pairwise reviewer preferences. This work identifies and addresses the limitations of using LLMs as reviewers and evaluators and enhances the quality of the reviewing process.																																	2024-09-11	PPRN:91499394		
J	Liao, Bencheng; Chen, Shaoyu; Jiang, Bo; Cheng, Tianheng; Zhang, Qian; Liu, Wenyu; Huang, Chang; Wang, Xinggang				Wang, Xinggang/LSL-0946-2024; Chen, Shaoyu/MGA-4947-2025; Cheng, Tianheng/MIP-3645-2025						Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction								Arxiv											3	3;2024-08-15;https://www.arxiv.org/abs/2303.08815v3| 2;2023-12-17;https://www.arxiv.org/abs/2303.08815v2| 1;2023-03-15;https://www.arxiv.org/abs/2303.08815v1	arXiv:2303.08815			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 15 2024	2024	Online lane graph construction is a promising but challenging task in autonomous driving. Previous methods usually model the lane graph at the pixel or piece level, and recover the lane graph by pixel-wise or piece-wise connection, which breaks down the continuity of the lane and results in suboptimal performance. Human drivers focus on and drive along the continuous and complete paths instead of considering lane pieces. Autonomous vehicles also require path-specific guidance from lane graph for trajectory planning. We argue that the path, which indicates the traffic flow, is the primitive of the lane graph. Motivated by this, we propose to model the lane graph in a novel path-wise manner, which well preserves the continuity of the lane and encodes traffic information for planning. We present a path-based online lane graph construction method, termed LaneGAP, which end-to-end learns the path and recovers the lane graph via a Path2Graph algorithm. We qualitatively and quantitatively demonstrate the superior accuracy and efficiency of LaneGAP over conventional pixel-based and piece-based methods on the challenging nuScenes and Argoverse2 datasets under controllable and fair conditions. Compared to the recent state-of-the-art piece-wise method TopoNet on the OpenLane-V2 dataset, LaneGAP still outperforms by 1.6 mIoU, further validating the effectiveness of path-wise modeling. Abundant visualizations in the supplementary material show LaneGAP can cope with diverse traffic conditions. 																																	2024-08-23	PPRN:46840385		
J	Friz, Peter K.; Hocquet, Antoine; Le, Khoa				Friz, Peter/NNG-7530-2025						ROUGH STOCHASTIC DIFFERENTIAL EQUATIONS								Arxiv											2	2;2024-08-13;https://www.arxiv.org/abs/2106.10340v5| 1;2021-06-18;https://www.arxiv.org/abs/2106.10340v2	arXiv:2106.10340			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 13 2024	2024	We establish a simultaneous generalization of Itô’s theory of stochastic and Lyons’ theory of rough differential equations. The interest in such a unification comes from a variety of applications, including pathwise stochastic filtering,- control and the conditional analysis of stochastic systems with common noise.																																	2024-08-22	PPRN:12054914		
J	Davari, MohammadReza; Belilovsky, Eugene				Davari, MohammadReza/NYS-5569-2025						Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks								Arxiv											2	2;2024-08-10;https://www.arxiv.org/abs/2312.06795v2| 1;2023-12-11;https://www.arxiv.org/abs/2312.06795v1	arXiv:2312.06795			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 10 2024	2024	The rapid development of AI systems has been greatly influenced by the emergence of foundation models. A common approach for targeted problems involves fine-tuning these pre-trained foundation models for specific target tasks, resulting in a rapid spread of models fine-tuned across a diverse array of tasks. This work focuses on the problem of merging multiple fine-tunings of the same foundation model derived from a spectrum of auxiliary tasks. We introduce a new simple method, Model Breadcrumbs, which consists of a sparsely defined weight set that guides model adaptation within the weight space of a pre-trained model. These breadcrumbs are constructed by subtracting the weights from a pre-trained model before and after fine-tuning, followed by a sparsification process that eliminates weight outliers and negligible perturbations. Our experiments demonstrate the effectiveness of Model Breadcrumbs to simultaneously improve performance across multiple tasks. This contribution aligns with the evolving paradigm of updatable machine learning, reminiscent of the collaborative principles underlying open-source software development, fostering a community-driven effort to reliably update machine learning models. Our method is shown to be more efficient and unlike previous proposals does not require hyperparameter tuning for each new task added. Through extensive experimentation involving various models, tasks, and modalities we establish that integrating Model Breadcrumbs offers a simple, efficient, and highly effective approach for constructing multi-task models and facilitating updates to foundation models.																																	2024-08-21	PPRN:86555246		
J	Sodhi, Paloma; Branavan, S.R.K.; Artzi, Yoav; McDonald, Ryan										SteP: Stacked LLM Policies for Web Actions								Arxiv											5	5;2024-08-08;https://www.arxiv.org/abs/2310.03720v4| 4;2024-08-06;https://www.arxiv.org/abs/2310.03720v3| 3;2024-04-22;https://www.arxiv.org/abs/2310.03720v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03720v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03720v1	arXiv:2310.03720			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 08 2024	2024	Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9% to 33.5%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data.																																	2024-08-21	PPRN:85436249		
J	Ma, Jiabo; Guo, Zhengrui; Zhou, Fengtao; Wang, Yihui; Xu, Yingxue; Cai, Yu; Zhu, Zhengjie; Jin, Cheng; Lin, Yi; Jiang, Xinrui; Han, Anjia; Liang, Li; Chan, Ronald Cheong Kin; Wang, Jiguang; Cheng, Kwang-Ting; Chen, Hao				Chen, Hao/JHU-3470-2023; Lin, Yi/JKH-5104-2023; Chen, Long/HZJ-7271-2023; Wang, Yihui/OLQ-4655-2025; Jiang, Xinrui/AHC-2016-2022; MA, Jiabo/NQF-7416-2025; Wang, Jiguang/B-2717-2012						Towards A Generalizable Pathology Foundation Model via Unified Knowledge Distillation								Arxiv											2	2;2024-08-03;https://www.arxiv.org/abs/2407.18449v2| 1;2024-07-26;https://www.arxiv.org/abs/2407.18449v1	arXiv:2407.18449			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 03 2024	2024	Foundation models pretrained on large-scale datasets are revolutionizing the field of computational pathology (CPath). The generalization ability of foundation models is crucial for the success in various downstream clinical tasks. However, current foundation models have only been evaluated on a limited type and number of tasks, leaving their generalization ability and overall performance unclear. To address this gap, we established a most comprehensive benchmark to evaluate the performance of off-the-shelf foundation models across six distinct clinical task types, encompassing a total of 39 specific tasks. Our findings reveal that existing foundation models excel at certain task types but struggle to effectively handle the full breadth of clinical tasks. To improve the generalization of pathology foundation models, we propose a unified knowledge distillation framework consisting of both expert and self knowledge distillation, where the former allows the model to learn from the knowledge of multiple expert models, while the latter leverages self-distillation to enable image representation learning via local-global alignment. Based on this framework, a Generalizable Pathology Foundation Model (GPFM) is pretrained on a large-scale dataset consisting of 190 million images from around 86,000 public H&E whole slides across 34 major tissue types. Evaluated on the established benchmark, GPFM achieves an impressive average rank of 1.36, with 29 tasks ranked 1st, while the the second-best model, UNI, attains an average rank of 2.96, with only 4 tasks ranked 1st. The superior generalization of GPFM demonstrates its exceptional modeling capabilities across a wide range of clinical tasks, positioning it as a new cornerstone for feature representation in CPath.																																	2024-08-11	PPRN:91119652		
J	Zhang, Qian; Dai, Xiangzi; Yang, Ninghua; An, Xiang; Feng, Ziyong; Ren, Xingyu				Ren, Xingyu/ABN-1790-2022						VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling								Arxiv											1	1;2024-08-02;https://www.arxiv.org/abs/2408.01181v1	arXiv:2408.01181			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 02 2024	2024	VAR is a new generation paradigm that employs 'next-scale prediction' as opposed to 'next-token prediction'. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model that integrates Visual Auto-Regressive techniques with the capabilities of CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets, such as ImageNet, we have constructed a substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR-CLIP's proficiency in generating fantasy images with high fidelity, textual congruence, and aesthetic excellence. 																																	2024-08-11	PPRN:91253638		
J	Zhang, Tao; Shen, Yanjun; Luo, Wenjing; Zhang, Yan; Liang, Hao; Zhang, T; Yang, Fan; Lin, Mingan; Qiao, Yujing; Chen, Weipeng; Cui, Bin; Zhang, Wentao; Zhou, Zenan				Chen, Weipeng/ITV-5921-2023; zhang, tao/GYA-5878-2022; YANG, FAN/W-8237-2019; Shen, Yan-Jun/AAD-2575-2021						CFBench: A Comprehensive Constraints-Following Benchmark for LLMs								Arxiv											1	1;2024-08-02;https://www.arxiv.org/abs/2408.01122v1	arXiv:2408.01122			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 02 2024	2024	The adeptness of Large Language Models (LLMs) in comprehending and following natural language instructions is critical for their deployment in sophisticated real-world applications. Existing evaluations mainly focus on fragmented constraints or narrow scenarios, but they overlook the comprehensiveness and authenticity of constraints from the user’s perspective. To bridge this gap, we propose CFBench, a largescale Comprehensive Constraints Following Benchmark for LLMs, featuring 1,000 curated samples that cover more than 200 real-life scenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from real-world instructions and constructs an innovative systematic framework for constraint types, which includes 10 primary categories and over 25 subcategories, and ensures each constraint is seamlessly integrated within the instructions. To make certain that the evaluation of LLM outputs aligns with user perceptions, we propose an advanced methodology that integrates multidimensional assessment criteria with requirement prioritization, covering various perspectives of constraints, instructions, and requirement fulfillment. Evaluating current leading LLMs on CFBench reveals substantial room for improvement in constraints following, and we further investigate influencing factors and enhancement strategies. 																																	2024-08-08	PPRN:91230164		
J	Kweon, Sunjun; Kim, Junu; Kim, Jiyoun; Im, Sujeong; Cho, Eunbyeol; Bae, Seongsu; Oh, Jungwoo; Lee, Gyubok; Moon, Jong Hak; You, Seng Chan; Baek, Seungjin; Han, Chang Hoon; Jung, Yoon Bin; Jo, Yohan; Choi, Edward				Oh, Jungwoo/MEP-0252-2025; Choi, Edward/AAC-8825-2020						Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes								Arxiv											4	4;2024-07-29;https://www.arxiv.org/abs/2309.00237v4| 3;2024-06-13;https://www.arxiv.org/abs/2309.00237v3| 2;2023-09-06;https://www.arxiv.org/abs/2309.00237v2| 1;2023-09-01;https://www.arxiv.org/abs/2309.00237v1	arXiv:2309.00237			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jul 29 2024	2024	The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructing high-performing clinical language models. This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals. All resources including weights, codes, and data used in the development of Asclepius are made publicly accessible for future research1.																																	2024-08-03	PPRN:84672972		
J	Lu, Yu; Liang, Yuanzhi; Zhu, Linchao; Yang, Yi				Zhu, Linchao/AAE-6700-2020						FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention								Arxiv											1	1;2024-07-29;https://www.arxiv.org/abs/2407.19918v1	arXiv:2407.19918			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 29 2024	2024	Video diffusion models have made substantial progress in various video generation applications. However, training models for long video generation tasks require significant computational and data resources, posing a challenge to developing long video diffusion models. This paper investigates a straightforward and training-free approach to extend an existing short video diffusion model (e.g. pre-trained on 16-frame videos) for consistent long video generation (e.g. 128 frames). Our preliminary observation has found that directly applying the short video diffusion model to generate long videos can lead to severe video quality degradation. Further investigation reveals that this degradation is primarily due to the distortion of high-frequency components in long videos, characterized by a decrease in spatial high-frequency components and an increase in temporal high-frequency components. Motivated by this, we propose a novel solution named FreeLong to balance the frequency distribution of long video features during the denoising process. FreeLong blends the low-frequency components of global video features, which encapsulate the entire video sequence, with the high-frequency components of local video features that focus on shorter subsequences of frames. This approach maintains global consistency while incorporating diverse and high-quality spatiotemporal details from local videos, enhancing both the consistency and fidelity of long video generation. We evaluated FreeLong on multiple base video diffusion models and observed significant improvements. Additionally, our method supports coherent multi-prompt generation, ensuring both visual coherence and seamless transitions between scenes.																																	2024-08-04	PPRN:91143757		
J	Shen, Junhong; Tenenholtz, Neil; Hall, James Brian; Alvarez-Melis, David; Fusi, Nicolo				Alvarez-Melis, David/AAV-1099-2021						Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains								Arxiv											3	3;2024-07-26;https://www.arxiv.org/abs/2402.05140v3| 2;2024-05-30;https://www.arxiv.org/abs/2402.05140v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.05140v1	arXiv:2402.05140			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 26 2024	2024	Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.																																	2024-08-02	PPRN:87572806		
J	Trivedi, Harsh; Khot, Tushar; Hartmann, Mareike; Manku, Ruskin; Dong, Vinty; Li, Edward; Gupta, Shashank; Sabharwal, Ashish; Balasubramanian, Niranjan										AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents								Arxiv											1	1;2024-07-26;https://www.arxiv.org/abs/2407.18901v1	arXiv:2407.18901			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 26 2024	2024	Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built $textbf{AppWorld Engine}$, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created $textbf{AppWorld Benchmark}$ (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our 'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least 16% fewer. This highlights the benchmark's difficulty and AppWorld's potential to push the frontiers of interactive coding agents. 																																	2024-08-04	PPRN:91150031		
J	Curras-Lorenzo, Guillermo; Pereira, Margarida; Kato, Go; Curty, Marcos; Tamaki, Kiyoshi				Curty, Marcos/L-2416-2014; Pereira, Margarida/AAM-3932-2020						A security framework for quantum key distribution implementations								Arxiv											2	2;2024-07-23;https://www.arxiv.org/abs/2305.05930v2| 1;2023-05-10;https://www.arxiv.org/abs/2305.05930v1	arXiv:2305.05930			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 23 2024	2024	Quantum key distribution (QKD) can theoretically achieve the Holy Grail of cryptography, information-theoretic security against eavesdropping. However, in practice, discrepancies between the mathematical models assumed in security proofs and the actual functioning of the devices used in implementations prevent it from reaching this goal. Device-independent QKD is currently not a satisfactory solution to this problem, as its performance is extremely poor and most of its security proofs assume that the user devices leak absolutely no information to the outside. On the other hand, measurement-device-independent (MDI) QKD can guarantee security with arbitrarily flawed receivers while achieving high performance, and the remaining challenge is ensuring its security in the presence of source imperfections. So far, all efforts in this regard have come at a price; some proofs are suitable only for particular source imperfections, while others severely compromise the system's performance, i.e., its communication speed and distance. Here, we overcome these crucial problems by presenting a security proof in the finite-key regime against coherent attacks that can incorporate general encoding imperfections and side channels while achieving much higher performances than previous approaches. Moreover, our proof requires minimal state characterization, which facilitates its application to real-life implementations.																																	2024-07-30	PPRN:69017972		
J	Unal, Caner; Papageorgiou, Alexandros; Obata, Ippei				Obata, Ippei/JQW-2297-2023; Unal, Caner/NYT-1890-2025						Axion-Gauge Dynamics During Inflation as the Origin of Pulsar Timing Array Signals and Primordial Black Holes								Arxiv											2	2;2024-07-18;https://www.arxiv.org/abs/2307.02322v3| 1;2023-07-05;https://www.arxiv.org/abs/2307.02322v1	arXiv:2307.02322			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 18 2024	2024	We demonstrate that the recently announced signal for a stochastic gravitational wave background (SGWB) from pulsar timing array (PTA) observations, if attributed to new physics, is compatible with primordial GW production due to axion-gauge dynamics during inflation. More specifically we find that axion-U(1) models may lead to sufficient particle production to explain the signal while simultaneously source some fraction of sub-solar mass primordial black holes (PBHs) as a signature. Moreover there is a parity violation in GW sector, hence the model suggests chiral GW search as a concrete target for future. We further analyze the axion-SU(2) coupling signatures and find that in the low/mild backreaction regime, it is incapable of producing PTA evidence and the tensor-to-scalar ratio is low at the peak, hence it overproduces scalar perturbations and PBHs.																																	2024-07-26	PPRN:73790269		
J	Majumder, Navonil; Hung, Chia-Yu; Ghosal, Deepanway; Hsu, Wei-Ning; Mihalcea, Rada; Poria, Soujanya				PORIA, SOUJANYA/KIJ-4789-2024						Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization								Arxiv											3	3;2024-07-17;https://www.arxiv.org/abs/2404.09956v4| 2;2024-07-04;https://www.arxiv.org/abs/2404.09956v3| 1;2024-04-16;https://www.arxiv.org/abs/2404.09956v2	arXiv:2404.09956			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jul 17 2024	2024	Generative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life. The generation of audio from text prompts is an important aspect of such processes in the music and film industry. Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs. These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt. Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data. As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from. The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order. We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics.																																	2024-07-26	PPRN:88539333		
J	Everett, Katie; Xiao, Lechao; Wortsman, Mitchell; Alemi, Alexander A.; Novak, Roman; Liu, Peter J.; Gur, Izzeddin; Sohl-Dickstein, Jascha; Kaelbling, Leslie Pack; Lee, Jaehoon; Pennington, Jeffrey										Scaling Exponents Across Parameterizations and Optimizers								Arxiv											2	2;2024-07-16;https://www.arxiv.org/abs/2407.05872v2| 1;2024-07-08;https://www.arxiv.org/abs/2407.05872v1	arXiv:2407.05872			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 16 2024	2024	Robust and effective scaling of models from small to large width typically requires the precise adjustment of many algorithmic and architectural details, such as parameterization and optimizer choices. In this work, we propose a new perspective on parameterization by investigating a key assumption in prior work about the alignment between parameters and data and derive new theoretical results under weaker assumptions and a broader set of optimizers. Our extensive empirical investigation includes tens of thousands of models trained with all combinations of three optimizers, four parameterizations, several alignment assumptions, more than a dozen learning rates, and fourteen model sizes up to 26.8B parameters. We find that the best learning rate scaling prescription would often have been excluded by the assumptions in prior work. Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer; moreover, our novel per-layer learning rate prescription for standard parameterization outperforms muP. Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow and propose Adam-atan2, a new numerically stable, scale-invariant version of Adam that eliminates the epsilon hyperparameter entirely.																																	2024-07-25	PPRN:90739932		
J	Chatziioannou, Katerina; Cromartie, H. Thankful; Gandolfi, Stefano; Tews, Ingo; Radice, David; Steiner, Andrew W.; Watts, Anna L.				Tews, Ingo/AAF-9124-2020; Reitze, David/IXD-3777-2023						Neutron stars and the dense matter equation of state: from microscopic theory to macroscopic observations								Arxiv											1	1;2024-07-15;https://www.arxiv.org/abs/2407.11153v1	arXiv:2407.11153			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 15 2024	2024	The past years have witnessed tremendous progress in understanding the properties of neutron stars and of the dense matter in their cores, made possible by electromagnetic observations of neutron stars and the detection of gravitational waves from their mergers. These observations provided novel constraints on neutron-star structure, that is intimately related to the properties of dense neutron-rich matter described by the nuclear equation of state. Nevertheless, constraining the equation of state over the wide range of densities probed by astrophysical observations is still challenging, as the physics involved is very broad and the system spans many orders of magnitude in densities. Here, we review theoretical approaches to calculate and model the neutron-star equation of state in various regimes of densities, and discuss the related consequent properties of neutron stars. We describe how the equation of state can be calculated from nuclear interactions that are constrained and benchmarked by nuclear experiments. We review neutron-star observations, with particular emphasis on information provided by gravitational-wave signals and electromagnetic observations. Finally, we discuss future challenges and opportunities in the field.																																	2024-07-25	PPRN:90850648		
J	Aakanksha; Ahmadian, Arash; Ermis, Beyza; Goldfarb-Tarrant, Seraphina; Kreutzer, Julia; Fadaee, Marzieh; Hooker, Sara				Ermis, Beyza Hilal/KJO-2003-2024						The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm								Arxiv											2	2;2024-07-08;https://www.arxiv.org/abs/2406.18682v2| 1;2024-06-26;https://www.arxiv.org/abs/2406.18682v1	arXiv:2406.18682			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 08 2024	2024	A key concern with the concept of "alignment" is the implicit question of "alignment to what?". AI systems are increasingly used across the world, yet safety alignment is often focused on homogeneous monolingual settings. Additionally, preference training and safety measures often overfit to harms common in Western-centric datasets. Here, we explore the viability of different alignment approaches when balancing dual objectives: addressing and optimizing for a non-homogeneous set of languages and cultural preferences while minimizing both global and local harms. We collect the first set of human annotated red-teaming prompts in different languages distinguishing between global and local harm, which serve as a laboratory for understanding the reliability of alignment techniques when faced with preference distributions that are non-stationary across geographies and languages. While this setting is seldom covered by the literature to date, which primarily centers on English harm mitigation, it captures real-world interactions with AI systems around the world. We establish a new precedent for state-of-the-art alignment techniques across 6 languages with minimal degradation in general performance. Our work provides important insights into cross-lingual transfer and novel optimization approaches to safeguard AI systems designed to serve global populations.																																	2024-07-21	PPRN:90118841		
J	Ferracin, Samuele; Hashim, Akel; Ville, Jean-Loup; Naik, Ravi; Carignan-Dugas, Arnaud; Qassim, Hammam; Morvan, Alexis; Santiago, David I.; Siddiqi, Irfan; Wallman, Joel J.				Wallman, Joel/X-5516-2019; Santiago, David/KFQ-3277-2024						Efficiently improving the performance of noisy quantum computers								Arxiv											3	3;2024-07-06;https://www.arxiv.org/abs/2201.10672v5| 2;2024-06-29;https://www.arxiv.org/abs/2201.10672v4| 1;2024-06-25;https://www.arxiv.org/abs/2201.10672v3	arXiv:2201.10672			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 06 2024	2024	Using near-term quantum computers to achieve a quantum advantage requires efficient strategies to improve the performance of the noisy quantum devices presently available. We develop and experimentally validate two efficient error mitigation protocols named “Noiseless Output Extrapolation” and “Pauli Error Cancellation” that can drastically enhance the performance of quantum circuits composed of noisy cycles of gates. By combining popular mitigation strategies such as probabilistic error cancellation and noise amplification with efficient noise reconstruction methods, our protocols can mitigate a wide range of noise processes that do not satisfy the assumptions underlying existing mitigation protocols, including nonlocal and gate-dependent processes. We test our protocols on a four-qubit superconducting processor at the Advanced Quantum Testbed. We observe significant improvements in the performance of both structured and random circuits, with up to 86% improvement in variation distance over the unmitigated outputs. Our experiments demonstrate the effectiveness of our protocols, as well as their practicality for current hardware platforms.																																	2024-07-21	PPRN:89523496		
J	Sreedhar, Karthik; Chilton, Lydia										Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs								Arxiv											2	2;2024-07-02;https://www.arxiv.org/abs/2402.08189v2| 1;2024-02-13;https://www.arxiv.org/abs/2402.08189v1	arXiv:2402.08189			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 02 2024	2024	When creating policies, plans, or designs for people, it is challenging for designers to foresee all of the ways in which people may reason and behave. Recently, Large Language Models (LLMs) have been shown to be able to simulate human reasoning. We extend this work by measuring LLMs ability to simulate strategic reasoning in the ultimatum game, a classic economics bargaining experiment. Experimental evidence shows human strategic reasoning is complex; people will often choose to punish other players to enforce social norms even at personal expense. We test if LLMs can replicate this behavior in simulation, comparing two structures: single LLMs and multi-agent systems. We compare their abilities to (1) simulate human-like reasoning in the ultimatum game, (2) simulate two player personalities, greedy and fair, and (3) create robust strategies that are logically complete and consistent with personality. Our evaluation shows that multi-agent systems are more accurate than single LLMs (88 percent vs. 50 percent) in simulating human reasoning and actions for personality pairs. Thus, there is potential to use LLMs to simulate human strategic reasoning to help decision and policy-makers perform preliminary explorations of how people behave in systems.																																	2024-07-19	PPRN:87672737		
J	Zhang, Yue; Ma, Ziqiao; Li, Jialu; Qiao, Yanyuan; Wang, Zun; Chai, Joyce; Wu, Qi; Bansal, Mohit; Kordjamshidi, Parisa				Bansal, Mohit/Q-9105-2016; Wu, Qi/ABD-6304-2021; Yanyuan, Qiao/KBY-0525-2024						Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models								Arxiv											1	1;2024-07-01;	arXiv:2407.07035			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on one hand, to milestone the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.																																	2024-11-18	PPRN:118690380		
J	Fuest, Michael; Ma, Pingchuan; Gui, Ming; Schusterbauer, Johannes; Hu, Vincent Tao; Ommer, Bjorn				Ma, Pingchuan/AFR-0634-2022						Diffusion Models and Representation Learning: A Survey								Arxiv											2	2;2024-06-30;https://www.arxiv.org/abs/2407.00783v1| 1;2024-06-30;https://www.arxiv.org/abs/2407.00783v1	arXiv:2407.00783			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 30 2024	2024	Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models' essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. 																																	2025-03-15	PPRN:90659322		
J	Wang, Haofan; Xing, Peng; Huang, Renyuan; Ai, Hao; Wang, Qixun; Bai, Xu				Wang, Qixun/ITT-4329-2023; Wong, Howard/HZJ-8545-2023; Xing, Peng/E-9238-2016						InstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation								Arxiv											1	1;2024-06-30;https://www.arxiv.org/abs/2407.00788v1	arXiv:2407.00788			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 30 2024	2024	Style transfer is an inventive process designed to create an image that maintains the essence of the original while embracing the visual style of another. Although diffusion models have demonstrated impressive generative power in personalized subject-driven or style-driven applications, existing state-of-the-art methods still encounter difficulties in achieving a seamless balance between content preservation and style enhancement. For example, amplifying the style's influence can often undermine the structural integrity of the content. To address these challenges, we deconstruct the style transfer task into three core elements: 1) Style, focusing on the image's aesthetic characteristics; 2) Spatial Structure, concerning the geometric arrangement and composition of visual elements; and 3) Semantic Content, which captures the conceptual meaning of the image. Guided by these principles, we introduce InstantStyle-Plus, an approach that prioritizes the integrity of the original content while seamlessly integrating the target style. Specifically, our method accomplishes style injection through an efficient, lightweight process, utilizing the cutting-edge InstantStyle framework. To reinforce the content preservation, we initiate the process with an inverted content latent noise and a versatile plug-and-play tile ControlNet for preserving the original image's intrinsic layout. We also incorporate a global semantic adapter to enhance the semantic content's fidelity. To safeguard against the dilution of style information, a style extractor is employed as discriminator for providing supplementary style guidance. Codes will be available at https://github.com/instantX-research/InstantStyle-Plus.																																	2024-07-18	PPRN:90656103		
J	Howard, Amanda A.; Jacob, Bruno; Murphy, Sarah H.; Heinlein, Alexander; Stinis, Panos				Heinlein, Alexander/AAS-9712-2020						Finite basis Kolmogorov-Arnold networks: domain decomposition for data-driven and physics-informed problems								Arxiv											1	1;2024-06-28;https://www.arxiv.org/abs/2406.19662v1	arXiv:2406.19662			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 28 2024	2024	Kolmogorov-Arnold networks (KANs) have attracted attention recently as an alternative to multilayer perceptrons (MLPs) for scientific machine learning. However, KANs can be expensive to train, even for relatively small networks. Inspired by finite basis physics-informed neural networks (FBPINNs), in this work, we develop a domain decomposition method for KANs that allows for several small KANs to be trained in parallel to give accurate solutions for multiscale problems. We show that finite basis KANs (FBKANs) can provide accurate results with noisy data and for physics-informed training.																																	2024-07-17	PPRN:90635649		
J	Wang, Junda; Yang, Zhichao; Yao, Zonghai; Yu, Hong				Yao, Zonghai/GNP-6756-2022; Yang, Zhichao/JLK-8359-2023						JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability								Arxiv											4	4;2024-06-28;https://www.arxiv.org/abs/2402.17887v4| 3;2024-04-16;https://www.arxiv.org/abs/2402.17887v3| 2;2024-03-02;https://www.arxiv.org/abs/2402.17887v2| 1;2024-02-27;https://www.arxiv.org/abs/2402.17887v1	arXiv:2402.17887			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 28 2024	2024	Large Language Models (LLMs) have demonstrated a remarkable potential in medical knowledge acquisition and question-answering. However, LLMs can potentially hallucinate and yield factually incorrect outcomes, even with domain-specific pretraining. Previously, retrieval augmented generation (RAG) has limited success in addressing hallucinations. Unlike previous methods in RAG where the retrieval model was trained separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning phase. The synchronized training mechanism enhances JMLR's ability to retrieve clinical guidelines and leverage medical knowledge to reason and answer questions and reduces the demand for computational resources. We evaluated JMLR on the important medical question-answering application. Our experimental results demonstrate that JMLR-13B (70.5%) outperforms a previous state-of-the-art open-source model using conventional pre-training and fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances reasoning quality and reduces hallucinations better than Claude3-Opus. Additionally, JMLR-13B (148 GPU hours) also trains much faster than Meditron-70B (42630 GPU hours). Through this work, we provide a new and efficient knowledge enhancement method for healthcare, demonstrating the potential of integrating retrieval and LLM training for medical question-answering systems.																																	2024-11-09	PPRN:87985326		
J	Kissane, Connor; Krzyzanowski, Robert; Bloom, Joseph; Conmy, Arthur; Nanda, Neel										Interpreting Attention Layer Outputs with Sparse Autoencoders								Arxiv											1	1;2024-06-25;https://www.arxiv.org/abs/2406.17759v1	arXiv:2406.17759			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 25 2024	2024	Decomposing model activations into interpretable components is a key open problem in mechanistic interpretability. Sparse autoencoders (SAEs) are a popular method for decomposing the internal activations of trained transformers into sparse, interpretable features, and have been applied to MLP layers and the residual stream. In this work we train SAEs on attention layer outputs and show that also here SAEs find a sparse, interpretable decomposition. We demonstrate this on transformers from several model families and up to 2B parameters. We perform a qualitative study of the features computed by attention layers, and find multiple families: long-range context, short-range context and induction features. We qualitatively study the role of every head in GPT-2 Small, and estimate that at least 90% of the heads are polysemantic, i.e. have multiple unrelated roles. Further, we show that Sparse Autoencoders are a useful tool that enable researchers to explain model behavior in greater detail than prior work. For example, we explore the mystery of why models have so many seemingly redundant induction heads, use SAEs to motivate the hypothesis that some are long-prefix whereas others are short-prefix, and confirm this with more rigorous analysis. We use our SAEs to analyze the computation performed by the Indirect Object Identification circuit (Wang et al. [66]), validating that the SAEs find causally meaningful intermediate variables, and deepening our understanding of the semantics of the circuit. We open-source the trained SAEs and a tool for exploring arbitrary prompts through the lens of Attention Output SAEs.																																	2024-07-15	PPRN:89530645		
J	Zeng, Yi; Klyman, Kevin; Zhou, Andy; Yang, Yu; Pan, Minzhou; Jia, Ruoxi; Song, Dawn; Liang, Percy; Li, Bo				zeng, yi/KFS-5661-2024						AI Risk Categorization Decoded (AIR 2024): From Government Regulations to Corporate Policies								Arxiv											1	1;2024-06-25;https://www.arxiv.org/abs/2406.17864v1	arXiv:2406.17864			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 25 2024	2024	We present a comprehensive AI risk taxonomy derived from eight government policies from the European Union, United States, and China and 16 company policies worldwide, making a significant step towards establishing a unified language for generative AI safety evaluation. We identify 314 unique risk categories organized into a four-tiered taxonomy. At the highest level, this taxonomy encompasses System & Operational Risks, Content Safety Risks, Societal Risks, and Legal & Rights Risks. The taxonomy establishes connections between various descriptions and approaches to risk, highlighting the overlaps and discrepancies between public and private sector conceptions of risk. By providing this unified framework, we aim to advance AI safety through information sharing across sectors and the promotion of best practices in risk mitigation for generative AI models and systems.																																	2024-08-25	PPRN:89898720		
J	Bordelon, Blake; Atanasov, Alexander; Pehlevan, Cengiz				Pehlevan, Cengiz/I-2259-2013						A Dynamical Model of Neural Scaling Laws								Arxiv											4	4;2024-06-23;https://www.arxiv.org/abs/2402.01092v4| 3;2024-05-27;https://www.arxiv.org/abs/2402.01092v3| 2;2024-04-12;https://www.arxiv.org/abs/2402.01092v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01092v1	arXiv:2402.01092			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 23 2024	2024	On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinitewidth dynamics at a rate 1 / width but at late time exhibit a rate width − c , where c depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.																																	2024-07-15	PPRN:87508975		
J	Feng, Jie; Du, Yuwei; Liu, Tianhui; Guo, Siqi; Lin, Yuming; Li, Yong				Feng, Jie/KGL-6704-2024; Guo, Siqi/JWP-9459-2024; Lin, Yuming/HGF-3286-2022						CityGPT: Empowering Urban Spatial Cognition of Large Language Models								Arxiv											1	1;2024-06-20;https://www.arxiv.org/abs/2406.13948v1	arXiv:2406.13948			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 20 2024	2024	Large language models(LLMs) with powerful language generation and reasoning capabilities have already achieved success in many domains, e.g., math and code generation. However, due to the lacking of physical world's corpus and knowledge during training, they usually fail to solve many real-life tasks in the urban space. In this paper, we propose CityGPT, a systematic framework for enhancing the capability of LLMs on understanding urban space and solving the related urban tasks by building a city-scale world model in the model. First, we construct a diverse instruction tuning dataset CityInstruction for injecting urban knowledge and enhancing spatial reasoning capability effectively. By using a mixture of CityInstruction and general instruction data, we fine-tune various LLMs (e.g., ChatGLM3-6B, Qwen1.5 and LLama3 series) to enhance their capability without sacrificing general abilities. To further validate the effectiveness of proposed methods, we construct a comprehensive benchmark CityEval to evaluate the capability of LLMs on diverse urban scenarios and problems. Extensive evaluation results demonstrate that small LLMs trained with CityInstruction can achieve competitive performance with commercial LLMs in the comprehensive evaluation of CityEval. The source codes are openly accessible to the research community via https://github.com/tsinghua-fib-lab/CityGPT.																																	2025-08-07	PPRN:123164961		
J	Jia, Shuyi; Zhang, Chao; Fung, Victor				Fung, Victor/A-9928-2016						LLMatDesign: Autonomous Materials Discovery with Large Language Models								Arxiv											1	1;2024-06-19;https://www.arxiv.org/abs/2406.13163v1	arXiv:2406.13163			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jun 19 2024	2024	Discovering new materials can have significant scientific and technological implications but remains a challenging problem today due to the enormity of the chemical space. Recent advances in machine learning have enabled data-driven methods to rapidly screen or generate promising materials, but these methods still depend heavily on very large quantities of training data and often lack the flexibility and chemical understanding often desired in materials discovery. We introduce LLMatDesign, a novel language-based framework for interpretable materials design powered by large language models (LLMs). LLMatDesign utilizes LLM agents to translate human instructions, apply modifications to materials, and evaluate outcomes using provided tools. By incorporating self-reflection on its previous decisions, LLMatDesign adapts rapidly to new tasks and conditions in a zero-shot manner. A systematic evaluation of LLMatDesign on several materials design tasks, in silico, validates LLMatDesign's effectiveness in developing new materials with user-defined target properties in the small data regime. Our framework demonstrates the remarkable potential of autonomous LLM-guided materials discovery in the computational setting and towards self-driving laboratories in the future.																																	2024-07-06	PPRN:89376865		
J	Cameron, Alex J.; Katz, Harley; Witten, Callum; Saxena, Aayush; Laporte, Nicolas; Bunker, Andrew J.										Nebular dominated galaxies: insights into the stellar initial mass function at high redshift								Arxiv											4	4;2024-06-19;https://www.arxiv.org/abs/2311.02051v5| 3;2024-06-18;https://www.arxiv.org/abs/2311.02051v4| 2;2023-11-21;https://www.arxiv.org/abs/2311.02051v3| 1;2023-11-06;https://www.arxiv.org/abs/2311.02051v2	arXiv:2311.02051			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	We identify a low-metallicity ($12+log({rm O}/{rm H})=7.59$) Ly$alpha$-emitting galaxy at $z=5.943$ with evidence of a strong Balmer jump, arising from nebular continuum. While Balmer jumps are sometimes observed in low-redshift star-forming galaxies, this galaxy also exhibits a steep turnover in the UV continuum. Such turnovers are typically attributed to absorption by a damped Ly$alpha$ system (DLA); however, the shape of the turnover and the high observed Ly$alpha$ escape fraction ($f_{rm esc,Lyalpha}~sim27%$) is also consistent with strong nebular two-photon continuum emission. Modelling the UV turnover with a DLA requires extreme column densities ($N_{rm HI}>10^{23}$~cm$^{-2}$), and simultaneously explaining the high $f_{rm esc,Lyalpha}$ requires a fine-tuned geometry. In contrast, modelling the spectrum as primarily nebular provides a good fit to both the continuum and emission lines, motivating scenarios in which (a) we are observing only nebular emission or (b) the ionizing source is powering extreme nebular emission that outshines the stellar emission. The nebular-only scenario could arise if the ionising source has `turned off' more recently than the recombination timescale ($sim$1,000~yr), hence we may be catching the object at a very specific time. Alternatively, hot stars with $T_{rm eff}gtrsim10^5$~K (e.g. Wolf-Rayet or low-metallicity massive stars) produce enough ionizing photons such that the two-photon emission becomes visible. While several stellar SEDs from the literature fit the observed spectrum well, the hot-star scenario requires that the number of $gtrsim50~{rm M}_odot$ stars relative to $sim5-50~{rm M}_odot$ stars is significantly higher than predicted by typical stellar initial mass functions (IMFs). The identification of more galaxies with similar spectra may provide evidence for a top-heavy IMF at high redshift.																																	2025-08-07	PPRN:86062807		
J	Xiao, Changrong; Ma, Wenxing; Song, Qingping; Xu, Sean Xin; Zhang, Kunpeng; Wang, Yufang; Fu, Qi										Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs								Arxiv											2	2;2024-06-15;https://www.arxiv.org/abs/2401.06431v2| 1;2024-01-12;https://www.arxiv.org/abs/2401.06431v1	arXiv:2401.06431			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 15 2024	2024	Receiving timely and personalized feedback is essential for second-language learners, especially when human instructors are unavailable. This study explores the effectiveness of Large Language Models (LLMs), including both proprietary and open-source models, for Automated Essay Scoring (AES). Through extensive experiments with public and private datasets, we find that while LLMs do not surpass conventional state-of-the-art (SOTA) grading models in performance, they exhibit notable consistency, generalizability, and explainability. We propose an open-source LLM-based AES system, inspired by the dual-process theory. Our system offers accurate grading and high-quality feedback, at least comparable to that of fine-tuned proprietary LLMs, in addition to its ability to alleviate misgrading. Furthermore, we conduct human-AI co-grading experiments with both novice and expert graders. We find that our system not only automates the grading process but also enhances the performance and efficiency of human graders, particularly for essays where the model has lower confidence. These results highlight the potential of LLMs to facilitate effective human-AI collaboration in the educational context, potentially transforming learning experiences through AI-generated feedback.																																	2024-07-04	PPRN:87156636		
J	Chen, Jiuhai; Qadri, Rifaa; Wen, Yuxin; Jain, Neel; Kirchenbauer, John; Zhou, Tianyi; Goldstein, Tom				Wen, Yuxin/AAA-4882-2019						GenQA: Generating Millions of Instructions from a Handful of Prompts								Arxiv											1	1;2024-06-14;https://www.arxiv.org/abs/2406.10323v1	arXiv:2406.10323			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Most public instruction finetuning datasets are relatively small compared to the closed source datasets used to train industry models. To study questions about finetuning at scale, such as curricula and learning rate cooldown schedules, there is a need for industrial-scale datasets. However, this scale necessitates a data generation process that is almost entirely automated. In this work, we study methods for generating large instruction datasets from a single prompt. With little human oversight, we get LLMs to write diverse sets of instruction examples ranging from simple completion tasks to complex multi-turn dialogs across a variety of subject areas. When finetuning a Llama-3 8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both knowledge-intensive leaderboard tasks as well as conversational evaluations. We release our dataset, the "generator" prompts that created it, and our finetuned model checkpoints.																																	2024-07-04	PPRN:89346233		
J	Ren, Jiawei; Xie, Kevin; Mirzaei, Ashkan; Liang, Hanxue; Zeng, Xiaohui; Kreis, Karsten; Liu, Ziwei; Torralba, Antonio; Fidler, Sanja; Kim, Seung Wook; Ling, Huan				Kim, Seung Wook/LCO-5791-2024						L4GM: Large 4D Gaussian Reconstruction Model								Arxiv											1	1;2024-06-14;https://www.arxiv.org/abs/2406.10324v1	arXiv:2406.10324			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	We present L4GM, the first 4D Large Reconstruction Model that produces animated objects from a single-view video input -- in a single feed-forward pass that takes only a second. Key to our success is a novel dataset of multiview videos containing curated, rendered animated objects from Objaverse. This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames. We keep our L4GM simple for scalability and build directly on top of LGM, a pretrained 3D Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview image input. L4GM outputs a per-frame 3D Gaussian Splatting representation from video frames sampled at a low fps and then upsamples the representation to a higher fps to achieve temporal smoothness. We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timestep multiview rendering loss to train the model. The representation is upsampled to a higher framerate by training an interpolation model which produces intermediate 3D Gaussian representations. We showcase that L4GM that is only trained on synthetic data generalizes extremely well on in-the-wild videos, producing high quality animated 3D assets.																																	2025-08-07	PPRN:123164588		
J	Du, Mingzhe; Luu, Anh Tuan; Ji, Bin; Liu, Qian; Ng, See-Kiong				Du, Mingzhe/HHM-3609-2022; Luu, Anh Tuan/AAG-3582-2021						Mercury: A Code Efficiency Benchmark for Code Large Language Models								Arxiv											4	4;2024-06-11;https://www.arxiv.org/abs/2402.07844v4| 3;2024-06-06;https://www.arxiv.org/abs/2402.07844v3| 2;2024-05-11;https://www.arxiv.org/abs/2402.07844v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.07844v1	arXiv:2402.07844			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. 1																																	2024-07-04	PPRN:87649505		
J	Ni, Shiyu; Bi, Keping; Guo, Jiafeng; Cheng, Xueqi				Bi, Keping/IST-1078-2023; Ni, Shiyu/KTI-0412-2024						When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation								Arxiv											2	2;2024-06-11;https://www.arxiv.org/abs/2402.11457v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11457v1	arXiv:2402.11457			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 11 2024	2024	Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls.																																	2024-07-04	PPRN:87762849		
J	Yen, Howard; Gao, Tianyu; Chen, Danqi										Long-Context Language Modeling with Parallel Context Encoding								Arxiv											2	2;2024-06-11;https://www.arxiv.org/abs/2402.16617v2| 1;2024-02-26;https://www.arxiv.org/abs/2402.16617v1	arXiv:2402.16617			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 11 2024	2024	Extending large language models (LLMs) to process longer inputs is crucial for a wide range of applications. However, the substantial computational cost of transformers and limited generalization of positional encoding restrict the size of their context window. We introduce C ontext E xpansion with P arallel E ncoding (CEPE ), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE employs a small encoder to process long inputs chunk by chunk, enabling the frozen decoder to utilize additional contexts via cross -attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, it extends the context window of LLAMA-2 to 128K tokens, offering 10× the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CEPE variant that can extend the context window of instruction-tuned models using only unlabeled data, and showcase its effectiveness on LL A MA-2-C HAT , leading to a strong instruction-following model that can leverage very long contexts on downstream tasks.																																	2024-07-04	PPRN:87890807		
J	Upadhyay, Shivani; Pradeep, Ronak; Thakur, Nandan; Craswell, Nick; Lin, Jimmy										UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor								Arxiv											1	1;2024-06-10;https://www.arxiv.org/abs/2406.06519v1	arXiv:2406.06519			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems. Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious. A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts. Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper. Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems. Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies. UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field. 																																	2024-06-22	PPRN:89263796		
J	Ma, Enhui; Zhou, Lijun; Tang, Tao; Zhang, Zhan; Han, Dong; Jiang, Junpeng; Zhan, Kun; Jia, Peng; Lang, Xianpeng; Sun, Haiyang; Lin, Di; Yu, Kaicheng				YANG, tao/KVB-8320-2024; Sun, Licai/JZD-9768-2024						Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation								Arxiv											2	2;2024-06-06;https://www.arxiv.org/abs/2406.01349v3| 1;2024-06-03;https://www.arxiv.org/abs/2406.01349v1	arXiv:2406.01349			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 06 2024	2024	Using generative models to synthesize new data has become a de-facto standard in autonomous driving to address the data scarcity issue. Though existing approaches are able to boost perception models, we discover that these approaches fail to improve the performance of planning of end-to-end autonomous driving models as the generated videos are usually less than 8 frames and the spatial and temporal inconsistencies are not negligible. To this end, we propose Delphi, a novel diffusion-based long video generation method with a shared noise modeling mechanism across the multi-views to increase spatial consistency, and a feature-aligned module to achieves both precise controllability and temporal consistency. Our method can generate up to 40 frames of video without loss of consistency which is about 5 times longer compared with state-of-the-art methods. Instead of randomly generating new data, we further design a sampling policy to let Delphi generate new data that are similar to those failure cases to improve the sample efficiency. This is achieved by building a failure-case driven framework with the help of pre-trained visual language models. Our extensive experiment demonstrates that our Delphi generates a higher quality of long videos surpassing previous state-of-the-art methods. Consequentially, with only generating 4% of the training dataset size, our framework is able to go beyond perception and prediction tasks, for the first time to the best of our knowledge, boost the planning performance of the end-to-end autonomous driving model by a margin of 25%.																																	2024-06-22	PPRN:89169113		
J	Ryan, Michael J.; Held, William; Yang, Diyi										Unintended Impacts of LLM Alignment on Global Representation								Arxiv											2	2;2024-06-06;https://www.arxiv.org/abs/2402.15018v2| 1;2024-02-22;https://www.arxiv.org/abs/2402.15018v1	arXiv:2402.15018			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 06 2024	2024	Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning. We make our code and data publicly available on Github1.																																	2024-11-09	PPRN:87870032		
J	Wang, Xiyuan; Yang, Haotong; Zhang, Muhan										Neural Common Neighbor with Completion for Link Prediction								Arxiv											3	3;2024-06-04;https://www.arxiv.org/abs/2302.00890v4| 2;2024-05-03;https://www.arxiv.org/abs/2302.00890v3| 1;2023-02-02;https://www.arxiv.org/abs/2302.00890v1	arXiv:2302.00890			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 04 2024	2024	In this work, we propose a novel link prediction model and further boost it by studying graph incompleteness. First, we introduce MPNN-then-SF, an innovative architecture leveraging structural feature (SF) to guide MPNN's representation pooling, with its implementation, namely Neural Common Neighbor (NCN). NCN exhibits superior expressiveness and scalability compared with existing models, which can be classified into two categories: SF-then-MPNN, augmenting MPNN's input with SF, and SF-and-MPNN, decoupling SF and MPNN. Second, we investigate the impact of graph incompleteness - the phenomenon that some links are unobserved in the input graph -on SF, like the common neighbor. Through dataset visualization, we observe that incompleteness reduces common neighbors and induces distribution shifts, significantly affecting model performance. To address this issue, we propose to use a link prediction model to complete the common neighbor structure. Combining this method with NCN, we propose Neural Common Neighbor with Completion (NCNC). NCN and NCNC outperform recent strong baselines by large margins, and NCNC further surpasses state-of-the-art models in standard link prediction benchmarks. 																																	2024-11-09	PPRN:36113503		
J	Jin, Ming; Zhang, Yifan; Chen, Wei; Zhang, Kexin; Liang, Yuxuan; Yang, Bin; Wang, Jindong; Pan, Shirui; Wen, Qingsong				Zhang, Yifan/MIQ-8814-2025; Wen, Qingsong/LTF-7625-2024; Liang, Yuxuan/KXR-3882-2024; wang, jindong/ACD-8485-2022; ZHANG, Kexin/OGO-5493-2025						Position: What Can Large Language Models Tell Us about Time Series Analysis								Arxiv											2	2;2024-06-01;https://www.arxiv.org/abs/2402.02713v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.02713v1	arXiv:2402.02713			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 01 2024	2024	Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision -making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.																																	2024-06-22	PPRN:87524092		
J	Bai, Jiamu; Chen, Daoyuan; Qian, Bingchen; Yao, Liuyi; Li, Yaliang										Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2402.11505v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11505v1	arXiv:2402.11505			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 30 2024	2024	Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients. This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "bucket effect'' in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving thousands of clients performing heterogeneous NLP tasks and client resources, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving consistently better improvement over SOTA FL methods in downstream NLP task performance across various heterogeneous distributions. FlexLoRA's practicality is further underscored by our theoretical analysis and its seamless integration with existing LoRA-based FL methods, offering a path toward cross-device, privacy-preserving federated tuning for LLMs.																																	2024-11-09	PPRN:87761729		
J	Lai, Yao; Lee, Sungyoung; Chen, Guojin; Poddar, Souradip; Hu, Mengkang; Pan, David Z.; Luo, Ping				Lai, Yao/HNQ-2386-2023; pluo/GPG-2707-2022						AnalogCoder: Analog Circuit Design via Training-Free Code Generation								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2405.14918v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14918v1	arXiv:2405.14918			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 30 2024	2024	Analog circuit design is a significant task in modern chip technology, focusing on the selection of component types, connectivity, and parameters to ensure proper circuit functionality. Despite advances made by Large Language Models (LLMs) in digital circuit design, the complexity and scarcity of data in analog circuitry pose significant challenges. To mitigate these issues, we introduce AnalogCoder, the first training-free LLM agent for designing analog circuits through Python code generation. Firstly, AnalogCoder incorporates a feedback-enhanced flow with tailored domain-specific prompts, enabling the automated and self-correcting design of analog circuits with a high success rate. Secondly, it proposes a circuit tool library to archive successful designs as reusable modular sub-circuits, simplifying composite circuit creation. Thirdly, extensive experiments on a benchmark designed to cover a wide range of analog circuit tasks show that AnalogCoder outperforms other LLM-based methods. It has successfully designed 20 circuits, 5 more than standard GPT-4o. We believe AnalogCoder can significantly improve the labor-intensive chip design process, enabling non-experts to design analog circuits efficiently. Codes and the benchmark are provided at github.com/laiyao1/AnalogCoder.																																	2024-06-16	PPRN:89018255		
J	Liu, Chunwei; Russo, Matthew; Cafarella, Michael; Cao, Lei; Chen, Peter Baille; Chen, Zui; Franklin, Michael; Kraska, Tim; Madden, Samuel; Vitagliano, Gerardo				LIU, CHUN-WEi/JSL-2104-2023; Vitagliano, Gerardo/LCD-3143-2024						A Declarative System for Optimizing AI Workloads								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2405.14696v2| 1;2024-05-23;https://www.arxiv.org/abs/2405.14696v1	arXiv:2405.14696			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	A long-standing goal of data management systems has been to build systems which can compute quantitative insights over large corpora of unstructured data in a cost-effective manner. Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or metrics from image and video corpora. Today’s models can accomplish these tasks with high accuracy. However, a programmer who wants to answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations. For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the ideal prompt design, and so on. The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape shifts. In this paper we present P ALIMPZEST , a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language. The system uses its cost optimization framework to implement the query plan with the best trade-offs between runtime, financial cost, and output data quality. We describe the workload of AI-powered analytics tasks, the optimization methods that P ALIMPZEST uses, and the prototype system itself. We evaluate P ALIMPZEST on tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching. We show that even our simple prototype offers a range of appealing plans, including one that is 3.3x faster and 2.9x cheaper than the baseline method, while also offering better data quality. With parallelism enabled, P ALIMPZEST can produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline. These require no additional work by the user.																																	2024-08-24	PPRN:88990011		
J	Ghosh, Sreyan; Evuru, Chandra Kiran Reddy; Kumar, Sonal; Aneja, Deepali; Jin, Zeyu; Duraiswami, Ramani; Manocha, Dinesh				Duraiswami, Ramani/J-6070-2012; Jin, Zeyu/JUV-6975-2023; Ghosh, Dr.Shyamasree/ABA-4456-2021						A Closer Look at the Limitations of Instruction Tuning								Arxiv											3	3;2024-05-28;https://www.arxiv.org/abs/2402.05119v4| 2;2024-02-28;https://www.arxiv.org/abs/2402.05119v3| 1;2024-02-03;https://www.arxiv.org/abs/2402.05119v1	arXiv:2402.05119			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed in this paper inspire future work in related directions.																																	2024-06-13	PPRN:87572721		
J	Jiang, Yitong; Zhang, Zhaoyang; Xue, Tianfan; Gu, Jinwei				Zhang, Zhaoyang/HJI-9395-2023						AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion								Arxiv											4	4;2024-05-28;https://www.arxiv.org/abs/2310.10123v5| 3;2023-12-02;https://www.arxiv.org/abs/2310.10123v4| 2;2023-11-19;https://www.arxiv.org/abs/2310.10123v3| 1;2023-10-17;https://www.arxiv.org/abs/2310.10123v2	arXiv:2310.10123			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	We present AutoDIR, an innovative all-in-one image restoration system incorporating latent diffusion. AutoDIR excels in its ability to automatically identify and restore images suffering from a range of unknown degradations. AutoDIR offers intuitive open-vocabulary image editing, empowering users to customize and enhance images according to their preferences. Specifically, AutoDIR consists of two key stages: a Blind Image Quality Assessment (BIQA) stage based on a semantic-agnostic vision-language model which automatically detects unknown image degradations for input images, an All-in-One Image Restoration (AIR) stage utilizes structural-corrected latent diffusion which handles multiple types of image degradations. Extensive experimental evaluation demonstrates that AutoDIR outperforms state-of-the-art approaches for a wider range of image restoration tasks. The design of AutoDIR also enables flexible user control (via text prompt) and generalization to new tasks as a foundation model of image restoration. 																																	2024-06-14	PPRN:85680226		
J	Zhang, Yihua; Li, Pingzhi; Hong, Junyuan; Li, Jiaxiang; Zhang, Yimeng; Zheng, Wenqing; Chen, Pin-Yu; Lee, Jason D.; Yin, Wotao; Hong, Mingyi; Wang, Zhangyang; Liu, Sijia; Chen, Tianlong				Zhang, Yimeng/ACD-2102-2022; Zhang, Yimeng/IUQ-7269-2023; Li, Pingzhi/LXA-9723-2024; Liu, Sijia/HOC-2459-2023; Zheng, Wenqing/ADW-8925-2022; Chen, Pin-Yu/AAA-1059-2020; Yin, Wotao/A-5472-2011; Zhihua, Wang/AFO-5263-2022						Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark								Arxiv											3	3;2024-05-28;https://www.arxiv.org/abs/2402.11592v3| 2;2024-02-26;https://www.arxiv.org/abs/2402.11592v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11592v1	arXiv:2402.11592			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. 																																	2024-06-13	PPRN:87763472		
J	Liu, Aiwei; Pan, Leyi; Hu, Xuming; Li, Shu'ang; Wen, Lijie; King, Irwin; Yu, Philip S.				Hu, Xuming/HTS-1538-2023; King, Irwin/C-9681-2015						An Unforgeable Publicly Verifiable Watermark for Large Language Models								Arxiv											3	3;2024-05-26;https://www.arxiv.org/abs/2307.16230v7| 2;2024-05-19;https://www.arxiv.org/abs/2307.16230v6| 1;2024-02-29;https://www.arxiv.org/abs/2307.16230v5	arXiv:2307.16230			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 26 2024	2024	Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection. To address this limitation, we propose an unforgeable publicly verifiable watermark algorithm named UPV that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently. Experiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code is available at https://github.com/THU-BPM/unforgeable_watermark. Additionally, our algorithm could also be accessed through MarkLLM (Pan et al., 2024) 1 .																																	2024-06-16	PPRN:88755933		
J	Qian, Rui; Dong, Xiaoyi; Zhang, Pan; Zang, Yuhang; Ding, Shuangrui; Lin, Dahua; Wang, Jiaqi				Zang, Yuhang/AES-3018-2022; WANG, JIAQI/KBB-8837-2024; Lin, Dahua/W-6576-2019; Dong, Xiaoyi/AAC-8666-2019						Streaming Long Video Understanding with Large Language Models								Arxiv											1	1;2024-05-25;https://www.arxiv.org/abs/2405.16009v1	arXiv:2405.16009			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 25 2024	2024	This paper presents VideoStreaming, , an advanced vision -language large model (VLLM) for video understanding, that capably understands arbitrary -length video with a constant number of video tokens streamingly encoded and adaptively selected. The challenge of video understanding in the vision language area mainly lies in the significant computational burden caused by the great number of tokens extracted from long videos. Previous works rely on sparse sampling or frame compression to reduce tokens. However, such approaches either disregard temporal information in a long time span or sacrifice spatial details, resulting in flawed compression. To address these limitations, our VideoStreaming has two core designs: Memory -Propagated Streaming Encoding and Adaptive Memory Selection. The Memory -Propagated Streaming Encoding architecture segments long videos into short clips and sequentially encodes each clip with a propagated memory. In each iteration, we utilize the encoded results of the preceding clip as historical memory, which is integrated with the current clip to distill a condensed representation that encapsulates the video content up to the current timestamp. This method not only incorporates long-term temporal dynamics into the streaming encoding process but also yields a fixed -length memory as a global representation for arbitrarily long videos. After the encoding process, the Adaptive Memory Selection strategy selects a constant number of question -related memories from all the historical memories, and feeds them into the LLM to generate informative responses. The question -related selection reduces redundancy within the memories, enabling efficient and precise video understanding. Meanwhile, the disentangled video extraction and reasoning design allows the LLM to answer different questions about a video by directly selecting corresponding memories, without the need to encode the whole video for each question. Through extensive experiments, our model achieves superior performance and higher efficiency on long video benchmarks, showcasing precise temporal comprehension for detailed question answering.																																	2024-06-11	PPRN:89072673		
J	Braun, Dan; Taylor, Jordan; Goldowsky-Dill, Nicholas; Sharkey, Lee										Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning								Arxiv											1	1;2024-05-24;https://www.arxiv.org/abs/2405.12241v2	arXiv:2405.12241			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network’s internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e_sae .																																	2024-06-08	PPRN:89011920		
J	Craig, Nathaniel; Green, Daniel; Meyers, Joel; Rajendran, Surjeet				Meyers, Joel/OCL-3371-2025						No ν s is Good News								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2405.00836v2| 1;2024-05-01;https://www.arxiv.org/abs/2405.00836v1	arXiv:2405.00836			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	The baryon acoustic oscillation (BAO) analysis from the first year of data from the Dark Energy Spectroscopic Instrument (DESI), when combined with data from the cosmic microwave background (CMB), has placed an upper-limit on the sum of neutrino masses, Emv < 70 meV (95%). In addition to excluding the minimum sum associated with the inverted hierarchy, the posterior is peaked at Emv = 0 and is close to excluding even the minumum sum, 58 meV at 2σ. In this paper, we explore the implications of this data for cosmology and particle physics. The sum of neutrino mass is determined in cosmology from the suppression of clustering in the late universe. Allowing the clustering to be enhanced, we extended the DESI analysis to Emv < 0 and find Emv= −160 ± 90 meV (68%), and that the suppression of power from the minimum sum of neutrino masses is excluded at 99% confidence. We show this preference for negative masses makes it challenging to explain the result by a shift of cosmic parameters, such as the optical depth or matter density. We then show how a result of Emv = 0 could arise from new physics in the neutrino sector, including decay, cooling, and/or time-dependent masses. These models are consistent with current observations but imply new physics that is accessible in a wide range of experiments. In addition, we discuss how an apparent signal with Emv < 0 can arise from new long range forces in the dark sector or from a primordial trispectrum that resembles the signal of CMB lensing.																																	2024-06-08	PPRN:88728606		
J	Zhong, Yifan; Ma, Chengdong; Zhang, Xiaoyuan; Yang, Ziran; Chen, Haojun; Zhang, Qingfu; Qi, Siyuan; Yang, Yaodong				Zhong, Yi/AAR-2522-2020; Zhang, Xiaoyuan/AGL-8800-2022; Qi, Siyuan/MFI-2587-2025; Zhang, Qingfu/K-4320-2015						Panacea: Pareto Alignment via Preference Adaptation for LLMs								Arxiv											2	2;2024-05-23;https://www.arxiv.org/abs/2402.02030v2| 1;2024-02-03;https://www.arxiv.org/abs/2402.02030v1	arXiv:2402.02030			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions. Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent an exponentially vast spectrum of human preferences through various optimization methods. Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.																																	2024-06-06	PPRN:87523918		
J	Caverly, Ryan James; Forbes, James Richard				Caverly, Ryan/H-9174-2019						LMI Properties and Applications in Systems, Stability, and Control Theory								Arxiv											3	3;2024-05-22;https://www.arxiv.org/abs/1903.08599v4| 2;2021-04-01;https://www.arxiv.org/abs/1903.08599v3| 1;2019-03-20;https://www.arxiv.org/abs/1903.08599v3	arXiv:1903.08599			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 22 2024	2024	Linear matrix inequalities (LMIs) commonly appear in systems, stability, and control applications. Many analysis and synthesis problems in these areas can be solved as feasibility or optimization problems subject to LMI constraints. Although most well-known LMI properties and manipulation tricks, such as the Schur complement and the congruence transformation, can be found in standard references, many useful LMI properties are scattered throughout the literature. The purpose of this document is to collect and organize properties, tricks, and applications related to LMIs from a number of references together in a single document. In this sense, the document can be thought of as an "LMI encyclopedia" or "LMI cookbook." Proofs of the properties presented in this document are not included when they can be found in the cited references in the interest of brevity. Illustrative examples are included whenever necessary to fully explain a certain property. Multiple equivalent forms of LMIs are often presented to give the reader a choice of which form may be best suited for a particular problem at hand. The equivalency of some of the LMIs in this document may be straightforward to more experienced readers, but the authors believe that some readers may benefit from the presentation of multiple equivalent LMIs.																																	2024-06-04	PPRN:13217078		
J	Hu, Hengyuan; Mirchandani, Suvir; Sadigh, Dorsa										Imitation Bootstrapped Reinforcement Learning								Arxiv											6	6;2024-05-20;https://www.arxiv.org/abs/2311.02198v6| 5;2024-05-04;https://www.arxiv.org/abs/2311.02198v5| 4;2024-02-29;https://www.arxiv.org/abs/2311.02198v4| 3;2023-11-24;https://www.arxiv.org/abs/2311.02198v3| 2;2023-11-20;https://www.arxiv.org/abs/2311.02198v2| 1;2023-11-03;https://www.arxiv.org/abs/2311.02198v1	arXiv:2311.02198			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 20 2024	2024	Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL policies since the beginning of training, which greatly accelerates exploration and training efficiency. We evaluate IBRL on 6 simulation and 3 real-world tasks spanning various difficulty levels. IBRL significantly outperforms prior methods and the improvement is particularly more prominent in harder tasks.																																	2024-06-15	PPRN:86054267		
J	Seiberg, Nathan; Seifnashri, Sahand; Shao, Shu-Heng										Non-invertible symmetries and LSM-type constraints on a tensor product Hilbert space								Arxiv											2	2;2024-05-17;https://www.arxiv.org/abs/2401.12281v2| 1;2024-01-22;https://www.arxiv.org/abs/2401.12281v1	arXiv:2401.12281			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 17 2024	2024	We discuss the exact non-invertible Kramers-Wannier symmetry of 1+1d lattice models on a tensor product Hilbert space of qubits. This symmetry is associated with a topological defect and a conserved operator, and the latter can be presented as a matrix product operator. Importantly, unlike its continuum counterpart, the symmetry algebra involves lattice translations. Consequently, it is not described by a fusion category. In the presence of this defect, the symmetry algebra involving parity/time-reversal is realized projectively, which is reminiscent of an anomaly. Different Hamiltonians with the same lattice non-invertible symmetry can flow in their continuum limits to infinitely many different fusion categories (with different Frobenius-Schur indicators), including, as a special case, the Ising CFT. The non-invertible symmetry leads to a constraint similar to that of Lieb-Schultz-Mattis, implying that the system cannot have a unique gapped ground state. It is either in a gapless phase or in a gapped phase with three (or a multiple of three) ground states, associated with the spontaneous breaking of the lattice non-invertible symmetry.																																	2024-06-14	PPRN:87291944		
J	Sadat, Seyedmorteza; Buhmann, Jakob; Bradley, Derek; Hilliges, Otmar; Weber, Romann M.				Hilliges, Otmar/Q-5899-2019						CADS: UNLEASHING THE DIVERSITY OF DIFFUSION MODELS THROUGH CONDITION-ANNEALED SAMPLING								Arxiv											3	3;2024-05-13;https://www.arxiv.org/abs/2310.17347v4| 2;2024-04-15;https://www.arxiv.org/abs/2310.17347v3| 1;2023-10-26;https://www.arxiv.org/abs/2310.17347v1	arXiv:2310.17347			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 13 2024	2024	While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. Our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing Gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. Our ConditionAnnealed Diffusion Sampler (CADS) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256 ×256 and 512 ×512 respectively.																																	2024-06-08	PPRN:85823105		
J	Maziarz, Krzysztof; Jackson-Flux, Henry; Cameron, Pashmina; Sirockin, Finton; Schneider, Nadine; Stiefl, Nikolaus; Segler, Marwin; Brockschmidt, Marc										Learning to Extend Molecular Scaffolds with Structural Motifs								Arxiv											1	1;2024-05-12;https://www.arxiv.org/abs/2103.03864v5	arXiv:2103.03864			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 12 2024	2024	Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom -by-atom and bond-by-bond or fragment by -fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the influence of a number of seemingly minor design choices on the overall performance.																																	2024-05-29	PPRN:88868739		
J	Lin, Sheng-Chieh; Gao, Luyu; Oguz, Barlas; Xiong, Wenhan; Lin, Jimmy; Yih, Wen-tau; Chen, Xilun										FLAME: Factuality-Aware Alignment for Large Language Models								Arxiv											1	1;2024-05-02;https://www.arxiv.org/abs/2405.01525v1	arXiv:2405.01525			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 02 2024	2024	Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination ). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware align ment (FLAME ￼ ), comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.																																	2024-07-04	PPRN:88723701		
J	Babel, Kushal; Chursin, Andrey; Danezis, George; Kichidis, Anastasios; Kokoris-Kogias, Lefteris; Koshy, Arun; Sonnino, Alberto; Tian, Mingwei										Mysticeti: Reaching the Limits of Latency with Uncertified DAGs								Arxiv											3	3;2024-04-30;https://www.arxiv.org/abs/2310.14821v3| 2;2024-02-13;https://www.arxiv.org/abs/2310.14821v2| 1;2023-10-23;https://www.arxiv.org/abs/2310.14821v1	arXiv:2310.14821			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	We introduce MYSTICETI-C, , the first DAG-based Byzantine consensus protocol to achieve the lower bounds of latency of 3 message rounds. Since MYSTICETI-C is built over DAGs it also achieves high resource efficiency and censorship resistance. MYSTICETI-C achieves this latency improvement by avoiding explicit certification of the DAG blocks and by proposing a novel commit rule such that every block can be committed without delays, resulting in optimal latency in the steady state and under crash failures. We further extend MYSTICETI-C to MYSTICETI-FPC, , which incorporates a fast commit path that achieves even lower latency for transferring assets. Unlike prior fast commit path protocols, MYSTICETI-FPC minimizes the number of signatures and messages by weaving the fast path transactions into the DAG. This frees up resources, which subsequently result in better performance. We prove the safety and liveness of the protocols in a Byzantine context. We evaluate MYSTICETI and compare it with state-of-the-art consensus and fast path protocols to demonstrate its low latency and resource efficiency, as well as its more graceful degradation under crash failures. MYSTICETI is the first Byzantine consensus protocol to achieve WAN latency of 0.5s for consensus commit while simultaneously maintaining state-of-the-art throughput of over 100k TPS. Finally, we report on integrating MYSTICETI-C as the consensus protocol into a major blockchain, resulting in 4x latency reduction.																																	2024-05-17	PPRN:85758995		
J	Huang, Jiehui; Dong, Xiao; Song, Wenhui; Li, Hanhui; Zhou, Jun; Cheng, Yuhao; Liao, Shutao; Chen, Long; Yan, Yiqiang; Liao, Shengcai; Liang, Xiaodan				song, wenhui/AAK-1869-2021; Li, Hanhui/AAJ-9546-2021; He, Zhongyuan/GRN-9274-2022; Li, Xiaofeng/LIC-9574-2024						ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving								Arxiv											1	1;2024-04-25;https://www.arxiv.org/abs/2404.16771v1	arXiv:2404.16771			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 25 2024	2024	Diffusion-based technologies have made significant strides, particularly in personalized and customized facialgeneration. However, existing methods face challenges in achieving high-fidelity and detailed identity (ID)consistency, primarily due to insufficient fine-grained control over facial areas and the lack of a comprehensive strategy for ID preservation by fully considering intricate facial details and the overall face. To address these limitations, we introduce ConsistentID, an innovative method crafted for diverseidentity-preserving portrait generation under fine-grained multimodal facial prompts, utilizing only a single reference image. ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions. Together, these components significantly enhance the accuracy of ID preservation by introducing fine-grained multimodal ID information from facial regions. To facilitate training of ConsistentID, we present a fine-grained portrait dataset, FGID, with over 500,000 facial images, offering greater diversity and comprehensiveness than existing public facial datasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental results substantiate that our ConsistentID achieves exceptional precision and diversity in personalized facial generation, surpassing existing methods in the MyStyle dataset. Furthermore, while ConsistentID introduces more multimodal ID information, it maintains a fast inference speed during generation.																																	2024-05-04	PPRN:88650609		
J	Ma, Chuofan; Jiang, Yi; Wu, Jiannan; Yuan, Zehuan; Qi, Xiaojuan				Ma, Chuofan/LNP-4715-2024; Qi, Xiaojuan/MVV-7776-2025						Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models								Arxiv											1	1;2024-04-19;https://www.arxiv.org/abs/2404.13013v1	arXiv:2404.13013			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Apr 19 2024	2024	We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images. Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization.   																																	2024-04-30	PPRN:88590330		
J	Zheng, Bowen; Hou, Yupeng; Lu, Hongyu; Chen, Yu; Zhao, Wayne Xin; Chen, Ming; Wen, Ji-Rong				Xia, Lianghao/IWV-0954-2023; Zheng, Bowen/AAJ-2019-2020; Hou, Yupeng/GNP-3072-2022						Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation								Arxiv											4	4;2024-04-19;https://www.arxiv.org/abs/2311.09049v4| 3;2023-12-22;https://www.arxiv.org/abs/2311.09049v3| 2;2023-11-28;https://www.arxiv.org/abs/2311.09049v2| 1;2023-11-15;https://www.arxiv.org/abs/2311.09049v1	arXiv:2311.09049			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 19 2024	2024	Recently, large language models (LLMs) have shown great potential in recommender systems, either improving existing recommendation models or serving as the backbone. However, there exists a large semantic gap between LLMs and recommender systems, since items to be recommended are often indexed by discrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs capture language semantics while recommender systems imply collaborative semantics, making it difficult to sufficiently leverage the model capacity of LLMs for recommendation. To address this challenge, in this paper, we propose a new LLM-based recommendation model called LC-Rec, which can better integrate language and collaborative semantics for recommender systems. Our approach can directly generate items from the entire item set for recommendation, without relying on candidate items. Specifically, we make two major contributions in our approach. For item indexing, we design a learning-based vector quantization method with uniform semantic mapping, which can assign meaningful and non-conflicting IDs (called item indices) for items. For alignment tuning, we propose a series of specially designed tuning tasks to enhance the integration of collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to deeply integrate language and collaborative semantics (characterized by the learned item indices), so as to achieve an effective adaptation to recommender systems. Extensive experiments demonstrate the effectiveness of our method, showing that our approach can outperform a number of competitive baselines including traditional recommenders and existing LLM-based recommenders. Our code is available at https://github.com/RUCAIBox/LC-Rec/.																																	2024-04-30	PPRN:86159950		
J	Gupta, Neha; Narasimhan, Harikrishna; Jitkrittum, Wittawat; Rawat, Ankit Singh; Menon, Aditya Krishna; Kumar, Sanjiv				Rawat, Ankit/V-3483-2019						Language Model Cascades: Token-level uncertainty and beyond								Arxiv											1	1;2024-04-15;https://www.arxiv.org/abs/2404.10136v1	arXiv:2404.10136			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. Cascading offers a simple strategy to achieve more favorable cost-quality tradeoffs: here, a small model is invoked for most "easy" instances, while a few "hard" instances are deferred to the large model. While the principles underpinning cascading are well-studied for classification tasks - with deferral based on predicted class uncertainty favored theoretically and practically - a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across examples. To mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.																																	2024-04-26	PPRN:88538536		
J	Chou, Yi-Hui; Chen, I-Chun; Chang, Chin-Jui; Ching, Joann; Yang, Yi-Hsuan				Lin, Chien-Chung/T-1496-2019						BERT-like Pre-training for Symbolic Piano Music Classification Tasks								Arxiv											2	2;2024-04-14;https://www.arxiv.org/abs/2107.05223v2| 1;2021-07-12;https://www.arxiv.org/abs/2107.05223v1	arXiv:2107.05223			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 14 2024	2024	This article presents a benchmark study of symbolic piano music classification using the masked language modelling approach of the Bidirectional Encoder Representations from Transformers (BERT). Specifically, we consider two types of MIDI data: MIDI scores, which are musical scores rendered directly into MIDI with no dynamics and precisely aligned with the metrical grid notated by its composer and MIDI performances, which are MIDI encodings of human performances of musical scoresheets. With five public-domain datasets of single-track piano MIDI files, we pre-train two 12-layer Transformer models using the BERT approach, one for MIDI scores and the other for MIDI performances, and fine-tune them for four downstream classification tasks. These include two note-level classification tasks (melody extraction and velocity prediction) and two sequence-level classification tasks (style classification and emotion classification). Our evaluation shows that the BERT approach leads to higher classification accuracy than recurrent neural network (RNN)-based baselines.																																	2024-04-25	PPRN:11862183		
J	Dong, Wenhao; Zhu, Haodong; Lin, Shaohui; Luo, Xiaoyan; Shen, Yunhang; Liu, Xuhui; Zhang, Juan; Guo, Guodong; Zhang, Baochang				Luo, Xiaoyan/KSM-6392-2024; Liu, shaohui/HKE-1383-2023; liu, xuhui/OCL-1630-2025						Fusion-Mamba for Cross-modality Object Detection								Arxiv											1	1;2024-04-14;https://www.arxiv.org/abs/2404.09146v1	arXiv:2404.09146			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 14 2024	2024	Cross -modality fusing complementary information from different modalities effectively improves object detection performance, making it more useful and robust for a wider range of applications. Existing fusion strategies combine different types of images or merge different backbone features through elaborated neural network modules. However, these methods neglect that modality disparities affect cross -modality fusion performance, as different modalities with different camera focal lengths, placements, and angles are hardly fused. In this paper, we investigate cross -modality fusion by associating cross -modal features in a hidden state space based on an improved Mamba with a gating mechanism. We design a Fusion -Mamba block (FMB) to map cross -modal features into a hidden state space for interaction, thereby reducing disparities between cross -modal features and enhancing the representation consistency of fused features. FMB contains two modules: the State Space Channel Swapping (SSCS) module facilitates shallow feature fusion, and the Dual State Space Fusion (DSSF) enables deep fusion in a hidden state space. Through extensive experiments on public datasets, our proposed approach outperforms the state-of-the-art methods on mAP with 5.9% on M3FD and 4.9% on FLIR-Aligned datasets, demonstrating superior object detection performance. To the best of our knowledge, this is the first work to explore the potential of Mamba for cross -modal fusion and establish a new baseline for cross -modality object detection.																																	2024-04-25	PPRN:88531318		
J	Panwar, Madhur; Ahuja, Kabir; Goyal, Navin										In-Context Learning through the Bayesian Prism								Arxiv											2	2;2024-04-14;https://www.arxiv.org/abs/2306.04891v2| 1;2023-06-08;https://www.arxiv.org/abs/2306.04891v1	arXiv:2306.04891			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 14 2024	2024	In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning like ICL setups have been devised that train transformers on sequences of input output pairs (x, f (x)). The function f comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses1.																																	2024-04-25	PPRN:73232836		
J	Rouze, Cambyse; Franca, Daniel Stilck										Learning quantum many-body systems from a few copies								Arxiv											2	2;2024-04-13;https://www.arxiv.org/abs/2107.03333v4| 1;2023-04-06;https://www.arxiv.org/abs/2107.03333v3	arXiv:2107.03333			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 13 2024	2024	Estimating physical properties of quantum states from measurements is one of the most fundamental tasks in quantum science. In this work, we identify conditions on states under which it is possible to infer the expectation values of all quasi-local observables of a state from a number of copies that scales polylogarithmically with the system's size and polynomially on the locality of the target observables. We show that this constitutes a provable exponential improvement in the number of copies over state-of-the-art tomography protocols. We achieve our results by combining the maximum entropy method with tools from the emerging fields of classical shadows and quantum optimal transport. The latter allows us to fine-tune the error made in estimating the expectation value of an observable in terms of how local it is and how well we approximate the expectation value of a fixed set of few-body observables. We conjecture that our condition holds for all states exhibiting some form of decay of correlations and establish it for several subsets thereof. These include widely studied classes of states such as one-dimensional thermal and high-temperature Gibbs states of local commuting Hamiltonians on arbitrary hypergraphs or outputs of shallow circuits. Moreover, we show improvements of the maximum entropy method beyond the sample complexity that are of independent interest. These include identifying regimes in which it is possible to perform the postprocessing efficiently as well as novel bounds on the condition number of covariance matrices of many-body states.																																	2024-04-25	PPRN:55182361		
J	Dohare, Shibhansh; Hernandez-Garcia, J. Fernando; Rahman, Parash; Mahmood, A. Rupam; Sutton, Richard S.										Maintaining Plasticity in Deep Continual Learning								Arxiv											2	2;2024-04-09;https://www.arxiv.org/abs/2306.13812v3| 1;2023-06-23;https://www.arxiv.org/abs/2306.13812v1	arXiv:2306.13812			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 09 2024	2024	Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to learn on new examples, a phenomenon called loss of plasticity. We provide direct demonstrations of loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89% accuracy on an early task down to 77%, about the level of a linear network, on the 2000th task. Loss of plasticity occurred with a wide range of deep network architectures, optimizers, activation functions, batch normalization, dropout, but was substantially eased by L2-regularization, particularly when combined with weight perturbation. Further, we introduce a new algorithm - continual backpropagation - which slightly modifies conventional backpropagation to reinitialize a small fraction of less-used units after each example and appears to maintain plasticity indefinitely.																																	2024-04-24	PPRN:73540942		
J	Bai, Jianhong; He, Tianyu; Wang, Yuchi; Guo, Junliang; Hu, Haoji; Liu, Zuozhu; Bian, Jiang				Bai, Jianhong/JFA-3919-2023; Liu, Zuozhu/IUN-8338-2023; Hu, Haoji/NPI-9351-2025; Wang, Yuchi/MYQ-5661-2025; HE, TIANYU/JXX-7116-2024						UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing								Arxiv											4	4;2024-04-07;https://www.arxiv.org/abs/2402.13185v4| 3;2024-02-24;https://www.arxiv.org/abs/2402.13185v3| 2;2024-02-22;https://www.arxiv.org/abs/2402.13185v2| 1;2024-02-20;https://www.arxiv.org/abs/2402.13185v1	arXiv:2402.13185			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 07 2024	2024	Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.																																	2024-04-21	PPRN:87779967		
J	Xu, Yifan; Liu, Xiao; Liu, Xinghan; Hou, Zhenyu; Li, Yueyan; Zhang, Xiaohan; Wang, Zihan; Zeng, Aohan; Du, Zhengxiao; Zhao, Wenyi; Tang, Jie; Dong, Yuxiao				Zhao, Wenyi/GQQ-5868-2022; Xu, Yifan/I-9273-2014; 航航, 张/KBC-0720-2024						ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline								Arxiv											1	1;2024-04-03;https://www.arxiv.org/abs/2404.02893v1	arXiv:2404.02893			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 03 2024	2024	Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems. In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger. Related techniques have been deployed to ChatGLMfootnote{url{https://chatglm.cn}}, an online serving LLM. 																																	2024-04-18	PPRN:88390269		
J	Barral, David; Cardama, F. Javier; Diaz, Guillermo; Failde, Daniel; Llovo, Iago F.; Juane, Mariamo Mussa; Vazquez-Perez, Jorge; Villasuso, Juan; Pineiro, Cesar; Costas, Natalia; Pichel, Juan C.; Pena, Tomas F.; Gomez, Andres				F Llovo, Iago/GRY-0101-2022; Pomar, César/ABC-9735-2021; Pichel, Juan C./AGY-6431-2022; Pena, Tomás/D-1677-2013						Review of Distributed Quantum Computing. From single QPU to High Performance Quantum Computing								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2404.01265v1	arXiv:2404.01265			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	The emerging field of quantum computing has shown it might change how we process information by using the unique principles of quantum mechanics. As researchers continue to push the boundaries of quantum technologies to unprecedented levels, distributed quantum computing raises as an obvious path to explore with the aim of boosting the computational power of current quantum systems. This paper presents a comprehensive survey of the current state of the art in the distributed quantum computing field, exploring its foundational principles, landscape of achievements, challenges, and promising directions for further research. From quantum communication protocols to entanglement-based distributed algorithms, each aspect contributes to the mosaic of distributed quantum computing, making it an attractive approach to address the limitations of classical computing. Our objective is to provide an exhaustive overview for experienced researchers and field newcomers.																																	2025-08-07	PPRN:122688639		
J	Ren, Jie; Xu, Han; Liu, Yiding; Cui, Yingqian; Wang, Shuaiqiang; Yin, Dawei; Tang, Jiliang				Cui, Yingqian/KBA-8229-2024; Yin, Dawei/JOR-9201-2023						A Robust Semantics-based Watermark for Large Language Model against Paraphrasing								Arxiv											2	2;2023-11-15;https://www.arxiv.org/abs/2311.08721v1| 1;2024-04-01;	arXiv:2311.08721			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	Large language models (LLMs) have show great ability in various natural language tasks. However, there are concerns that LLMs are possible to be used improperly or even illegally. To prevent the malicious usage of LLMs, detecting LLM-generated text becomes crucial in the deployment of LLM applications. Watermarking is an effective strategy to detect the LLM-generated content by encoding a pre-defined secret watermark to facilitate the detection process. However, the majority of existing watermark methods leverage the simple hashes of precedent tokens to partition vocabulary. Such watermark can be easily eliminated by paraphrase and correspondingly the detection effectiveness will be greatly compromised. Thus, to enhance the robustness against paraphrase, we propose a semantics-based watermark framework SemaMark. It leverages the semantics as an alternative to simple hashes of tokens since the paraphrase will likely preserve the semantic meaning of the sentences. Comprehensive experiments are conducted to demonstrate the effectiveness and robustness of SemaMark under different paraphrases.																																	2024-11-16	PPRN:86159388		
J	Zhao, Qi; Wang, Shijie; Zhang, Ce; Fu, Changcheng; Do, Minh Quan; Agarwal, Nakul; Lee, Kwonjoon; Sun, Chen										AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?								Arxiv											3	3;2024-04-01;https://www.arxiv.org/abs/2307.16368v3| 2;2023-10-09;https://www.arxiv.org/abs/2307.16368v2| 1;2023-07-31;https://www.arxiv.org/abs/2307.16368v1	arXiv:2307.16368			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 01 2024	2024	Can we better anticipate an actor’s future actions (e.g. mix eggs) by knowing what commonly happens after the current action (e.g. crack eggs)? What if the actor also shares the goal (e.g. make fried rice) with us? The long-term action anticipation (LTA) task aims to predict an actor’s future behavior from video observations in the form of verb and noun sequences, and it is crucial for human -machine interaction. We propose to formulate the LTA task from two perspectives: a bottom -up approach that predicts the next actions autoregressively by modeling temporal dynamics; and a top -down approach that infers the goal of the actor and plans the needed procedure to accomplish the goal. We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos), have the potential to help LTA from both perspectives. It can help provide the prior knowledge on the possible next actions, and infer the goal given the observed part of a procedure, respectively. We propose AntGPT, which represents video observations as sequences of human actions, and uses the action representation for an LLM to infer the goals and model temporal dynamics. AntGPT achieves state of-the-art performance on Ego4D LTA v1 and v2, EPIC -Kitchens -55, as well as EGTEA GAZE+, thanks to LLMs’ goal inference and temporal dynamics modeling capabilities. We further demonstrate that these capabilities can be effectively distilled into a compact neural network 1.3% of the original LLM model size. 																																	2024-04-18	PPRN:74185027		
J	Faysse, Manuel; Fernandes, Patrick; Guerreiro, Nuno M.; Loison, Antonio; Alves, Duarte M.; Corro, Caio; Boizard, Nicolas; Alves, Joao; Rei, Ricardo; Martins, Pedro H.; Casademunt, Antoni Bigata; Yvon, Francois; Martins, Andre F.T.; Viaud, Gautier; Hudelot, Celine; Colombo, Pierre				HUDELOT, CELINE/IRZ-2920-2023; Corro, Caio/GSO-3608-2022; Alves, João/AAH-7435-2019; Torres Martins, Andre Filipe/JXL-9782-2024						CroissantLLM: A Truly Bilingual French-English Language Model								Arxiv											4	4;2024-03-29;https://www.arxiv.org/abs/2402.00786v4| 3;2024-02-13;https://www.arxiv.org/abs/2402.00786v3| 2;2024-02-02;https://www.arxiv.org/abs/2402.00786v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.00786v1	arXiv:2402.00786			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 29 2024	2024	We introduce CroissantLLM, a 1.3B language model pre-trained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-toFrench pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework (Bommasani et al., 2023) and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work to strengthen our understanding of multilingualism in language models.																																	2024-04-17	PPRN:87456788		
J	Zhang, Yuxin; Chen, Haoyu; Lin, Zheng; Chen, Zhe; Zhao, Jin				Chen, Haoyu/JOK-4487-2023						FedAC: An Adaptive Clustered Federated Learning Framework for Heterogeneous Data								Arxiv											2	2;2024-03-29;https://www.arxiv.org/abs/2403.16460v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16460v1	arXiv:2403.16460			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 29 2024	2024	Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments. Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different nonIID settings compared to SOTA methods.																																	2024-04-17	PPRN:88279682		
J	Kim, Dahyun; Kim, Yungi; Song, Wonho; Kim, Hyeonwoo; Kim, Yunsu; Kim, Sanghoon; Park, Chanjun				Park, Chanjun/KLZ-1990-2024						sDPO: Don't Use Your Data All at Once								Arxiv											1	1;2024-03-28;https://www.arxiv.org/abs/2403.19270v1	arXiv:2403.19270			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 28 2024	2024	As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.																																	2024-04-15	PPRN:88335948		
J	Ghanem, Dalia; Sant'Anna, Pedro H.C.; Wuthrich, Kaspar				Ghanem, Dalia/JNT-2719-2023						Selection and parallel trends								Arxiv											5	5;2023-12-30;https://www.arxiv.org/abs/2203.09001v9| 4;2023-10-16;https://www.arxiv.org/abs/2203.09001v8| 3;2022-03-17;https://www.arxiv.org/abs/2203.09001v4| 2;2024-03-27;https://www.arxiv.org/abs/2203.09001v11| 1;2024-03-08;https://www.arxiv.org/abs/2203.09001v10	arXiv:2203.09001			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 27 2024	2024	We study the role of selection into treatment in difference-in-differences (DiD) designs. We derive necessary and sufficient conditions for parallel trends assumptions under general classes of selection mechanisms. These conditions characterize the empirical content of parallel trends. For settings where the necessary conditions are questionable, we propose tools for selection-based sensitivity analysis. We also provide templates for justifying DiD in applications with and without covariates. A reanalysis of the causal effect of NSW training programs demonstrates the usefulness of our selection-based approach to sensitivity analysis.																																	2024-04-14	PPRN:11964328		
J	Shi, Jiawen; Yuan, Zenghui; Liu, Yinuo; Huang, Yue; Zhou, Pan; Sun, Lichao; Gong, Neil Zhenqiang				Yuan, Zenghui/ITV-5827-2023						Optimization-based Prompt Injection Attack to LLM-as-a-Judge								Arxiv											4	4;2025-03-03;https://www.arxiv.org/abs/2403.17710v4| 3;2024-11-15;https://www.arxiv.org/abs/2403.17710v3| 2;2024-08-24;https://www.arxiv.org/abs/2403.17710v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17710v1	arXiv:2403.17710			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 26 2024	2024	LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.																																	2025-08-07	PPRN:88295873		
J	Xu, Cheng; Mao, Ning; Zeng, Tiansheng; Zhang, Yang				mao, ning/HKW-6510-2023; null, Cheng/NWG-8560-2025; Zhang, Yang/AAH-8138-2021						Multiple Chern bands in twisted MoTe$_2$ and possible non-Abelian states								Arxiv											2	2;2024-10-23;https://www.arxiv.org/abs/2403.17003v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.17003v1	arXiv:2403.17003			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	We investigate the moiré band structures and possible even denominator fractional quantum Hall state in small angle twisted bilayer MoTe$_2$, using combined large-scale local basis density functional theory calculation and continuum model exact diagonalization. Via large-scale first principles calculations at $theta=1.89^{circ}$, we find a sequence of $C=1$ moiré Chern bands, in analogy to Landau levels. Constructing the continuum model with multiple Chern bands and uniform Berry curvature in the second moiré band, we undertake band-projected exact diagonalization using unscreened Coulomb repulsion to pinpoint possible $nu=-3/2$ non-Abelian states across a wide range of twist angles below $theta=2.5^{circ}$.																																	2025-08-07	PPRN:88281710		
J	Bastek, Jan-Hendrik; Sun, WaiChing; Kochmann, Dennis M.				Sun, WaiChing/U-1654-2019; Kochmann, Dennis/A-6827-2012; Bastek, Jan-Hendrik/JJF-7710-2023						Physics-Informed Diffusion Models								Arxiv											4	4;2025-03-13;https://www.arxiv.org/abs/2403.14404v4| 3;2025-02-16;https://www.arxiv.org/abs/2403.14404v3| 2;2024-05-23;https://www.arxiv.org/abs/2403.14404v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14404v1	arXiv:2403.14404			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.																																	2025-08-07	PPRN:88261116		
J	Li, Yanzhou; Li, Tianlin; Chen, Kangjie; Zhang, Jian; Liu, Shangqing; Wang, Wenhan; Zhang, Tianwei; Liu, Yang				Liu, Shangqing/LCD-8169-2024; w, wh/IAP-2639-2023; LI, TIANLIN/NWH-2947-2025; Zhang, Tianwei/AAV-8818-2020; Liu, Yang/D-2306-2013						BadEdit: Backdooring large language models by model editing								Arxiv											1	1;2024-03-20;https://www.arxiv.org/abs/2403.13355v1	arXiv:2403.13355			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 20 2024	2024	Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique. It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. (3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. (4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning. Experimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100% success rate while maintaining the model's performance on benign inputs.																																	2024-04-12	PPRN:88244908		
J	Sun, Yuan; Wang, Xuan; Zhang, Yunfan; Zhang, Jie; Jiang, Caigui; Guo, Yu; Wang, Fei				Jiang, Caigui/B-8381-2016						iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching								Arxiv											2	2;2024-03-20;https://www.arxiv.org/abs/2312.09031v2| 1;2023-12-14;https://www.arxiv.org/abs/2312.09031v1	arXiv:2312.09031			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 20 2024	2024	We present a method named iComMa to address the 6D camera pose estimation problem in computer vision. Conventional pose estimation methods typically rely on the target's CAD model or necessitate specific network training tailored to particular object classes. Some existing methods have achieved promising results in mesh-free object and scene pose estimation by inverting the Neural Radiance Fields (NeRF). However, they still struggle with adverse initializations such as large rotations and translations. To address this issue, we propose an efficient method for accurate camera pose estimation by inverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based differentiable framework optimizes camera pose by minimizing the residual between the query image and the rendered image, requiring no training. An end-to-end matching module is designed to enhance the model's robustness against adverse initializations, while minimizing pixel-level comparing loss aids in precise pose estimation. Experimental results on synthetic and complex real-world data demonstrate the effectiveness of the proposed approach in challenging conditions and the accuracy of camera pose estimation.																																	2024-04-12	PPRN:86587647		
J	Chen, Yangyi; Sikka, Karan; Cogswell, Michael; Ji, Heng; Divakaran, Ajay				Chen, Yangyi/LSL-4051-2024						Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models								Arxiv											2	2;2024-03-19;https://www.arxiv.org/abs/2309.04461v2| 1;2023-09-08;https://www.arxiv.org/abs/2309.04461v1	arXiv:2309.04461			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 19 2024	2024	Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.																																	2024-04-12	PPRN:84979301		
J	Liu, Jian; Huang, Xiaoshui; Huang, Tianyu; Chen, Lu; Hou, Yuenan; Tang, Shixiang; Liu, Ziwei; Ouyang, Wanli; Zuo, Wangmeng; Jiang, Junjun; Liu, Xianming				Hou, Yuenan/LWK-5131-2024; Huang, Xiaoshui/HPG-0735-2023; shixiang, tang/JQX-3091-2023; huang, tianyu/AFA-9231-2022; Ouyang, Wanli/I-7135-2018; Liu, Ziwei/AAG-6939-2021						A Comprehensive Survey on 3D Content Generation								Arxiv											2	2;2024-03-19;https://www.arxiv.org/abs/2402.01166v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01166v1	arXiv:2402.01166			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	Recent years have witnessed remarkable advances in artificial intelligence generated content(AIGC), with diverse input modalities, e.g., text, image, video, audio and 3D. The 3D is the most close visual modality to real-world 3D environment and carries enormous knowledge. The 3D content generation shows both academic and practical values while also presenting formidable technical challenges. This review aims to consolidate developments within the burgeoning domain of 3D content generation. Specifically, a new taxonomy is proposed that categorizes existing approaches into three types: 3D native generative methods, 2D prior-based 3D generative methods, and hybrid 3D generative methods. The survey covers approximately 60 papers spanning the major techniques. Besides, we discuss limitations of current 3D content generation techniques, and point out open challenges as well as promising directions for future work. Accompanied with this survey, we have established a project website where the resources on 3D content generation research are provided.																																	2024-04-12	PPRN:87509779		
J	Lee, Suhyeon; Kim, Won Jun; Chang, Jinho; Ye, Jong Chul				Kim, Wonjun/KDO-4306-2024; Ye, Jong/C-1623-2011						LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2305.11490v5| 2;2023-10-17;https://www.arxiv.org/abs/2305.11490v4| 1;2023-05-19;https://www.arxiv.org/abs/2305.11490v1	arXiv:2305.11490			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 18 2024	2024	Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual IO. This direction of research is particularly relevant to medical imaging because medical image analysis and generation consist of reasoning based on a combination of visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our model, LLM-CXR, trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks. The code is at https://github.com/hyn2028/llm-cxr.																																	2024-04-11	PPRN:70568973		
J	Ren, Yilong; Chen, Yue; Liu, Shuai; Wang, Boyue; Yu, Haiyang; Cui, Zhiyong				Cui, Zhiyong/AGV-6207-2022; Ren, Yilong/MXL-4162-2025; Yu, Haiyang/JFK-3502-2023						TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models								Arxiv											2	2;2024-03-18;https://www.arxiv.org/abs/2403.02221v2| 1;2024-03-04;https://www.arxiv.org/abs/2403.02221v1	arXiv:2403.02221			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning. Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging LLMs. In this framework, we construct a sequence embedding layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer based on Graph Convolutional Networks (GCNs) to extract sequence features and spatial features, respectively. These are subsequently integrated to form inputs that are suitable for LLMs. A Low-Rank Adaptation (LoRA) fine-tuning approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands. Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and few-shot prediction scenarios, effectively supporting the development of ITS in regions with scarce historical traffic data.																																	2024-04-11	PPRN:88014029		
J	Yao, Xufeng; Li, Haoyang; Chan, Tsz Ho; Xiao, Wenyi; Yuan, Mingxuan; Huang, Yu; Chen, Lei; Yu, Bei				Xiao, Wenyi/LPP-6401-2024; Yao, Xufeng/Q-1251-2018						HDLdebugger: Streamlining HDL debugging with Large Language Models								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.11671v1	arXiv:2403.11671			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 18 2024	2024	In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.																																	2024-04-11	PPRN:88198141		
J	Zeng, Yuchen; Lee, Kangwook				Zeng, Yuchen/W-4920-2017						The Expressive Power of Low-Rank Adaptation								Arxiv											3	3;2024-03-18;https://www.arxiv.org/abs/2310.17513v3| 2;2023-10-27;https://www.arxiv.org/abs/2310.17513v2| 1;2023-10-26;https://www.arxiv.org/abs/2310.17513v1	arXiv:2310.17513			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model f to accurately represent any smaller target model f if LoRA-rank ≥ (width of f) × depth of f / depth of f , under a mild assumption. We quantify the approximation error when the LoRArank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-( embedding size/2 ) LoRA adapters. Our study reveals numerous theoretical insights on hyperparameter tuning and algorithm development for LoRA, all of which are empirically validated.																																	2024-04-11	PPRN:85821860		
J	Wu, Qianyang; Shi, Ye; Huang, Xiaoshui; Yu, Jingyi; Xu, Lan; Wang, Jingya				Huang, Xiaoshui/HPG-0735-2023; wang, Jingya/W-8031-2019						THOR: Text to Human-Object Interaction Diffusion via Relation Intervention								Arxiv											1	1;2024-03-17;https://www.arxiv.org/abs/2403.11208v1	arXiv:2403.11208			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 17 2024	2024	This paper addresses new methodologies to deal with the challenging task of generating dynamic Human-Object Interactions from textual descriptions (Text2HOI). While most existing works assume interactions with limited body parts or static objects, our task involves addressing the variation in human motion, the diversity of object shapes, and the semantic vagueness of object motion simultaneously. To tackle this, we propose a novel Text-guided Human-Object Interaction diffusion model with Relation Intervention (THOR). THOR is a cohesive diffusion model equipped with a relation intervention mechanism. In each diffusion step, we initiate text-guided human and object motion and then leverage human-object relations to intervene in object motion. This intervention enhances the spatial-temporal relations between humans and objects, with human-centric interaction representation providing additional guidance for synthesizing consistent motion from text. To achieve more reasonable and realistic results, interaction losses is introduced at different levels of motion granularity. Moreover, we construct Text-BEHAVE, a Text2HOI dataset that seamlessly integrates textual descriptions with the currently largest publicly available 3D HOI dataset. Both quantitative and qualitative experiments demonstrate the effectiveness of our proposed model.																																	2024-04-11	PPRN:88197193		
J	Cheng, Xinhua; Yang, Tianyu; Wang, Jianan; Li, Yu; Zhang, Lei; Zhang, Jian; Yuan, Li				Zhang, Chao/ADO-6062-2022; Yuan, Li/AET-1324-2022; Li, Yumeng/JBS-1868-2023						Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts								Arxiv											2	2;2024-03-16;https://www.arxiv.org/abs/2310.11784v2| 1;2023-10-18;https://www.arxiv.org/abs/2310.11784v1	arXiv:2310.11784			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 16 2024	2024	Recent text-to-3D generation methods achieve impressive 3D content creation capacity thanks to the advances in image diffusion models and optimizing strategies. However, current methods struggle to generate correct 3D content for a complex prompt in semantics, i.e., a prompt describing multiple interacted objects binding with different attributes. In this work, we propose a general framework named Progressive3D, which decomposes the entire generation into a series of locally progressive editing steps to create precise 3D content for complex prompts, and we constrain the content change to only occur in regions determined by user-defined region prompts in each editing step. Furthermore, we propose an overlapped semantic component suppression technique to encourage the optimization process to focus more on the semantic differences between prompts. Extensive experiments demonstrate that the proposed Progressive3D framework generates precise 3D content for prompts with complex semantics and is general for various text-to-3D methods driven by different 3D representations.																																	2024-04-11	PPRN:85699543		
J	Ding, Zihan; Jin, Chi										Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2309.16984v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.16984v1	arXiv:2309.16984			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 14 2024	2024	Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.																																	2024-04-11	PPRN:85338824		
J	Liu, Kunhao; Zhan, Fangneng; Xu, Muyu; Theobalt, Christian; Shao, Ling; Lu, Shijian				Zhan, Fangneng/MYS-1836-2025; Lu, Shijian/AAU-4831-2021; Liu, Kunhao/JXL-3939-2024						StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting								Arxiv											1	1;2024-03-12;https://www.arxiv.org/abs/2403.07807v1	arXiv:2403.07807			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 12 2024	2024	We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image’s style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi -view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low -dimensional features and then maps them into high -dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high -dimensional memory -intensive features. The second is a K-nearest-neighb or -based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi -view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi -view consistency. Project page: https://kunhao-liu.github.io/StyleGaussian/																																	2024-04-08	PPRN:88113186		
J	Tan, Weihao; Zhang, Wentao; Liu, Shanqi; Zheng, Longtao; Wang, Xinrun; An, Bo										TRUE KNOWLEDGE COMES FROM PRACTICE: ALIGNING LLMS WITH EMBODIED ENVIRONMENTS VIA REINFORCEMENT LEARNING								Arxiv											2	2;2024-03-11;https://www.arxiv.org/abs/2401.14151v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.14151v1	arXiv:2401.14151			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 11 2024	2024	Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.																																	2024-04-08	PPRN:87336974		
J	Qin, Yijian; Wang, Xin; Zhang, Ziwei; Zhu, Wenwu				Zhang, Ziwei/AAD-7704-2021; Zhu, Wenwu/C-5025-2018						Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs								Arxiv											4	4;2024-03-09;https://www.arxiv.org/abs/2310.18152v4| 3;2024-01-12;https://www.arxiv.org/abs/2310.18152v3| 2;2023-11-06;https://www.arxiv.org/abs/2310.18152v2| 1;2023-10-27;https://www.arxiv.org/abs/2310.18152v1	arXiv:2310.18152			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 09 2024	2024	Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing computational costs and allowing much more flexibility in combining with different LLM models. Experimental evaluations demonstrate the effectiveness of the proposed DGTL model on achieving superior or comparable performance over state-of-the-art baselines. Additionally, we also demonstrate that our DGTL model can offer natural language explanations for predictions, thereby significantly enhancing model interpretability.																																	2024-04-04	PPRN:85858678		
J	Guo, Xu; Chen, Yiqiang										Generative AI for Synthetic Data Generation: Methods, Challenges and the Future								Arxiv											1	1;2024-03-07;https://www.arxiv.org/abs/2403.04190v1	arXiv:2403.04190			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 07 2024	2024	The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real -world data positions this approach as a compelling solution to low -resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task -specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.																																	2024-04-05	PPRN:88055616		
J	Jiang, Yibo; Rajendran, Goutham; Ravikumar, Pradeep; Aragam, Bryon; Veitch, Victor										On the Origins of Linear Representations in Large Language Models								Arxiv											1	1;2024-03-06;https://www.arxiv.org/abs/2403.03867v1	arXiv:2403.03867			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 06 2024	2024	Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.																																	2024-04-03	PPRN:88046872		
J	Ding, Shuangrui; Liu, Zihan; Dong, Xiaoyi; Zhang, Pan; Qian, Rui; He, Conghui; Lin, Dahua; Wang, Jiaqi				Dong, Xiaoyi/AAC-8666-2019; Lin, Dahua/W-6576-2019; WANG, JIAQI/KBB-8837-2024; He, Conghui/AAZ-3323-2021						SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.17645v1	arXiv:2402.17645			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English. After adequate pre-training, 10K carefully crafted QA pairs are used to empower the LLM with the instruction-following capability and solve diverse tasks. With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced LLMs like GPT-4. We showcase the generated samples on our project page.																																	2024-03-27	PPRN:87920066		
J	Li, Xiaoxia; Liang, Siyuan; Zhang, Jiyi; Fang, Han; Liu, Aishan; Chang, Ee-Chien				李, 晓霞/IAR-4559-2023; Liang, Siyuan/KHW-1891-2024						Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/2402.14872v2| 1;2024-02-21;https://www.arxiv.org/abs/2402.14872v1	arXiv:2402.14872			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization problem and employ a standardized set of genetic algorithms for generating eligible prompts. Compared to the baseline AutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4% higher without ONION defense and 85.2% higher with ONION defense. SMJ's better performance in all three semantic meaningfulness metrics of Jailbreak Prompt, Similarity, and Outlier, also means that SMJ is resistant to defenses that use those metrics as thresholds.																																	2024-03-27	PPRN:87864515		
J	Li, Haoyang; Zhang, Jing; Liu, Hanbing; Fan, Ju; Zhang, Xiaokang; Zhu, Jun; Wei, Renjie; Pan, Hongyan; Li, Cuiping; Chen, Hong				li, haoyang/HPD-1672-2023; Wei, Renjie/IQS-2348-2023						CodeS: Towards Building Open-source Language Models for Text-to-SQL								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2402.16347v1	arXiv:2402.16347			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.																																	2024-11-09	PPRN:87890764		
J	Huang, Dong; Bu, Qingwen; Qing, Yuhao; Cui, Heming				Qing, Yuhao/AAC-5562-2022						CodeCoT: Tackling Code Syntax Errors in CoT Reasoning for Code Generation								Arxiv											2	2;2024-02-23;https://www.arxiv.org/abs/2308.08784v2| 1;2023-08-17;https://www.arxiv.org/abs/2308.08784v1	arXiv:2308.08784			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	Chain-of-thought (CoT) has emerged as a groundbreaking tool in NLP, notably for its efficacy in complex reasoning tasks, such as mathematical proofs. However, its application in code generation faces a distinct challenge, i.e., although the code generated with CoT reasoning is logically correct, it faces the problem of syntax error (e.g., invalid syntax error report) during code execution, which causes the CoT result's pass@1 in HumanEval even lower than the zero-shot result. In this paper, we present Code Chain-of-Thought (CodeCoT) that integrates CoT with a self-examination process for code generation. CodeCoT begins with the LLMs using CoT for initial code development to ensure the generated code follows the correct logic flow. Then, CodeCoT will generate test cases to validate whether the code has syntax errors during the execution. CodeCoT then employs a self-examination phase, in which the generated code is executed against these test cases in the local environment. If the local environment raises error information (e.g., invalid syntax error), CodeCoT will iteratively refine the code based on the feedback information. Within this loop, CodeCoT can make sure their generated codes not only follow the logic flow of the code description, but the syntax error will also be addressed with the self-examination process. Our evaluation results reveal that CodeCoT improves the effectiveness of code generation. For example, CodeCoT increases pass@1 from 75.6% to 79.3% for the HumanEval dataset.																																	2024-03-27	PPRN:86006050		
J	Jin, Zhuoran; Cao, Pengfei; Chen, Yubo; Liu, Kang; Jiang, Xiaojian; Xu, Jiexin; Li, Qiuxia; Zhao, Jun				li, qiuxia/HCI-3877-2022; Liu, Kang/AAG-8964-2020; Chen, Yubo/LSK-8832-2024; Cao, Pengfei/F-8723-2012						Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models								Arxiv											1	1;2024-02-22;https://www.arxiv.org/abs/2402.14409v1	arXiv:2402.14409			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 22 2024	2024	Retrieval -augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning -Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability bias towards common knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence: We reveal that RALMs follow the principle of majority rule, leaning towards placing trust in evidence that appears more frequently. Moreover, we find that RALMs exhibit confirmation bias, and are more willing to choose evidence that is consistent with their internal memory. To solve the challenge of knowledge conflicts, we propose a method called Conflict -Disentangle Contrastive Decoding (CD2) to better calibrate the model’s confidence. Experimental results demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.																																	2024-03-21	PPRN:87800396		
J	Mahabadi, Rabeeh Karimi; Ivison, Hamish; Tae, Jaesung; Henderson, James; Beltagy, Iz; Peters, Matthew E.; Cohan, Arman										TESS: Text-to-Text Self-Conditioned Simplex Diffusion								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2305.08379v2| 1;2023-05-15;https://www.arxiv.org/abs/2305.08379v1	arXiv:2305.08379			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 21 2024	2024	Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop in performance, and is competitive with pretrained autoregressive sequence-to-sequence models. We publicly release our codebase at https://github.com/allenai/tess-diffusion.																																	2024-03-20	PPRN:69613519		
J	Skreta, Marta; Zhou, Zihan; Yuan, Jia Lin; Darvish, Kourosh; Aspuru-Guzik, Alan; Garg, Animesh				Yuan, Jialin/HNJ-0567-2023; Aspuru-Guzik, Alan/A-4984-2008						RePLan: Robotic Replanning with Perception and Language Models								Arxiv											2	2;2024-02-20;https://www.arxiv.org/abs/2401.04157v2| 1;2024-01-08;https://www.arxiv.org/abs/2401.04157v1	arXiv:2401.04157			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 20 2024	2024	Advancements in large language models (LLMs) have demonstrated their potential in facilitating high-level reasoning, logical reasoning and robotics planning. Recently, LLMs have also been able to generate reward functions for low-level robot actions, effectively bridging the interface between high-level planning and low-level robot control. However, the challenge remains that even with syntactically correct plans, robots can still fail to achieve their intended goals due to imperfect plans or unexpected environmental issues. To overcome this, Vision Language Models (VLMs) have shown remarkable success in tasks such as visual question answering. Leveraging the capabilities of VLMs, we present a novel framework called Robotic Replanning with Perception and Language Models (RePLan) that enables online replanning capabilities for long-horizon tasks. This framework utilizes the physical grounding provided by a VLM's understanding of the world's state to adapt robot actions when the initial plan fails to achieve the desired goal. We developed a Reasoning and Control (RC) benchmark with eight long-horizon tasks to test our approach. We find that RePLan enables a robot to successfully adapt to unforeseen obstacles while accomplishing open-ended, long-horizon goals, where baseline models cannot, and can be readily applied to real robots.																																	2024-03-20	PPRN:87082832		
J	Lei, Xuanyu; Yang, Zonghan; Chen, Xinrui; Li, Peng; Liu, Yang				Chen, Xinrui/KQU-5547-2024						Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models								Arxiv											1	1;2024-02-19;https://www.arxiv.org/abs/2402.12058v1	arXiv:2402.12058			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 19 2024	2024	State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the textual CoT prompting. Our code is released in https://github.com/leixy20/Scaffold.																																	2024-03-19	PPRN:87763128		
J	Liang, Jiawei; Liang, Siyuan; Liu, Aishan; Jia, Xiaojun; Kuang, Junhao; Cao, Xiaochun				Liang, Siyuan/KHW-1891-2024; Kuang, Junhao/KSL-6164-2024; Jia, Xiaojun/IUM-2172-2023						Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection								Arxiv											1	1;2024-02-18;https://www.arxiv.org/abs/2402.11473v1	arXiv:2402.11473			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 18 2024	2024	The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper introduces a novel and previously unrecognized threat in face forgery detection scenarios caused by backdoor attack. By embedding backdoors into models and incorporating specific trigger patterns into the input, attackers can deceive detectors into producing erroneous predictions for forged faces. To achieve this goal, this paper proposes Poisoned Forgery Face framework, which enables clean-label backdoor attacks on face forgery detectors. Our approach involves constructing a scalable trigger generator and utilizing a novel convolving process to generate translation-sensitive trigger patterns. Moreover, we employ a relative embedding method based on landmark-based regions to enhance the stealthiness of the poisoned samples. Consequently, detectors trained on our poisoned samples are embedded with backdoors. Notably, our approach surpasses SoTA backdoor baselines with a significant improvement in attack success rate (+16.39% BDAUC) and reduction in visibility (-12.65% L∞). Furthermore, our attack exhibits promising performance against backdoor defenses. We anticipate that this paper will draw greater attention to the potential threats posed by backdoor attacks in face forgery detection scenarios. 																																	2024-03-19	PPRN:87761857		
J	Zhang, Yazhou; Wang, Mengyao; Ren, Chenyu; Li, Qiuchi; Tiwari, Prayag; Wang, Benyou; Qin, Jing				Wang, Benyou/Y-5146-2019; Li, Qiuchi/MDT-3578-2025; Tiwari, Prayag/N-6261-2017; cy, R/KHW-8983-2024						Pushing The Limit of LLM Capacity for Text Classification								Arxiv											2	2;2024-02-16;https://www.arxiv.org/abs/2402.07470v2| 1;2024-02-12;https://www.arxiv.org/abs/2402.07470v1	arXiv:2402.07470			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 16 2024	2024	The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average. Further evaluation experiments show a clear surpassing of RGPT over human classification performance1.																																	2024-03-21	PPRN:87636822		
J	Sanford, Clayton; Hsu, Daniel; Telgarsky, Matus										Transformers, parallel computation, and logarithmic depth								Arxiv											1	1;2024-02-14;https://www.arxiv.org/abs/2402.09268v1	arXiv:2402.09268			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 14 2024	2024	We show that a constant number of self -attention layers can efficiently simulate—and be simulated by—a constant number of communication rounds of Massively Parallel Computation. As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub -quadratic transformer approximations. We thus establish parallelism as a key distinguishing property of transformers.																																	2024-05-25	PPRN:87684019		
J	Dagan, Gautier; Synnaeve, Gabriel; Roziere, Baptiste										Getting the most out of your tokenizer for pre-training and domain adaptation								Arxiv											2	2;2024-02-07;https://www.arxiv.org/abs/2402.01035v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.01035v1	arXiv:2402.01035			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 07 2024	2024	Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pretokenization regular expression, and training data of a tokenizer can significantly impact the model’s generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte -Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper -parameters selection and switching the tokenizer in a pre -trained LLM. We perform our experiments on models trained from scratch and from pre -trained models, verifying their applicability to a wide range of use -cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre -trained LLM to obtain large gains in generation speed and effective context size.																																	2024-05-25	PPRN:87503898		
J	Wu, Zhengxuan; Geiger, Atticus; Icard, Thomas; Potts, Christopher; Goodman, Noah D.										Interpretability at Scale: Identifying Causal Mechanisms in Alpaca								Arxiv											3	3;2024-02-06;https://www.arxiv.org/abs/2305.08809v3| 2;2024-01-23;https://www.arxiv.org/abs/2305.08809v2| 1;2023-05-15;https://www.arxiv.org/abs/2305.08809v1	arXiv:2305.08809			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 06 2024	2024	Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward faithfully understanding the inner-workings of our ever-growing and most widely deployed language models. 																																	2024-05-25	PPRN:69591951		
J	Aguina-Kang, Rio; Gumin, Maxim; Han, Do Heon; Morris, Stewart; Yoo, Seung Jean; Ganeshan, Aditya; Jones, R. Kenny; Wei, Qiuhong Anna; Fu, Kailiang; Ritchie, Daniel				Fu, Kailiang/NDT-5473-2025						Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases								Arxiv											1	1;2024-02-05;https://www.arxiv.org/abs/2403.09675v1	arXiv:2403.09675			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 05 2024	2024	We present a system for generating indoor scenes in response to text prompts. The prompts are not limited to a fixed vocabulary of scene descriptions, and the objects in generated scenes are not restricted to a fixed set of object categories -- we call this setting indoor scene generation. Unlike most prior work on indoor scene generation, our system does not require a large training dataset of existing 3D scenes. Instead, it leverages the world knowledge encoded in pre-trained large language models (LLMs) to synthesize programs in a domain-specific layout language that describe objects and spatial relations between them. Executing such a program produces a specification of a constraint satisfaction problem, which the system solves using a gradient-based optimization scheme to produce object positions and orientations. To produce object geometry, the system retrieves 3D meshes from a database. Unlike prior work which uses databases of category-annotated, mutually-aligned meshes, we develop a pipeline using vision-language models (VLMs) to retrieve meshes from massive databases of un-annotated, inconsistently-aligned meshes. Experimental evaluations show that our system outperforms generative models trained on 3D data for traditional, closed-universe scene generation tasks; it also outperforms a recent LLM-based layout generation method on open-universe scene generation.																																	2024-04-11	PPRN:88168618		
J	Jiang, Yushan; Pan, Zijie; Zhang, Xikun; Garg, Sahil; Schneider, Anderson; Nevmyvaka, Yuriy; Song, Dongjin				Pan, Zijie/MEQ-2078-2025; Jiang, Yushan/LCE-4310-2024						Empowering Time Series Analysis with Large Language Models: A Survey								Arxiv											1	1;2024-02-05;https://www.arxiv.org/abs/2402.03182v1	arXiv:2402.03182			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 05 2024	2024	Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.																																	2024-05-25	PPRN:87517666		
J	Yang, Zonglin; Dong, Li; Du, Xinya; Cheng, Hao; Cambria, Erik; Liu, Xiaodong; Gao, Jianfeng; Wei, Furu				Wang, Xiaodong/E-4202-2017; Gao, Jianfeng/AAP-8200-2021; Cambria, Erik/C-2103-2013; Yang, Zonglin/AGA-2162-2022						Language Models as Inductive Reasoners								Arxiv											3	3;2024-02-05;https://www.arxiv.org/abs/2212.10923v3| 2;2022-12-21;https://www.arxiv.org/abs/2212.10923v1| 1;2022-12-21;https://www.arxiv.org/abs/2212.10923v1	arXiv:2212.10923			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 05 2024	2024	Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule -fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language models as “reasoners”. Moreover, we provide the first and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations. We discuss our future perspectives on inductive reasoning in detail in Section 7. Dataset and code are available at https://github.com/ ZonglinY/Inductive_Reasoning.																																	2024-02-19	PPRN:35883693		
J	Fan, Xiaoran; Ji, Tao; Jiang, Changhao; Li, Shuo; Jin, Senjie; Song, Sirui; Wang, Junke; Hong, Boyang; Chen, Lu; Zheng, Guodong; Zhang, Ming; Huang, Caishuang; Zheng, Rui; Xi, Zhiheng; Zhou, Yuhao; Dou, Shihan; Ye, Junjie; Yan, Hang; Gui, Tao; Zhang, Qi; Qiu, Xipeng; Huang, Xuanjing; Wu, Zuxuan; Jiang, Yu-Gang				Ji, Tao/KIK-1554-2024; Ye, Junjie/AAW-8101-2021; Xi, Zhiheng/KUD-1665-2024; Gui, Tao/LWI-6783-2024; Zhou, Yuhao/ABC-4280-2022						MouSi: Poly-Visual-Expert Vision-Language Models								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2401.17221v1	arXiv:2401.17221			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 30 2024	2024	Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website1.																																	2024-05-25	PPRN:87421533		
J	Kwan, Wai-Chung; Zeng, Xingshan; Jiang, Yuxin; Wang, Yufei; Li, Liangyou; Shang, Lifeng; Jiang, Xin; Liu, Qun; Wong, Kam-Fai				SHANG, LIFENG/KIC-9695-2024; JIANG, Yuxin/AAB-9309-2022						MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2401.16745v1	arXiv:2401.16745			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 30 2024	2024	Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance. MT-Eval is released publicly to encourage future research towards more robust conversational models.																																	2024-02-16	PPRN:87420334		
J	Ju, Yuanchen; Hu, Kaizhe; Zhang, Guowei; Zhang, Gu; Jiang, Mingrun; Xu, Huazhe				Zhang, Guowei/T-3190-2017						Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation								Arxiv											1	1;2024-01-15;https://www.arxiv.org/abs/2401.07487v1	arXiv:2401.07487			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 15 2024	2024	Enabling robotic manipulation that generalizes to out-of-distribution scenes is a crucial step toward open-world embodied intelligence. For human beings, this ability is rooted in the understanding of semantic correspondence among objects, which naturally transfers the interaction experience of familiar objects to novel ones. Although robots lack such a reservoir of interaction experience, the vast availability of human videos on the Internet may serve as a valuable resource, from which we extract an affordance memory including the contact points. Inspired by the natural way humans think, we propose Robo-ABC: when confronted with unfamiliar objects that require generalization, the robot can acquire affordance by retrieving objects that share visual or semantic similarities from the affordance memory. The next step is to map the contact points of the retrieved objects to the new object. While establishing this correspondence may present formidable challenges at first glance, recent research finds it naturally arises from pre-trained diffusion models, enabling affordance mapping even across disparate object categories. Through the Robo-ABC framework, robots may generalize to manipulate out-of-category objects in a zero-shot manner without any manual annotation, additional training, part segmentation, pre-coded knowledge, or viewpoint restrictions. Quantitatively, Robo-ABC significantly enhances the accuracy of visual affordance retrieval by a large margin of 31.6% compared to state-of-the-art (SOTA) end-to-end affordance models. We also conduct real-world experiments of cross-category object-grasping tasks. Robo-ABC achieved a success rate of 85.7%, proving its capacity for real-world tasks.																																	2024-05-25	PPRN:87194337		
J	Pawar, Saurav; Tonmoy, S. Towhidul Islam; Zaman, S M Mehedi; Jain, Vinija; Chadha, Aman; Das, Amitava				Zaman, S M Mehedi/OON-0475-2025; Chadha, Dr. Aman/GNM-9565-2022						The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey								Arxiv											1	1;2024-01-15;https://www.arxiv.org/abs/2401.07872v1	arXiv:2401.07872			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 15 2024	2024	The advent of Large Language Models (LLMs) represents a notable breakthrough in Natural Language Processing (NLP), contributing to substantial progress in both text comprehen-sion and generation. However, amidst these advancements, it is noteworthy that LLMs of -ten face a limitation in terms of context length extrapolation. Understanding and extending the context length for LLMs is crucial in en-hancing their performance across various NLP applications. In this survey paper, we delve into the multifaceted aspects of exploring why it is essential, and the potential transformations that superior techniques could bring to NLP applications. We study the inherent challenges associated with extending context length and present an organized overview of the existing strategies employed by researchers. Addition-ally, we discuss the intricacies of evaluating context extension techniques and highlight the open challenges that researchers face in this do-main. Furthermore, we explore whether there is a consensus within the research community regarding evaluation standards and identify areas where further agreement is needed. This comprehensive survey aims to serve as a valuable resource for researchers, guiding them through the nuances of context length extension techniques and fostering discussions on future advancements in this evolving field.																																	2024-05-25	PPRN:87188715		
J	Lyu, Qing; Apidianaki, Marianna; Callison-Burch, Chris				Callison-Burch, Chris/A-3393-2010						Towards Faithful Model Explanation in NLP: A Survey								Arxiv											3	3;2024-01-12;https://www.arxiv.org/abs/2209.11326v4| 2;2023-12-20;https://www.arxiv.org/abs/2209.11326v3| 1;2022-09-22;https://www.arxiv.org/abs/2209.11326v1	arXiv:2209.11326			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jan 12 2024	2024	End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.																																	2024-02-02	PPRN:18421186		
J	Chung, Jaeyoung; Oh, Jeongtaek; Lee, Kyoung Mu				Chung, Jaeyoung/LQK-2801-2024; Lee, Hyo-Suk/J-5618-2012						Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images								Arxiv											3	3;2024-01-04;https://www.arxiv.org/abs/2311.13398v3| 2;2023-12-04;https://www.arxiv.org/abs/2311.13398v2| 1;2023-11-22;https://www.arxiv.org/abs/2311.13398v1	arXiv:2311.13398			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 04 2024	2024	In this paper, we present a method to optimize Gaussian splatting with a limited number of images while avoiding overfitting. Representing a 3D scene by combining numerous Gaussian splats has yielded outstanding visual quality. However, it tends to overfit the training views when only a small number of images are available. To address this issue, we introduce a dense depth map as a geometry guide to mitigate overfitting. We obtained the depth map using a pre-trained monocular depth estimation model and aligning the scale and offset using sparse COLMAP feature points. The adjusted depth aids in the color-based optimization of 3D Gaussian splatting, mitigating floating artifacts, and ensuring adherence to geometric constraints. We verify the proposed method on the NeRF-LLFF dataset with varying numbers of few images. Our approach demonstrates robust geometry compared to the original method that relies solely on images. 																																	2024-01-13	PPRN:86239321		
J	Delcourt, Michelle; Postle, Luke										Finding an almost perfect matching in a hypergraph avoiding forbidden submatchings								Arxiv											2	2;2024-12-24;https://www.arxiv.org/abs/2204.08981v3| 1;2022-04-19;https://www.arxiv.org/abs/2204.08981v1	arXiv:2204.08981			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 24 2024	2024	In 1973, Erdos conjectured the existence of high girth (n, 3, 2)-Steiner systems. Recently, Glock, Kuhn, Lo, and Osthus and independently Bohman and Warnke proved the approximate version of Erdos’ conjecture. Just this year, Kwan, Sah, Sawhney, and Simkin proved Erdos’ conjecture. As for Steiner systems with more general parameters, Glock, Kuhn, Lo, and Osthus conjectured the existence of high girth (n, q, r )-Steiner systems. We prove the approximate version of their conjecture. This result follows from our general main results which concern finding perfect or almost perfect matchings in a hypergraph G avoiding a given set of submatchings (which we view as a hypergraph H where V(H)=E(G)). Our first main result is a common generalization of the classical theorems of Pippenger (for finding an almost perfect matching) and Ajtai, Komlos, Pintz, Spencer, and Szemere´di (for finding an independent set in girth five hypergraphs). More generally, we prove this for coloring and even list coloring, and also generalize this further to when H is a hypergraph with small codegrees (for which high girth designs is a specific instance). Indeed, the coloring version of our result even yields an almost partition of Knr into approximate high girth (n,q,r)-Steiner systems. Our main results also imply the existence of a perfect matching in a bipartite hypergraph where the parts have slightly unbalanced degrees. This has a number of other applications; for example, it proves the existence of Δ pairwise disjoint list colorings in the setting of Kahn’s theorem for list coloring the edges of a hypergraph; it also proves asymptotic versions of various rainbow matching results in the sparse setting (where the number of times a color appears could be much smaller than the number of colors) and even the existence of many pairwise disjoint rainbow matchings in such circumstances.																																	2025-03-27	PPRN:12138612		
J	Kuhne, Lars										Equidistribution in Families of Abelian Varieties and Uniformity								Arxiv											2	2;2024-12-23;https://www.arxiv.org/abs/2101.10272v4| 1;2021-01-25;https://www.arxiv.org/abs/2101.10272v3	arXiv:2101.10272			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 23 2024	2024	Using equidistribution techniques from Arakelov theory as well as recent results obtained by Dimitrov, Gao, and Habegger, we deduce uniform results on the Manin-Mumford and the Bogomolov conjecture. For each given integer g ≥ 2, we prove that the number of torsion points lying on a smooth complex algebraic curve of genus g embedded into its Jacobian is uniformly bounded. Complementing recent works of Dimitrov, Gao, and Habegger, we obtain a rather uniform version of the Mordell conjecture as well. In particular, the number of rational points on a smooth algebraic curve defined over a number field can be bounded solely in terms of its genus and the Mordell-Weil rank of its Jacobian.																																	2025-02-02	PPRN:11916644		
J	Pang, Chao; Weng, Xingxing; Wu, Jiang; Li, Jiayu; Liu, Yi; Sun, Jiaxing; Li, Weijia; Wang, Shuai; Feng, Litong; Xia, Gui-Song; He, Conghui				Li, Weijia/ABG-1523-2021; He, Conghui/AAZ-3323-2021						VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis								Arxiv											3	3;2024-12-19;https://www.arxiv.org/abs/2403.20213v4| 2;2024-11-06;https://www.arxiv.org/abs/2403.20213v3| 1;2024-03-29;https://www.arxiv.org/abs/2403.20213v1	arXiv:2403.20213			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 19 2024	2024	This paper develops a Versatile and Honest vision language Model (VHM) for remote sensing image analysis. VHM is built on a large-scale remote sensing image-text dataset with rich-content captions (VersaD), and an honest instruction dataset comprising both factual and deceptive questions (HnstD). Unlike prevailing remote sensing image-text datasets, in which image captions focus on a few prominent objects and their relationships, VersaD captions provide detailed information about image properties, object attributes, and the overall scene. This comprehensive captioning enables VHM to thoroughly understand remote sensing images and perform diverse remote sensing tasks. Moreover, different from existing remote sensing instruction datasets that only include factual questions, HnstD contains additional deceptive questions stemming from the non-existence of objects. This feature prevents VHM from producing affirmative answers to nonsense queries, thereby ensuring its honesty. In our experiments, VHM significantly outperforms various vision language models on common tasks of scene classification, visual question answering, and visual grounding. Additionally, VHM achieves competent performance on several unexplored tasks, such as building vectorizing, multi-label classification and honest question answering.																																	2025-01-28	PPRN:88342995		
J	Xu, Yi; Hu, Yuxin; Zhang, Zaiwei; Meyer, Gregory P.; Mustikovela, Siva Karthik; Srinivasa, Siddhartha; Wolff, Eric M.; Huang, Xin										VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision								Arxiv											2	2;2025-08-29;https://www.arxiv.org/abs/2412.14446v2| 1;2024-12-19;https://www.arxiv.org/abs/2412.14446v1	arXiv:2412.14446			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 19 2024	2024	Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes. This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset.																																	2025-01-24	PPRN:120064314		
J	Yu, Puxuan; Merrick, Luke; Nuti, Gaurav; Campos, Daniel										Arctic-Embed 2.0: Multilingual Retrieval Without Compromise								Arxiv											1	1;2024-12-14;https://www.arxiv.org/abs/2412.04506v2	arXiv:2412.04506			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 14 2024	2024	This paper presents the training methodology of Arctic-Embed 2.0, a set of open-source text embedding models built for accurate and efficient multilingual retrieval. While prior works have suffered from degraded English retrieval quality, Arctic-Embed 2.0 delivers competitive retrieval quality on multilingual and English-only benchmarks, and supports Matryoshka Representation Learning (MRL) for efficient embedding storage with significantly lower compressed quality degradation compared to alternatives. We detail the design and implementation, presenting several important open research questions that arose during model development. We conduct experiments exploring these research questions and include extensive discussion aimed at fostering further discussion in this field.																																	2025-01-23	PPRN:119964264		
J	Cheng, Yunfei; Zhang, Aonan; Zhang, Xuanyu; Wang, Chong; Wang, Yi				Cheng, Yunfei/N-9567-2018; Zhang, Aonan/GOK-2166-2022						Recurrent Drafter for Fast Speculative Decoding in Large Language Models								Arxiv											5	5;2024-12-13;https://www.arxiv.org/abs/2403.09919v5| 4;2024-10-09;https://www.arxiv.org/abs/2403.09919v4| 3;2024-05-30;https://www.arxiv.org/abs/2403.09919v3| 2;2024-03-22;https://www.arxiv.org/abs/2403.09919v2| 1;2024-03-14;https://www.arxiv.org/abs/2403.09919v1	arXiv:2403.09919			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 13 2024	2024	We present Recurrent Drafter (ReDrafter), an advanced speculative decoding approach that achieves state-of-the-art speedup for large language models (LLMs) inference. The performance gains are driven by three key aspects: (1) leveraging a recurrent neural network (RNN) as the draft model conditioning on LLM’s hidden states, (2) applying a dynamic tree attention algorithm over beam search results to eliminate duplicated prefixes in candidate sequences, and (3) training through knowledge distillation from the LLM. ReDrafter accelerates Vicuna inference in MT-Bench by up to 2.8x with a PyTorch implementation on Nvidia H100 GPUs. To demonstrate its practicality in real environments, we also validated its effectiveness for on-device applications by implementing the approach in MLX and benchmarking performance on Metal GPUs in Apple Silicon chips, achieving up to 2.3x speedup. We summarize our experimental results in Figure 1.																																	2025-01-24	PPRN:88169496		
J	Huguet, Guillaume; Vuckovic, James; Fatras, Kilian; Thibodeau-Laufer, Eric; Lemos, Pablo; Islam, Riashat; Liu, Cheng-Hao; Rector-Brooks, Jarrid; Akhound-Sadegh, Tara; Bronstein, Michael; Tong, Alexander; Bose, Avishek Joey				Huguet, Guillaume/GPX-3656-2022; Lemos, Pedro/G-5758-2013; Liu, Cheng-Hao/JAX-3401-2023						Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation								Arxiv											2	2;2024-12-11;https://www.arxiv.org/abs/2405.20313v2| 1;2024-05-30;https://www.arxiv.org/abs/2405.20313v1	arXiv:2405.20313			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 11 2024	2024	Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FOLD FLOW-2 4 , a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation. FOLD FLOW-2 presents substantial new architectural features over the previous FOLD FLOW family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples—crucial for de-novo drug design—we train FOLD FLOW-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FOLD FLOW-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FOLD FLOW-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FOLD FLOW-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.																																	2025-01-19	PPRN:89113608		
J	Ren, Xubin; Wei, Wei; Xia, Lianghao; Su, Lixin; Cheng, Suqi; Wang, Junfeng; Yin, Dawei; Huang, Chao				Ren, Xubin/KOD-3622-2024; Yin, Dawei/JOR-9201-2023						Representation Learning with Large Language Models for Recommendation								Arxiv											5	5;2024-12-11;https://www.arxiv.org/abs/2310.15950v5| 4;2024-02-25;https://www.arxiv.org/abs/2310.15950v4| 3;2024-01-25;https://www.arxiv.org/abs/2310.15950v3| 2;2023-12-15;https://www.arxiv.org/abs/2310.15950v2| 1;2023-10-24;https://www.arxiv.org/abs/2310.15950v1	arXiv:2310.15950			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 11 2024	2024	Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text- only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLMempowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, employs LLMs for user/item profiling, and aligns the semantic space of LLMs with collaborative relational signals through cross-view alignment. This work further demonstrates the theoretical foundation of incorporating textual signals through mutual information maximization, which improves the quality of representations. Our evaluation integrates RLMRec with state-of-the-art recommender models, while also analyzin its efficiency and robustness to noise data. 																																	2025-01-19	PPRN:85769250		
J	Fu, Chaoyou; Zhang, Yi-Fan; Yin, Shukang; Li, Bo; Fang, Xinyu; Zhao, Sirui; Duan, Haodong; Sun, Xing; Liu, Ziwei; Wang, Liang; Shan, Caifeng; He, Ran				Zhao, Sirui/OYE-9902-2025; Zhang, Yifan/MIQ-8814-2025; Liu, Ziwei/AAG-6939-2021; Fang, Xinyu/IYT-2547-2023; Wang, Liang/MTE-7117-2025; Shan, Caifeng/W-6178-2019; Duan, Haodong/ITV-1505-2023; Yin, Shukang/JCP-4961-2023						MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs								Arxiv											1	1;2024-12-08;https://www.arxiv.org/abs/2411.15296v2	arXiv:2411.15296			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 08 2024	2024	As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that are impressive, such as writing code given a flow chart or creating stories based on an image. In the development process, evaluation is critical since it provides intuitive feedback and guidance on improving models. Distinct from the traditional train-eval-test paradigm that only favors a single task like image classification, the versatility of MLLMs has spurred the rise of various new benchmarks and evaluation methods. In this paper, we aim to present a comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extented applications; 2) the typical process of benchmark counstruction, consisting of data collection, annotation, and precautions; 3) the systematic evaluation manner composed of judge, metric, and toolkit; 4) the outlook for the next benchmark. This work aims to offer researchers an easy grasp of how to effectively evaluate MLLMs according to different needs and to inspire better evaluation methods, thereby driving the progress of MLLM research.																																	2025-01-17	PPRN:119798753		
J	Magnusson, Ian; Bhagia, Akshita; Hofmann, Valentin; Soldaini, Luca; Jha, Ananya Harsh; Tafjord, Oyvind; Schwenk, Dustin; Walsh, Evan Pete; Elazar, Yanai; Lo, Kyle; Groeneveld, Dirk; Beltagy, Iz; Hajishirzi, Hannaneh; Smith, Noah A.; Richardson, Kyle; Dodge, Jesse										Paloma: A Benchmark for Evaluating Language Model Fit								Arxiv											2	2;2024-12-07;https://www.arxiv.org/abs/2312.10523v2| 1;2023-12-16;https://www.arxiv.org/abs/2312.10523v1	arXiv:2312.10523			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 07 2024	2024	Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains—varying distributions of language. We introduce P ERPLEXITY A NALYSIS FOR L ANGUAGE M ODEL A SSESSMENT (P ALOMA ) 1 , a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from P ALOMA surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.																																	2025-01-17	PPRN:86685468		
J	Dang, Yunkai; Huang, Kaichen; Huo, Jiahao; Yan, Yibo; Huang, Sirui; Liu, Dongrui; Gao, Mengxi; Zhang, Jie; Qian, Chen; Wang, Kun; Liu, Yong; Shao, Jing; Xiong, Hui; Hu, Xuming				Yan, Yibo/LOR-9828-2024; Hu, Xuming/HTS-1538-2023; Huang, Sirui/JBS-1025-2023; Xiong, Hui/KIK-5457-2024; Liu, DongRui/NLN-8967-2025; Gao, Mengxi/OJU-4957-2025						Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey<br>								Arxiv											1	1;2024-12-03;https://www.arxiv.org/abs/2412.02104v1	arXiv:2412.02104			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 03 2024	2024	The rapid development of Artificial Intelligence (AI) has revolutionized numerous fields, with large language models (LLMs) and computer vision (CV) systems driving advancements in natural language understanding and visual processing, respectively. The convergence of these technologies has catalyzed the rise of multimodal AI, enabling richer, cross-modal understanding that spans text, vision, audio, and video modalities. Multimodal large language models (MLLMs), in particular, have emerged as a powerful framework, demonstrating impressive capabilities in tasks like image-text generation, visual question answering, and cross-modal retrieval. Despite these advancements, the complexity and scale of MLLMs introduce significant challenges in interpretability and explainability, essential for establishing transparency, trustworthiness, and reliability in high-stakes applications. This paper provides a comprehensive survey on the interpretability and explainability of MLLMs, proposing a novel framework that categorizes existing research across three perspectives: (I) Data, (II) Model, (III) Training & Inference. We systematically analyze interpretability from token-level to embedding-level representations, assess approaches related to both architecture analysis and design, and explore training and inference strategies that enhance transparency. By comparing various methodologies, we identify their strengths and limitations and propose future research directions to address unresolved challenges in multimodal explainability. This survey offers a foundational resource for advancing interpretability and transparency in MLLMs, guiding researchers and practitioners toward developing more accountable and robust multimodal AI systems.																																	2025-01-15	PPRN:119686439		
J	Che, Haoxuan; He, Xuanhua; Liu, Quande; Jin, Cheng; Chen, Hao				Chen, Hao/JHU-3470-2023; Liu, Quande/AAU-8189-2020						GameGen-X: Interactive Open-world Game Video Generation								Arxiv											2	2;2024-12-02;https://www.arxiv.org/abs/2411.00769v2| 1;2024-11-01;https://www.arxiv.org/abs/2411.00769v1	arXiv:2411.00769			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 02 2024	2024	We introduce GameGen-X, the first diffusion transformer model specifically designed for both generating and interactively controlling open-world game videos. This model facilitates high-quality, open-domain generation by simulating an extensive array of game engine features, such as innovative characters, dynamic environments, complex actions, and diverse events. Additionally, it provides interactive controllability, predicting and altering future content based on the current clip, thus allowing for gameplay simulation. To realize this vision, we first collected and built an Open-World Video Game Dataset (OGameData) from scratch. It is the first and largest dataset for open-world game video generation and control, which comprises over one million diverse gameplay video clips with informative captions from GPT-4o. GameGen-X undergoes a two-stage training process, consisting of pre-training and instruction tuning. Firstly, the model was pre-trained via text-to-video generation and video continuation, endowing it with the capability for long-sequence, high-quality open-domain game video generation. Further, to achieve interactive controllability, we designed InstructNet to incorporate game- related multi-modal control signal experts. This allows the model to adjust latent representations based on user inputs, unifying character interaction, and scene content control for the first time in video generation. During instruction tuning, only the InstructNet is updated while the pre-trained foundation model is frozen, enabling the integration of interactive controllability without loss of diversity and quality of generated content. GameGen-X represents a significant leap forward in open-world game design using generative models. It demonstrates the potential of generative models to serve as auxiliary tools to traditional rendering techniques, effectively merging creative generation with interactive capabilities. 																																	2025-01-11	PPRN:119011811		
J	Gao, Zhaolin; Brantley, Kiante; Joachims, Thorsten										Reviewer2: Optimizing Review Generation Through Prompt Generation								Arxiv											2	2;2024-12-02;https://www.arxiv.org/abs/2402.10886v2| 1;2024-02-16;https://www.arxiv.org/abs/2402.10886v1	arXiv:2402.10886			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 02 2024	2024	Recent developments in LLMs offer new opportunities for assisting authors in improving their work. In this paper, we envision a use case where authors can receive LLM-generated reviews that uncover weak points in the current draft. While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce. To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2. Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address. We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft. As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect prompts, which we make available as a resource for future research.																																	2025-01-11	PPRN:87753868		
J	Zhou, Han; Wan, Xingchen; Proleev, Lev; Mincu, Diana; Chen, Jilin; Heller, Katherine A; Roy, Subhrajit				Teng, Chieh-Lin/AAC-7441-2020; Heller, Keith/F-7517-2010; Wan, Xingchen/AAE-2146-2022						Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering								Arxiv											3	3;2024-12-01;https://www.arxiv.org/abs/2309.17249v3| 2;2024-01-24;https://www.arxiv.org/abs/2309.17249v2| 1;2023-09-29;https://www.arxiv.org/abs/2309.17249v1	arXiv:2309.17249			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 01 2024	2024	Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.																																	2025-01-11	PPRN:85337683		
J	d'Enterria, D.; Kluth, S.; Zanderighi, G.; Ayala, C.; Benitez-Rathgeb, M.A.; Bluemlein, J.; Boito, D.; Brambilla, N.; Britzger, D.; Camarda, S.; Cooper-Sarkar, A.M.; Cridge, T.; Cvetic, G.; Dalla Brida, M.; Deur, A.; Giuli, F.; Golterman, M.; Hoang, A.H.; Huston, J.; Jamin, M.; Kotikov, A.V.; Krivokhizhin, V.G.; Kronfeld, A.S.; Leino, V.; Lipka, K.; Makela, T.; Malaescu, B.; Maltman, K.; Marzani, S.; Mateu, V.; Moch, S.; Monni, P.F.; Nadolsky, P.; Nason, P.; Nesterenko, A.V.; Perez-Ramos, R.; Peris, S.; Petreczky, P.; Pich, A.; Rabbertz, K.; Ramos, A.; Reichelt, D.; Rodriguez-Sanchez, A.; Rojo, J.; Saragnese, M.; Sawyer, L.; Schott, M.; Schumann, S.; Shaikhatdenov, B.G.; Sint, S.; Soyez, G.; Teca, D.; Vairo, A.; Vos, M.; Waits, C.; Weber, J.H.; Wobisch, M.; Xie, K.				Maleev, Victor/R-4140-2016; Brambilla, Nora/O-9943-2015; Rabbertz, Klaus/OYD-3255-2025; Dalla Brida, Mattia/T-2879-2017; Mateu, Vicent/B-5036-2017; PEREZ-RAMOS, Redamy/AAM-7540-2021; Reichelt, Daniel/ABC-9713-2021; Schumann, Steffen/AAP-2924-2020; Boito, Diogo/C-2727-2015						The strong coupling constant: State of the art and the decade ahead								Arxiv											1	1;2024-11-29;https://www.arxiv.org/abs/2203.08271v2	arXiv:2203.08271			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 29 2024	2024	Theoretical predictions for particle production cross sections and decays at colliders rely heavily on perturbative Quantum Chromo dynamics (QCD) calculations, expressed as an expansion in powers of the strong coupling constant αS. The current O (1%) uncertainty of the QCD coupling evaluated at the reference Z boson mass, αS αS( mZ2 ) = 0.1179±0.0009, is one of the limiting factors to more precisely describe multiple processes at current and future colliders. A reduction of this uncertainty is thus a prerequisite to perform precision tests of the Standard Model as well as searches for new physics. This report provides a comprehensive summary of the state-of-the-art, challenges, and prospects in the experimental and theoretical study of the strong coupling. The current αS(mZ2) world average is derived from a combination of seven categories of observables: (i) lattice QCD, (ii) hadronic τ decays, (iii) deep-inelastic scattering and parton distribution functions fits, (iv) electroweak boson decays, hadronic final-states in (v) e+e− , (vi) e-p, and (vii) p-p collisions, and (viii) quarkonia decays and masses. We review the current status of each of these seven αS (mZ2) extraction methods, discuss novel αS determinations, and examine the averaging method used to obtain the world-average value. Each of the methods discussed provides a “wish list” of experimental and theoretical developments required in order to achieve the goal of a per-mille precision on αS (mZ2) within the next decade.																																	2025-01-24	PPRN:119580393		
J	Tu, Shuyuan; Xing, Zhen; Han, Xintong; Cheng, Zhi-Qi; Dai, Qi; Luo, Chong; Wu, Zuxuan				Xing, Zhen/NKP-8149-2025; Cheng, Zhiqi/KJM-4038-2024						StableAnimator: High-Quality Identity-Preserving Human Image Animation								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/2411.17697v2	arXiv:2411.17697			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	Current diffusion models for human image animation struggle to ensure identity (ID) consistency. This paper presents StableAnimator, the first end-to-end ID-preserving video diffusion framework, which synthesizes high-quality videos without any post-processing, conditioned on a reference image and a sequence of poses. Building upon a video diffusion model, StableAnimator contains carefully designed modules for both training and inference striving for identity consistency. In particular, StableAnimator begins by computing image and face embeddings with off-the- shelf extractors, respectively and face embeddings are further refined by interacting with image embeddings using a global content-aware Face Encoder. Then, StableAnimator introduces a novel distribution-aware ID Adapter that prevents interference caused by temporal layers while preserving ID via alignment. During inference, we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to further enhance the face quality. We demonstrate that solving the HJB equation can be integrated into the diffusion denoising process, and the resulting solution constrains the denoising path and thus benefits ID preservation. Experiments on multiple benchmarks show the effectiveness of StableAnimator both qualitatively and quantitatively.																																	2025-01-08	PPRN:119466593		
J	Li, Mingrui; Liu, Shuhong; Zhou, Heng; Zhu, Guohao; Cheng, Na; Deng, Tianchen; Wang, Hongyu				Li, Ming-Rui/HHN-5152-2022; Deng, Tianchen/KBB-4126-2024						SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM								Arxiv											5	5;2024-11-24;https://www.arxiv.org/abs/2402.03246v6| 4;2024-03-26;https://www.arxiv.org/abs/2402.03246v5| 3;2024-03-13;https://www.arxiv.org/abs/2402.03246v4| 2;2024-03-02;https://www.arxiv.org/abs/2402.03246v3| 1;2024-02-05;https://www.arxiv.org/abs/2402.03246v1	arXiv:2402.03246			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Nov 24 2024	2024	We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian Splatting. It incorporates appearance, geometry, and semantic features through multi-channel optimization, addressing the oversmoothing limitations of neural implicit SLAM systems in high-quality rendering, scene understanding, and object-level geometry. We introduce a unique semantic feature loss that effectively compensates for the shortcomings of traditional depth and color losses in object optimization. Through a semantic-guided keyframe selection strategy, we prevent erroneous reconstructions caused by cumulative errors. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, precise semantic segmentation, and object-level geometric accuracy, while ensuring real-time rendering capabilities.																																	2025-01-08	PPRN:87523583		
J	Liao, Haoran; Wang, Derek S.; Sitdikov, Iskandar; Salcedo, Ciro; Seif, Alireza; Minev, Zlatko K.				Liao, Haoran/OYE-5765-2025						Machine Learning for Practical Quantum Error Mitigation								Arxiv											1	1;2024-11-22;https://www.arxiv.org/abs/2309.17368v2	arXiv:2309.17368			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 22 2024	2024	Quantum computers progress toward outperforming classical supercomputers, but quantum errors remain their primary obstacle. The key to overcoming errors on near-term devices has emerged through the field of quantum error mitigation, enabling improved accuracy at the cost of additional run time. Here, through experiments on state-of-the-art quantum computers using up to 100 qubits, we demonstrate that without sacrificing accuracy machine learning for quantum error mitigation (ML-QEM) drastically reduces the cost of mitigation. We benchmark ML-QEM using a variety of machine learning models -- linear regression, random forests, multi-layer perceptrons, and graph neural networks -- on diverse classes of quantum circuits, over increasingly complex device-noise profiles, under interpolation and extrapolation, and in both numerics and experiments. These tests employ the popular digital zero-noise extrapolation method as an added reference. Finally, we propose a path toward scalable mitigation by using ML-QEM to mimic traditional mitigation methods with superior runtime efficiency. Our results show that classical machine learning can extend the reach and practicality of quantum error mitigation by reducing its overheads and highlight its broader potential for practical quantum computations.																																	2025-08-07	PPRN:123342435		
J	Huang, Zhengchao; Xia, Bin; Lin, Zicheng; Mou, Zhun; Yang, Wenming; Jia, Jiaya				Jia, Jiaya/I-3251-2012; Yang, Wenming/HJH-8634-2023						FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant								Arxiv											2	2;2024-11-21;https://www.arxiv.org/abs/2408.10072v2| 1;2024-08-19;https://www.arxiv.org/abs/2408.10072v1	arXiv:2408.10072			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 21 2024	2024	The rapid advancement of deepfake technologies has sparked widespread public concern, particularly as face forgery poses a serious threat to public information security. However, the unknown and diverse forgery techniques, varied facial features and complex environmental factors pose significant challenges for face forgery analysis. Existing datasets lack descriptive annotations of these aspects, making it difficult for models to distinguish between real and forged faces using only visual information amid various confounding factors. In addition, existing methods fail to yield user-friendly and explainable results, hindering the understanding of the model's decision-making process. To address these challenges, we introduce a novel Open-World Face Forgery Analysis VQA (OW-FFA-VQA) task and its corresponding benchmark. To tackle this task, we first establish a dataset featuring a diverse collection of real and forged face images with essential descriptions and reliable forgery reasoning. Based on this dataset, we introduce FFAA: Face Forgery Analysis Assistant, consisting of a fine-tuned Multimodal Large Language Model (MLLM) and Multi-answer Intelligent Decision System (MIDS). By integrating hypothetical prompts with MIDS, the impact of fuzzy classification boundaries is effectively mitigated, enhancing model robustness. Extensive experiments demonstrate that our method not only provides user-friendly and explainable results but also significantly boosts accuracy and robustness compared to previous methods.																																	2024-12-31	PPRN:91496757		
J	O'Brien, Kyle; Majercak, David; Fernandes, Xavier; Edgar, Richard; Chen, Jingya; Nori, Harsha; Carignan, Dean; Horvitz, Eric; Poursabzi-Sangde, Forough										STEERING LANGUAGE MODEL REFUSAL WITH SPARSE AUTOENCODERS								Arxiv											1	1;2024-11-18;https://www.arxiv.org/abs/2411.11296v1	arXiv:2411.11296			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 18 2024	2024	Responsible practices for deploying language models include guiding models to recognize and refuse answering prompts that are considered unsafe, while complying with safe prompts. Achieving such behavior typically requires updating model weights, which is costly and inflexible. We explore opportunities to steering model activations at inference time, which does not require updating weights. Using sparse autoencoders, we identify and steer features in Phi-3 Mini that mediate refusal behavior. We find that feature steering can improve Phi-3 Mini’s robustness to jailbreak attempts across various harms, including challenging multi-turn attacks. However, we discover that feature steering can adversely affect overall performance on benchmarks. These results suggest that identifying steerable mechanisms for refusal via sparse autoencoders is a promising approach for enhancing language model safety, but that more research is needed to mitigate feature steering’s adverse effects on performance.																																	2024-12-28	PPRN:119261518		
J	He, Yun; Jin, Di; Wang, Chaoqi; Bi, Chloe; Mandyam, Karishma; Zhang, Hejia; Zhu, Chen; Li, Ning; Xu, Tengyu; Lv, Hongjiang; Bhosale, Shruti; Zhu, Chenguang; Sankararaman, Karthik Abinav; Helenowski, Eryk; Kambadur, Melanie; Tayade, Aditya; Ma, Hao; Fang, Han; Wang, Sinong				Xu, Tengyu/ABC-1399-2020; lv, hong/KCK-5097-2024; Wang, Sinong/AAQ-6664-2020						Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions Following								Arxiv											1	1;2024-11-13;https://www.arxiv.org/abs/2410.15553v2	arXiv:2410.15553			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Nov 13 2024	2024	Large Language Models (LLMs) have demonstrated impressive capabilities in various tasks, including instruction following, which is crucial for aligning model outputs with user expectations. However, evaluating LLMs' ability to follow instructions remains challenging due to the complexity and subjectivity of human language. Current benchmarks primarily focus on single-turn, monolingual instructions, which do not adequately reflect the complexities of real-world applications that require handling multi-turn and multilingual interactions. To address this gap, we introduce Multi-IF, a new benchmark designed to assess LLMs' proficiency in following multi-turn and multilingual instructions. Multi-IF, which utilizes a hybrid framework combining LLM and human annotators, expands upon the IFEval by incorporating multi-turn sequences and translating the English prompts into another 7 languages, resulting in a dataset of 4,501 multilingual conversations, where each has three turns. Our evaluation of 14 state-of-the-art LLMs on Multi-IF reveals that it presents a significantly more challenging task than existing benchmarks. All the models tested showed a higher rate of failure in executing instructions correctly with each additional turn. For example, o1-preview drops from 0.877 at the first turn to 0.707 at the third turn in terms of average accuracy over all languages. Moreover, languages with non-Latin scripts (Hindi, Russian, and Chinese) generally exhibit higher error rates, suggesting potential limitations in the models' multilingual capabilities. We release Multi-IF prompts and the evaluation code base to encourage further research in this critical area.																																	2024-12-21	PPRN:119218660		
J	Zhang, Shenao; Yu, Donghan; Sharma, Hiteshi; Zhong, Han; Liu, Zhihan; Yang, Ziyi; Wang, Shuohang; Hassan, Hany; Wang, Zhaoran				YANG, ZIYI/ONK-3216-2025; Zhang, Shenao/HLW-0562-2023; Liu, Zhihan/NGS-3762-2025						Self-Exploring Language Models: Active Preference Elicitation for Online Alignment								Arxiv											3	3;2024-11-05;https://www.arxiv.org/abs/2405.19332v3| 2;2024-10-09;https://www.arxiv.org/abs/2405.19332v2| 1;2024-05-29;https://www.arxiv.org/abs/2405.19332v1	arXiv:2405.19332			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 05 2024	2024	Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings. 																																	2024-12-09	PPRN:89105317		
J	Alamdari, Sarah; Thakkar, Nitya; van den Berg, Rianne; Tenenholtz, Neil; Strome, Robert; Moses, Alan M.; Lu, Alex X; Fusi, Nicolo; Amini, Ava P.; Yang, Kevin K.										Protein generation with evolutionary diffusion: sequence is all you need								bioRxiv											2	2;2024-11-04;https://www.biorxiv.org//content/10.1101/2023.09.11.556673v2| 1;2023-09-12;https://www.biorxiv.org//content/10.1101/2023.09.11.556673v1	10.1101/2023.09.11.556673							preprint	Nov 04 2024	2024	Deep generative models are increasingly powerful tools for the in silico design of novel proteins. Recently, a family of generative models called diffusion models has demonstrated the ability to generate biologically plausible proteins that are dissimilar to any actual proteins seen in nature, enabling unprecedented capability and control in de novo protein design. However, current state-of-the-art diffusion models generate protein structures, which limits the scope of their training data and restricts generations to a small and biased subset of protein design space. Here, we introduce a general-purpose diffusion framework, EvoDiff, that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models for controllable protein generation in sequence space. EvoDiff generates high-fidelity, diverse, and structurally-plausible proteins that cover natural sequence and functional space. We show experimentally that EvoDiff generations express, fold, and exhibit expected secondary structure elements. Critically, EvoDiff can generate proteins inaccessible to structure-based models, such as those with disordered regions, while maintaining the ability to design scaffolds for functional structural motifs. We validate the universality of our sequence-based formulation by experimentally characterizing intrinsically-disordered mitochondrial targeting signals, metal-binding proteins, and protein binders designed using EvoDiff. We envision that EvoDiff will expand capabilities in protein engineering beyond the structure-function paradigm toward programmable, sequence-first design.																																	2024-12-09	PPRN:84948145		
J	Zemlevskiy, Nikita A.										Scalable Quantum Simulations of Scattering in Scalar Field Theory on 120 Qubits								Arxiv											1	1;2024-11-04;https://www.arxiv.org/abs/2411.02486v1	arXiv:2411.02486			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	Simulations of collisions of fundamental particles on a quantum computer are expected to have an exponential advantage over classical methods and promise to enhance searches for new physics. Furthermore, scattering in scalar field theory has been shown to be BQP-complete, making it a representative problem for which quantum computation is efficient. As a step toward large-scale quantum simulations of collision processes, scattering of wavepackets in one-dimensional scalar field theory is simulated using 120 qubits of IBM’s Heron superconducting quantum computer ibm fez. Variational circuits compressing vacuum preparation, wavepacket initialization, and time evolution are determined using classical resources. By leveraging physical properties of states in the theory, such as symmetries and locality, the variational quantum algorithm constructs scalable circuits that can be used to simulate arbitrarily-large system sizes. A new strategy is introduced to mitigate errors in quantum simulations, which enables the extraction of meaningful results from circuits with up to 4924 two-qubit gates and two-qubit gate depths of 103. The effect of interactions is clearly seen, and is found to be in agreement with classical Matrix Product State simulations. The developments that will be necessary to simulate high-energy inelastic collisions on a quantum computer are discussed.																																	2024-12-10	PPRN:119042119		
J	Bai, Yuelin; Du, Xinrun; Liang, Yiming; Jin, Yonggang; Zhou, Junting; Liu, Ziqiang; Fang, Feiteng; Chang, Mingshan; Zheng, Tianyu; Zhang, Xincheng; Ma, Nuo; Wang, Zekun; Yuan, Ruibin; Wu, Haihong; Lin, Hongquan; Huang, Wenhao; Zhang, Jiajun; Lin, Chenghua; Fu, Jie; Yang, Min; Ni, Shiwen; Zhang, Ge				Zheng, Tianyu/OIS-2506-2025; Zheng, Tianyu/JXM-4664-2024; 常, 铭珊/IXX-1480-2023; Wang, ZK/GXV-0814-2022; Wu, Haihong/LDG-4833-2024; Huang, Wenhao/GWU-9337-2022						COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning								Arxiv											2	2;2024-11-02;https://www.arxiv.org/abs/2403.18058v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.18058v1	arXiv:2403.18058			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 02 2024	2024	Remarkable progress on English instruction tuning has facilitated the efficacy and reliability of large language models (LLMs). However, there remains a noticeable gap in instruction tuning for Chinese, where the complex linguistic features pose significant challenges. Existing datasets, generally distilled from English-centric LLMs, are not well-aligned with Chinese users’ interaction patterns. To bridge this gap, we introduce COIG-CQIA, a new Chinese instruction tuning dataset derived from various real-world resources and undergoing rigorous human verification. We conduct extensive experiments on COIG-CQIA, and compare them with strong baseline models and datasets. The experimental results show that models trained on COIG-CQIA achieve highly competitive performance in diverse benchmarks. Additionally, our findings offer several insights for designing effective Chinese instruction-tuning datasets and data-mixing strategies. 																																	2024-12-09	PPRN:88333089		
J	Yang, Xiao; Sun, Kai; Xin, Hao; Sun, Yushi; Bhalla, Nikita; Chen, Xiangsen; Choudhary, Sajal; Gui, Rongze Daniel; Jiang, Ziran Will; Jiang, Ziyu; Kong, Lingkun; Moran, Brian; Wang, Jiaqi; Xu, Yifan Ethan; Yan, An; Yang, Chenyu; Yuan, Eting; Zha, Hanwen; Tang, Nan; Chen, Lei; Scheffer, Nicolas; Liu, Yue; Shah, Nirav; Wanga, Rakesh; Kumar, Anuj; Yih, Wen-tau; Dong, Xin Luna				Yang, Chenyu/LSL-3224-2024; Yan, An/GLT-4045-2022; Sun, Yushi/KIH-7682-2024						CRAG -- Comprehensive RAG Benchmark								Arxiv											1	1;2024-11-01;https://www.arxiv.org/abs/2406.04744v2	arXiv:2406.04744			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 01 2024	2024	Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation of this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve <=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% of questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of participants and submissions. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions. 																																	2024-12-06	PPRN:89243726		
J	Buhl, Marie Davidsen; Sett, Gaurav; Koessler, Leonie; Schuett, Jonas; Anderljung, Markus										Safety cases for frontier AI								Arxiv											1	1;2024-10-28;https://www.arxiv.org/abs/2410.21572v1	arXiv:2410.21572			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 28 2024	2024	As frontier artificial intelligence (AI) systems become more capable, it becomes more important that developers can explain why their systems are sufficiently safe. One way to do so is via safety cases: reports that make a structured argument, supported by evidence, that a system is safe enough in a given operational context. Safety cases are already common in other safety-critical industries such as aviation and nuclear power. In this paper, we explain why they may also be a useful tool in frontier AI governance, both in industry self-regulation and government regulation. We then discuss the practicalities of safety cases, outlining how to produce a frontier AI safety case and discussing what still needs to happen before safety cases can substantially inform decisions. [GRAPHICS]																																	2024-11-30	PPRN:118901877		
J	Chen, Zixuan; He, Xialin; Wang, Yen-Jen; Liao, Qiayuan; Ze, Yanjie; Li, Zhongyu; Sastry, S. Shankar; Wu, Jiajun; Sreenath, Koushil; Gupta, Saurabh; Peng, Xue Bin				Ze, Yanjie/MBV-8320-2025; chen, zixuan/JBJ-4564-2023; Wu, Jiajun/AFA-0504-2022						Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies								Arxiv											2	2;2024-10-28;https://www.arxiv.org/abs/2410.11825v3| 1;2024-10-16;https://www.arxiv.org/abs/2410.11825v2	arXiv:2410.11825			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Oct 28 2024	2024	Reinforcement learning combined with sim-to-real transfer offers a general framework for developing locomotion controllers for legged robots. To facilitate successful deployment in the real world, smoothing techniques, such as low-pass filters and smoothness rewards, are often employed to develop policies with smooth behaviors. However, because these techniques are non-differentiable and usually require tedious tuning of a large set of hyperparameters, they tend to require extensive manual tuning for each robotic platform. To address this challenge and establish a general technique for enforcing smooth behaviors, we propose a simple and effective method that imposes a Lipschitz constraint on a learned policy, which we refer to as Lipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can be implemented in the form of a gradient penalty, which provides a differentiable objective that can be easily incorporated with automatic differentiation frameworks. We demonstrate that LCP effectively replaces the need for smoothing rewards or low-pass filters and can be easily integrated into training frameworks for many distinct humanoid robots. We extensively evaluate LCP in both simulation and real-world humanoid robots, producing smooth and robust locomotion controllers. All simulation and deployment code, along with complete checkpoints, is available 																																	2024-12-06	PPRN:114273541		
J	Nikankin, Yaniv; Reusch, Anja; Mueller, Aaron; Belinkov, Yonatan										Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics								Arxiv											1	1;2024-10-28;https://www.arxiv.org/abs/2410.21272v1	arXiv:2410.21272			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 28 2024	2024	Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a "bag of heuristics".																																	2024-12-06	PPRN:118941528		
J	Zhang, Haochen; Dong, Yuyang; Xiao, Chuan; Oyamada, Masafumi										Large Language Models as Data Preprocessors								Arxiv											2	2;2024-10-27;https://www.arxiv.org/abs/2308.16361v2| 1;2023-08-30;https://www.arxiv.org/abs/2308.16361v1	arXiv:2308.16361			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 27 2024	2024	Large Language Models (LLMs), typified by OpenAI's GPT, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. Aiming at tabular data, we delve into the applicability of state-of-the-art LLMs such as GPT-4 and GPT-4o for a series of preprocessing tasks, including error detection, data imputation, schema matching, and entity matching. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the performance and efficiency of these models. The effectiveness of LLMs in data preprocessing is evaluated through an experimental study spanning a variety of public datasets. GPT-4 emerged as a standout, achieving 100% accuracy or F1 score on 4 of these datasets, suggesting LLMs' immense potential in these tasks. Despite certain limitations, our study underscores the promise of LLMs in this domain and anticipates future developments to overcome current hurdles.																																	2024-12-06	PPRN:84623799		
J	Qin, Yiran; Shi, Zhelun; Yu, Jiwen; Wang, Xijun; Zhou, Enshen; Li, Lijun; Yin, Zhenfei; Liu, Xihui; Sheng, Lu; Shao, Jing; Bai, Lei; Ouyang, Wanli; Zhang, Ruimao				Liu, Xihui/LHA-5141-2024; Qin, Yiran/KIJ-8882-2024						WorldSimBench: Towards Video Generation Models as World Simulators								Arxiv											1	1;2024-10-23;https://www.arxiv.org/abs/2410.18072v1	arXiv:2410.18072			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 23 2024	2024	Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulators. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.																																	2024-11-24	PPRN:118788170		
J	Chen, Hao; Waheed, Abdul; Li, Xiang; Wang, Yidong; Wang, Jindong; Raj, Bhiksha; Abdin, Marah I.				wang, jindong/ACD-8485-2022						On the Diversity of Synthetic Data and its Impact on Training Large Language Models								Arxiv											2	2;2024-10-22;https://www.arxiv.org/abs/2410.15226v2| 1;2024-10-19;https://www.arxiv.org/abs/2410.15226v1	arXiv:2410.15226			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 22 2024	2024	The rise of Large Language Models (LLMs) has accentuated the need for diverse, high-quality pre-training data. Synthetic data emerges as a viable solution to the challenges of data scarcity and inaccessibility. While previous literature has focused predominantly on the quality and quantity of real data, our work enables the measurement of diversity in synthetic data and explores its impact on LLM performance. We study the downstream effects of synthetic data diversity during both the pre-training and fine-tuning stages by introducing a new diversity metric, textit{LLM cluster-agent}, designed to evaluate the diversity of synthetic datasets. Through a series of controlled experiments with models of 350M and 1.4B parameters, we demonstrate that the proposed cluster-based LLM scoring of diversity correlates positively with both pre-training and supervised fine-tuning performance. Our findings also reveal that synthetic data diversity in pre-training affects supervised fine-tuning more significantly than pre-training itself, even for smaller models. We hope this study advances our understanding of the optimal use of synthetic data in LLM training and opens new avenues for efficient data generation processes.																																	2024-11-23	PPRN:118747250		
J	Gandikota, Rohit; Orgad, Hadas; Belinkov, Yonatan; Materzynska, Joanna; Bau, David				Rohit, Gandikota/AAV-5659-2021; Bau, David/KGM-5427-2024						Unified Concept Editing in Diffusion Models								Arxiv											2	2;2024-10-22;https://www.arxiv.org/abs/2308.14761v2| 1;2023-08-25;https://www.arxiv.org/abs/2308.14761v1	arXiv:2308.14761			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 22 2024	2024	Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demonstrating improved efficacy and scalability over prior work. 																																	2024-11-27	PPRN:84523320		
J	Shen, Ruizhe; Chen, Tianqi; Yang, Bo; Lee, Ching Hua				Yang, Bo/AAH-5086-2020; Lee, Ching Hua/U-8215-2019; Chen, Tianqi/AAT-2978-2020						Observation of the non-Hermitian skin effect and Fermi skin on a digital quantum computer								Arxiv											3	3;2024-10-21;https://www.arxiv.org/abs/2311.10143v3| 2;2023-12-17;https://www.arxiv.org/abs/2311.10143v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.10143v1	arXiv:2311.10143			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Oct 21 2024	2024	Non-Hermitian physics has attracted considerable attention in recent years, particularly the non-Hermitian skin effect (NHSE) for its extreme sensitivity and non-locality. While the NHSE has been physically observed in various classical metamaterials and even ultracold atomic arrays, its highly-nontrivial implications in many-body dynamics have never been experimentally investigated. In this work, we report the first observation of the NHSE on a universal quantum processor, as well as its characteristic but elusive Fermi skin from many-fermion statistics. To implement NHSE dynamics on a quantum computer, the effective time-evolution circuit not only needs to be non-reciprocal and non-unitary but must also be scaled up to a sufficient number of lattice qubits to achieve spatial non-locality. We show how such a non-unitary operation can be systematically realized by post-selecting multiple ancilla qubits, as demonstrated through two paradigmatic non-reciprocal models on a noisy IBM quantum processor, with clear signatures of asymmetric spatial propagation and many-body Fermi skin accumulation. To minimize errors from inevitable device noise, time evolution is performed using a trainable, optimized quantum circuit produced with variational quantum algorithms. Our study represents a critical milestone in the quantum simulation of non-Hermitian lattice phenomena on present-day quantum computers and can be readily generalized to more sophisticated many-body models with the remarkable programmability of quantum computers.																																	2024-11-20	PPRN:86200406		
J	Yu, Miao; Wang, Shilong; Zhang, Guibin; Mao, Junyuan; Yin, Chenlong; Liu, Qijiong; Wen, Qingsong; Wang, Kun; Wang, Yang				Wang, Kun/NHP-5583-2025; Liu, Qijiong/OFN-4546-2025; Wen, Qingsong/LTF-7625-2024						NetSafe: Exploring the Topological Safety of Multi-agent Networks								Arxiv											1	1;2024-10-21;https://www.arxiv.org/abs/2410.15686v1	arXiv:2410.15686			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 21 2024	2024	Large language models (LLMs) have empowered nodes within multi-agent networks with intelligence, showing growing applications in both academia and industry. However, how to prevent these networks from generating malicious information remains unexplored with previous research on single LLM's safety be challenging to transfer. In this paper, we focus on the safety of multi-agent networks from a topological perspective, investigating which topological properties contribute to safer networks. To this end, we propose a general framework, NetSafe along with an iterative RelCom interaction to unify existing diverse LLM-based agent frameworks, laying the foundation for generalized topological safety research. We identify several critical phenomena when multi-agent networks are exposed to attacks involving misinformation, bias, and harmful information, termed as Agent Hallucination and Aggregation Safety. Furthermore, we find that highly connected networks are more susceptible to the spread of adversarial attacks, with task performance in a Star Graph Topology decreasing by 29.7%. Besides, our proposed static metrics aligned more closely with real-world dynamic evaluations than traditional graph-theoretic metrics, indicating that networks with greater average distances from attackers exhibit enhanced safety. In conclusion, our work introduces a new topological perspective on the safety of LLM-based multi-agent networks and discovers several unreported phenomena, paving the way for future research to explore the safety of such networks.																																	2024-11-20	PPRN:118754714		
J	Azam, Basim; Akhtar, Naveed				Azam, Basim/IZV-9025-2023; AKHTAR, NAVEED/AAT-1283-2020						Suitability of KANs for Computer Vision: A preliminary investigation								Arxiv											2	2;2024-10-17;https://www.arxiv.org/abs/2406.09087v2| 1;2024-06-13;https://www.arxiv.org/abs/2406.09087v1	arXiv:2406.09087			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling that implements learnable functions on the edges of the networks, diverging from the traditional nodecentric activations in neural networks. This work assesses the applicability and efficacy of KANs in visual modeling, focusing on fundamental recognition and segmentation tasks. We mainly analyze the performance and efficiency of different network architectures built using KAN concepts along with conventional building blocks of convolutional and linear layers, enabling a comparative analysis with the conventional models. Our findings are aimed at contributing to understanding the potential of KANs in computer vision, highlighting both their strengths and areas for further research. Our evaluation 1 point toward the fact that while KAN-based architectures perform in line with the original claims, it may often be important to employ more complex functions on the network edges to retain the performance advantage of KANs on more complex visual data.																																	2024-11-09	PPRN:89302654		
J	Shao, Yijia; Li, Tianshi; Shi, Weiyan; Liu, Yanchen; Yang, Diyi										PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action								Arxiv											2	2;2024-10-17;https://www.arxiv.org/abs/2409.00138v2| 1;2024-08-29;https://www.arxiv.org/abs/2409.00138v1	arXiv:2409.00138			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 17 2024	2024	As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. 																																	2024-11-13	PPRN:91722971		
J	Shao, Zezhi; Wang, Fei; Xu, Yongjun; Wei, Wei; Yu, Chengqing; Zhang, Zhao; Yao, Di; Sun, Tao; Jin, Guangyin; Cao, Xin; Cong, Gao; Jensen, Christian S.; Cheng, Xueqi				Wang, Fei/AAD-7044-2020; Cong, Gao/A-3726-2011; Yao, Di/AAB-1433-2020						Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis								Arxiv											2	2;2024-10-17;https://www.arxiv.org/abs/2310.06119v2| 1;2023-10-09;https://www.arxiv.org/abs/2310.06119v1	arXiv:2310.06119			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 17 2024	2024	Multivariate Time Series (MTS) analysis is crucial to understanding and managing complex systems, such as traffic and energy systems, and a variety of approaches to MTS forecasting have been proposed recently. However, we often observe inconsistent or seemingly contradictory performance findings across different studies. This hinders our understanding of the merits of different approaches and slows down progress. We address the need for means of assessing MTS forecasting proposals reliably and fairly, in turn enabling better exploitation of MTS as seen in different applications. Specifically, we first propose BasicTS+, a benchmark designed to enable fair, comprehensive, and reproducible comparison of MTS forecasting solutions. BasicTS+ establishes a unified training pipeline and reasonable settings, enabling an unbiased evaluation. Second, we identify the heterogeneity across different MTS as an important consideration and enable classification of MTS based on their temporal and spatial characteristics. Disregarding this heterogeneity is a prime reason for difficulties in selecting the most promising technical directions. Third, we apply BasicTS+ along with rich datasets to assess the capabilities of more than 45 MTS forecasting solutions. This provides readers with an overall picture of the cutting-edge research on MTS forecasting. 																																	2024-11-13	PPRN:85521273		
J	Dai, Wenxun; Chen, Ling-Hao; Wang, Jingbo; Liu, Jinpeng; Dai, Bo; Tang, Yansong				Chen, Ling-Hao/JXX-2220-2024						MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model								Arxiv											2	2;2024-10-15;https://www.arxiv.org/abs/2404.19759v2| 1;2024-04-30;https://www.arxiv.org/abs/2404.19759v1	arXiv:2404.19759			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 15 2024	2024	This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatial-temporal control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model. By adopting one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., initial poses) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models for motion generation. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency.																																	2024-11-09	PPRN:88700514		
J	Jiao, Fangkai; Qin, Chengwei; Liu, Zhengyuan; Chen, Nancy F.; Joty, Shafiq				Jiao, Fangkai/HNB-4284-2023; Chen, Nancy/GSD-8813-2022; Liu, Zhengyuan/ABB-5058-2020; qin, chengwei/MBV-9309-2025						Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing								Arxiv											3	3;2024-10-15;https://www.arxiv.org/abs/2402.00658v3| 2;2024-04-15;https://www.arxiv.org/abs/2402.00658v2| 1;2024-02-01;https://www.arxiv.org/abs/2402.00658v1	arXiv:2402.00658			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 15 2024	2024	Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to our synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo. 1																																	2024-11-10	PPRN:87456232		
J	Tang, Wenpin; Zhao, Hanyang				Zhao, Hanyang/HLW-9612-2023						Contractive Diffusion Probabilistic Models								Arxiv											3	3;2024-10-12;https://www.arxiv.org/abs/2401.13115v3| 2;2024-05-23;https://www.arxiv.org/abs/2401.13115v2| 1;2024-01-23;https://www.arxiv.org/abs/2401.13115v1	arXiv:2401.13115			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 12 2024	2024	Diffusion probabilistic models (DPMs) have emerged as a promising technique in generative modeling. The success of DPMs relies on two ingredients: time reversal of diffusion processes and score matching. In view of possibly unguaranteed score matching, we propose a new criterion – the contraction property of backward sampling in the design of DPMs, leading to a novel class of contractive DPMs (CDPMs). Our key insight is that, the contraction property can provably narrow score matching errors and discretization errors, thus our proposed CDPMs are robust to both sources of error. For practical use, we show that CDPM can leverage weights of pretrained DPMs by a simple transformation, and does not need retraining. We corroborated our approach by experiments on synthetic 1-dim examples, Swiss Roll, MNIST, CIFAR-10 32×32  and AFHQ 64×64 dataset. Notably, CDPM steadily improves the performance of baseline score-based diffusion models.																																	2024-10-26	PPRN:87311374		
J	Yan, Yibo; Wang, Shen; Huo, Jiahao; Li, Hang; Li, Boyan; Su, Jiamin; Gao, Xiong; Zhang, Yi-Fan; Xu, Tianlong; Chu, Zhendong; Zhong, Aoxiao; Wang, Kun; Xiong, Hui; Yu, Philip S.; Hu, Xuming; Wen, Qingsong				Wen, Qingsong/LTF-7625-2024; Hu, Xuming/HTS-1538-2023; Yan, Yibo/LOR-9828-2024; LI, Boyan/KJM-0700-2024; Chu, Zhendong/PBV-5776-2025; Xiong, Hui/KIK-5457-2024; Zhang, Yifan/KMY-8838-2024						ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection								Arxiv											2	2;2024-10-08;https://www.arxiv.org/abs/2410.04509v2| 1;2024-10-06;https://www.arxiv.org/abs/2410.04509v1	arXiv:2410.04509			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 08 2024	2024	As the field of Multimodal Large Language Models (MLLMs) continues to evolve, their potential to revolutionize artificial intelligence is particularly promising, especially in addressing mathematical reasoning tasks. Current mathematical benchmarks predominantly focus on evaluating MLLMs’ problem-solving ability, yet there is a crucial gap in addressing more complex scenarios such as error detection, for enhancing reasoning capability in complicated settings. To fill this gap, we formally formulate the new task — multimodal error detection, , and introduce ER- R- RORRADAR, the first benchmark designed to assess MLLMs’ capabilities in such a task. ERROR RADAR evaluates two sub-tasks: error step identification and error categorization, , providing a comprehensive framework for evaluating MLLMs’ complex mathematical reasoning ability. It consists of 2,500 high-quality multimodal K-12 mathematical problems, collected from real-world student interactions in an educational organization, with rigorous annotation and rich metadata such as problem type and error category. Through extensive experiments, we evaluated both open-source and closed-source representative MLLMs, benchmarking their performance against educational expert evaluators. Results indicate significant challenges still remain, as GPT-4o with best performance is still around 10% behind human evaluation. The dataset will be available upon acceptance.																																	2024-10-31	PPRN:104388897		
J	Kim, Yubin; Park, Chanwoo; Jeong, Hyewon; Chan, Yik Siu; Xu, Xuhai; Mcduff, Daniel; Lee, Hyeonhoon; Ghassemi, Marzyeh; Breazeal, Cynthia; Park, Hae Won				Xu, Xuhai/JQK-5168-2023						MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making								Arxiv											2	2;2024-10-04;https://www.arxiv.org/abs/2404.15155v2| 1;2024-04-22;https://www.arxiv.org/abs/2404.15155v1	arXiv:2404.15155			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 04 2024	2024	Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named Medical Decision-making Agents (MDAgents) that helps address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, emulating real-world medical decision-making processes adapted to tasks of varying complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and medical diagnosis benchmarks. MDAgents achieved the best performance in seven out of ten benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant improvement of up to 6.5% (p < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy improvement of 11.8%.																																	2024-10-24	PPRN:88622264		
J	Hahn, Jeremy; Raksit, Arpon; Wilson, Dylan										A motivic filtration on the topological cyclic homology of commutative ring spectra								Arxiv											2	2;2024-10-03;https://www.arxiv.org/abs/2206.11208v2| 1;2022-06-22;https://www.arxiv.org/abs/2206.11208v1	arXiv:2206.11208			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	For a prime number p and a p-quasisyntomic commutative ring R, Bhatt–Morrow–Scholze defined motivic filtrations on the p-completions of THH (R), TC− (R), TP (R), and TC (R), with the associated graded objects for TP (R) and TC (R) recovering the prismatic and syntomic cohomology of R, respectively. We give an alternate construction of these filtrations that applies also when R is a well-behaved commutative ring spectrum; for example, we can take R to be S, MU, ku, ko, or tmf. We compute the mod (p, v1) syntomic cohomology of the Adams summand € and observe that, when p ≥ 3, the motivic spectral sequence for V (1)∗ TC (ℓ) collapses at the E2-page.																																	2024-10-24	PPRN:12186709		
J	Yang, Zitong; Band, Neil; Li, Shuangping; Candes, Emmanuel; Hashimoto, Tatsunori										Synthetic continued pretraining								Arxiv											2	2;2024-10-03;https://www.arxiv.org/abs/2409.07431v2| 1;2024-09-11;https://www.arxiv.org/abs/2409.07431v1	arXiv:2409.07431			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 03 2024	2024	Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient--to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If, instead, the source documents are available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can "rearrange" knowledge to enable more data-efficient learning.																																	2024-10-18	PPRN:91838832		
J	Mousavi, Seyed Mahed; Alghisi, Simone; Riccardi, Giuseppe										DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs								Arxiv											3	3;2024-10-02;https://www.arxiv.org/abs/2404.08700v3| 2;2024-06-12;https://www.arxiv.org/abs/2404.08700v2| 1;2024-04-10;https://www.arxiv.org/abs/2404.08700v1	arXiv:2404.08700			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 02 2024	2024	LLMs acquire knowledge from massive data snapshots collected at different timestamps. Their knowledge is then commonly evaluated using static benchmarks. However, factual knowledge is generally subject to time-sensitive changes, and static benchmarks cannot address those cases. We present an approach to dynamically evaluate the knowledge in LLMs and their time-sensitiveness against Wikidata, a publicly available up-to-date knowledge graph. We evaluate the time-sensitive knowledge in twenty-four private and open-source LLMs, as well as the effectiveness of four editing methods in updating the outdated facts. Our results show that 1) outdatedness is a critical problem across state-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with slight variations of the question prompt; and 3) the performance of the state-of-the-art knowledge editing algorithms is very limited, as they can not reduce the cases of outdatedness and output inconsistency.																																	2024-10-16	PPRN:88529600		
J	Nicole, El Karoui; Tan, Xiaolu				Tan, Xiaolu/IVU-7868-2023						Capacities, Measurable Selection and Dynamic Programming Part II: Application in Stochastic Control Problems								Arxiv											2	2;2024-10-02;https://www.arxiv.org/abs/1310.3364v3| 1;2015-11-17;https://www.arxiv.org/abs/1310.3364v2	arXiv:1310.3364			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 02 2024	2024	We provide an overview on how to use the measurable selection techniques to derive the dynamic programming principle for a general stochastic optimal control/stopping problem. By considering its martingale problem formulation on the canonical space of paths, one can check the required measurability conditions. This covers in particular the most classical controlled/stopped diffusion processes problems. Further, we study the approximation property of the optimal control problems by piecewise constant control problems. As a byproduct, we obtain an equivalence result of the strong, weak and relaxed formulations of the controlled/stopped diffusion processes problem.																																	2024-10-14	PPRN:12059336		
J	Gu, Wei; Mihalcea, Leonardo C.; Sharpe, Eric; Zou, Hao				Sharpe, Eric/M-2889-2016						Quantum K theory of Grassmannians, Wilson line operators, and Schur bundles								Arxiv											2	2;2024-09-23;https://www.arxiv.org/abs/2208.01091v3| 1;2022-08-01;https://www.arxiv.org/abs/2208.01091v1	arXiv:2208.01091			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 23 2024	2024	We prove a `Whitney' presentation, and a 'Coulomb branch' presentation, for the torus equivariant quantum K theory of the Grassmann manifold Gr(k; n), inspired from physics, and stated in an earlier paper. The first presentation is obtained by quantum deforming the product of the Hirzebruch λy classes of the tautological bundles. In physics, the λy classes arise as certain Wilson line operators. The second presentation is obtained from the Coulomb branch equations involving the partial derivatives of a twisted superpotential from supersymmetric gauge theory. This is closest to a presentation obtained by Gorbounov and Korff, utilizing integrable systems techniques. Algebraically, we relate the Coulomb and Whitney presentations utilizing transition matrices from the (equivariant) Grothendieck polynomials to the (equivariant) complete homogeneous symmetric polynomials. Along the way, we calculate K-theoretic Gromov-Witten invariants of wedge powers of the tautological bundles on Gr(k; n), using the `quantum=classical' statement.																																	2024-10-07	PPRN:11870041		
J	Dorka, Nicolai										QUANTILE REGRESSION FOR DISTRIBUTIONAL REWARD MODELS IN RLHF								Arxiv											1	1;2024-09-16;https://www.arxiv.org/abs/2409.10164v1	arXiv:2409.10164			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 16 2024	2024	Reinforcement learning from human feedback (RLHF) has become a key method for aligning large language models (LLMs) with human preferences through the use of reward models. However, traditional reward models typically generate point estimates, which oversimplify the diversity and complexity of human values and preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel approach to reward modeling that learns a distribution over rewards instead of a single scalar value. Our method uses quantile regression to estimate a full, potentially multimodal distribution over preferences, providing a more powerful and nuanced representation of preferences. This distributional approach can better capture the diversity of human values, addresses label noise, and accommodates conflicting preferences by modeling them as distinct modes in the distribution. Our experimental results show that QRM outperforms comparable traditional point-estimate models on RewardBench. Furthermore, we demonstrate that the additional information provided by the distributional estimates can be utilized in downstream applications, such as risk-aware reinforcement learning, resulting in LLM policies that generate fewer extremely negative responses. 																																	2024-12-24	PPRN:119223101		
J	Crane, Eleanor; Smith, Kevin C.; Tomesh, Teague; Eickbusch, Alec; Martyn, John M.; Kuehn, Stefan; Funcke, Lena; Demarco, Michael Austin; Chuang, Isaac L.; Wiebe, Nathan; Schuckert, Alexander; Girvin, Steven M.				Girvin, Steven/C-1471-2012; Funcke, Lena/AAV-6911-2021						Hybrid Oscillator-Qubit Quantum Processors: Simulating Fermions, Bosons, and Gauge Fields								Arxiv											1	1;2024-09-05;https://www.arxiv.org/abs/2409.03747v1	arXiv:2409.03747			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 05 2024	2024	We develop a hybrid oscillator-qubit processor framework for quantum simulation of strongly correlated fermions and bosons that avoids the boson-to-qubit mapping overhead encountered in qubit hardware. This framework gives exact decompositions of particle interactions such as density-density terms and gauge-invariant hopping, as well as approximate methods based on the Baker-Campbell Hausdorff formulas including the magnetic field term for the U(1) quantum link model in (2 + 1)D. We use this framework to show how to simulate dynamics using Trotterisation, perform ancilla-free partial error detection using Gauss’s law, measure non-local observables, estimate ground state energies using a oscillator-qubit variational quantum eigensolver as well as quantum signal processing, and we numerically study the influence of hardware errors in circuit QED experiments. To show the advantages over all-qubit hardware, we perform an end-to-end comparison of the gate complexity for the gauge-invariant hopping term and find an improvement of the asymptotic scaling with the boson number cutoff S from O(log(S) 2) to O (1) in our framework as well as, for bosonic matter, a constant factor improvement of better than 104 . We also find an improvement from O(log( S)) to O (1) for the U(1) magnetic field term. While our work focusses on an implementation in superconducting hardware, our framework can also be used in trapped ion, and neutral atom hardware. This work establishes digital quantum simulation with hybrid oscillator-qubit hardware as a viable and advantageous method for the study of qubit-b oson models in materials science, chemistry, and high-energy physics.																																	2024-09-24	PPRN:91750151		
J	Ryee, Siheon; Witt, Niklas; Wehling, Tim O.				Wehling, Tim/O-4642-2014						Quenched pair breaking by interlayer correlations as a key to superconductivity in La3Ni2O7								Arxiv											4	4;2024-08-30;https://www.arxiv.org/abs/2310.17465v5| 3;2024-07-29;https://www.arxiv.org/abs/2310.17465v4| 2;2024-03-13;https://www.arxiv.org/abs/2310.17465v2| 1;2023-10-26;https://www.arxiv.org/abs/2310.17465v1	arXiv:2310.17465			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 30 2024	2024	The recent discovery of superconductivity in La3Ni2O7 with Tc ≃ 80 K under high pressure opens up a new route to high- T c superconductivity. This material realizes a bilayer square lattice model featuring a strong interlayer hybridization unlike many unconventional superconductors. A key question in this regard concerns how electronic correlations driven by the interlayer hybridization affect the low-energy electronic structure and the concomitant superconductivity. Here, we demonstrate using a cluster dynamical mean-field theory that the interlayer electronic correlations (IECs) induce a Lifshitz transition resulting in a change of Fermi surface topology. By solving an appropriate gap equation, we further show that the leading pairing instability, s ±- wave, is enhanced by the IECs. The underlying mechanism is the quenching of a strong ferromagnetic channel, resulting from the Lifshitz transition driven by the IECs. Based on this picture, we provide a possible reason of why superconductivity emerges only under high pressure.																																	2024-09-07	PPRN:85821642		
J	Ye, Tian; Xu, Zicheng; Li, Yuanzhi; Allen-Zhu, Zeyuan				Ye, Tianyong/MSX-9881-2025						Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems								Arxiv											1	1;2024-08-29;https://www.arxiv.org/abs/2408.16293v1	arXiv:2408.16293			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 29 2024	2024	Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to "self-correct" their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating "error-correction" data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others.																																	2024-09-23	PPRN:91783365		
J	Ren, Yixuan; Zhou, Yang; Yang, Jimei; Shi, Jing; Liu, Difan; Liu, Feng; Kwon, Mingi; Shrivastava, Abhinav				Shrivastava, Abhinav/IQR-5079-2023; Zhou, Yang/ABA-8626-2020						Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models								Arxiv											2	2;2024-08-28;https://www.arxiv.org/abs/2402.14780v3| 1;2024-02-22;https://www.arxiv.org/abs/2402.14780v1	arXiv:2402.14780			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 28 2024	2024	Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot video motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapts it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling. To disentangle the spatial and temporal information during training, we introduce a novel concept of appearance absorbers that detach the original appearance from the reference video prior to motion learning. The proposed modules are trained in a staged pipeline and inferred in a plug-and-play fashion, enabling easy extensions to various downstream tasks such as custom video generation and editing, video appearance customization and multiple motion combination. 																																	2024-09-19	PPRN:87799850		
J	Li, Yunxin; Wang, Longyue; Hu, Baotian; Chen, Xinyu; Zhong, Wanqi; Lyu, Chenyang; Wang, Wei; Zhang, Min				Hu, Baotian/AAA-4102-2022; Wang, Wei/JGL-6789-2023						A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering								Arxiv											3	3;2024-08-24;https://www.arxiv.org/abs/2311.07536v3| 2;2024-01-27;https://www.arxiv.org/abs/2311.07536v2| 1;2023-11-13;https://www.arxiv.org/abs/2311.07536v1	arXiv:2311.07536			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 24 2024	2024	The emergence of multimodal large models (MLMs) has significantly advanced the field of visual understanding, offering remarkable capabilities in the realm of visual question answering (VQA). Yet, the true challenge lies in the domain of knowledge-intensive VQA tasks, which necessitate not just recognition of visual elements, but also a deep comprehension of the visual information in conjunction with a vast repository of learned knowledge. To uncover such capabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we provide an in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which assesses how well models can understand visual cues and connect to general knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in reasoning out specific knowledge from images, showcasing their proficiency across various specialized fields; 3) Comprehensive Knowledge with Decision-making Rationales, which examines model's capability to provide logical explanations for its inference, facilitating a deeper analysis from the interpretability perspective. Additionally, we utilize a visual knowledge-enhanced training strategy and multimodal retrieval-augmented generation approach to enhance MLMs, highlighting the future need for advancements in this research direction. Extensive experiments indicate that: a) GPT-4V demonstrates enhanced explanation generation when using composite images as few-shots; b) GPT-4V and other MLMs produce severe hallucinations when dealing with world knowledge; c) Visual knowledge enhanced training and prompting technicals present potential to improve performance.																																	2024-09-04	PPRN:86138325		
J	Frantar, Elias; Castro, Roberto L.; Chen, Jiale; Hoefler, Torsten; Alistarh, Dan				Chen, Jiale/HPE-9186-2023; Hoefler, Torsten/HKF-3023-2023						MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large Language Models								Arxiv											1	1;2024-08-21;https://www.arxiv.org/abs/2408.11743v1	arXiv:2408.11743			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 21 2024	2024	As inference on Large Language Models (LLMs) emerges as an important workload in machine learning applications, weight quantization has become a standard technique for efficient GPU deployment. Quantization not only reduces model size, but has also been shown to yield substantial speedups for single-user inference, due to reduced memory movement, with low accuracy impact. Yet, it remains open whether speedups are achievable also in batched settings with multiple parallel clients, which are highly relevant for practical serving. It is unclear whether GPU kernels can be designed to remain practically memory-bound, while supporting the substantially increased compute requirements of batched workloads. This paper resolves this question positively by describing the design of Mixed-precision AutoRegressive LINear kernels, called MARLIN. Concretely, given a model whose weights are compressed via quantization to, e.g., 4 bits per element, MARLIN shows that batchsizes up to 1632 can be supported with close to maximum (4×) 4 × ) quantization speedup, and larger batchsizes up to 64-128 with gradually decreasing, but still significant, acceleration. MARLIN accomplishes this via a combination of techniques, such as asynchronous memory access, complex task scheduling and pipelining, and bespoke quantization support. Our experiments show that MARLIN’s nearoptimal performance on individual LLM layers across different scenarios can also lead to endto-end LLM inference speedups (of up to 2 . 8 × ) when integrated with the popular vLLM serving engine. Finally, MARLIN is extensible to further compression techniques, like NVIDIA 2:4 sparsity, leading to additional speedups.																																	2024-08-31	PPRN:91500466		
J	Phan, Huy N.; Phan, Hoang N.; Nguyen, Tien N.; Bui, Nghi D.Q.										RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion								Arxiv											3	3;2024-08-14;https://www.arxiv.org/abs/2403.06095v4| 2;2024-03-16;https://www.arxiv.org/abs/2403.06095v2| 1;2024-03-10;https://www.arxiv.org/abs/2403.06095v1	arXiv:2403.06095			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Aug 14 2024	2024	Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present tool, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHYPER is the {em Repo-level Semantic Graph} (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that tool markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines. 																																	2024-08-22	PPRN:88098986		
J	Tang, Zhicong; Gu, Shuyang; Wang, Chunyu; Zhang, Ting; Bao, Jianmin; Chen, Dong; Guo, Baining				Li, Tingting/AFT-3796-2022; Zhang, Bo/AAN-7181-2020						VolumeDiffusion: Flexible Text-to-3D Generation with Efficient Volumetric Encoder								Arxiv											3	3;2024-08-13;https://www.arxiv.org/abs/2312.11459v3| 2;2024-04-29;https://www.arxiv.org/abs/2312.11459v2| 1;2023-12-18;https://www.arxiv.org/abs/2312.11459v1	arXiv:2312.11459			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 13 2024	2024	This paper introduces a pioneering 3D volumetric encoder designed for text-to-3D generation. To scale up the training data for the diffusion model, a lightweight network is developed to efficiently acquire feature volumes from multi-view images. The 3D volumes are then trained on a diffusion model for text-to-3D generation using a 3D U-Net. This research further addresses the challenges of inaccurate object captions and high-dimensional feature volumes. The proposed model, trained on the public Objaverse dataset, demonstrates promising outcomes in producing diverse and recognizable samples from text prompts. Notably, it empowers finer control over object part characteristics through textual cues, fostering model creativity by seamlessly combining multiple concepts within a single object. This research significantly contributes to the progress of 3D generation by introducing an efficient, flexible, and scalable representation methodology.																																	2024-08-22	PPRN:86696755		
J	Ma, Xinbei; Wang, Yiting; Yao, Yao; Yuan, Tongxin; Zhang, Aston; Zhang, Zhuosheng; Zhao, Hai				Zhang, Zhuosheng/AAF-4919-2020; Wang, Yi-Ting/ABE-2435-2021; Yao, Yao/OCL-4665-2025						Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions								Arxiv											2	2;2025-09-05;https://www.arxiv.org/abs/2408.02544v3| 1;2024-08-05;https://www.arxiv.org/abs/2408.02544v1	arXiv:2408.02544			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 05 2024	2024	This paper investigates the faithfulness of multimodal large language model (MLLM) agents in the graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general setting is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using our simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness (i.e., action accuracy) of multimodal agents, our findings indicate that these agents are prone to environmental distractions, resulting in unfaithful behaviors. Furthermore, we switch to the adversarial perspective and implement environment injection, demonstrating that such unfaithfulness can be exploited, leading to unexpected risks.																																	2024-08-11	PPRN:91247371		
J	Zeng, Yi; Yang, Yu; Zhou, Andy; Tan, Jeffrey Ziwei; Tu, Yuheng; Mai, Yifan; Klyman, Kevin; Pan, Minzhou; Jia, Ruoxi; Song, Dawn; Liang, Percy; Li, Bo				zeng, yi/KFS-5661-2024						AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies								Arxiv											1	1;2024-08-05;https://www.arxiv.org/abs/2407.17436v2	arXiv:2407.17436			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 05 2024	2024	Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.2																																	2024-08-13	PPRN:91260598		
J	Sawarkar, Kunal; Mangal, Abhilasha; Solanki, Shivam Raj										Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers								Arxiv											2	2;2024-08-04;https://www.arxiv.org/abs/2404.07220v2| 1;2024-03-22;https://www.arxiv.org/abs/2404.07220v1	arXiv:2404.07220			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Aug 04 2024	2024	Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the 'Blended RAG' method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.																																	2024-08-11	PPRN:88500262		
J	Copetti, Christian										Defect Charges, Gapped Boundary Conditions, and the Symmetry TFT								Arxiv											1	1;2024-08-02;https://www.arxiv.org/abs/2408.01490v1	arXiv:2408.01490			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 02 2024	2024	We offer a streamlined and computationally powerful characterization of higher representations (higher charges) for defect operators under generalized symmetries, employing the powerful framework of Symmetry TFT Z(C). For a defect D of codimension p, these representations (charges) are in one-to-one correspondence with gapped boundary conditions for the SymTFT Z ( C ) on a manifold Y = Σd−p+1 × Sp−1 , and can be efficiently described through dimensional reduction. We explore numerous applications of our construction, including scenarios where an anomalous bulk theory can host a symmetric defect. This generalizes the connection between ’t Hooft anomalies and the absence of symmetric boundary conditions to defects of any codimension. Finally we describe some properties of surface charges for 3 + 1d duality symmetries, which should be relevant to the study of Gukov-Witten operators in gauge theories.																																	2024-08-09	PPRN:91244585		
J	Liu, Aiwei; Pan, Leyi; Lu, Yijian; Li, Jingjing; Hu, Xuming; Zhang, Xi; Wen, Lijie; King, Irwin; Xiong, Hui; Yu, Philip S.				Li, Jingjing/JDM-6942-2023; Hu, Xuming/HTS-1538-2023; King, Irwin/C-9681-2015						A Survey of Text Watermarking in the Era of Large Language Models								Arxiv											2	2;2024-08-02;https://www.arxiv.org/abs/2312.07913v6| 1;2024-08-01;https://www.arxiv.org/abs/2312.07913v5	arXiv:2312.07913			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 02 2024	2024	Text watermarking algorithms are crucial for protecting the copyright of textual content. Historically, their capabilities and application scenarios were limited. However, recent advancements in large language models (LLMs) have revolutionized these techniques. LLMs not only enhance text watermarking algorithms with their advanced abilities but also create a need for employing these algorithms to protect their own copyrights or prevent potential misuse. This paper conducts a comprehensive survey of the current state of text watermarking technology, covering four main aspects: (1) an overview and comparison of different text watermarking techniques; (2) evaluation methods for text watermarking algorithms, including their detectability, impact on text or LLM quality, robustness under target or untargeted attacks; (3) potential application scenarios for text watermarking technology; (4) current challenges and future directions for text watermarking. This survey aims to provide researchers with a thorough understanding of text watermarking technology in the era of LLM, thereby promoting its further advancement.																																	2024-08-21	PPRN:91261010		
J	Choshen, Leshem; Cotterell, Ryan; Hu, Michael Y.; Linzen, Tal; Mueller, Aaron; Ross, Candace; Warstadt, Alex; Wilcox, Ethan; Williams, Adina; Zhuang, Chengxu										[Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus								Arxiv											2	2;2024-07-27;https://www.arxiv.org/abs/2404.06214v2| 1;2024-04-09;https://www.arxiv.org/abs/2404.06214v1	arXiv:2404.06214			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 27 2024	2024	After last year’s successful BabyLM Challenge, the competition will be hosted again in 2024/2025. The overarching goals of the challenge remain the same; however, some of the competition rules will be different. The big changes for this year’s competition are as follows: First, we replace the loose track with a paper track , which allows (for example) nonmodel-based submissions, novel cognitivelyinspired benchmarks, or analysis techniques. Second, we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget. Third, we introduce a multimodal visionand-language track , and will release a corpus of 50% text-only and 50% image–text multimodal data as a starting point for LM model training. The purpose of this CfP is to provide rules for this year’s challenge, explain these rule changes and their rationale in greater detail, give a timeline of this year’s competition, and provide answers to frequently asked questions from last year’s challenge.																																	2024-08-06	PPRN:88466146		
J	Huang, Jing; Yang, Diyi; Potts, Christopher										Demystifying Verbatim Memorization in Large Language Models								Arxiv											1	1;2024-07-25;https://www.arxiv.org/abs/2407.17817v1	arXiv:2407.17817			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 25 2024	2024	Large Language Models (LLMs) frequently memorize long sequences verbatim, often with serious legal and privacy implications. Much prior work has studied such verbatim memorization using observational data. To complement such work, we develop a framework to study verbatim memorization in a controlled setting by continuing pre-training from Pythia checkpoints with injected sequences. We find that (1) non-trivial amounts of repetition are necessary for verbatim memorization to happen; (2) later (and presumably better) checkpoints are more likely to verbatim memorize sequences, even for out-of-distribution sequences; (3) the generation of memorized sequences is triggered by distributed model states that encode high-level features and makes important use of general language modeling capabilities. Guided by these insights, we develop stress tests to evaluate unlearning methods and find they often fail to remove the verbatim memorized information, while also degrading the LM. Overall, these findings challenge the hypothesis that verbatim memorization stems from specific model weights or mechanisms. Rather, verbatim memorization is intertwined with the LM's general capabilities and thus will be very difficult to isolate and suppress without degrading model quality.																																	2024-08-02	PPRN:91102660		
J	Poulin, Vivian; Smith, Tristan L.; Calderon, Rodrigo; Simon, Theo				Calderon, Rodrigo/LIH-6706-2024						On the implications of the 'cosmic calibration tension' beyond&nbsp;<italic>H</italic>0&nbsp;and the synergy between early- and late-time new physics								Arxiv											1	1;2024-07-25;https://www.arxiv.org/abs/2407.18292v1	arXiv:2407.18292			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 25 2024	2024	The “cosmic calibration tension” is a > 5 σ discrepancy between the cosmological distance ladder built from baryonic acoustic oscillations (BAO) calibrated by the Planck/ΛCDM sound horizon (rs) and Type Ia supernovae (SN1a) calibrated instead with the S H 0 ES absolute magnitude, assuming the distance-duality relationship (DDR) holds. In this work, we emphasize the consequences of this tension beyond the value of the Hubble constant H0, and the implications for physics beyond ΛCDM. Of utmost importance, it implies a larger physical matter density ωm = Ωmh2, as both the fractional matter density Ωm and h = H0/100 km/s/Mpc are well constrained from late-time data. New physics in the pre-recombination era must thus be able to decrease r s while either reducing the value of Ωm, or increasing the value of ωm. Assuming a ΛCDM-like primordial power spectrum, this necessarily results in an increase in the clustering amplitude σ 8 . Deviations from ΛCDM in the latetime expansion history cannot resolve the calibrator tension but can help relax the required shifts to the matter density and σ8: it is in that sense that a combination of early and late-time new physics may help alleviate the tension . More precisely, models that modify the pre-recombination expansion history can accommodate the increase in ωm without the need for additional modifications. It is those models which only affect recombination that require additional deviations at late-times to be successful. Hence, the “cosmic calibration tension” points either to a targeted modification of the pre-recombination expansion history, or to a broader change affecting multiple cosmic epochs.																																	2024-08-02	PPRN:91118863		
J	Sabour, Sahand; Liu, Siyang; Zhang, Zheyuan; Liu, June M.; Zhou, Jinfeng; Sunaryo, Alvionna S.; Li, Juanzi; Lee, Tatia M.C.; Mihalcea, Rada; Huang, Minlie				Li, Zhiyuan/ESQ-7168-2022; Zhou, Jinfeng/JYP-1934-2024						EmoBench: Evaluating the Emotional Intelligence of Large Language Models								Arxiv											3	3;2024-07-17;https://www.arxiv.org/abs/2402.12071v3| 2;1800-01-01;https://www.arxiv.org/abs/2402.12071v2| 1;2024-02-19;https://www.arxiv.org/abs/2402.12071v1	arXiv:2402.12071			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 17 2024	2024	Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. 																																	2024-07-26	PPRN:87758993		
J	Manolescu, Ciprian; Ozsvath, Peter										Heegaard Floer homology and integer surgeries on links								Arxiv											2	2;2024-07-16;https://www.arxiv.org/abs/1011.1317v6| 1;2017-04-03;https://www.arxiv.org/abs/1011.1317v4	arXiv:1011.1317			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 16 2024	2024	Let L be a link in an integral homology three-sphere. We give a description of the Heegaard Floer homology of integral surgeries on L in terms of some data associated to L , which we call a complete system of hyperboxes for L . Roughly, a complete systems of hyperboxes consists of chain complexes for (some versions of) the link Floer homology of L and all its sublinks, together with several chain maps between these complexes. Further, we introduce a way of presenting closed four-manifolds with b+2 ≥ 2 by four-colored framed links in the three-sphere. Given a link presentation of this kind for a four-manifold X , we then describe the Ozsvath-Szabo mixed invariants of X in terms of a complete system of hyperboxes for the link. Finally, we explain how a grid diagram produces a particular complete system of hyperboxes for the corresponding link.																																	2024-12-06	PPRN:12427351		
J	Shehzad, Ahsan; Xia, Feng; Abid, Shagufta; Peng, Ciyuan; Yu, Shuo; Zhang, Dongyu; Verspoor, Karin				Peng, Ciyuan/HHN-2745-2022; Verspoor, Karin/G-6034-2016; Zhang, Dongyu/MYR-3898-2025; Shehzad, Ahsan/ABQ-5039-2022						Graph Transformers: A Survey								Arxiv											1	1;2024-07-13;https://www.arxiv.org/abs/2407.09777v1	arXiv:2407.09777			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 13 2024	2024	Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.																																	2024-07-27	PPRN:91019452		
J	Baumli, Kate; Baveja, Satinder; Behbahani, Feryal; Chan, Harris; Comanici, Gheorghe; Flennerhag, Sebastian; Gazeau, Maxime; Holsheimer, Kristian; Horgan, Dan; Laskin, Michael; Lyle, Clare; Masoom, Hussain; McKinney, Kay; Mnih, Volodymyr; Neitz, Alexander; Nikulin, Dmitry; Pardo, Fabio; Parker-Holder, Jack; Quan, John; Rocktaschel, Tim; Sahni, Himanshu; Schaul, Tom; Schroecker, Yannick; Spencer, Stephen; Steigerwald, Richie; Wang, Luyu; Zhang, Lei				Schaul, Tom/C-4349-2011						Vision-Language Models as a Source of Rewards								Arxiv											3	3;2024-07-12;https://www.arxiv.org/abs/2312.09187v3| 2;2024-02-22;https://www.arxiv.org/abs/2312.09187v2| 1;2023-12-14;https://www.arxiv.org/abs/2312.09187v1	arXiv:2312.09187			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 12 2024	2024	Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.																																	2024-07-23	PPRN:86587366		
J	Kenton, Zachary; Siegel, Noah Y.; Kramar, Janos; Brown-Cohen, Jonah; Albanie, Samuel; Bulian, Jannis; Agarwal, Rishabh; Lindner, David; Tang, Yunhao; Goodman, Noah D.; Shah, Rohin										On scalable oversight with weak LLMs judging strong LLMs								Arxiv											2	2;2024-07-12;https://www.arxiv.org/abs/2407.04622v2| 1;2024-07-05;https://www.arxiv.org/abs/2407.04622v1	arXiv:2407.04622			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 12 2024	2024	Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. In this paper we study debate, where two AI's compete to convince a judge; consultancy, where a single AI tries to convince a judge that asks questions; and compare to a baseline of direct question-answering, where the judge just answers outright without the AI. We use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. We benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. We find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed. Previous work assigned debaters/consultants an answer to argue for. When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy. Further, we find that stronger debater models increase judge accuracy, though more modestly than in previous studies.																																	2024-07-23	PPRN:90726259		
J	Kunstner, Frederik; Yadav, Robin; Milligan, Alan; Schmidt, Mark; Bietti, Alberto				Kunstner, Fred/LDG-3559-2024						Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models								Arxiv											2	2;2024-07-12;https://www.arxiv.org/abs/2402.19449v2| 1;2024-02-29;https://www.arxiv.org/abs/2402.19449v1	arXiv:2402.19449			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	Adam has been shown to outperform gradient descent on large language models by a larger margin than on other tasks, but it is unclear why. We show that a key factor in this performance gap is the heavy-tailed class imbalance found in language tasks. When trained with gradient descent, the loss of infrequent words decreases more slowly than the loss of frequent ones. This leads to a slow decrease on the average loss as most samples come from infrequent words. On the other hand, Adam and sign-based methods are less sensitive to this problem. To establish that this behavior is caused by class imbalance, we show empirically that it can be reproduced across architectures and data types, on language transformers, vision CNNs, and linear models. On a linear model with cross-entropy loss, we show that class imbalance leads to imbalanced, correlated gradients and Hessians that have been hypothesized to benefit Adam. We also prove that, in continuous time, gradient descent converges slowly on low-frequency classes while sign descent does not.																																	2024-07-23	PPRN:87990784		
J	Li, Qingyun; Chen, Zhe; Wang, Weiyun; Wang, Wenhai; Ye, Shenglong; Jin, Zhenjiang; Chen, Guanzhou; He, Yinan; Gao, Zhangwei; Cui, Erfei; Yu, Jiashuo; Tian, Hao; Zhou, Jiasheng; Xu, Chao; Wang, Bin; Wei, Xingjian; Li, Wei; Zhang, Wenjian; Zhang, Bo; Cai, Pinlong; Wen, Licheng; Yan, Xiangchao; Li, Zhenxiang; Chu, Pei; Wang, Yi; Dou, Min; Tian, Changyao; Zhu, Xizhou; Lu, Lewei; Chen, Yushi; He, Junjun; Tu, Zhongying; Lu, Tong; Wang, Yali; Wang, Limin; Lin, Dahua; Qiao, Yu; Shi, Botian; He, Conghui; Dai, Jifeng				Dai, Jifeng/HGU-8741-2022; Lin, Dahua/W-6576-2019; Shi, Botian/HTT-0363-2023; Wang, Wen-Jing/HOH-7164-2023; Zhang, Bo/ABF-8476-2021; He, Conghui/AAZ-3323-2021; Cai, Pinlong/P-6490-2017; Qiao, Yu/ABD-5787-2021; Chen, Yushi/AFG-5685-2022; Li, Qing-Yun/AFX-5150-2022; TIAN, Hao/GMU-6101-2022; Chen, Guanzhou/JAX-4341-2023; zhou, jiasheng/AAA-5906-2021; Ye, Shenglong/NFT-0127-2025; Wang, Limin/AAE-3419-2019; Wang, Bin/MVU-8917-2025						OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text								Arxiv											3	3;2024-07-12;https://www.arxiv.org/abs/2406.08418v3| 2;2024-06-13;https://www.arxiv.org/abs/2406.08418v2| 1;2024-06-12;https://www.arxiv.org/abs/2406.08418v1	arXiv:2406.08418			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 12 2024	2024	Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits. Recent studies have shown that such data aids multimodal in-context learning and maintains the capabilities of large language models during multimodal fine-tuning. However, the limited scale and diversity of current image-text interleaved data restrict the development of multimodal large language models. In this paper, we introduce OmniCorpus, a 10 billion-scale image-text interleaved dataset. Using an efficient data engine, we filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion text tokens. Compared to counterparts (e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while maintaining good data quality; 2) features more diverse sources, including both English and non-English websites as well as video-centric websites; 3) is more flexible, easily degradable from an image-text interleaved format to pure text corpus and image-text pairs. Through comprehensive analysis and experiments, we validate the quality, usability, and effectiveness of the proposed dataset. We hope this could provide a solid data foundation for future multimodal model research. Code and data are released at https://github.com/OpenGVLab/OmniCorpus.																																	2024-07-23	PPRN:89290989		
J	Liu, Yixin; Fabbri, Alexander R.; Chen, Jiawen; Zhao, Yilun; Han, Simeng; Joty, Shafiq; Liu, Pengfei; Radev, Dragomir; Wu, Chien-Sheng; Cohan, Arman				Liu, Pengfei/JUV-0307-2023; Wu, Chien-Sheng/AAH-1354-2019; Zhao, Ziang/IAR-5845-2023; Radev, Dragomir/E-9641-2012; Liu, Yixin/ABC-7725-2021						Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization								Arxiv											2	2;2024-07-12;https://www.arxiv.org/abs/2311.09184v2| 1;2023-11-15;https://www.arxiv.org/abs/2311.09184v1	arXiv:2311.09184			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 12 2024	2024	While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction.																																	2024-07-23	PPRN:86160227		
J	He, Wanggui; Fu, Siming; Liu, Mushui; Wang, Xierui; Xiao, Wenyi; Shu, Fangxun; Wang, Yi; Zhang, Lei; Yu, Zhelun; Li, Haoyuan; Huang, Ziwei; Gan, Leilei; Jiang, Hao				Huang, Ziwei/IXW-8363-2023; Xiao, Wenyi/LPP-6401-2024						MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image Synthesis								Arxiv											1	1;2024-07-11;https://www.arxiv.org/abs/2407.07614v2	arXiv:2407.07614			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 11 2024	2024	Auto-regressive models have made significant progress in the realm of language generation, yet they do not perform on par with diffusion models in the domain of image synthesis. In this work, we introduce MARS, a novel framework for T2I generation that incorporates a specially designed Semantic Vision-Language Integration Expert (SemVIE). This innovative component integrates pre-trained LLMs by independently processing linguistic and visual information, freezing the textual component while fine-tuning the visual component. This methodology preserves the NLP capabilities of LLMs while imbuing them with exceptional visual understanding. Building upon the powerful base of the pre-trained Qwen-7B, MARS stands out with its bilingual generative capabilities corresponding to both English and Chinese language prompts and the capacity for joint image and text generation. The flexibility of this framework lends itself to migration towards any-to-any task adaptability. Furthermore, MARS employs a multi-stage training strategy that first establishes robust image-text alignment through complementary bidirectional tasks and subsequently concentrates on refining the T2I generation process, significantly augmenting text-image synchrony and the granularity of image details. Notably, MARS requires only 9% of the GPU days needed by SD1.5, yet it achieves remarkable results across a variety of benchmarks, illustrating the training efficiency and the potential for swift deployment in various applications.																																	2024-07-23	PPRN:90770712		
J	Lin, Yen Ting; Lowrie, Robert B.; Aslangil, Denis; Subasi, Yigit; Sornborger, Andrew T.				Lin, Yen/ABB-8491-2020; Subasi, Yigit/H-8224-2017						Challenges for quantum computation of nonlinear dynamical systems using linear representations								Arxiv											1	1;2024-07-08;https://www.arxiv.org/abs/2202.02188v3	arXiv:2202.02188			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jul 08 2024	2024	A number of recent studies have proposed that linear representations are appropriate for solving nonlinear dynamical systems with quantum computers, which fundamentally act linearly on a wave function in a Hilbert space. Linear representations, such as the Koopman representation and Koopman von Neumann mechanics, have regained attention from the dynamical-systems research community. Here, we aim to present a unified theoretical framework, currently missing in the literature, with which one can compare and relate existing methods, their conceptual basis, and their representations. We also aim to show that, despite the fact that quantum simulation of nonlinear classical systems may be possible with such linear representations, a necessary projection into a feasible finite-dimensional space will in practice eventually induce numerical artifacts which can be hard to eliminate or even control. As a result, a practical, reliable and accurate way to use quantum computation for solving general nonlinear dynamical systems is still an open problem.																																	2024-07-21	PPRN:90752336		
J	Alkhaldi, Asma; Alnajim, Raneem; Alabdullatef, Layan; Alyahya, Rawan; Chen, Jun; Zhu, Deyao; Alsinan, Ahmed; Elhoseiny, Mohamed				Elhoseiny, Mohamed/X-6406-2019						MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis								Arxiv											1	1;2024-07-04;https://www.arxiv.org/abs/2407.04106v1	arXiv:2407.04106			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	Recent advancements in artificial intelligence (AI) have precipitated significant breakthroughs in healthcare, particularly in refining diagnostic procedures. However, previous studies have often been constrained to limited functionalities. This study introduces MiniGPT-Med, a vision-language model derived from large-scale language models and tailored for medical applications. MiniGPT-Med demonstrates remarkable versatility across various imaging modalities, including X-rays, CT scans, and MRIs, enhancing its utility. The model is capable of performing tasks such as medical report generation, visual question answering (VQA), and disease identification within medical imagery. Its integrated processing of both image and textual clinical data markedly improves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's superior performance in disease grounding, medical report generation, and VQA benchmarks, representing a significant step towards reducing the gap in assisting radiology practice. Furthermore, it achieves state-of-the-art performance on medical report generation, higher than the previous best model by 19% accuracy. MiniGPT-Med promises to become a general interface for radiology diagnoses, enhancing diagnostic efficiency across a wide range of medical imaging applications.																																	2024-07-21	PPRN:90733766		
J	Katsumata, Kai; Vo, Duc Minh; Nakayama, Hideki				Nakayama, Hideki/AAT-5270-2020						A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis								Arxiv											2	2;2024-07-04;https://www.arxiv.org/abs/2311.12897v2| 1;2023-11-21;https://www.arxiv.org/abs/2311.12897v1	arXiv:2311.12897			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 04 2024	2024	3D Gaussian Splatting (3DGS) has shown remarkable success in synthesizing novel views given multiple views of a static scene. Yet, 3DGS faces challenges when applied to dynamic scenes because 3D Gaussian parameters need to be updated per timestep, requiring a large amount of memory and at least a dozen observations per timestep. To address these limitations, we present a compact dynamic 3D Gaussian representation that models positions and rotations as functions of time with a few parameter approximations while keeping other properties of 3DGS including scale, color and opacity invariant. Our method can dramatically reduce memory usage and relax a strict multi-view assumption. In our experiments on monocular and multi-view scenarios, we show that our method not only matches state-of-the-art methods, often linked with slower rendering speeds, in terms of high rendering quality but also significantly surpasses them by achieving a rendering speed of 118 frames per second (FPS) at a resolution of 1,352×1,014 on a single GPU.																																	2024-07-21	PPRN:86244178		
J	Zhang, Huaxin; Xu, Xiaohao; Wang, Xiang; Zuo, Jialong; Han, Chuchu; Huang, Xiaonan; Gao, Changxin; Wang, Yuehuan; Sang, Nong				han, chuchu/NFS-2743-2025; Huang, Xiaonan/AAP-6905-2020						Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM								Arxiv											2	2;2024-06-29;https://www.arxiv.org/abs/2406.12235v2| 1;2024-06-18;https://www.arxiv.org/abs/2406.12235v1	arXiv:2406.12235			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 29 2024	2024	Towards open-ended Video Anomaly Detection (VAD), existing methods often exhibit biased detection when faced with challenging or unseen events and lack interpretability. To address these drawbacks, we propose Holmes-VAD, a novel framework that leverages precise temporal supervision and rich multimodal instructions to enable accurate anomaly localization and comprehensive explanations. Firstly, towards unbiased and explainable VAD system, we construct the first largescale multimodal VAD instruction-tuning benchmark, i.e. , VAD-Instruct50k . This dataset is created using a carefully designed semi-automatic labeling paradigm. Efficient single-frame annotations are applied to the collected untrimmed videos, which are then synthesized into high-quality analyses of both abnormal and normal video clips using a robust off-the-shelf video captioner and a large language model (LLM). Building upon the VAD-Instruct50k dataset, we develop a customized solution for interpretable video anomaly detection. We train a lightweight temporal sampler to select frames with high anomaly response and fine -tune a multimodal large language model (LLM) to generate explanatory content. Extensive experimental results validate the generality and interpretability of the proposed Holmes-VAD , establishing it as a novel interpretable technique for real-world video anomaly analysis. To support the community, our benchmark and model will be publicly available at https://holmesvad.github.io/ .																																	2024-07-18	PPRN:89360733		
J	Chaves, Juan Manuel Zambrano; Huang, Shih-Cheng; Xu, Yanbo; Xu, Hanwen; Usuyama, Naoto; Zhang, Sheng; Wang, Fei; Xie, Yujia; Khademi, Mahmoud; Yang, Ziyi; Awadalla, Hany; Gong, Julia; Hu, Houdong; Yang, Jianwei; Li, Chunyuan; Gao, Jianfeng; Gu, Yu; Wong, Cliff; Wei, Mu; Naumann, Tristan; Chen, Muhao; Lungren, Matthew P.; Chaudhari, Akshay; Yeung-Levy, Serena; Langlotz, Curtis P.; Wang, Sheng; Poon, Hoifung				徐, 彦波/IAN-4376-2023; Li, Chunyuan/KHY-0771-2024; yang, ziyi/JGD-5349-2023; Chaudhari, Akshay/ABE-1734-2021; Gao, Jianfeng/AAP-8200-2021; Usuyama, Naoto/NCV-1655-2025; Gu, Yu/LEL-7846-2024; Xu, Hanwen/HPD-3102-2023; Chen, Muhao/AAA-3634-2021; Khademi, Mahmoud/E-6888-2013						Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation								Arxiv											5	5;2024-06-27;https://www.arxiv.org/abs/2403.08002v5| 4;2024-05-10;https://www.arxiv.org/abs/2403.08002v4| 3;2024-05-04;https://www.arxiv.org/abs/2403.08002v3| 2;2024-03-20;https://www.arxiv.org/abs/2403.08002v2| 1;2024-03-12;https://www.arxiv.org/abs/2403.08002v1	arXiv:2403.08002			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 27 2024	2024	The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real -world clinics. Frontier general -domain models such as GPT-4V still have significant performance gaps in multimodal biomedical applications. More importantly, less -acknowledged pragmatic issues, including accessibility, model cost, and tedious manual evaluation make it hard for clinicians to use state-of-the-art large models directly on private patient data. Here, we explore training open -source small multimodal models (SMMs) to bridge competency gaps for unmet clinical needs in radiology. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre -trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space. For training, we assemble a large dataset of over 697 thousand radiology image -text pairs. For evaluation, we propose CheXprompt, a GPT-4based metric for assessing factual accuracy, and demonstrate its parity with expert evaluation. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LLaVA-Rad (7B) model attains state-of-the-art results on standard radiology tasks such as report generation and cross -modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). Moreover, LLaVA-Rad requires only one day to be trained on over 697 thousand image -text pairs using a standard 8-A100 GPU cluster, allowing further fine-tuning by clinicians using their own data. The inference of LLaVA-Rad is fast and can be performed on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real -world clinical applications.																																	2024-07-17	PPRN:88133029		
J	Harikane, Yuichi; Inoue, Akio K.; Ellis, Richard S.; Ouchi, Masami; Nakazato, Yurina; Yoshida, Naoki; Ono, Yoshiaki; Sun, Fengwu; Sato, Riku A.; Fujimoto, Seiji; Kashikawa, Nobunari; McLeod, Derek J.; Perez-Gonzalez, Pablo G.; Sawicki, Marcin; Sugahara, Yuma; Xu, Yi; Yamanaka, Satoshi; Carnall, Adam C.; Cullen, Fergus; Dunlop, James S.; Egami, Eiichi; Grogin, Norman; Isobe, Yuki; Koekemoer, Anton M.; Laporte, Nicolas; Lee, Chien-Hsiu; Magee, Dan; Matsuo, Hiroshi; Matsuoka, Yoshiki; Mawatari, Ken; Nakajima, Kimihiko; Nakane, Minami; Tamura, Yoichi; Umeda, Hiroya; Yanagisawa, Hiroto				Ouchi, Masami/AAA-9826-2019; Yoshida, Naoki/A-4305-2011; Perez-Gonzalez, Pablo G./IVH-0781-2023; Dunlop, James/ADB-7947-2022; Sawicki, Marcin/JZT-9160-2024; Matsuoka, Yoshiki/MTB-5673-2025; Ellis, Richard/ABL-1310-2022; Harikane, Yuichi/KHY-2680-2024; Ono, Yoshiaki/AAY-4463-2020						JWST, ALMA, and Keck Spectroscopic Constraints on the UV Luminosity Functions at z~7-14: Clumpiness and Compactness of the Brightest Galaxies in the Early Universe								Arxiv											1	1;2024-06-26;https://www.arxiv.org/abs/2406.18352v1	arXiv:2406.18352			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 26 2024	2024	We present the number densities and physical properties of the bright galaxies spectroscopically confirmed at z∼7−14. Our sample is composed of 53 galaxies at zspec∼7−14, including recently-confirmed galaxies at zspec=12.34−14.32 with JWST, as well as new confirmations at zspec=6.583−7.643 with −24<MUV<−21 mag using ALMA and Keck. Our JWST/NIRSpec observations have also revealed that very bright galaxy candidates at z∼10−13 identified from ground-based telescope images before JWST are passive galaxies at z∼3−4, emphasizing the necessity of strict screening and spectroscopy in the selection of the brightest galaxies at z > 10. The UV luminosity functions derived from these spectroscopic results are consistent with a double power-law function, showing tensions with theoretical models at the bright end. To understand the origin of the overabundance of bright galaxies, we investigate their morphologies using JWST/NIRCam high-resolution images obtained in various surveys including PRIMER and COSMOS-Web. We find that ∼70% of the bright galaxies at z∼7 exhibit clumpy morphologies with multiple sub-components, suggesting merger-induced starburst activity, which is consistent with SED fitting results showing bursty star formation histories. At z ≳ 10, bright galaxies are classified into two types of galaxies; extended ones with weak high-ionization emission lines, and compact ones with strong high-ionization lines including NIV]λ1486, indicating that at least two different processes (e.g., merger-induced starburst and compact star formation/AGN) are shaping the physical properties of the brightest galaxies at z ≳ 10 and are responsible for their overabundance.																																	2024-12-06	PPRN:89904505		
J	Abadi, Sebastien; Xu, Ke-Jun; Lomeli, Eder G.; Puphal, Pascal; Isobe, Masahiko; Zhong, Yong; Fedorov, Alexei V.; Mo, Sung-Kwan; Hashimoto, Makoto; Lu, Dong-Hui; Moritz, Brian; Keimer, Bernhard; Devereaux, Thomas P.; Hepting, Matthias; Shen, Zhi-Xun				Puphal, Pascal/ODK-9911-2025; Moritz, Brian/D-7505-2015; Xu, Ke-Jun/AAZ-5194-2020						Electronic structure of the alternating monolayer-trilayer phase of La3Ni2O7								Arxiv											2	2;2024-06-25;https://www.arxiv.org/abs/2402.07143v2| 1;2024-02-11;https://www.arxiv.org/abs/2402.07143v1	arXiv:2402.07143			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 25 2024	2024	Recent studies of La3Ni2O7 have identified a bilayer (2222) structure and an unexpected alternating monolayer-trilayer (1313) structure, both of which feature signatures of superconductivity near 80 K under high pressures. Using angle -resolved photoemission spectroscopy, we measure the electronic structure of 1313 samples. In contrast to the previously studied 2222 structure, we find that the 1313 structure hosts a flat band with a markedly different binding energy, as well as an additional electron pocket and band splittings. By comparison to local -density approximation calculations, we find renormalizations of the Ni-dz2 and Ni-dx2−y2 derived bands to be about 5 to 7 and about 4 respectively, suggesting strong correlation effects. These results reveal important differences in the electronic structure brought about by the distinct structural motifs with the same stoichiometry. Such differences may be relevant to the putative high temperature superconductivity.																																	2024-07-15	PPRN:87649929		
J	Feng, Zhaopeng; Zhang, Yan; Li, Hao; Wu, Bei; Liao, Jiayu; Liu, Wenqiang; Lang, Jun; Feng, Yang; Wu, Jian; Liu, Zuozhu				Liao, Jiayu/E-6129-2011; Liu, Zuozhu/IUN-8338-2023; feng, zhaopeng/OPN-4686-2025						TEaR: Improving LLM-based Machine Translation with Systematic Self-Refinement								Arxiv											3	3;2024-06-21;https://www.arxiv.org/abs/2402.16379v3| 2;2024-03-04;https://www.arxiv.org/abs/2402.16379v2| 1;2024-02-26;https://www.arxiv.org/abs/2402.16379v1	arXiv:2402.16379			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 21 2024	2024	Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self -refinement and result in improved translation performance. Motivated by these insights, we introduce a systematic LLMbased self -refinement translation framework, named TEaR, , which stands for T ranslate, E stimate, a nd R efine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self -refinement framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it’s from high -resource languages to low -resource ones or whether it’s English -centric or centered around other languages; 2) TEaR exhibits superior systematicity and interpretability; 3) different estimation strategies yield varied impacts, directly affecting the effectiveness of the final corrections. Additionally, traditional neural translation models and evaluation models operate separately, often focusing on singular tasks due to their limited capabilities, while general-purpose LLMs possess the capability to undertake both tasks simultaneously. We further conduct crossmodel correction experiments to investigate the potential relationship between the translation and evaluation capabilities of general-purpose LLMs. Our code and data are available at https://github.com/fzp0424/self_correct_mt.																																	2024-07-11	PPRN:87883272		
J	Mousavi, Pooneh; Della Libera, Luca; Duret, Jarod; Ploujnikov, Artem; Subakan, Cem; Ravanelli, Mirco				Ravanelli, Mirco/AAK-4199-2020						DASB -- Discrete Audio and Speech Benchmark								Arxiv											1	1;2024-06-20;https://www.arxiv.org/abs/2406.14294v1	arXiv:2406.14294			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 20 2024	2024	Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text -to -speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.																																	2024-07-06	PPRN:89375057		
J	An, Wenbin; Tian, Feng; Leng, Sicong; Nie, Jiahao; Lin, Haonan; Wang, Qianying; Dai, Guang; Chen, Ping; Lu, Shijian				Haonan, Lin/LMQ-3599-2024; Leng, Sicong/MNO-5959-2025; wang, yingxiong/C-4060-2009; Lu, Shijian/AAU-4831-2021						AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention								Arxiv											3	3;2025-03-14;https://www.arxiv.org/abs/2406.12718v3| 2;2024-06-21;https://www.arxiv.org/abs/2406.12718v2| 1;2024-06-18;https://www.arxiv.org/abs/2406.12718v1	arXiv:2406.12718			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 18 2024	2024	Despite their great success across various multimodal tasks, Large Vision-Language Models (LVLMs) are facing a prevalent problem with object hallucinations, where the generated textual responses are inconsistent with ground-truth objects in the given image. This paper investigates various LVLMs and pinpoints attention deficiency toward discriminative local image features as one root cause of object hallucinations. Specifically, LVLMs predominantly attend to prompt-independent global image features, while failing to capture prompt-relevant local features, consequently undermining the visual grounding capacity of LVLMs and leading to hallucinations. To this end, we propose Assembly of Global and Local Attention (AGLA), a training-free and plug-and-play approach that mitigates object hallucinations by exploring an ensemble of global features for response generation and local features for visual discrimination simultaneously. Our approach exhibits an image-prompt matching scheme that captures prompt-relevant local features from images, leading to an augmented view of the input image where prompt-relevant content is reserved while irrelevant distractions are masked. With the augmented view, a calibrated decoding distribution can be derived by integrating generative global features from the original image and discriminative local features from the augmented image. Extensive experiments show that AGLA consistently mitigates object hallucinations and enhances general perception capability for LVLMs across various discriminative and generative benchmarks. Our code will be released at https://github.com/Lackel/AGLA.																																	2025-08-07	PPRN:89361194		
J	Smith, Victoria; Shamsabadi, Ali Shahin; Ashurst, Carolyn; Weller, Adrian				Shahin Shamsabadi, Ali/HGC-3874-2022						Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey								Arxiv											2	2;2024-06-18;https://www.arxiv.org/abs/2310.01424v2| 1;2023-09-27;https://www.arxiv.org/abs/2310.01424v1	arXiv:2310.01424			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 18 2024	2024	Large Language Models (LLMs) have shown greatly enhanced performance in recent years, attributed to increased size and extensive training data. This advancement has led to widespread interest and adoption across industries and the public. However, training data memorization in Machine Learning models scales with model size, particularly concerning for LLMs. Memorized text sequences have the potential to be directly leaked from LLMs, posing a serious threat to data privacy. Various techniques have been developed to attack LLMs and extract their training data. As these models continue to grow, this issue becomes increasingly critical. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first SoK on data privacy for LLMs. We (i) identify a taxonomy of salient dimensions where attacks differ on LLMs, (ii) systematize existing attacks, using our taxonomy of dimensions to highlight key trends, (iii) survey existing mitigation strategies, highlighting their strengths and limitations, and (iv) identify key gaps, demonstrating open problems and areas for concern.																																	2024-07-04	PPRN:85377452		
J	Chang, Hoyeon; Park, Jinho; Ye, Seonghyeon; Yang, Sohee; Seo, Youngkyung; Chang, Du-Seong; Seo, Minjoon										How Do Large Language Models Acquire Factual Knowledge During Pretraining?								Arxiv											2	2;2024-11-12;https://www.arxiv.org/abs/2406.11813v3| 1;2024-06-17;https://www.arxiv.org/abs/2406.11813v1	arXiv:2406.11813			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 17 2024	2024	Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model's capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models' robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.																																	2025-08-07	PPRN:89352540		
J	Mou, Xinyi; Wei, Zhongyu; Huang, Xuanjing				Wei, Zhongyu/KSL-9373-2024; Mou, Xinyi/KPA-9492-2024						Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation								Arxiv											2	2;2024-06-17;https://www.arxiv.org/abs/2402.16333v2| 1;2024-02-26;https://www.arxiv.org/abs/2402.16333v1	arXiv:2402.16333			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework HiSim for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent -based models. We further construct a Twitter -like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi -faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real -world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.																																	2024-07-04	PPRN:87943751		
J	Qiang, Yao; Zhou, Xiangyu; Zade, Saleh Zare; Khanduri, Prashant; Zhu, Dongxiao				Zhu, Dongxiao/G-4049-2010; Khanduri, Prashant/NGR-6579-2025						Hijacking Large Language Models via Adversarial In-Context Learning								Arxiv											2	2;2024-06-15;https://www.arxiv.org/abs/2311.09948v2| 1;2023-11-16;https://www.arxiv.org/abs/2311.09948v1	arXiv:2311.09948			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 15 2024	2024	In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. This work introduces a novel transferable attack against ICL to address these issues, aiming to hijack LLMs to generate the target response or jailbreak. Our hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos without directly contaminating the user queries. Comprehensive experimental results across different generation and jailbreaking tasks highlight the effectiveness of our hijacking attack, resulting in distracted attention towards adversarial tokens and consequently leading to unwanted target outputs. We also propose a defense strategy against hijacking attacks through the use of extra clean demos, which enhances the robustness of LLMs during ICL. Broadly, this work reveals the significant security vulnerabilities of LLMs and emphasizes the necessity for in-depth studies on their robustness.																																	2024-08-25	PPRN:86177063		
J	Fatemi, Bahare; Kazemi, Mehran; Tsitsulin, Anton; Malkan, Karishma; Yim, Jinyeong; Palowitch, John; Seo, Sungyong; Halcrow, Jonathan; Perozzi, Bryan										Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning								Arxiv											1	1;2024-06-13;https://www.arxiv.org/abs/2406.09170v1	arXiv:2406.09170			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 13 2024	2024	Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks. However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies. In this work, we address these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios. The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance. Our findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks. 																																	2024-07-02	PPRN:89301763		
J	Xia, Peng; Chen, Ze; Tian, Juanxi; Gong, Yangrui; Hou, Ruibo; Xu, Yue; Wu, Zhenbang; Fan, Zhiyuan; Zhou, Yiyang; Zhu, Kangyu; Zheng, Wenhao; Wang, Zhaoyang; Wang, Xiao; Zhang, Xuchao; Bansal, Chetan; Niethammer, Marc; Huang, Junzhou; Zhu, Hongtu; Li, Yun; Sun, Jimeng; Ge, Zongyuan; Li, Gang; Zou, James; Yao, Huaxiu				Huang, Jianzhong/KEI-1516-2024; tang, lei/JCO-4117-2023; Wang, Zhaoyang/KHU-1670-2024; Yao, Huaxiu/V-3516-2019; Zhou, Yiyang/AAU-7705-2021; Zheng, Wenhao/KBR-3871-2024						CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models								Arxiv											1	1;2024-06-10;https://www.arxiv.org/abs/2406.06007v1	arXiv:2406.06007			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 10 2024	2024	Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to C omprehensively ev A luate the t R ustworthin ES s of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES .																																	2024-07-04	PPRN:89264747		
J	Cao, Yuanpu; Cao, Bochuan; Chen, Jinghui										Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections								Arxiv											2	2;2024-06-09;https://www.arxiv.org/abs/2312.00027v2| 1;2023-11-15;https://www.arxiv.org/abs/2312.00027v1	arXiv:2312.00027			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 09 2024	2024	Recent developments in Large Language Models (LLMs) have manifested significant advancements. To facilitate safeguards against malicious exploitation, a body of research has concentrated on aligning LLMs with human preferences and inhibiting their generation of inappropriate content. Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal amount of harmful data can easily unalign the target LLM. While being effective, such fine-tuning-based unalignment approaches also have their own limitations: (1) non-stealthiness , after fine-tuning, safety audits or red-teaming can easily expose the potential weaknesses of the unaligned models, thereby precluding their release/use. (2) nonpersistence , the unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning again with aligned data points. In this work, we show that it is possible to conduct stealthy and persistent unalignment on large language models via backdoor injections. We also provide a novel understanding of the relationship between the backdoor persistence and the activation pattern and further provide guidelines for potential trigger design. Through extensive experiments, we demonstrate that our proposed stealthy and persistent unalignment can successfully pass the safety evaluation while maintaining strong persistence against re-alignment defense.																																	2024-06-22	PPRN:86359727		
J	Liu, Xinliang; Xu, Bo; Cao, Shuhao; Zhang, Lei				liu, xinliang/OTG-5644-2025; Zhang, Lei/B-1068-2014						Mitigating spectral bias for the multiscale operator learning								Arxiv											2	2;2024-06-09;https://www.arxiv.org/abs/2210.10890v3| 1;2022-10-19;https://www.arxiv.org/abs/2210.10890v1	arXiv:2210.10890			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 09 2024	2024	Neural operators have emerged as a powerful tool for learning the mapping between infinitedimensional parameter and solution spaces of partial differential equations (PDEs). In this work, we focus on multiscale PDEs that have important applications such as reservoir modeling and turbulence prediction. We demonstrate that for such PDEs, the spectral bias towards low-frequency components presents a significant challenge for existing neural operators. To address this challenge, we propose a hierarchical attention neural operator (HANO) inspired by the hierarchical matrix approach. HANO features a scale-adaptive interaction range and self-attentions over a hierarchy of levels, enabling nested feature computation with controllable linear cost and encoding/decoding of multiscale solution space. We also incorporate an empirical H1 loss function to enhance the learning of high-frequency components. Our numerical experiments demonstrate that HANO outperforms state-of-the-art (SOTA) methods for representative multiscale problems.																																	2024-11-09	PPRN:22131244		
J	Liu, Zhijun; Wang, Shuai; Inoue, Sho; Bai, Qibing; Li, Haizhou				WANG, SHUAI/IUM-6008-2023						Autoregressive Diffusion Transformer for Text-to-Speech Synthesis								Arxiv											1	1;2024-06-08;https://www.arxiv.org/abs/2406.05551v1	arXiv:2406.05551			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 08 2024	2024	Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space Rd and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate 170 ms of 24 kHz speech per evaluation step with minimal degradation in performance.																																	2024-07-04	PPRN:89267757		
J	Fan, Wenqi; Wang, Shijie; Huang, Jiani; Chen, Zhikai; Song, Yu; Tang, Wenzhuo; Mao, Haitao; Liu, Hui; Liu, Xiaorui; Yin, Dawei; Li, Qing				Li, Qing/H-4100-2011; Yin, Dawei/JOR-9201-2023; Tang, Wenzhuo/ISR-9909-2023; liu, xiaorui/GVU-5497-2022; song, yu/KCZ-2003-2024; WANG, Shijie/JTS-6827-2023						Graph Machine Learning in the Era of Large Language Models (LLMs)								Arxiv											2	2;2024-06-04;https://www.arxiv.org/abs/2404.14928v2| 1;2024-04-23;https://www.arxiv.org/abs/2404.14928v1	arXiv:2404.14928			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 04 2024	2024	Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML’s generalization, transferability, and few -shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out -of -distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre -training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.																																	2024-07-04	PPRN:88626568		
J	Lin, Xinyu; Wang, Wenjie; Li, Yongqi; Yang, Shuo; Feng, Fuli; Wei, Yinwei; Chua, Tat-Seng				Wang, Meng/AEZ-9059-2022; Wei, Yinwei/JHX-9398-2023						Data-efficient Fine-tuning for LLM-based Recommendation								Arxiv											1	1;2024-06-04;https://www.arxiv.org/abs/2401.17197v2	arXiv:2401.17197			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 04 2024	2024	Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data. To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance. To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. Empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.																																	2024-07-04	PPRN:87421122		
J	Rozado, David				Rozado, David/O-6934-2014						The Political Preferences of LLMs								Arxiv											2	2;2024-06-02;https://www.arxiv.org/abs/2402.01789v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01789v1	arXiv:2402.01789			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 02 2024	2024	I report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, I administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both closed and open source. When probed with questions/statements with political connotations, most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. This does not appear to be the case for five additional base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, the weak performance of the base models at coherently answering the tests’ questions makes this subset of results inconclusive. Finally, I demonstrate that LLMs can be steered towards specific locations in the political spectrum through Supervised Fine-Tuning (SFT) with only modest amounts of politically aligned data, suggesting SFT's potential to embed political orientation in LLMs. With LLMs beginning to partially displace traditional information sources like search engines and Wikipedia, the societal implications of political biases embedded in LLMs are substantial.																																	2024-06-22	PPRN:87523602		
J	Chen, Jiajie; Hou, Thomas Y.										Stable nearly self-similar blowup of the 2D Boussinesq and 3D Euler equations with smooth data II: Rigorous Numerics								Arxiv											2	2;2023-05-09;https://www.arxiv.org/abs/2305.05660v1| 1;2024-06-01;	arXiv:2305.05660			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 01 2024	2024	This is Part II of our paper in which we prove finite time blowup of the 2D Boussinesq and 3D axisymmetric Euler equations with smooth initial data of finite energy and boundary. In Part I of our paper [14], we establish an analytic framework to prove nonlinear stability of an approximate self-similar blowup profile using a combination of weighted L∞ and weighted C1/2 energy estimates. We reduce proving nonlinear stability to verifying several inequalities for the constants in the energy estimate which depend on the approximate steady state and the weights in the energy functional only. In Part II of our paper, we construct approximate space-time solutions with rigorous error control, which are used to obtain sharp stability estimates of the linearized operator in Part I. We also obtain sharp estimates of the regular part of the velocity using numerical integration with computer assistance. These results enable us to verify that the constants in the energy estimate obtained in Part I [14] indeed satisfy the inequalities for nonlinear stability. The nonlinear stability further implies the finite time singularity of the axisymmetric 3D Euler equations with smooth initial data and boundary.																																	2024-11-17	PPRN:68802255		
J	Mao, Haitao; Chen, Zhikai; Tang, Wenzhuo; Zhao, Jianan; Ma, Yao; Zhao, Tong; Shah, Neil; Galkin, Mikhail; Tang, Jiliang				Zhao, Tong/GQB-5245-2022; Tang, Wenzhuo/ISR-9909-2023						Position: Graph Foundation Models are Already Here								Arxiv											3	3;2024-05-30;https://www.arxiv.org/abs/2402.02216v3| 2;2024-02-06;https://www.arxiv.org/abs/2402.02216v2| 1;2024-02-03;https://www.arxiv.org/abs/2402.02216v1	arXiv:2402.02216			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 30 2024	2024	Graph Foundation Models (GFMs) are emerging as a significant research topic in the graph domain, aiming to develop graph models trained on extensive and diverse data to enhance their applicability across various tasks and domains. Developing GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which are typically trained from scratch for specific tasks on particular datasets The primary challenge in constructing GFMs lies in effectively leveraging vast and diverse graph data to achieve positive transfer. Drawing inspiration from existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a “graph vocabulary”, in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, expressiveness, and stability. Such a vocabulary perspective can potentially advance the future GFM design in line with the neural scaling laws. All relevant resources for GFMs design can be found at here.																																	2024-06-17	PPRN:87523341		
J	Zhu, Siting; Qin, Renjie; Wang, Guangming; Liu, Jiuming; Wang, Hesheng				Wang, Hesheng/P-3192-2015; Wang, Guangming/IWM-4556-2023; Liu, Jiuming/LKL-2382-2024						SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2403.07494v3| 1;2024-04-01;https://www.arxiv.org/abs/2403.07494v2	arXiv:2403.07494			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 29 2024	2024	We propose SemGauss-SLAM, a dense semantic SLAM system utilizing 3D Gaussian representation, that enables accurate 3D semantic mapping, robust camera tracking, and high-quality rendering simultaneously. In this system, we incorporate semantic feature embedding into 3D Gaussian representation, which effectively encodes semantic information within the spatial layout of the environment for precise semantic scene representation. Furthermore, we propose feature-level loss for updating 3D Gaussian representation, enabling higher-level guidance for 3D Gaussian optimization. In addition, to reduce cumulative drift in tracking and improve semantic reconstruction accuracy, we introduce semantic-informed bundle adjustment leveraging multi-frame semantic associations for joint optimization of 3D Gaussian representation and camera poses, leading to low-drift tracking and accurate mapping. Our SemGauss-SLAM method demonstrates superior performance over existing radiance field-based SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in high-precision semantic segmentation and dense semantic mapping.																																	2024-08-24	PPRN:88360677		
J	Li, Lingyao; Zhou, Jiayan; Gao, Zhenxiang; Hua, Wenyue; Fan, Lizhou; Yu, Huizi; Hagen, Loni; Zhang, Yongfeng; Assimes, Themistocles L.; Hemphill, Libby; Ma, Siyuan				Zhou, Jiayan/ABC-4378-2020; Assimes, Themistocles/D-9696-2015; Ma, Siyuan/IVV-8174-2023; Yu, Huizi/HPD-6726-2023; Hagen, Loni/HJY-7705-2023						A scoping review of using Large Language Models (LLMs) to investigate Electronic Health Records (EHRs)								Arxiv											2	2;2024-05-22;https://www.arxiv.org/abs/2405.03066v2| 1;2024-05-05;https://www.arxiv.org/abs/2405.03066v1	arXiv:2405.03066			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 22 2024	2024	Electronic Health Records (EHRs) play an important role in the healthcare system. However, their complexity and vast volume pose significant challenges to data interpretation and analysis. Recent advancements in Artificial Intelligence (AI), particularly the development of Large Language Models (LLMs), open up new opportunities for researchers in this domain. Although prior studies have demonstrated their potential in language understanding and processing in the context of EHRs, a comprehensive scoping review is lacking. This study aims to bridge this research gap by conducting a scoping review based on 329 related papers collected from OpenAlex. We first performed a bibliometric analysis to examine paper trends, model applications, and collaboration networks. Next, we manually reviewed and categorized each paper into one of the seven identified topics: named entity recognition, information extraction, text similarity, text summarization, text classification, dialogue system, and diagnosis and prediction. For each topic, we discussed the unique capabilities of LLMs, such as their ability to understand context, capture semantic relations, and generate human-like text. Finally, we highlighted several implications for researchers from the perspectives of data resources, prompt engineering, fine-tuning, performance measures, and ethical concerns. In conclusion, this study provides valuable insights into the potential of LLMs to transform EHR research and discusses their applications and ethical considerations.																																	2024-06-04	PPRN:88791276		
J	Lu, Li-Chun; Chen, Shou-Jen; Pai, Tsung-Min; Yu, Chan-Hung; Lee, Hung-yi; Sun, Shao-Hua				Lu, Li-Chun/HNS-4975-2023; Sun, Shao-Hua/AAB-6903-2019						LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play								Arxiv											1	1;2024-05-18;https://www.arxiv.org/abs/2405.06373v3	arXiv:2405.06373			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 18 2024	2024	Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, , a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. Our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics.																																	2024-06-15	PPRN:89098274		
J	Takahashi, Jun; Shao, Hui; Zhao, Bowen; Guo, Wenan; Sandvik, Anders W.				Guo, WenAn/G-3827-2013; Takahashi, Jun/GRX-8704-2022						SO(5) multicriticality in two-dimensional quantum magnets								Arxiv											1	1;2024-05-10;https://www.arxiv.org/abs/2405.06607v1	arXiv:2405.06607			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 10 2024	2024	We resolve the long-standing problem of the nature of the quantum phase transition between a Neel antiferromagnet and a spontaneously dimerized valence-bond solid in two-dimensional spin-1/2 magnets. We study a class of J-Q models, in which the standard Heisenberg exchange J competes with multi-spin interactions Qn formed by products of n singlet projectors on adjacent parallel links of the lattice. Using large-scale quantum Monte Carlo (QMC) calculations, we provide unambiguous evidence for first-order transitions in these models, with the strength of the discontinuities increasing with n. In the case of the widely studied n = 2 and n = 3 models, the first-order signatures are very weak, but observable in correlation functions on large lattices. On intermediate length scales (up to hundreds of lattice constants, depending on the observable) we can extract well-defined scaling dimensions (critical exponents) that are common to the models with small n, indicating close proximity to a universal quantum critical point. By combining two different Q terms, specifically we consider the J-Q2-Q6 model, the transition can be continuously tuned from weak to more strongly first-order. In the plane (Q2, Q6), with J = 1 − Q2, the two coexisting order parameters on the first-order line scale with an unusually large exponent β ≈ 0.85. This exponent and others coincide closely with known rigorous bounds for an SO(5) symmetric conformal field theory (CFT), but, in contrast to prevailing scenarios, the leading SO(5) singlet operator is relevant and responsible for the first-order transition ending at a fine-tuned multicritical point. We quantitatively characterize the emergent SO(5) symmetry by computing the scaling dimensions of its leading irrelevant perturbations. The large β value and a large correlation length exponent, ν ≈ 1.4, partially explain why the transition remains near-critical on the first-order line even quite far away from the critical point and in many different models without fine-tuning. In addition, we find that few-spin lattice operators are dominated by their content of the SO(5) violating field (the traceless symmetric tensor), and interactions involving many spins are required to observe strong effects of the relevant SO(5) singlet that brings the system into the coexistence line. Beyond the scaling dimensions that can be directly explained by the CFT, the exponent that had previously been identified with the divergent correlation length when crossing between the two phases does not have a corresponding level in the CFT spectrum. We explain this emergent “pseudo critical” length scale by a mechanism relying on a dangerously irrelevant SO(5) perturbation in combination with repulsive interactions between the two order parameters. This length scale is reflected in crossover behaviors of observables when traversing the weak first-order line. We argue that the multicritical point is also most likely the top of a gapless spin liquid phase recently discovered in frustrated Heisenberg models, into which the J-Q models can be continuously deformed. Our results are at variance with the conventional scenario of generic deconfined quantum critical points, including the complex CFT proposal. The multicritical point should exists within real Hamiltonians, though perhaps only outside the regime amenable to sign-free QMC simulations.																																	2024-06-08	PPRN:89042571		
J	Dutta, Subhabrata; Singh, Joykirat; Chakrabarti, Soumen; Chakraborty, Tanmoy										How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2402.18312v2| 1;2024-02-28;https://www.arxiv.org/abs/2402.18312v1	arXiv:2402.18312			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of Llama-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token appear in the later half, attention heads that move information along ontological relationships appear in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.																																	2024-05-24	PPRN:87985336		
J	Madan, Neelu; Mogelmose, Andreas; Modi, Rajat; Rawat, Yogesh S.; Moeslund, Thomas B.				Madan, Neelu/LXV-4797-2024						Foundation Models for Video Understanding: A Survey								Arxiv											1	1;2024-05-06;https://www.arxiv.org/abs/2405.03770v1	arXiv:2405.03770			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	May 06 2024	2024	Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image -based ViFMs, which adapt existing image models for video tasks, 2) Video -Based ViFMs, which utilize video -specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image -based foundation models consistently outperform video -based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. 																																	2024-06-04	PPRN:88975366		
J	Yang, Shiyuan; Hou, Liang; Huang, Haibin; Ma, Chongyang; Wan, Pengfei; Zhang, Di; Chen, Xiaodong; Liao, Jing				Huang, Haibin/R-1823-2019; Chen, Xiaodong/JGD-8455-2023						Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2402.03162v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.03162v1	arXiv:2402.03162			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 06 2024	2024	Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for multiple objects as well as camera's pan and zoom movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. 																																	2024-05-25	PPRN:87523326		
J	Arawjo, Ian; Swoopes, Chelse; Vaithilingam, Priyan; Wattenberg, Martin; Glassman, Elena										ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing								Arxiv											3	3;2024-05-03;https://www.arxiv.org/abs/2309.09128v3| 2;2023-12-20;https://www.arxiv.org/abs/2309.09128v2| 1;2023-09-17;https://www.arxiv.org/abs/2309.09128v1	arXiv:2309.09128			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 03 2024	2024	Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.																																	2024-05-25	PPRN:85165923		
J	Gu, Chenchen; Li, Xiang Lisa; Liang, Percy; Hashimoto, Tatsunori										On the Learnability of Watermarks for Language Models								Arxiv											3	3;2024-05-02;https://www.arxiv.org/abs/2312.04469v3| 2;2024-01-01;https://www.arxiv.org/abs/2312.04469v2| 1;2023-12-07;https://www.arxiv.org/abs/2312.04469v1	arXiv:2312.04469			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 02 2024	2024	Watermarking of language model outputs enables statistical detection of modelgenerated text, which can mitigate harms and misuses of language models. Existing watermarking strategies operate by altering the decoder of an existing language model. In this paper, we ask whether language models can directly learn to generate watermarked text, which would have significant implications for the real -world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, enabling watermarking for open models, where users can control the decoding procedure. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding -based watermarking. We test our approach on three decoding -based watermarking strategies and various hyperparameter settings, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low -distortion watermarks.																																	2024-05-19	PPRN:86443379		
J	Hu, Teng; Zhang, Jiangning; Yi, Ran; Wang, Yating; Huang, Hongrui; Weng, Jieyu; Wang, Yabiao; Ma, Lizhuang				Wang, Yunxiao/AAQ-2686-2020; Wang, Zhiyong/F-7955-2010; YI, Ran/IVV-5458-2023						MotionMaster: Training-free Camera Motion Transfer For Video Generation								Arxiv											2	2;2024-05-01;https://www.arxiv.org/abs/2404.15789v2| 1;2024-04-24;https://www.arxiv.org/abs/2404.15789v1	arXiv:2404.15789			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 01 2024	2024	The emergence of diffusion models has greatly propelled the progress in image and video generation. Recently, some efforts have been made in controllable video generation, including text-to-video generation and video motion control, among which camera motion control is an important topic. However, existing camera motion control methods rely on training a temporal camera module, and necessitate substantial computation resources due to the large amount of parameters in video generation models. Moreover, existing methods pre-define camera motion types during training, which limits their flexibility in camera control. Therefore, to reduce training costs and achieve flexible camera control, we propose COMD, a novel training-free video motion transfer model, which disentangles camera motions and object motions in source videos and transfers the extracted camera motions to new videos. We first propose a one-shot camera motion disentanglement method to extract camera motion from a single source video, which separates the moving objects from the background and estimates the camera motion in the moving objects region based on the motion in the background by solving a Poisson equation. Furthermore, we propose a few-shot camera motion disentanglement method to extract the common camera motion from multiple videos with similar camera motions, which employs a window-based clustering technique to extract the common features in temporal attention maps of multiple videos. Finally, we propose a motion combination method to combine different types of camera motions together, enabling our model a more controllable and flexible camera control. Extensive experiments demonstrate that our training-free approach can effectively decouple camera-object motion and apply the decoupled camera motion to a wide range of controllable video generation tasks, achieving flexible and diverse camera motion control.																																	2024-05-21	PPRN:88640517		
J	Kim, Yoonsik; Yim, Moonbin; Song, Ka Yeon										TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains								Arxiv											1	1;2024-04-30;https://www.arxiv.org/abs/2404.19205v1	arXiv:2404.19205			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	In this paper, we establish a benchmark for table visual question answering, referred to as the TableVQA-Bench, derived from pre-existing table question-answering (QA) and table structure recognition datasets. It is important to note that existing datasets have not incorporated images or QA pairs, which are two crucial components of TableVQA. As such, the primary objective of this paper is to obtain these necessary components. Specifically, images are sourced either through the application of a stylesheet or by employing the proposed table rendering system. QA pairs are generated by exploiting the large language model (LLM) where the input is a text-formatted table. Ultimately, the completed TableVQA-Bench comprises 1,500 QA pairs. We comprehensively compare the performance of various multi-modal large language models (MLLMs) on TableVQA-Bench. GPT-4V achieves the highest accuracy among commercial and open-sourced MLLMs from our experiments. Moreover, we discover that the number of vision queries plays a significant role in TableVQA performance. To further analyze the capabilities of MLLMs in comparison to their LLM backbones, we investigate by presenting image-formatted tables to MLLMs and text-formatted tables to LLMs, respectively. Our findings suggest that processing visual inputs is more challenging than text inputs, as evidenced by the lower performance of MLLMs, despite generally requiring higher computational costs than LLMs. 																																	2024-12-24	PPRN:119222455		
J	Wang, Hong-Yi; Song, Fei; Wang, Zhong				Song, Fei/MEP-5805-2025						Amoeba Formulation of Non-Bloch Band Theory in Arbitrary Dimensions								Arxiv											3	3;2024-04-30;https://www.arxiv.org/abs/2212.11743v3| 2;2024-04-25;https://www.arxiv.org/abs/2212.11743v2| 1;2022-12-22;https://www.arxiv.org/abs/2212.11743v1	arXiv:2212.11743			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 30 2024	2024	The non-Hermitian skin effect dramatically reshapes the energy bands of non-Hermitian systems, meaning that the usual Bloch band theory is fundamentally inadequate as their characterization. The non-Bloch band theory, in which the concept of Brillouin zone is generalized, has been widely applied to investigate non-Hermitian systems in one spatial dimension. However, its generalization to higher dimensions has been challenging. Here, we develop a formulation of the non-Hermitian skin effect and non-Bloch band theory in arbitrary spatial dimensions, which is based on a natural geometrical object known as the amoeba. Our theory provides a general framework for studying non-Hermitian bands beyond one dimension. Key quantities of non-Hermitian bands, including the energy spectrum, eigenstates profiles, and the generalized Brillouin zone, can be efficiently obtained from this approach.																																	2024-05-21	PPRN:35895059		
J	Ascher, Kenneth; Devleming, Kristin; Liu, Yuchen										Wall crossing for K-moduli spaces of plane curves								Arxiv											2	2;2024-04-29;https://www.arxiv.org/abs/1909.04576v2| 1;2019-09-10;https://www.arxiv.org/abs/1909.04576v1	arXiv:1909.04576			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 29 2024	2024	We construct proper good moduli spaces parametrizing K-polystable Q -Gorenstein smoothable log Fano pairs (X, cD ), where X is a Fano variety and D is a rational multiple of the anti-canonical divisor. We then establish a wall-crossing framework of these K-moduli spaces as c varies. The main application in this paper is the case of plane curves of degree d ≥ 4 as boundary divisors of P2 . In this case, we show that when the coefficient c is small, the K-moduli space of these pairs is isomorphic to the GIT moduli space. We then show that the first wall crossing of these K-moduli spaces are weighted blow-ups of Kirwan type. We also describe all wall crossings for degree 4 , 5 , 6 and relate the final K-moduli spaces to Hacking’s compactification and the moduli of K3 surfaces.																																	2024-05-18	PPRN:12907362		
J	Liu, Fuxiao; Xu, Paiheng; Li, Zongxia; Feng, Yue; Song, Hyemi				Xu, Paiheng/NRB-2409-2025						Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps								Arxiv											3	3;2024-04-26;https://www.arxiv.org/abs/2307.05052v4| 2;2024-04-15;https://www.arxiv.org/abs/2307.05052v3| 1;2023-07-11;https://www.arxiv.org/abs/2307.05052v1	arXiv:2307.05052			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 26 2024	2024	We investigate the role of various demonstration components in the in -context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground -truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground -truth labels significantly affects the saliency, though it’s more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment -indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground -truth labels. Finally, we find that the effectiveness of complementary explanations in boosting ICL performance is task -dependent, with limited benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks. These insights are critical for understanding the functionality of LLMs and guiding the development of effective demonstrations, which is increasingly relevant in light of the growing use of LLMs in applications such as ChatGPT.																																	2024-05-06	PPRN:73870816		
J	Zhao, Ze-Wei; Zhang, Ji-Guo; Li, Yichao; Zhang, Jing-Fei; Zhang, Xin				张, 继国/AAB-5652-2020; Zhang, Xin/AET-6134-2022; Zhang, Jingfei/HHZ-2743-2022; Zhao, Zewei/LEM-2736-2024						FRB dark sirens: Measuring the Hubble constant with unlocalized fast radio bursts								Arxiv											2	2;2024-04-25;https://www.arxiv.org/abs/2212.13433v2| 1;2022-12-27;https://www.arxiv.org/abs/2212.13433v1	arXiv:2212.13433			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 25 2024	2024	Fast radio bursts (FRBs) can be used to measure cosmological parameters by employing the Macquart relation. However, at present, only a small number of FRB events are localized to host galaxies with known redshifts. Inspired by the dark siren method in gravitational wave cosmology, we develop a Bayesian method to statistically measure the Hubble constant using unlocalized FRBs and galaxy catalog data, which makes it possible to constrain cosmological parameters from a large number of FRB data without known redshifts, meanwhile including the real galaxy information. We assume that the probability for a galaxy to host an FRB is proportional to the luminosity of this galaxy and use the results from the IllustrisTNG simulation as the priors of FRB host galaxy parameters. Ignoring some systematic errors, we obtain the first statistical H0 measurement only using twelve unlocalized FRB events combined with the big bang nucleosynthesis result, i.e., H0 = 80.4+24.1 −19.4 km s−1 Mpc−1 (68% highest-density interval). This method can also be refined to constrain other cosmological and FRB parameters. It is applicable to well-localized FRBs that still have several potential hosts.																																	2024-05-04	PPRN:35880257		
J	Lian, Zheng; Sun, Licai; Ren, Yong; Gu, Hao; Sun, Haiyang; Chen, Lan; Liu, Bin; Tao, Jianhua				Tao, Jianhua/LUZ-0363-2024; Lian, Zheng/AEV-4241-2022						MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition								Arxiv											3	3;2024-04-21;https://www.arxiv.org/abs/2401.03429v3| 2;2024-01-10;https://www.arxiv.org/abs/2401.03429v2| 1;2024-01-07;https://www.arxiv.org/abs/2401.03429v1	arXiv:2401.03429			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 21 2024	2024	Multimodal emotion recognition plays a crucial role in enhancing user experience in human-computer interaction. Over the past few decades, researchers have proposed a series of algorithms and achieved impressive progress. Although each method shows its superior performance, different methods lack a fair comparison due to inconsistencies in feature extractors, evaluation manners, and experimental settings. These inconsistencies severely hinder the development of this field. Therefore, we build MERBench, a unified evaluation benchmark for multimodal emotion recognition. We aim to reveal the contribution of some important techniques employed in previous works, such as feature selection, multimodal fusion, robustness analysis, fine-tuning, pre-training, etc. We hope this benchmark can provide clear and comprehensive guidance for follow-up researchers. Based on the evaluation results of MERBench, we further point out some promising research directions. Additionally, we introduce a new emotion dataset MER2023, focusing on the Chinese language environment. This dataset can serve as a benchmark dataset for research on multi-label learning, noise robustness, and semi-supervised learning. We encourage the follow-up researchers to evaluate their algorithms under the same experimental setup as MERBench for fair comparisons. 																																	2024-05-01	PPRN:87044903		
J	Klingefjord, Oliver; Lowe, Ryan; Edelman, Joe										What are human values, and how do we align AI to them?								Arxiv											1	1;2024-04-17;https://www.arxiv.org/abs/2404.10636v2	arXiv:2404.10636			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 17 2024	2024	There is an emerging consensus that we need to align AI systems with human values (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply this to language models in practice. We split the problem of "aligning to human values" into three parts: first, eliciting values from people; second, reconciling those values into an alignment target for training ML models; and third, actually training the model. In this paper, we focus on the first two parts, and ask the question: what are "good" ways to synthesize diverse human inputs about values into a target for aligning language models? To answer this question, we first define a set of 6 criteria that we believe must be satisfied for an alignment target to shape model behavior in accordance with human values. We then propose a process for eliciting and reconciling values called Moral Graph Elicitation (MGE), which uses a large language model to interview participants about their values in particular contexts; our approach is inspired by the philosophy of values advanced by Taylor (1977), Chang (2004), and others. We trial MGE with a representative sample of 500 Americans, on 3 intentionally divisive prompts (e.g. advice about abortion). Our results demonstrate that MGE is promising for improving model alignment across all 6 criteria. For example, almost all participants (89.1%) felt well represented by the process, and (89%) thought the final moral graph was fair, even if their value wasn't voted as the wisest. Our process often results in "expert" values (e.g. values from women who have solicited abortion advice) rising to the top of the moral graph, without defining who is considered an expert in advance.																																	2024-04-27	PPRN:88553964		
J	Wang, Renxi; Li, Haonan; Han, Xudong; Zhang, Yixuan; Baldwin, Timothy				Han, Xudong/PAV-5225-2025; Wang, Renxi/AAM-1326-2020; Li, Haonan/IAQ-4402-2023						Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents								Arxiv											2	2;2024-04-16;https://www.arxiv.org/abs/2402.11651v2| 1;2024-02-18;https://www.arxiv.org/abs/2402.11651v1	arXiv:2402.11651			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Apr 16 2024	2024	Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making finetuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. In this paper, we argue that unsuccessful trajectories offer valuable insights, and LLMs can learn from these trajectories through appropriate quality control and fine-tuning strategies. By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi -hop question answering, and strategic question answering tasks. We further analyze the inference results and find that our method provides a better tradeoff between valuable information and errors in unsuccessful trajectories. To our knowledge, we are the first to demonstrate the value of negative trajectories and their application in agenttunning scenarios. Our findings offer guidance for developing better agent -tuning methods and low -resource data usage techniques. 1																																	2024-04-26	PPRN:87762988		
J	Chandra, Ajay; Chevyrev, Ilya; Hairer, Martin; Shen, Hao				Hairer, Martin/LSJ-5848-2024						Stochastic quantisation of Yang-Mills-Higgs in 3D								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2201.03487v2| 1;2022-01-10;https://www.arxiv.org/abs/2201.03487v1	arXiv:2201.03487			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	We define a state space and a Markov process associated to the stochastic quantisation equation of Yang–Mills–Higgs (YMH) theories. The state space S is a nonlinear metric space of distributions, elements of which can be used as initial conditions for the (deterministic and stochastic) YMH flow with good continuity properties. Using gauge covariance of the deterministic YMH flow, we extend gauge equivalence ∼ to S and thus define a quotient space of “gauge orbits” O. We use the theory of regularity structures to prove local in time solutions to the renormalised stochastic YMH flow. Moreover, by leveraging symmetry arguments in the small noise limit, we show that there is a unique choice of renormalisation counterterms such that these solutions are gauge covariant in law. This allows us to define a canonical Markov process on O (up to a potential finite time blow-up) associated to the stochastic YMH flow.																																	2024-07-27	PPRN:12029197		
J	Zhao, Haiyan; Yang, Fan; Shen, Bo; Lakkaraju, Himabindu; Du, Mengnan				YANG, FAN/W-8237-2019; Zhao, Haiyan/HGD-3793-2022; Du, Mengnan/MXL-9283-2025						Towards Uncovering How Large Language Model Works: An Explainability Perspective								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2402.10688v2| 1;2024-02-16;https://www.arxiv.org/abs/2402.10688v1	arXiv:2402.10688			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 15 2024	2024	Large language models (LLMs) have led to breakthroughs in language tasks, yet the internal mechanisms that enable their remarkable generalization and reasoning abilities remain opaque. This lack of transparency presents challenges such as hallucinations, toxicity, and misalignment with human values, hindering the safe and beneficial deployment of LLMs. This paper aims to uncover the mechanisms underlying LLM functionality through the lens of explainability. First, we review how knowledge is architecturally composed within LLMs and encoded in their internal parameters via mechanistic interpretability techniques. Then, we summarize how knowledge is embedded in LLM representations by leveraging probing techniques and representation engineering. Additionally, we investigate the training dynamics through a mechanistic perspective to explain phenomena such as grokking and memorization. Lastly, we explore how the insights gained from these explanations can enhance LLM performance through model editing, improve efficiency through pruning, and better align with human values.																																	2024-04-26	PPRN:87754178		
J	Yamada, Yutaro; Bao, Yihan; Lampinen, Andrew K.; Kasai, Jungo; Yildirim, Ilker				Bao, Yihan/KAM-2515-2024						Evaluating Spatial Understanding of Large Language Models								Arxiv											2	2;2024-04-13;https://www.arxiv.org/abs/2310.14540v3| 1;2023-10-23;https://www.arxiv.org/abs/2310.14540v1	arXiv:2310.14540			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 13 2024	2024	Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge — spatial relationships. We design natural -language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs’ mistakes reflect both spatial and non -spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains.1																																	2024-04-25	PPRN:85757262		
J	Qiu, Haoran; Mao, Weichao; Patke, Archit; Cui, Shengkun; Jha, Saurabh; Wang, Chen; Franke, Hubertus; Kalbarczyk, Zbigniew T.; Basar, Tamer; Iyer, Ravishankar K.				Qiu, Haoran/AFE-3899-2022; Wang, Chen/AAG-2436-2019						Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction								Arxiv											1	1;2024-04-12;https://www.arxiv.org/abs/2404.08509v1	arXiv:2404.08509			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 12 2024	2024	Large language models (LLMs) have been driving a new wave of interactive AI applications across numerous domains. However, efficiently serving LLM inference requests is challenging due to their unpredictable execution times originating from the autoregressive nature of generative models. Existing LLM serving systems exploit first-come-first-serve (FCFS) scheduling, suffering from head-of-line blocking issues. To address the non-deterministic nature of LLMs and enable efficient interactive LLM serving, we present a speculative shortest-job-first (SSJF) scheduler that uses a light proxy model to predict LLM output sequence lengths. Our open-source SSJF implementation does not require changes to memory management or batching strategies. Evaluations on real-world datasets and production workload traces show that SSJF reduces average job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x compared to FCFS schedulers, across no batching, dynamic batching, and continuous batching settings.																																	2024-04-26	PPRN:88550462		
J	Chen, Yu-Hsueh; Grover, Tarun				Grover, Tarun/AAX-6598-2021						Separability transitions in topological states induced by local decoherence								Arxiv											1	1;2024-04-10;https://www.arxiv.org/abs/2309.11879v2	arXiv:2309.11879			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 10 2024	2024	We study states with intrinsic topological order subjected to local decoherence from the perspective of separability, i.e., whether a decohered mixed state can be expressed as an ensemble of short-range entangled (SRE) pure states. We focus on toric codes and the X -cube fracton state and provide evidence for the existence of decoherence-induced separability transitions that precisely coincide with the threshold for the feasibility of active error correction. A key insight is that local decoherence acting on the ‘parent’ cluster states of these models results in a Gibbs state. As an example, for the 2d (3d) toric code subjected to bit -flip errors, we show that the decohered density matrix can be written as a convex sum of SRE states for p > pc, where pc is related to the paramagneticferromagnetic transition in the 2d (3d) random -field bond Ising model along the Nishimori line.																																	2024-04-24	PPRN:86281329		
J	Li, Shufan; Kallidromitis, Konstantinos; Gokul, Akash; Kato, Yusuke; Kozuka, Kazuki				Li, Shuangning/GLT-4682-2022						Aligning Diffusion Models by Optimizing Human Utility								Arxiv											1	1;2024-04-06;https://www.arxiv.org/abs/2404.04465v1	arXiv:2404.04465			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 06 2024	2024	We present Diffusion-KTO, a novel approach for aligning text-to-image diffusion models by formulating the alignment objective as the maximization of expected human utility. Since this objective applies to each generation independently, Diffusion-KTO does not require collecting costly pairwise preference data nor training a complex reward model. Instead, our objective requires simple per-image binary feedback signals, e.g. likes or dislikes, which are abundantly available. After fine-tuning using Diffusion-KTO, text-to-image diffusion models exhibit superior performance compared to existing techniques, including supervised fine-tuning and Diffusion-DPO, both in terms of human judgment and automatic evaluation metrics such as PickScore and ImageReward. Overall, Diffusion-KTO unlocks the potential of leveraging readily available per-image binary signals and broadens the applicability of aligning text-to-image diffusion models with human preferences.																																	2024-05-22	PPRN:88447185		
J	Yadkori, Yasin Abbasi; Kuzborskij, Ilja; Stutz, David; Gyorgy, Andras; Fisch, Adam; Doucet, Arnaud; Beloshapka, Iuliya; Weng, Wei-Hung; Yang, Yao-Yuan; Szepesvari, Csaba; Cemgil, Ali Taylan; Tomasev, Nenad				Cemgil, Ali/A-3068-2016						Mitigating LLM Hallucinations via Conformal Abstention								Arxiv											1	1;2024-04-04;https://www.arxiv.org/abs/2405.01563v1	arXiv:2405.01563			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 04 2024	2024	We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying “I don’t know”) in a general domain, instead of resorting to possibly “hallucinating” a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.																																	2024-05-28	PPRN:88837997		
J	Harman, Nate; Snowden, Andrew										Oligomorphic groups and tensor categories								Arxiv											1	1;2024-04-02;https://www.arxiv.org/abs/2204.04526v2	arXiv:2204.04526			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	Given an oligomorphic group G and a measure µ for G (in a sense that we introduce), we define a rigid tensor category Perm(G; µ) of “permutation modules,” and, in certain cases, an abelian envelope Rep(G; µ) of this category. When G is the infinite symmetric group, this recovers Deligne’s interpolation category. Other choices for G lead to fundamentally new tensor categories. For example, we construct the first known semisimple pre-Tannakian categories in positive characteristic with super-exponential growth. One interesting aspect of our construction is that, unlike previous work in this direction, our categories are concrete: the objects are modules over a ring, and the tensor product receives a universal bi-linear map. Central to our constructions is a novel theory of integration on oligomorphic groups, which could be of more general interest. Classifying the measures on an oligomorphic group appears to be a difficult problem, which we solve in only a few cases. [GRAPHICS]																																	2024-04-18	PPRN:88376793		
J	Redfield, Seth; Batalha, Natasha; Benneke, Bjoern; Biller, Beth; Espinoza, Nestor; France, Kevin; Konopacky, Quinn; Kreidberg, Laura; Rauscher, Emily; Sing, David										Report of the Working Group on Strategic Exoplanet Initiatives with HST and JWST								Arxiv											1	1;2024-04-02;https://www.arxiv.org/abs/2404.02932v1	arXiv:2404.02932			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	This STScI Working Group (WG) was charged with soliciting community feedback and evaluating the strategic planning for exoplanet science with JWST and HST given the high quality of exoplanet observations, the significantly lengthened mission lifetime for JWST, and the pronounced expansion of the field over the last decade. We were charged with identifying key science themes, providing recommendations on issues associated with optimal timing and scale of resources, as well as providing a recommended DDT concept achievable with 500 hours of JWST time. We recommend a DDT concept to survey the atmospheres of rocky-M dwarf exoplanets. It is critical to quickly survey a wide sample of such targets to ascertain if they indeed host significant atmospheres, i.e., define the cosmic shoreline, and to identify high priority targets for future follow-up. It is important for this effort to occur early in the mission lifetime. In the context of strategic planning of exoplanet observations, it is useful to estimate the expected exoplanet observational commitment over JWST's lifetime. Given the current usage associated with exoplanets, extended over 20 cycles, it is anticipated that JWST will dedicate $approx$30,000 hours to exoplanet observations. We recommend efforts to support GO-driven programs that will contribute to this unprecedented data product of JWST. Of the $approx$30,000 hours of anticipated JWST full-mission time dedicated to exoplanets, we expect that 1/3 of it could, and perhaps inevitably would, form a comprehensive, high S/N, panchromatic, 10$^4$ hour atmospheric survey of planets. Such an observational sample would be a legacy archive that would address a broad range of science questions across various populations of planets. It would also bridge the direct imaging and transit communities and involve a multitude of techniques to detect and characterize exoplanets.																																	2024-04-19	PPRN:88413990		
J	Lu, Yifan; Hu, Yue; Zhong, Yiqi; Wang, Dequan; Wang, Yanfeng; Chen, Siheng				Lu, Yifan/GWQ-8871-2022; Wang, Yan-Feng/F-3288-2016						AN EXTENSIBLE FRAMEWORK FOR OPEN HETEROGE- NEOUS COLLABORATIVE PERCEPTION								Arxiv											3	3;2024-04-01;https://www.arxiv.org/abs/2401.13964v3| 2;2024-02-16;https://www.arxiv.org/abs/2401.13964v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.13964v1	arXiv:2401.13964			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types.																																	2024-04-18	PPRN:87336897		
J	Qiu, Ri-Zhao; Yang, Ge; Zeng, Weijia; Wang, Xiaolong				Zeng, Weijia/OUH-9298-2025						Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2404.01223v1	arXiv:2404.01223			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. 																																	2024-04-18	PPRN:88367025		
J	Shen, Yue; Grier, Catherine J.; Horne, Keith; Stone, Zachary; Li, Jennifer I.; Yang, Qian; Homayouni, Yasaman; Trump, Jonathan R.; Anderson, Scott F.; Brandt, W.N.; Hall, Patrick B.; Ho, Luis C.; Jiang, Linhua; Petitjean, Patrick; Schneider, Donald P.; Tao, Charling; Donnan, Fergus.R.; Alsayyad, Yusra; Bershady, Matthew A.; Blanton, Michael R.; Bizyaev, Dmitry; Bundy, Kevin; Chen, Yuguang; Davis, Megan C.; Dawson, Kyle; Fan, Xiaohui; Greene, Jenny E.; Groller, Hannes; Guo, Yucheng; Ibarra-Medel, Hector; Jiang, Yuanzhe; Keenan, Ryan P.; Kollmeier, Juna A.; Lejoly, Cassandra; Li, Zefeng; Macorra, Axel de la; Moe, Maxwell; Nie, Jundan; Rossi, Graziano; Smith, Paul S.; Tee, Wei Leong; Weijmans, Anne-Marie; Xu, Jiachuan; Yue, Minghao; Zhou, Xu; Zhou, Zhimin; Zou, Hu				Guo, Y/MTF-3808-2025; Bundy, Kevin/ABE-6356-2021; Fan, Xiaohui/F-6458-2010; Trump, Jonathan/NPI-4613-2025; Schneider, Donald/ACH-4274-2022; Ibarra, Hector/ITT-0741-2023; Bershady, Matthew/HOH-5118-2023; Weijmans, Anne-Marie/KIL-5813-2024; Nie, Jundan/NQF-2071-2025; Zou, Hu/HGD-9992-2022; Hall, Patrick/PBU-6720-2025; Brandt, William/N-2844-2015						The Sloan Digital Sky Survey Reverberation Mapping Project: Key Results								Arxiv											1	1;2024-04-01;https://www.arxiv.org/abs/2305.01014v2	arXiv:2305.01014			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 01 2024	2024	We present the final data from the Sloan Digital Sky Survey Reverberation Mapping (SDSS-RM) project, a precursor to the SDSS-V Black Hole Mapper Reverberation Mapping program. This data set includes 11-year photometric and 7-year spectroscopic light curves for 849 broad-line quasars over a redshift range of 0.1 < z < 4.5 and a luminosity range of Lbol = 1044−47.5 erg s−1, along with spectral and variability measurements. We report 23, 81, 125, and 110 reverberation mapping lags (relative to optical continuum variability) for broad Hα, Hβ, Mg II and C Iv using the SDSS-RM sample, spanning much of the luminosity and redshift ranges of the sample. Using 30 low-redshift RM AGNs with dynamical-modeling black hole masses, we derive a new estimate of the average virial factor of ⟨log f⟩ = 0.62 ± 0.07 for the line dispersion measured from the RMS spectrum. The intrinsic scatter of individual virial factors is 0.31 ± 0.07 dex, indicating a factor of two systematic uncertainty in RM black hole masses. Our lag measurements reveal significant R− L relations for Hβ and Mg II at high redshift, consistent with the latest measurements based on heterogeneous samples. While we are unable to robustly constrain the slope of the R − L relation for C Iv given the limited dynamic range in luminosity, we found substantially larger scatter in C Iv lags at fixed L1350. Using the SDSS-RM lag sample, we derive improved single-epoch (SE) mass recipes for Hβ, Mg II and C Iv, which are consistent with their respective RM masses as well as between the SE recipes from two different lines, over the luminosity range probed by our sample. The new Hβ and Mg II recipes are approximately unbiased estimators at given RM masses, but there are systematic biases in the C Iv recipe. The intrinsic scatter of SE masses around RM masses is ∼ 0.45 dex for Hβ and Mg II, increasing to ∼ 0.58 dex for C Iv.																																	2024-05-11	PPRN:66775336		
J	Andukuri, Chinmaya; Fraenken, Jan-Philipp; Gerstenberg, Tobias; Goodman, Noah D.										STaR-GATE: Teaching Language Models to Ask Clarifying Questions								Arxiv											2	2;2024-03-29;https://www.arxiv.org/abs/2403.19154v2| 1;2024-03-28;https://www.arxiv.org/abs/2403.19154v1	arXiv:2403.19154			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 29 2024	2024	When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model's ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions-a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model—the Questioner—and a Roleplayer whose preferences are unknown to the Questioner. By asking questions, the Questioner elicits preferences from the Roleplayer. The Questioner is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer's latent preferences. After two iterations of self-improvement, the Questioner asks better questions, allowing it to generate responses that are preferred over responses from the initial model on 72% of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.																																	2024-04-17	PPRN:88331078		
J	Stoehr, Niklas; Gordon, Mitchell; Zhang, Chiyuan; Lewis, Owen										Localizing Paragraph Memorization in Language Models								Arxiv											1	1;2024-03-28;https://www.arxiv.org/abs/2403.19851v1	arXiv:2403.19851			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 28 2024	2024	Can we localize the weights and mechanisms used by a language model to memorize and recite entire paragraphs of its training data? In this paper, we show that while memorization is spread across multiple layers and model components, gradients of memorized paragraphs have a distinguishable spatial pattern, being larger in lower model layers than gradients of non-memorized examples. Moreover, the memorized examples can be unlearned by fine-tuning only the high-gradient weights. We localize a low-layer attention head that appears to be especially involved in paragraph memorization. This head is predominantly focusing its attention on distinctive, rare tokens that are least frequent in a corpus-level unigram distribution. Next, we study how localized memorization is across the tokens in the prefix by perturbing tokens and measuring the caused change in the decoding. A few distinctive tokens early in a prefix can often corrupt the entire continuation. Overall, memorized continuations are not only harder to unlearn, but also to corrupt than non-memorized ones.																																	2024-04-17	PPRN:88345387		
J	Xia, Chunqiu Steven; Deng, Yinlin; Zhang, Lingming				Deng, Yinlin/KBQ-0846-2024						Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM								Arxiv											1	1;2024-03-28;https://www.arxiv.org/abs/2403.19114v1	arXiv:2403.19114			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 28 2024	2024	LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and complete LLM generations at https://github.com/evo-eval/evoeval																																	2024-04-14	PPRN:88333302		
J	Shi, Zhelun; Wang, Zhipin; Fan, Hongxing; Zhang, Zaibin; Li, Lijun; Zhang, Yongting; Yin, Zhenfei; Sheng, Lu; Qiao, Yu; Shao, Jing				Sheng, Lu/V-2526-2019						Assessment of Multimodal Large Language Models in Alignment with Human Values								Arxiv											1	1;2024-03-26;https://www.arxiv.org/abs/2403.17830v1	arXiv:2403.17830			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 26 2024	2024	Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations. To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations. Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle. We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives. Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field.																																	2025-08-07	PPRN:123162437		
J	Wang, Binxu; Vastola, John J.				Wang, Binxu/JAX-3811-2023						Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later								Arxiv											2	2;2024-03-26;https://www.arxiv.org/abs/2303.02490v2| 1;2023-03-04;https://www.arxiv.org/abs/2303.02490v1	arXiv:2303.02490			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 26 2024	2024	How do diffusion generative models convert pure noise into meaningful images? In a variety of pretrained diffusion models (including conditional latent space models like Stable Diffusion), we observe that the reverse diffusion process that underlies image generation has the following properties: (i) individual trajectories tend to be low-dimensional and resemble 2D `rotations'; (ii) high-variance scene features like layout tend to emerge earlier, while low-variance details tend to emerge later; and (iii) early perturbations tend to have a greater impact on image content than later perturbations. To understand these phenomena, we derive and study a closed-form solution to the probability flow ODE for a Gaussian distribution, which shows that the reverse diffusion state rotates towards a gradually-specified target on the image manifold. It also shows that generation involves first committing to an outline, and then to finer and finer details. We find that this solution accurately describes the initial phase of image generation for pretrained models, and can in principle be used to make image generation more efficient by skipping reverse diffusion steps. Finally, we use our solution to characterize the image manifold in Stable Diffusion. Our viewpoint reveals an unexpected similarity between generation by GANs and diffusion and provides a conceptual link between diffusion and image retrieval. [GRAPHICS]																																	2024-04-14	PPRN:43599347		
J	Malekjani, Mohammad; Mc Conville, Ruairi; Colgain, Eoin O; Pourojaghi, Saeed; Sheikh-Jabbari, M.M.				Malekjani, Mohammad/O-8692-2019; Pourojaghi, Saeed/GWZ-5118-2022; Sheikh-Jabbari, Mohammad/AAM-7848-2020						Negative Dark Energy Density from High Redshift Pantheon+ Supernovae								Arxiv											2	2;2024-03-21;https://www.arxiv.org/abs/2301.12725v3| 1;2023-08-11;https://www.arxiv.org/abs/2301.12725v2	arXiv:2301.12725			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 21 2024	2024	Within the Friedmann-Lemaıtre-Robertson-Walker (FLRW) framework, the Hubble constant H0 is an integration constant. Thus, consistency of the model demands observational constancy of H0. We demonstrate redshift evolution of best fit ACDM parameters (H0, Ωm) in Pantheon+ supernove (SNe). Redshift evolution of best fit cosmological parameters is a prerequisite to finding a statistically significant evolution as well as identifying alternative models that are competitive with ACDM in a Bayesian model comparison. To assess statistical significance, we employ three different methods: i) Bayesian model comparison, ii) mock simulations and iii) profile distributions. The first shows a marginal preference for the vanilla ACDM model over an ad hoc model with 3 additional parameters and an unphysical jump in cosmological parameters at z = 1. From mock simulations, we estimate the statistical significance of redshift evolution of best fit parameters and negative dark energy density (Ωm > 1) to be in the 1 − 2σ range, depending on the criteria employed. Importantly, in direct comparison to the same analysis with the earlier Pantheon sample we find that statistical significance of redshift evolution of best fit parameters has increased, as expected for a physical effect. Our profile distribution analysis demonstrates a shift in (H0, Ωm) in excess of 95% confidence level for SNe with redshifts z > 1 and also shows that a degeneracy in MCMC posteriors is not equivalent to a curve of constant x2. Our findings can be interpreted as a statistical fluctuation or unexplored systematics in Pantheon+ or ACDM model breakdown. The first two possibilities are disfavoured by similar trends in independent probes.																																	2024-04-13	PPRN:76983089		
J	Lu, Yuhang; Zhu, Xinge; Wang, Tai; Ma, Yuexin				Wang, Tai/MVV-1100-2025; zhu, xinge/LGZ-7330-2024						OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries								Arxiv											3	3;2024-03-19;https://www.arxiv.org/abs/2312.03774v3| 2;2023-12-09;https://www.arxiv.org/abs/2312.03774v2| 1;2023-12-06;https://www.arxiv.org/abs/2312.03774v1	arXiv:2312.03774			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	Occupancy prediction has increasingly garnered attention in recent years for its fine-grained understanding of 3D scenes. Traditional approaches typically rely on dense, regular grid representations, which often leads to excessive computational demands and a loss of spatial details for small objects. This paper introduces OctreeOcc, an innovative 3D occupancy prediction framework that leverages the octree representation to adaptively capture valuable information in 3D, offering variable granularity to accommodate object shapes and semantic regions of varying sizes and complexities. In particular, we incorporate image semantic information to improve the accuracy of initial octree structures and design an effective rectification mechanism to refine the octree structure iteratively. Our extensive evaluations show that OctreeOcc not only surpasses state-of-the-art methods in occupancy prediction, but also achieves a 15%-24% reduction in computational overhead compared to dense-grid-based methods.																																	2024-04-12	PPRN:86442589		
J	Ma, Yuexiao; Li, Huixia; Zheng, Xiawu; Ling, Feng; Xiao, Xuefeng; Wang, Rui; Wen, Shilei; Chao, Fei; Ji, Rongrong				Li, Shiyan/H-3445-2016						AFFINEQUANT: AFFINE TRANSFORMATION QUANTIZATION FOR LARGE LANGUAGE MODELS								Arxiv											1	1;2024-03-19;https://www.arxiv.org/abs/2403.12544v1	arXiv:2403.12544			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. This constraint results in significant errors after quantization, particularly in low-bit configurations. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and generalization capabilities. To ensure the invertibility of the transformation during optimization, we further introduce a gradual mask optimization method. This method initially focuses on optimizing the diagonal elements and gradually extends to the other elements. Such an approach aligns with the Levy-Desplanques theorem, theoretically ensuring invertibility of the transformation. As a result, significant performance improvements are evident across different LLMs on diverse datasets. Notably, these improvements are most pronounced when using very low-bit quantization, enabling the deployment of large models on edge devices. To illustrate, we attain a C4 perplexity of 15.76 (2.26↓ vs 18.02 in OmniQuant) on the LLaMA2-7B model of W4A4 quantization without overhead. On zero-shot tasks, AffineQuant achieves an average of 58.61% accuracy (1.98% ↑ vs 56.63 in OmniQuant) when using 4/4-bit quantization for LLaMA-30B, which setting a new state-of-the-art benchmark for PTQ in LLMs.																																	2024-04-12	PPRN:88239349		
J	Fu, Hengyu; Yang, Zhuoran; Wang, Mengdi; Chen, Minshuo				Chen, Minshuo/NQE-7177-2025; Yang, Zhuoran/IYJ-4459-2023						Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.11968v1	arXiv:2403.11968			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation.																																	2024-04-11	PPRN:88196140		
J	Chernozhukov, Victor; Newey, Whitney K.; Quintas-Martinez, Victor; Syrgkanis, Vasilis										Automatic Debiased Machine Learning via Riesz Regression								Arxiv											2	2;2024-03-14;https://www.arxiv.org/abs/2104.14737v3| 1;2021-04-30;https://www.arxiv.org/abs/2104.14737v1	arXiv:2104.14737			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 14 2024	2024	A variety of interesting parameters may depend on high dimensional regressions. Machine learning can be used to estimate such parameters. However estimators based on machine learners can be severely biased by regularization and/or model selection. Debiased machine learning uses Neyman orthogonal estimating equations to reduce such biases. Debiased machine learning generally requires estimation of unknown Riesz representers. A primary innovation of this paper is to provide Riesz regression estimators of Riesz representers that depend on the parameter of interest, rather than explicit formulae, and that can employ any machine learner, including neural nets and random forests. End-to-end algorithms emerge where the researcher chooses the parameter of interest and the machine learner and the debiasing follows automatically. Another innovation here is debiased machine learners of parameters depending on generalized regressions, including high-dimensional generalized linear models. An empirical example of automatic debiased machine learning using neural nets is given. We find in Monte Carlo examples that automatic debiasing sometimes performs better than debiasing via inverse propensity scores and never worse. Finite sample mean square error bounds for Riesz regression estimators and asymptotic theory are also given.																																	2024-04-11	PPRN:11880497		
J	Yang, Shu; Wang, Yihui; Chen, Hao				Wang, Yihui/OLQ-4655-2025; Yang, Shu/I-2051-2014; Chen, Hao/JHU-3470-2023						MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology								Arxiv											1	1;2024-03-11;https://www.arxiv.org/abs/2403.06800v1	arXiv:2403.06800			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 11 2024	2024	Multiple Instance Learning (MIL) has emerged as a dominant paradigm to extract discriminative feature representations within Whole Slide Images (WSIs) in computational pathology. Despite driving notable progress, existing MIL approaches suffer from limitations in facilitating comprehensive and efficient interactions among instances, as well as challenges related to time-consuming computations and overfitting. In this paper, we incorporate the Selective Scan Space State Sequential Model (Mamba) in Multiple Instance Learning (MIL) for long sequence modeling with linear complexity, termed as MambaMIL. By inheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability to comprehensively understand and perceive long sequences of instances. Furthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the order and distribution of instances, which exploits the inherent valuable information embedded within the long sequences. With the SR-Mamba as the core component, MambaMIL can effectively capture more discriminative features and mitigate the challenges associated with overfitting and high computational overhead. Extensive experiments on two public challenging tasks across nine diverse datasets demonstrate that our proposed framework performs favorably against state-of-the-art MIL methods.																																	2024-04-07	PPRN:88103881		
J	Aghaee, Morteza; Akkala, Arun; Alam, Zulfi; Ali, Rizwan; Ramirez, Alejandro Alcaraz; Andrzejczuk, Mariusz; Antipov, Andrey E.; Aseev, Pavel; Astafev, Mikhail; Bauer, Bela; Becker, Jonathan; Boddapati, Srini; Boekhout, Frenk; Bommer, Jouri; Hansen, Esben Bork; Bosma, Tom; Bourdet, Leo; Boutin, Samuel; Caroff, Philippe; Casparis, Lucas; Cassidy, Maja; Chatoor, Sohail; Christensen, Anna Wulf; Clay, Noah; Cole, William S.; Corsetti, Fabiano; Cui, Ajuan; Dalampiras, Paschalis; Dokania, Anand; de Lange, Gijs; de Moor, Michiel; Saldana, Juan Carlos Estrada; Fallahi, Saeed; Fathabad, Zahra Heidarnia; Gamble, John; Gardner, Geoff; Govender, Deshan; Griggio, Flavio; Grigoryan, Ruben; Gronin, Sergei; Gukelberger, Jan; Heedt, Sebastian; Zamorano, Jesus Herranz; Ho, Samantha; Holgaard, Ulrik Laurens; Nielsen, William Hvidtfelt Padkaer; Ingerslev, Henrik; Johansson, Linda; Jones, Jeffrey; Kallaher, Ray; Karimi, Farhad; Karzig, Torsten; King, Cameron; Kloster, Maren Elisabeth; Knapp, Christina; Kocon, Dariusz; Koski, Jonne; Kostamo, Pasi; Krogstrup, Peter; Kumar, Mahesh; Laeven, Tom; Larsen, Thorvald; Li, Kongyi; Lindemann, Tyler; Love, Julie; Lutchyn, Roman; Madsen, Morten Hannibal; Manfra, Michael; Markussen, Signe; Martinez, Esteban; McNeil, Robert; Memisevic, Elvedin; Morgan, Trevor; Mullally, Andrew; Nayak, Chetan; Nielsen, Jens; Nijholt, Bas; Nurmohamed, Anne; O'Farrell, Eoin; Otani, Keita; Pauka, Sebastian; Petersson, Karl; Petit, Luca; Pikulin, Dima; Preiss, Frank; Perez, Marina Quintero; Rajpalke, Mohana; Rasmussen, Katrine; Razmadze, Davydas; Reentila, Outi; Reilly, David; Rouse, Richard; Sadovskyy, Ivan A.; Sainiemi, Lauri; Schreppler, Sydney; Sidorkin, Vadim; Singh, Amrita; Singh, Shilpi; Sinha, Sarat; Sohr, Patrick; Stankevic, Tomas; Stek, Lieuwe; Suominen, Henri; Suter, Judith; Svidenko, Vicky; Teicher, Sam; Temuerhan, Mine; Thiyagarajah, Nivetha; Tholapi, Raj; Thomas, Mason; Toomey, Emily; Upadhyay, Shivendra; Urban, Ivan; Vaitiekenas, Saulius; Van Hoogdalem, Kevin; Van Woerkom, David; Viazmitinov, Dmitrii V.; Vogel, Dominik; Waddy, Steven; Watson, John; Weston, Joseph; Winkler, Georg W.; Yang, Chung Kai; Yau, Sean; Yi, Daniel; Yucelen, Emrah; Webster, Alex; Zeisel, Roland; Zhao, Ruichen		Microsoft Quantum		Gamble, John/HKV-1362-2023; Kumar, Mahesh/P-3551-2019; Webster, Alex/KCY-6083-2024; Sainiemi, Lauri/JSK-9162-2023; Astafev, Mikhail/B-7801-2015; SINGH, AMRITA/KZU-7012-2024; Ali, Rizwan/P-3566-2017						InAs-Al Hybrid Devices Passing the Topological Gap Protocol								Arxiv											2	2;2024-03-08;https://www.arxiv.org/abs/2207.02472v4| 1;2022-07-07;https://www.arxiv.org/abs/2207.02472v2	arXiv:2207.02472			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 08 2024	2024	We present measurements and simulations of semiconductor -superconductor heterostructure devices that are consistent with the observation of topological superconductivity and Majorana zero modes. The devices are fabricated from high -mobility two-dimensional electron gases in which quasione -dimensional wires are defined by electrostatic gates. These devices enable measurements of local and non -local transport properties and have been optimized via extensive simulations to ensure robustness against non -uniformity and disorder. Our main result is that several devices, fabricated according to the design’s engineering specifications, have passed the topological gap protocol defined in Pikulin et al. [arXiv:2103.12217]. This protocol is a stringent test composed of a sequence of three -terminal local and non -local transport measurements performed while varying the magnetic field, semiconductor electron density, and junction transparencies. Passing the protocol indicates a high probability of detection of a topological phase hosting Majorana zero modes as determined by large-scale disorder simulations. Our experimental results are consistent with a quantum phase transition into a topological superconducting phase that extends over several hundred millitesla in magnetic field and several millivolts in gate voltage, corresponding to approximately one hundred micro -electron -volts in Zeeman energy and chemical potential in the semiconducting wire. These regions feature a closing and re -opening of the bulk gap, with simultaneous zero -bias conductance peaks at both ends of the devices that withstand changes in the junction transparencies. The extracted maximum topological gaps in our devices are 20-60 µeV. This demonstration is a prerequisite for experiments involving fusion and braiding of Majorana zero modes.																																	2025-05-24	PPRN:10395490		
J	Chakrabarty, Tuhin; Laban, Philippe; Agarwal, Divyansh; Muresan, Smaranda; Wu, Chien-Sheng				Wu, Chien-Sheng/AAH-1354-2019						Art or Artifice? Large Language Models and the False Promise of Creativity								Arxiv											3	3;2024-03-08;https://www.arxiv.org/abs/2309.14556v3| 2;2024-02-25;https://www.arxiv.org/abs/2309.14556v2| 1;2023-09-25;https://www.arxiv.org/abs/2309.14556v1	arXiv:2309.14556			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 08 2024	2024	Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.																																	2024-04-07	PPRN:85287053		
J	Barkeshli, Maissam; Chen, Yu-An; Hsin, Po-Shen; Kobayashi, Ryohei				Kobayashi, Ryohei/GVT-0939-2022						Higher-group symmetry in finite gauge theory and stabilizer codes								Arxiv											2	2;2024-03-06;https://www.arxiv.org/abs/2211.11764v3| 1;2022-11-21;https://www.arxiv.org/abs/2211.11764v1	arXiv:2211.11764			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 06 2024	2024	A large class of gapped phases of matter can be described by topological finite group gauge theories. In this paper, we show how such gauge theories possess a higher -group global symmetry, which we study in detail. We derive the d -group global symmetry and its ’t Hooft anomaly for topological finite group gauge theories in (d + 1) space-time dimensions, including non -Abelian gauge groups and Dijkgraaf-Witten twists. We focus on the 1 -form symmetry generated by invertible (Abelian) magnetic defects and the higher -form symmetries generated by invertible topological defects decorated with lower dimensional gauged symmetry -protected topological (SPT) phases. We show that due to a generalization of the Witten effect and charge -flux attachment, the 1 -form symmetry generated by the magnetic defects mixes with other symmetries into a higher group. We describe such higher -group symmetry in various lattice model examples. We discuss several applications, including the classification of fermionic SPT phases in (3+1)D for general fermionic symmetry groups, where we also derive a simpler formula for the [O5] ∈ H5(BG, U(1)) obstruction that has appeared in prior work. We also show how the d -group symmetry is related to fault -tolerant non -Pauli logical gates and a refined Clifford hierarchy in stabilizer codes. We discover new logical gates in stabilizer codes using the d -group symmetry, such as a controlled Z gate in the (3+1) D Z2 toric code.																																	2024-04-03	PPRN:23119553		
J	Ji, Sijie; Zheng, Xinzhe; Wu, Chenshu				Wu, Chenshu/E-8190-2012						HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?								Arxiv											1	1;2024-03-05;https://www.arxiv.org/abs/2403.02727v1	arXiv:2403.02727			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 05 2024	2024	There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR)? Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and “think step-by-step” strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and stateof -the -art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform all the baselines on both datasets. Our findings indicate that by effective prompting, LLMs can interpret raw IMU data based on their knowledge base, possessing a promising potential to analyze raw sensor data of the physical world effectively.																																	2024-04-02	PPRN:88028870		
J	Zhang, Zhenyu; Chen, Runjin; Liu, Shiwei; Yao, Zhewei; Ruwase, Olatunji; Chen, Beidi; Wu, Xiaoxia; Wang, Zhangyang				Yao, Zhewei/ABE-1531-2021; Zhang, Zhenyu/ISS-1688-2023; Zhihua, Wang/AFO-5263-2022; Chen, Runjin/LZH-7447-2025						Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding								Arxiv											1	1;2024-03-05;https://www.arxiv.org/abs/2403.04797v1	arXiv:2403.04797			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 05 2024	2024	This paper aims to overcome the "lost-in-the-middle" challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-scale context fusion from short to long distance. Extensive experiments with a wide range of LLMs demonstrate the efficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of up to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are available at https://github.com/VITA-Group/Ms-PoE.																																	2024-04-07	PPRN:88081763		
J	Ahdritz, Gustaf; Qin, Tian; Vyas, Nikhil; Barak, Boaz; Edelman, Benjamin L.										Distinguishing the Knowable from the Unknowable with Language Models								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/2402.03563v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.03563v1	arXiv:2402.03563			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings.																																	2024-03-27	PPRN:87529258		
J	Parmar, Jupinder; Prabhumoye, Shrimai; Jennings, Joseph; Patwary, Mostofa; Subramanian, Sandeep; Su, Dan; Zhu, Chen; Narayanan, Deepak; Jhunjhunwala, Aastha; Dattagupta, Ayush; Jawa, Vibhu; Liu, Jiwei; Mahabaleshwarkar, Ameya; Nitski, Osvald; Brundyn, Annika; Maki, James; Martinez, Miguel; You, Jiaxuan; Kamalu, John; LeGresley, Patrick; Fridman, Denys; Casper, Jared; Aithal, Ashwath; Kuchaiev, Oleksii; Shoeybi, Mohammad; Cohen, Jonathan; Catanzaro, Bryan				You, Jiaxuan/ABC-7506-2020; Jawa, Vibhu/NFT-6031-2025						Nemotron-4 15B Technical Report								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.16819v2	arXiv:2402.16819			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	We introduce Nemotron-4 15B, a 15 -billion -parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly -sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarlysized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.																																	2024-11-09	PPRN:87921246		
J	Zhu, Jiacheng; Greenewald, Kristjan; Nadjahi, Kimia; Borde, Haitz Saez de Ocariz; Gabrielsson, Rickard Brueel; Choshen, Leshem; Ghassemi, Marzyeh; Yurochkin, Mikhail; Solomon, Justin				Zhu, Jiacheng/HPF-7304-2023						Asymmetry in Low-Rank Adapters of Foundation Models								Arxiv											1	1;2024-02-27;https://www.arxiv.org/abs/2402.16842v2	arXiv:2402.16842			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 27 2024	2024	Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product BA, we observe that the B and A matrices have distinct functions: A extracts features from the input, while B uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning B is inherently more effective than fine-tuning A, and that a random untrained A should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training B improves the bound. We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs. The code and data is available at https: //github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA																																	2024-03-25	PPRN:87922041		
J	Wu, Fangzhou; Wu, Shutong; Cao, Yulong; Xiao, Chaowei				Wu, Fangzhou/IST-1642-2023; Xiao, Chaowei/AAT-8772-2021; Wu, Shutong/KFQ-9016-2024						WIPI: A New Web Threat for LLM-Driven Web Agents								Arxiv											1	1;2024-02-26;https://www.arxiv.org/abs/2402.16965v1	arXiv:2402.16965			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 26 2024	2024	With the fast development of large language models (LLMs), LLM-driven Web Agents (Web Agents for short) have obtained tons of attention due to their superior capability where LLMs serve as the core part of making decisions like the human brain equipped with multiple web tools to actively interact with external deployed websites. As uncountable Web Agents have been released and such LLM systems are experiencing rapid development and drawing closer to widespread deployment in our daily lives, an essential and pressing question arises: “Are these Web Agents secure?”. In this paper, we introduce a novel threat, WIPI, that indirectly controls Web Agent to execute malicious instructions embedded in publicly accessible webpages. To launch a successful WIPI works in a black -box environment. This methodology focuses on the form and content of indirect instructions within external webpages, enhancing the efficiency and stealthiness of the attack. To evaluate the effectiveness of the proposed methodology, we conducted extensive experiments using 7 plugin-based ChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The results reveal that our methodology achieves an average attack success rate (ASR) exceeding 90% even in pure black -box scenarios. Moreover, through an ablation study examining various user prefix instructions, we demonstrated that the WIPI exhibits strong robustness, maintaining high performance across diverse prefix instructions.																																	2024-11-10	PPRN:88004181		
J	Baptista, Ricardo; Marzouk, Youssef; Zahm, Olivier				Baptista, Ricardo/N-6383-2013						On the representation and learning of monotone triangular transport maps								Arxiv											2	2;2024-02-24;https://www.arxiv.org/abs/2009.10303v3| 1;2020-09-22;https://www.arxiv.org/abs/2009.10303v2	arXiv:2009.10303			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 24 2024	2024	Transportation of measure provides a versatile approach for modeling complex probability distributions, with applications in density estimation, Bayesian inference, generative modeling, and beyond. Monotone triangular transport maps—approximations of the Knothe–Rosenblatt (KR) rearrangement—are a canonical choice for these tasks. Yet the representation and parameterization of such maps have a significant impact on their generality and expressiveness, and on properties of the optimization problem that arises in learning a map from data (e.g., via maximum likelihood estimation). We present a general framework for representing monotone triangular maps via invertible transformations of smooth functions. We establish conditions on the transformation such that the associated infinite -dimensional minimization problem has no spurious local minima, i.e., all local minima are global minima; and we show for target distributions satisfying certain tail conditions that the unique global minimizer corresponds to the KR map. Given a sample from the target, we then propose an adaptive algorithm that estimates a sparse semi -parametric approximation of the underlying KR map. We demonstrate how this framework can be applied to joint and conditional density estimation, likelihood -free inference, and structure learning of directed graphical models, with stable generalization performance across a range of sample sizes.																																	2024-03-24	PPRN:10389521		
J	Fang, Fang; Wang, Kenneth; Liu, Vincent S.; Wang, Yu; Cimmino, Ryan; Wei, Julia; Bintz, Marcus; Parr, Avery; Kemp, Jack; Ni, Kang-Kuen; Yao, Norman Y.				Yao, Norman/A-3929-2009						Probing critical phenomena in open quantum systems using atom arrays								Arxiv											1	1;2024-02-23;https://www.arxiv.org/abs/2402.15376v1	arXiv:2402.15376			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 23 2024	2024	At continuous phase transitions, quantum many-body systems exhibit scale-invariance and complex, emergent universal behavior. Most strikingly, at a quantum critical point, correlations decay as a power law, with exponents determined by a set of universal scaling dimensions. Experimentally probing such power-law correlations is extremely challenging, owing to the complex interplay between decoherence, the vanishing energy gap, and boundary effects. Here, we employ a Rydberg quantum simulator to adiabatically prepare critical ground states of both a one-dimensional ring and a two-dimensional square lattice. By accounting for and tuning the openness of our quantum system, which is well-captured by the introduction of a single phenomenological length scale, we are able to directly observe power-law correlations and extract the corresponding scaling dimensions. Moreover, in two dimensions, we observe a decoupling between phase transitions in the bulk and on the boundary, allowing us to identify two distinct boundary universality classes. Our work demonstrates that direct adiabatic preparation of critical states in quantum simulators can complement recent approaches to studying quantum criticality using the Kibble-Zurek mechanism or digital quantum circuits.																																	2024-03-23	PPRN:87864836		
J	Wei, Shao-Wen; Liu, Yu-Xiao				Liu, Yuxiao/IUN-7857-2023						Testing the nature of Gauss-Bonnet gravity by four-dimensional rotating black hole shadow								Arxiv											2	2;2024-02-21;https://www.arxiv.org/abs/2003.07769v4| 1;2020-09-02;https://www.arxiv.org/abs/2003.07769v3	arXiv:2003.07769			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 21 2024	2024	The recent discovery of the novel four-dimensional static and spherically symmetric Gauss -Bonnet black hole provides a promising bed to test Gauss -Bonnet gravity by using astronomical observations [Phys. Rev. Lett. 124, 081301 (2020)]. In this paper, we first obtain the rotating Gauss -Bonnet black hole solution by using the Newman -Janis algorithm, and then study the shadow cast by the nonrotating and rotating candidate Gauss -Bonnet black holes. The result indicates that positive metric parameter α shrinks the shadow, while negative one enlarges it. Meanwhile, both the distortion and ratio of two diameters of the shadow are found to increase with the metric parameter for certain spin. Comparing with the Kerr black hole, the shadow gets more distorted for α, and less distorted for negative one. Furthermore, we calculate the angular diameter of the shadow by making use of the observation of M87*. The result indicates that negative metric parameter α in (-4.5, 0) is more favored. Since the negative energy appears for negative α, our results extends the study of Gauss -Bonnet gravity. We believe further study on the four-dimensional rotating black hole may shed new light on Gauss -Bonnet gravity.																																	2024-03-20	PPRN:13158427		
J	Dai, Yongfu; Feng, Duanyu; Huang, Jimin; Jia, Haochen; Xie, Qianqian; Zhang, Yifang; Han, Weiguang; Tian, Wei; Wang, Hao				Zhang, Yifang/F-1806-2018; Feng, Duanyu/OML-4880-2025						LAiW: A Chinese Legal Large Language Models Benchmark								Arxiv											2	2;2024-02-18;https://www.arxiv.org/abs/2310.05620v2| 1;2023-10-09;https://www.arxiv.org/abs/2310.05620v1	arXiv:2310.05620			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 18 2024	2024	General and legal domain LLMs have demonstrated strong performance in various tasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities. To address this challenge, we are the first to build the Chinese legal LLMs benchmark LAiW, based on the logic of legal practice. To align with the thinking process of legal experts and legal practice (syllogism), we divide the legal capabilities of LLMs from easy to difficult into three levels: basic information retrieval, legal foundation inference, and complex legal application. Each level contains multiple tasks to ensure a comprehensive evaluation. Through automated evaluation of current general and legal domain LLMs on our benchmark, we indicate that these LLMs may not align with the logic of legal practice. LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks, which may pose obstacles to their practical application and acceptance by legal experts. To further confirm the complex legal application capabilities of current LLMs in legal application scenarios, we also incorporate human evaluation with legal experts. The results indicate that while LLMs may demonstrate strong performance, they still require reinforcement of legal logic.																																	2024-03-15	PPRN:85573355		
J	AhmadiTeshnizi, Ali; Gao, Wenzhi; Udell, Madeleine										OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models								Arxiv											1	1;2024-02-15;https://www.arxiv.org/abs/2402.10172v1	arXiv:2402.10172			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 15 2024	2024	Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than 20% and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than 30%.																																	2024-03-08	PPRN:87699082		
J	Ye, Rui; Wang, Wenhao; Chai, Jingyi; Li, Dihan; Li, Zexi; Xu, Yinda; Du, Yaxin; Wang, Yanfeng; Chen, Siheng				Li, Zexi/GYQ-5430-2022; Wang, Wenhao/OFN-2602-2025						OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning								Arxiv											1	1;2024-02-10;https://www.arxiv.org/abs/2402.06954v1	arXiv:2402.06954			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 10 2024	2024	Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high -quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy -preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research -friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction -following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cover 30+ evaluation metrics. Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings. Notably, in a financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can outperform GPT-4 by a significant margin while the model obtained through individual training cannot, demonstrating strong motivation for clients to participate in FL. The code is available at https://github.com/rui-ye/OpenFedLLM.																																	2024-05-25	PPRN:87630586		
J	Alman, Josh; Song, Zhao										The Fine-Grained Complexity of Gradient Computation for Training Large Language Models								Arxiv											1	1;2024-02-07;https://www.arxiv.org/abs/2402.04497v1	arXiv:2402.04497			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 07 2024	2024	Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run ‘forward’ computations and ‘backward’ computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.																																	2024-02-23	PPRN:87560170		
J	Frisch, Ivar; Giulianelli, Mario										LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models								Arxiv											1	1;2024-02-05;https://www.arxiv.org/abs/2402.02896v1	arXiv:2402.02896			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 05 2024	2024	While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.																																	2024-02-21	PPRN:87523883		
J	Carr, B. J.; Clesse, S.; Garcia-Bellido, J.; Hawkins, M. R. S.; Kuhnel, Florian										Observational Evidence for Primordial Black Holes: A Positivist Perspective								Arxiv											2	2;2024-02-04;https://www.arxiv.org/abs/2306.03903v2| 1;2023-06-06;https://www.arxiv.org/abs/2306.03903v1	arXiv:2306.03903			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 04 2024	2024	We review numerous arguments for primordial black holes (PBHs) based on observational evidence from a variety of lensing, dynamical, accretion and gravitational -wave effects. This represents a shift from the usual emphasis on PBH constraints and provides what we term a positivist perspective. Microlensing observations of stars and quasars suggest that PBHs of around 1 M⊙ could provide much of the dark matter in galactic halos, this being allowed by the Large Magellanic Cloud microlensing observations if the PBHs have an extended mass function. More generally, providing the mass and dark matter fraction of the PBHs is large enough, the associated Poisson fluctuations could generate the first bound objects at a much earlier epoch than in the standard cosmological scenario. This simultaneously explains the recent detection of high-redshift dwarf galaxies, puzzling correlations of the source -subtracted infrared and X-ray cosmic backgrounds, the size and the mass -to -light ratios of ultra -faint -dwarf galaxies, the dynamical heating of the Galactic disk, and the binary coalescences observed by LIGO/Virgo/KAGRA in a mass range not usually associated with stellar remnants. Even if PBHs provide only a small fraction of the dark matter, they could explain various other observational conundra, and sufficiently large ones could seed the supermassive black holes in galactic nuclei or even early galaxies themselves. We argue that PBHs would naturally have formed around the electroweak, quantum chromodynamics and electron -positron annihilation epochs, when the sound -speed inevitably dips. This leads to an extended PBH mass function with a number of distinct bumps, the most prominent one being at around 1 M⊙, and this would allow PBHs to explain many of the observations in a unified way.																																	2024-05-25	PPRN:72864721		
J	Chern, Steffi; Chern, Ethan; Neubig, Graham; Liu, Pengfei				Liu, Pengfei/JUV-0307-2023						Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2401.16788v1	arXiv:2401.16788			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 30 2024	2024	Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable metaevaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose SCALEE- VAL, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during metaevaluation. We release the code for our framework, which is publicly available at: https: //github.com/GAIR-NLP/scaleeval.																																	2024-02-15	PPRN:87417801		
J	Wang, Tiannan; Chen, Jiamin; Jia, Qingrui; Wang, Shuai; Fang, Ruoyu; Wang, Huilin; Gao, Zhaowei; Xie, Chunzhao; Xu, Chuou; Dai, Jihong; Liu, Yibin; Wu, Jialong; Ding, Shengwei; Li, Long; Huang, Zhiwei; Deng, Xinle; Yu, Teng; Ma, Gangan; Xiao, Han; Chen, Zixin; Xiang, Danjun; Wang, Yunxia; Zhu, Yuanyuan; Xiao, Yi; Wang, Jing; Wang, Yiru; Ding, Siran; Huang, Jiayang; Xu, Jiayi; Tayier, Yilihamu; Hu, Zhenyu; Gao, Yuan; Zheng, Chengfeng; Ye, Yueshu; Li, Yihang; Wan, Lei; Jiang, Xinyue; Wang, Yujie; Cheng, Siyu; Song, Zhule; Tang, Xiangru; Xu, Xiaohua; Zhang, Ningyu; Chen, Huajun; Jiang, Yuchen Eleanor; Zhou, Wangchunshu				Huang, Zhiwei/KBA-4600-2024; Zheng, Chengfeng/KMA-0326-2024; Li, Yihang/HSF-6332-2023; Wang, Benyou/Y-5146-2019; hu, zhenyu/KIB-5028-2024; Huang, Jiayang/OML-8138-2025; wang, huilin/JGD-7298-2023; Xu, Jiayi/A-2736-2017; chen, jiamin/KWU-2539-2024; Wang, Yiru/HPG-4487-2023; Chen, Zixin/GQZ-7920-2022; Huajun, Chen/B-6340-2013; Zhang, Ningyu/AAQ-7391-2021						Weaver: Foundation Models for Creative Writing								Arxiv											1	1;2024-01-30;https://www.arxiv.org/abs/2401.17268v1	arXiv:2401.17268			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 30 2024	2024	This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.																																	2024-02-16	PPRN:87418941		
J	Jia, Jinghan; Liu, Jiancheng; Ram, Parikshit; Yao, Yuguang; Liu, Gaowen; Liu, Yang; Sharma, Pranay; Liu, Sijia				Liu, Gaowen/AAP-9890-2021; Liu, Sijia/HOC-2459-2023; Liu, JC/LPI-0149-2024						Model Sparsity Can Simplify Machine Unlearning								Arxiv											7	7;2023-10-22;https://www.arxiv.org/abs/2304.04934v9| 6;2023-09-28;https://www.arxiv.org/abs/2304.04934v8| 5;2023-04-11;https://www.arxiv.org/abs/2304.04934v2| 4;2024-01-27;https://www.arxiv.org/abs/2304.04934v13| 3;2024-01-04;https://www.arxiv.org/abs/2304.04934v12| 2;2023-12-06;https://www.arxiv.org/abs/2304.04934v11| 1;2023-11-17;https://www.arxiv.org/abs/2304.04934v10	arXiv:2304.04934			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 27 2024	2024	In response to recent data regulation requirements, machine unlearning (MU) has emerged as a critical process to remove the influence of specific examples from a given model. Although exact unlearning can be achieved through complete model retraining using the remaining dataset, the associated computational costs have driven the development of efficient, approximate unlearning techniques. Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective: model sparsification via weight pruning, which is capable of reducing the gap between exact unlearning and approximate unlearning. We show in both theory and practice that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. This leads to a new MU paradigm, termed prune first, then unlearn, which infuses a sparse model prior into the unlearning process. Building on this insight, we also develop a sparsity-aware unlearning method that utilizes sparsity regularization to enhance the training process of approximate unlearning. Extensive experiments show that our proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest unlearning methods) when using sparsity-aware unlearning. Furthermore, we demonstrate the practical impact of our proposed MU methods in addressing other machine learning challenges, such as defending against backdoor attacks and enhancing transfer learning. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse.																																	2024-05-25	PPRN:57983232		
J	Li, Mengtian; Yao, Shengxiang; Xie, Zhifeng; Chen, Keyu				Li, Mengtian/GSD-5852-2022; Chen, Ke-Yu/S-2094-2017						GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting								Arxiv											2	2;2024-01-27;https://www.arxiv.org/abs/2401.09720v2| 1;2024-01-18;https://www.arxiv.org/abs/2401.09720v1	arXiv:2401.09720			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 27 2024	2024	In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.																																	2024-02-15	PPRN:87221502		
J	Kupfer, Thomas; Korol, Valeriya; Littenberg, Tyson B.; Shah, Sweta; Savalle, Etienne; Groot, Paul J.; Marsh, Thomas R.; Le Jeune, Maude; Nelemans, Gijs; Pala, Anna F.; Petiteau, Antoine; Ramsay, Gavin; Steeghs, Danny; Babak, Stanislav				Pala, Anna Francesca/HKV-9190-2023; Kupfer, Thomas/LPQ-1800-2024; Nelemans, Gijs/D-3177-2012; Babak, Stanislav/H-9689-2017; Groot, Paul/K-4391-2016						LISA Galactic binaries with astrometry from Gaia DR3								Arxiv											3	3;2024-01-25;https://www.arxiv.org/abs/2302.12719v3| 2;2024-01-18;https://www.arxiv.org/abs/2302.12719v2| 1;2023-02-24;https://www.arxiv.org/abs/2302.12719v1	arXiv:2302.12719			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 25 2024	2024	Galactic compact binaries with orbital periods shorter than a few hours emit detectable gravitational waves at low frequencies. Their gravitational wave signals can be detected with the future Laser Interferometer Space Antenna (LISA). Crucially, they may be useful in the early months of the mission operation in helping to validate LISA’s performance in comparison to pre-launch expectations. We present an updated list of 55 candidate LISA detectable binaries with measured properties, for which we derive distances based on Gaia Data release 3 astrometry. Based on the known properties from EM observations, we predict the LISA detectability after 1, 3, 6, and 48 months using Bayesian analysis methods. We distinguish between verification and detectable binaries as being detectable after 3 and 48 months respectively. We find 18 verification binaries and 22 detectable sources, which triples the number of known LISA binaries over the last few years. These include detached double white dwarfs, AM CVn binaries, one ultracompact X-ray binary and two hot subdwarf binaries. We find that across this sample the gravitational wave amplitude is expected to be measured to : 10% on average, while the inclination is expected to be determined with : 15° precision. For detectable binaries these average errors increase to ≈ 50% and to ≈ 40° respectively.																																	2024-02-11	PPRN:44165788		
J	Kumar, Deepak; Abuhashem, Yousef; Durumeric, Zakir										Watch Your Language: Investigating Content Moderation with Large Language Models								Arxiv											2	2;2024-01-17;https://www.arxiv.org/abs/2309.14517v2| 1;2023-09-25;https://www.arxiv.org/abs/2309.14517v1	arXiv:2309.14517			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 17 2024	2024	Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs perform in content moder-ation settings. In this work, we evaluate a suite of commodity LLMs on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we instantiate 95 subcommu-nity specific LLMs by prompting GPT-3.5 with rules from 95 Reddit subcommunities. We find that GPT-3.5 is effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. For toxicity detection, we evaluate a suite of commodity LLMs (GPT-3, GPT-3.5, GPT-4, Gemini Pro, LLAMA 2) and show that LLMs significantly outperform currently widespread tox-icity classifiers. However, recent increases in model size add only marginal benefit to toxicity detection, suggesting a po-tential performance plateau for LLMs on toxicity detection tasks. We conclude by outlining avenues for future work in studying LLMs and content moderation.																																	2024-05-25	PPRN:85229610		
J	Lee, Seongyun; Kim, Seungone; Park, Sue Hyun; Kim, Geewook; Seo, Minjoon				Kim, Geewook/IYS-2231-2023						Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation								Arxiv											1	1;2024-01-12;https://www.arxiv.org/abs/2401.06591v1	arXiv:2401.06591			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 12 2024	2024	Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. 																																	2024-01-30	PPRN:87156587		
J	Ulmer, Dennis; Mansimov, Elman; Lin, Kaixiang; Sun, Justin; Gao, Xibin; Zhang, Yi										Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk								Arxiv											1	1;2024-01-10;https://www.arxiv.org/abs/2401.05033v1	arXiv:2401.05033			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 10 2024	2024	Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via "self-talk" of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.																																	2024-01-31	PPRN:87164171		
J	Owen, David				Owen, David/AAS-1605-2020						How predictable is language model benchmark performance?								Arxiv											1	1;2024-01-09;https://www.arxiv.org/abs/2401.04757v1	arXiv:2401.04757			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 09 2024	2024	We investigate large language model performance across five orders of magnitude of compute scaling in eleven recent model architectures. We show that average benchmark performance, aggregating over many individual tasks and evaluations as in the commonly-used BIG-Bench dataset, is decently predictable as a function of training compute scale. Specifically, when extrapolating BIG-Bench Hard performance across one order of magnitude in compute, we observe average absolute errors of 6 percentage points (pp). By contrast, extrapolation for individual BIG-Bench tasks across an order of magnitude in compute yields higher average errors of 18pp. Nonetheless, individual task performance remains significantly more predictable than chance. Overall, our work suggests compute scaling provides a promising basis to forecast AI capabilities in diverse benchmarks, though predicting performance in specific tasks poses challenges.																																	2024-01-31	PPRN:87164188		
J	Hooper, Coleman; Kim, Sehoon; Mohammadzadeh, Hiva; Genc, Hasan; Keutzer, Kurt; Gholami, Amir; Shao, Sophia										SPEED: Speculative Pipelined Execution for Efficient Decoding								Arxiv											2	2;2024-01-03;https://www.arxiv.org/abs/2310.12072v2| 1;2023-10-18;https://www.arxiv.org/abs/2310.12072v1	arXiv:2310.12072			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 03 2024	2024	Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us to accelerate generative LLM inference. We demonstrate the efficiency of our method in terms of latency reduction relative to model accuracy and demonstrate how speculation allows for training deeper decoders with parameter sharing with minimal runtime overhead.																																	2024-01-11	PPRN:85700974		
J	Divol, Vincent; Niles-Weed, Jonathan; Pooladian, Aram-Alexandre										Optimal transport map estimation in general function spaces								Arxiv											2	2;2024-01-02;https://www.arxiv.org/abs/2212.03722v2| 1;2022-12-07;https://www.arxiv.org/abs/2212.03722v1	arXiv:2212.03722			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 02 2024	2024	We study the problem of estimating a function T given independent samples from a distribution P and from the pushforward distribution T♯P. This setting is motivated by applications in the sciences, where T represents the evolution of a physical system over time, and in machine learning, where, for example, T may represent a transformation learned by a deep neural network trained for a generative modeling task. To ensure identifiability, we assume that T = ∇ϕ0 is the gradient of a convex function, in which case T is known as an optimal transport map. Prior work has studied the estimation of T under the assumption that it lies in a Ho¨lder class, but general theory is lacking. We present a unified methodology for obtaining rates of estimation of optimal transport maps in general function spaces. Our assumptions are significantly weaker than those appearing in the literature: we require only that the source measure P satisfy a Poincare´ inequality and that the optimal map be the gradient of a smooth convex function that lies in a space whose metric entropy can be controlled. As a special case, we recover known estimation rates for Ho¨lder transport maps, but also obtain nearly sharp results in many settings not covered by prior work. For example, we provide the first statistical rates of estimation when P is the normal distribution and the transport map is given by an infinite-width shallow neural network.																																	2024-01-11	PPRN:24982920		
J	Friedl, Stefan; Nagel, Matthias; Orson, Patrick; Powell, Mark										A survey of the foundations of four-manifold theory in the topological category								Arxiv											2	2;2024-01-02;https://www.arxiv.org/abs/1910.07372v3| 1;2020-07-09;https://www.arxiv.org/abs/1910.07372v2	arXiv:1910.07372			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 02 2024	2024	This survey aims to provide a guide to the literature on topological 4-manifolds. Foundational theorems on 4-manifolds are stated, especially in the topological category. Precise references are given, with indications of the strategies employed in the proofs. Where appropriate we give statements for manifolds of all dimensions. Many intuitively plausible theorems which are standard results in differential topology are either extraordinarily deep results in the topological category, are open, or are known to be false. Hence one must proceed with caution. This book seeks to help 4-manifold topologists navigate potential pitfalls, and to apply the many powerful results that do exist with confidence.																																	2024-05-25	PPRN:15214235		
J	Handa, Ankur; Allshire, Arthur; Makoviychuk, Viktor; Petrenko, Aleksei; Singh, Ritvik; Liu, Jingzhou; Makoviichuk, Denys; Van Wyk, Karl; Zhurkevich, Alexander; Sundaralingam, Balakumar; Narang, Yashraj; Lafleche, Jean-Francois; Fox, Dieter; State, Gavriel				Sundaralingam, Balakumar/AAC-8731-2021						DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality								Arxiv											1	1;2024-01-02;https://www.arxiv.org/abs/2210.13702v2	arXiv:2210.13702			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 02 2024	2024	Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found at https://dextreme.org/																																	2024-01-11	PPRN:86953855		
J	Fernandez, Yuriel Nunez; Ritter, Marc K.; Jeannin, Matthieu; Li, Jheng-Wei; Kloss, Thomas; Louvet, Thibaud; Terasaki, Satoshi; Parcollet, Olivier; von Delft, Jan; Shinaoka, Hiroshi; Waintal, Xavier				Parcollet, Olivier/AAE-2863-2021; Shinaoka, Hiroshi/L-9288-2018						Learning tensor networks with tensor cross interpolation: new algorithms and libraries								Arxiv											2	2;2024-12-26;https://www.arxiv.org/abs/2407.02454v3| 1;2024-07-04;https://www.arxiv.org/abs/2407.02454v2	arXiv:2407.02454			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 26 2024	2024	The tensor cross interpolation (TCI) algorithm is a rank-revealing algorithm for decomposing low-rank, high-dimensional tensors into tensor trains/matrix product states (MPS). TCI learns a compact MPS representation of the entire object from a tiny training data set. Once obtained, the large existing MPS toolbox provides exponentially fast algorithms for performing a large set of operations. We discuss several improvements and variants of TCI. In particular, we show that replacing the cross interpolation by the partially rank-revealing LU decomposition yields a more stable and more flexible algorithm than the original algorithm. We also present two open source libraries, xfac in Python/C++ and TensorCrossInterpolation.jl in Julia, that implement these improved algorithms, and illustrate them on several applications. These include sign-problem-free integration in large dimension, the superhigh-resolution quantics representation of functions, the solution of partial differential equations, the superfast Fourier transform, the computation of partition functions, and the construction of matrix product operators.																																	2025-02-15	PPRN:90673389		
J	Xiao, Xu; Venkatraman, Jayameenakshi; Cortinas, Rodrigo G.; Chowdhury, Shoumik; Devoret, Michel H.										A diagrammatic method to compute the effective Hamiltonian of driven nonlinear oscillators								Arxiv											2	2;2024-12-24;https://www.arxiv.org/abs/2304.13656v2| 1;2023-04-26;https://www.arxiv.org/abs/2304.13656v1	arXiv:2304.13656			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 24 2024	2024	In this work, we present a new diagrammatic method for computing the effective Hamiltonian of driven nonlinear oscillators. At the heart of our method is a self-consistent perturbation expansion developed in phase space, which establishes a direct correspondence between the diagram and algebra. Each diagram corresponds to a Hamiltonian term, the prefactor of which, like those in Feynman diagrams, involves a simple counting of topologically equivalent diagrams. Leveraging the algorithmic simplicity of our diagrammatic method, we provide a readily available computer program that generates the effective Hamiltonian to arbitrary order. We show the consistency of our schemes with existing perturbation methods such as the Schrieffer-Wolff method. Furthermore, we recover the classical harmonic balance scheme from our result in the limit of h → 0. Our method contributes to the understanding of dynamic control within quantum systems and achieves precision essential for advancing future quantum information processors. To demonstrate its value and versatility, we analyze five examples from the field of superconducting circuits. These include an experimental proposal for the Hamiltonian stabilization of a three-legged Schro¨dinger cat, modeling of energy renormalization phenomena in superconducting circuits experiments, a comprehensive characterization of multiphoton resonances in a driven transmon, a proposal for an inductively shunted transmon circuit, and a characterization of classical ultra-subharmonic bifurcation in driven oscillators. Lastly, we benchmark the performance of our method by comparing it with experimental data and exact Floquet numerical diagonalization.																																	2025-02-02	PPRN:65567887		
J	Kahatapitiya, Kumara; Ranasinghe, Kanchana; Park, Jongwoo; Ryoo, Michael S.				Kahatapitiya, Kumara/AAU-3656-2021						Language Repository for Long Video Understanding								Arxiv											2	2;2024-12-20;https://www.arxiv.org/abs/2403.14622v2| 1;2024-03-21;https://www.arxiv.org/abs/2403.14622v1	arXiv:2403.14622			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 20 2024	2024	Language has become a prominent modality in computer vision with the rise of LLMs. Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length. This becomes critical, especially in applications such as long-form video understanding. In this paper, we introduce a Language Repository (LangRepo) for LLMs, that maintains concise and structured information as an interpretable (i.e., all-textual) representation. Our repository is updated iteratively based on multi-scale video chunks. We introduce write and read operations that focus on pruning redundancies in text, and extracting information at various temporal scales. The proposed framework is evaluated on zero-shot visual question-answering benchmarks including EgoSchema, NExT-QA, IntentQA and NExT-GQA, showing state-of-the-art performance at its scale. 																																	2025-01-29	PPRN:88259552		
J	Chen, Shuo; Han, Zhen; He, Bailan; Ding, Zifeng; Yu, Wenqian; Torr, Philip; Tresp, Volker; Gu, Jindong				Han, Zhen/AAA-4764-2022						RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI /MULTI-MODAL JAILBREAK ATTACKS ?								Arxiv											2	2;2024-12-15;https://www.arxiv.org/abs/2404.03411v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.03411v1	arXiv:2404.03411			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 15 2024	2024	Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. 																																	2025-01-24	PPRN:88405008		
J	Song, Jiwon; Oh, Kyungseok; Kim, Taesu; Kim, Hyungjun; Kim, Yulhwa; Kim, Jae-Joon				Kim, Joo Hyun/C-6604-2019						SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks								Arxiv											5	5;2024-12-13;https://www.arxiv.org/abs/2402.09025v6| 4;2024-06-12;https://www.arxiv.org/abs/2402.09025v4| 3;2024-06-01;https://www.arxiv.org/abs/2402.09025v3| 2;2024-05-29;https://www.arxiv.org/abs/2402.09025v2| 1;2024-02-14;https://www.arxiv.org/abs/2402.09025v1	arXiv:2402.09025			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 13 2024	2024	Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB outperforms previous LLM pruning methods in accelerating LLM inference while also maintaining superior perplexity and accuracy, making SLEB as a promising technique for enhancing the efficiency of LLMs. 																																	2025-01-21	PPRN:87687883		
J	Oh, Tadahiro; Okamoto, Mamoru; Tolomeo, Leonardo				Tolomeo, Leonardo/AAX-8588-2021						Stochastic quantization of the Φ33-model								Arxiv											3	3;2024-12-12;https://www.arxiv.org/abs/2108.06777v3| 2;2024-04-26;https://www.arxiv.org/abs/2108.06777v2| 1;2021-08-15;https://www.arxiv.org/abs/2108.06777v1	arXiv:2108.06777			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 12 2024	2024	We study the construction of the Φ33-measure and complete the program on the (non-)construction of the focusing Gibbs measures, initiated by Lebowitz, Rose, and Speer (1988). This problem turns out to be critical, exhibiting the following phase transition. In the weakly nonlinear regime, we prove normalizability of the Φ33-measure and show that it is singular with respect to the massive Gaussian free field. Moreover, we show that there exists a shifted measure with respect to which the Φ33-measure is absolutely continuous. In the strongly nonlinear regime, by further developing the machinery introduced by the authors, we establish non-normalizability of the Φ33-measure. Due to the singularity of the Φ33-measure with respect to the massive Gaussian free field, this non-normalizability part poses a particular challenge as compared to our previous works. In order to overcome this issue, we first construct a σ- finite version of the Φ33-measure and show that this measure is not normalizable. Furthermore, we prove that the truncated Φ33-measures have no weak limit in a natural space, even up to a subsequence. We also study the dynamical problem for the canonical stochastic quantization of the Φ33-measure, namely, the three-dimensional stochastic damped nonlinear wave equation with a quadratic nonlinearity forced by an additive space-time white noise (= the hyperbolic Φ33model). By adapting the paracontrolled approach, in particular from the works by Gubinelli, Koch, and the first author (2018) and by the authors (2020), we prove almost sure global well-posedness of the hyperbolic Φ33-model and invariance of the Gibbs measure in the weakly nonlinear regime. In the globalization part, we introduce a new, conceptually simple and straightforward approach, where we directly work with the (truncated) Gibbs measure, using the Boue-Dupuis variational formula and ideas from theory of optimal transport.																																	2025-01-22	PPRN:11899843		
J	Song, Yuda; Zhang, Hanlin; Eisenach, Carson; Kakade, Sham; Foster, Dean; Ghai, Udaya										Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models								Arxiv											1	1;2024-12-03;https://www.arxiv.org/abs/2412.02674v1	arXiv:2412.02674			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 03 2024	2024	Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the generation-verification gap. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries.																																	2025-01-15	PPRN:119686614		
J	Cheng, Daixuan; Gu, Yuxian; Huang, Shaohan; Bi, Junyu; Huang, Minlie; Wei, Furu				cheng, daixuan/LZE-0736-2025; Huang, Shaohan/LDF-3300-2024						Instruction Pre-Training: Language Models are Supervised Multitask Learners								Arxiv											2	2;2024-11-28;https://www.arxiv.org/abs/2406.14491v2| 1;2024-06-20;https://www.arxiv.org/abs/2406.14491v1	arXiv:2406.14491			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 28 2024	2024	Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama370B. 																																	2025-01-10	PPRN:89380210		
J	Liu, Haohe; Xu, Xuenan; Yuan, Yi; Wu, Mengyue; Wang, Wenwu; Plumbley, Mark D.				liu, haohe/JBS-1030-2023; wang, wenwu/HOF-4371-2023; Plumbley, Mark/A-7298-2008						SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound								Arxiv											2	2;2024-11-28;https://www.arxiv.org/abs/2405.00233v2| 1;2024-04-30;https://www.arxiv.org/abs/2405.00233v1	arXiv:2405.00233			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 28 2024	2024	Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general sound, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra- low bit rates between 0.31 kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates. 																																	2025-01-10	PPRN:88712096		
J	Chen, Shimin; Lan, Xiaohan; Yuan, Yitian; Jie, Zequn; Ma, Lin										TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/2411.18211v1	arXiv:2411.18211			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	Rapid development of large language models (LLMs) has significantly advanced multimodal large language models (LMMs), particularly in vision-language tasks. However, existing video-language models often overlook precise temporal localization and struggle with videos of varying lengths. We introduce TimeMarker, a versatile Video-LLM designed for high-quality dialogue based on video content, emphasizing temporal localization. TimeMarker integrates Temporal Separator Tokens to enhance temporal awareness, accurately marking specific moments within videos. It employs the AnyLength mechanism for dynamic frame sampling and adaptive token merging, enabling effective handling of both short and long videos. Additionally, TimeMarker utilizes diverse datasets, including further transformed temporal-related video QA datasets, to bolster its temporal understanding capabilities. Image and interleaved data are also employed to further enhance the model's semantic perception ability. Evaluations demonstrate that TimeMarker achieves state-of-the-art performance across multiple benchmarks, excelling in both short and long video categories. 																																	2025-01-10	PPRN:119466794		
J	Dong, Hao; Gu, Weihao; Zhang, Xianjing; Xu, Jintao; Ai, Rui; Lu, Huimin; Kannala, Juho; Chen, Xieyuanli				Chen, Xieyuanli/AAH-6401-2020; Ai, Rui/OPN-0502-2025; jintao, xu/JBI-8473-2023; Xu, Jintao/D-8308-2018; Lu, Huimin/H-5571-2011						SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation								Arxiv											3	3;2024-11-27;https://www.arxiv.org/abs/2211.15656v4| 2;2024-10-31;https://www.arxiv.org/abs/2211.15656v3| 1;2022-11-28;https://www.arxiv.org/abs/2211.15656v2	arXiv:2211.15656			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 27 2024	2024	High-definition (HD) semantic map generation of the environment is an essential component of autonomous driving. Existing methods have achieved good performance in this task by fusing different sensor modalities, such as LiDAR and camera. However, current works are based on raw data or network feature-level fusion and only consider short-range HD map generation, limiting their deployment to realistic autonomous driving applications. In this paper, we focus on the task of building the HD maps in both short ranges, i.e., within 30 m, and also predicting long-range HD maps up to 90 m, which is required by downstream path planning and control tasks to improve the smoothness and safety of autonomous driving. To this end, we propose a novel network named SuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels. We use LiDAR depth to improve image depth estimation and use image features to guide long-range LiDAR feature prediction. We benchmark our SuperFusion on the nuScenes dataset and a self-recorded dataset and show that it outperforms the state-of-the-art baseline methods with large margins on all intervals. Additionally, we apply the generated HD map to a downstream path planning task, demonstrating that the long-range HD maps predicted by our method can lead to better path planning for autonomous vehicles. 																																	2025-01-10	PPRN:46905133		
J	Zhao, Xuandong; Gunn, Sam; Christ, Miranda; Fairoze, Jaiden; Fabrega, Andres; Carlini, Nicholas; Garg, Sanjam; Hong, Sanghyun; Nasr, Milad; Tramer, Florian; Jha, Somesh; Li, Lei; Wang, Yu-Xiang; Song, Dawn				Zhao, Xuandong/LIG-4204-2024; Wang, Yuyan/GQZ-8255-2022						SoK: Watermarking for AI-Generated Content								Arxiv											1	1;2024-11-27;https://www.arxiv.org/abs/2411.18479v1	arXiv:2411.18479			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.																																	2025-01-10	PPRN:119466766		
J	Fang, Zihan; Lin, Zheng; Hu, Senkang; Cao, Hangcheng; Deng, Yiqin; Chen, Xianhao; Fang, Yuguang				Fang, Yuguang/JJF-2146-2023; Deng, Yiqin/CAG-8820-2022; CHEN, XIANHAO/AAX-6311-2021; Hu, Senkang/KYP-8998-2024						IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers								Arxiv											3	3;2024-11-21;https://www.arxiv.org/abs/2410.02592v4| 2;2024-10-10;https://www.arxiv.org/abs/2410.02592v3| 1;2024-10-03;https://www.arxiv.org/abs/2410.02592v1	arXiv:2410.02592			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 21 2024	2024	Recently, in-car monitoring has emerged as a promising technology for detecting early-stage abnormal status of the driver and providing timely alerts to prevent traffic accidents. Although training models with multimodal data enhances the reliability of abnormal status detection, the scarcity of labeled data and the imbalance of class distribution impede the extraction of critical abnormal state features, significantly deteriorating training performance. Furthermore, missing modalities due to environment and hardware limitations further exacerbate the challenge of abnormal status identification. More importantly, monitoring abnormal health conditions of passengers, particularly in elderly care, is of paramount importance but remains underexplored. To address these challenges, we introduce our IC3M, an efficient camera-rotation-based multimodal framework for monitoring both driver and passengers in a car. Our IC3M comprises two key modules: an adaptive threshold pseudo-labeling strategy and a missing modality reconstruction. The former customizes pseudo-labeling thresholds for different classes based on the class distribution, generating class-balanced pseudo labels to guide model training effectively, while the latter leverages cross- modality relationships learned from limited labels to accurately recover missing modalities by distribution transferring from available modalities. Extensive experimental results demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy, precision, and recall while exhibiting superior robustness under limited labeled data and severe missing modality.																																	2024-12-31	PPRN:102599260		
J	Wang, Zhengyi; Lorraine, Jonathan; Wang, Yikai; Su, Hang; Zhu, Jun; Fidler, Sanja; Zeng, Xiaohui				wang, yikai/HLW-7052-2023						LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models								Arxiv											1	1;2024-11-14;https://www.arxiv.org/abs/2411.09595v1	arXiv:2411.09595			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 14 2024	2024	This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce LLaMA-Mesh, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. LLaMA-Mesh achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance.																																	2024-12-21	PPRN:119218651		
J	Li, Tian-Nuo; Li, Yun-He; Du, Guo-Hong; Wu, Peng-Ju; Feng, Lu; Zhang, Jing-Fei; Zhang, Xin				Du, Guo-Hong/LZE-3894-2025; Wu, Peng-Ju/OCL-1116-2025; Zhang, Xin/AET-6134-2022; Zhang, Jingfei/HHZ-2743-2022						Revisiting holographic dark energy after DESI 2024								Arxiv											1	1;2024-11-13;https://www.arxiv.org/abs/2411.08639v1	arXiv:2411.08639			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 13 2024	2024	New insights from the Dark Energy Spectroscopic Instrument (DESI) 2024 baryon acoustic oscillations (BAO) data, in conjunction with cosmic microwave background (CMB) and Type Ia supernova (SN) data, suggest that dark energy may not be a cosmological constant. In this work, we investigate the cosmological implications of holographic dark energy (HDE) and interacting holographic dark energy (IHDE) models, utilizing CMB, DESI BAO, and SN data. By considering the combined DESI BAO and SN data, we determine that in the IHDE model, the parameter c > 1 and the dark-energy equation of state w does not cross 1 at the 1σ confidence level, whereas in the HDE model, it marginally falls below this threshold. Upon incorporating CMB data, we observe that in the HDE model, the parameter c < 1 and w crosses 1 at a level beyond 10σ. Conversely, for the IHDE model, the likelihood of w crossing 1 is considerably diminished, implying that the introduction of interaction within the HDE model could potentially resolve or mitigate the cosmic big rip conundrum. Furthermore, our analysis reveals that the HDE and IHDE models are statistically as viable as the ΛCDM model when assessing Bayesian evidence with DESI BAO data combined with SN data. However, when CMB data are added, the HDE and IHDE models are significantly less favored compared to the ΛCDM model. Our findings advocate for further exploration of the HDE and IHDE models using forthcoming, more precise late-universe observations.																																	2024-12-21	PPRN:119218681		
J	Goemans, Arthur; Buhl, Marie Davidsen; Schuett, Jonas; Korbak, Tomek; Wang, Jessica; Hilton, Benjamin; Irving, Geoffrey										Safety case template for frontier AI: A cyber inability argument								Arxiv											1	1;2024-11-12;https://www.arxiv.org/abs/2411.08088v1	arXiv:2411.08088			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 12 2024	2024	Frontier artificial intelligence (AI) systems pose increasing risks to society, making it essential for developers to provide assurances about their safety. One approach to offering such assurances is through a safety case: a structured, evidence-based argument aimed at demonstrating why the risk associated with a safety-critical system is acceptable. In this article, we propose a safety case template for offensive cyber capabilities. We illustrate how developers could argue that a model does not have capabilities posing unacceptable cyber risks by breaking down the main claim into progressively specific sub-claims, each supported by evidence. In our template, we identify a number of risk models, derive proxy tasks from the risk models, define evaluation settings for the proxy tasks, and connect those with evaluation results. Elements of current frontier safety techniques - such as risk models, proxy tasks, and capability evaluations - use implicit arguments for overall system safety. This safety case template integrates these elements using the Claims Arguments Evidence (CAE) framework in order to make safety arguments coherent and explicit. While uncertainties around the specifics remain, this template serves as a proof of concept, aiming to foster discussion on AI safety cases and advance AI assurance.																																	2024-12-21	PPRN:119217920		
J	Penic, Rafael Josip; Vlasic, Tin; Huber, Roland G.; Wan, Yue; Sikic, Mile				Sikic, Mile/A-9325-2015; Huber, Roland/AAG-6353-2019; Vlasic, Tin/HGC-3044-2022						RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks								Arxiv											2	2;2024-11-12;https://www.arxiv.org/abs/2403.00043v2| 1;2024-02-29;https://www.arxiv.org/abs/2403.00043v1	arXiv:2403.00043			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 12 2024	2024	While RNA has recently been recognized as an interesting small-molecule drug target, many challenges remain to be addressed before we take full advantage of it. This emphasizes the necessity to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides a huge potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date, with 650M parameters pre-trained on 36M non-coding RNA sequences from several databases. It can extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabilities overcome the inability of other deep learning methods for secondary structure prediction to generalize on unseen RNA families.																																	2024-12-18	PPRN:87998200		
J	Gupta, Vipul; Pantoja, David; Ross, Candace; Williams, Adina; Ung, Megan										Changing Answer Order Can Decrease MMLU Accuracy								Arxiv											2	2;2024-11-11;https://www.arxiv.org/abs/2406.19470v2| 1;2024-06-27;https://www.arxiv.org/abs/2406.19470v1	arXiv:2406.19470			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Nov 11 2024	2024	As large language models (LLMs) have grown in prevalence, particular benchmarks have become essential for the evaluation of these models and for understanding model capabilities. Most commonly, we use test accuracy averaged across multiple subtasks in order to rank models on leaderboards, to determine which model is best for our purposes. In this paper, we investigate the robustness of the accuracy measurement on a widely used multiple choice question answering dataset, MMLU. When shuffling the answer label contents, we find that all explored models decrease in accuracy on MMLU, but not every model is equally sensitive. These findings suggest a possible adjustment to the standard practice of leaderboard testing, where we additionally consider the percentage of examples each model answers correctly by random chance.																																	2024-12-19	PPRN:90629397		
J	Bhardwaj, Lakshya; Copetti, Christian; Pajer, Daniel; Schafer-Nameki, Sakura				Bhardwaj, Lakshya/ISU-5186-2023						Boundary SymTFT								Arxiv											1	1;2024-11-08;https://www.arxiv.org/abs/2409.02166v2	arXiv:2409.02166			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 08 2024	2024	We study properties of boundary conditions (BCs) in theories with categorical (or non- invertible) symmetries. We describe how the transformation properties, or (generalized) charges, of BCs are captured by topological BCs of Symmetry Topological Field Theory (SymTFT), which is a topological field theory in one higher spacetime dimension. As an application of the SymTFT chracterization, we discuss the symmetry properties of boundary conditions for (1+1)d gapped and gapless phases. We provide a number of concrete examples in spacetime dimensions d = 2, 3. We furthermore expand the lattice description for (1+1)d anyon chains with categorical symmetries to include boundary conditions carrying arbitrary 1-charges under the symmetry.																																	2024-12-16	PPRN:119108254		
J	Prasad, Archiki; Yuan, Weizhe; Pang, Richard Yuanzhe; Xu, Jing; Fazel-Zarandi, Maryam; Bansal, Mohit; Sukhbaatar, Sainbayar; Weston, Jason; Yu, Jane				Bansal, Mohit/Q-9105-2016						Self-Consistency Preference Optimization								Arxiv											2	2;2024-11-07;https://www.arxiv.org/abs/2411.04109v2| 1;2024-11-06;https://www.arxiv.org/abs/2411.04109v1	arXiv:2411.04109			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 07 2024	2024	Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.																																	2024-12-16	PPRN:119060341		
J	Zhang, Zhexin; Lu, Yida; Ma, Jingyuan; Zhang, Di; Li, Rui; Ke, Pei; Sun, Hao; Sha, Lei; Sui, Zhifang; Wang, Hongning; Huang, Minlie				Sha, Lei/HGB-4806-2022; Wang, Hongning/GPK-7527-2022; Ma, Jing-Yuan/GPW-8009-2022						ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors								Arxiv											2	2;2024-11-05;https://www.arxiv.org/abs/2402.16444v2| 1;2024-02-26;https://www.arxiv.org/abs/2402.16444v1	arXiv:2402.16444			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 05 2024	2024	The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with common safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective as a safety evaluator for advanced LLMs. 																																	2024-12-09	PPRN:87889773		
J	D'Angelo, Francesco; Andriushchenko, Maksym; Varre, Aditya; Flammarion, Nicolas										Why Do We Need Weight Decay in Modern Deep Learning?								Arxiv											3	3;2024-11-04;https://www.arxiv.org/abs/2310.04415v2| 2;2023-10-06;https://www.arxiv.org/abs/2310.04415v1| 1;2023-10-06;https://www.arxiv.org/abs/2310.04415v1	arXiv:2310.04415			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. 																																	2024-12-10	PPRN:85520749		
J	Ding, Yangruibo; Peng, Jinjun; Min, Marcus J.; Kaiser, Gail; Yang, Junfeng; Ray, Baishakhi				Ding, Yangruibo/JEP-6503-2023; Peng, Jinjun/NRB-1301-2025						SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning								Arxiv											2	2;2024-10-31;https://www.arxiv.org/abs/2406.01006v2| 1;2024-06-03;https://www.arxiv.org/abs/2406.01006v1	arXiv:2406.01006			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Oct 31 2024	2024	Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs’ reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, monologue reasoning, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states. We begin by collecting PYX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of S EM C ODER , a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. S EM C ODER achieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I (GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5turbo: 59.0%). We also study the effectiveness of S EM C ODER ’s monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs’ debugging and self-refining capabilities. 																																	2024-12-11	PPRN:89163811		
J	Ding, Zhiyan; Li, Bowen; Lin, Lin				Lin, Lin/I-2726-2012; BOWEN, LI/LVA-1576-2024						Efficient quantum Gibbs samplers with Kubo--Martin--Schwinger detailed balance condition								Arxiv											4	4;2024-10-29;https://www.arxiv.org/abs/2404.05998v5| 3;2024-08-20;https://www.arxiv.org/abs/2404.05998v3| 2;2024-04-25;https://www.arxiv.org/abs/2404.05998v2| 1;2024-04-09;https://www.arxiv.org/abs/2404.05998v1	arXiv:2404.05998			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 29 2024	2024	Lindblad dynamics and other open-system dynamics provide a promising path towards efficient Gibbs sampling on quantum computers. In these proposals, the Lindbladian is obtained via an algorithmic construction akin to designing an artificial thermostat in classical Monte Carlo or molecular dynamics methods, rather than treated as an approximation to weakly coupled system-bath unitary dynamics. Recently, Chen, Kastoryano, and Gilyén (arXiv:2311.09207) introduced the first efficiently implementable Lindbladian satisfying the Kubo--Martin--Schwinger (KMS) detailed balance condition, which ensures that the Gibbs state is a fixed point of the dynamics and is applicable to non-commuting Hamiltonians. This Gibbs sampler uses a continuously parameterized set of jump operators, and the energy resolution required for implementing each jump operator depends only logarithmically on the precision and the mixing time. In this work, we build upon the structural characterization of KMS detailed balanced Lindbladians by Fagnola and Umanità, and develop a family of efficient quantum Gibbs samplers using a finite set of jump operators (the number can be as few as one), re{akin to the classical Markov chain-based sampling algorithm. Compared to the existing works, our quantum Gibbs samplers have a comparable quantum simulation cost but with greater design flexibility and a much simpler implementation and error analysis.} Moreover, it encompasses the construction of Chen, Kastoryano, and Gilyén as a special instance.																																	2025-08-07	PPRN:88467064		
J	Kelley, Zander; Meka, Raghu										Strong Bounds for 3-Progressions								Arxiv											4	4;2024-10-28;https://www.arxiv.org/abs/2302.05537v6| 3;2024-01-05;https://www.arxiv.org/abs/2302.05537v5| 2;2023-09-26;https://www.arxiv.org/abs/2302.05537v4| 1;2023-08-09;https://www.arxiv.org/abs/2302.05537v3	arXiv:2302.05537			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 28 2024	2024	We show that for some constant β > 0, any subset A of integers {1, …, N} of size at least 2−O((logN)β)⋅N contains a non-trivial three-term arithmetic progression. Previously, three-term arithmetic progressions were known to exist only for sets of size at least N/(logN)1+c for a constant c > 0.<br /> Our approach is first to develop new analytic techniques for addressing some related questions in the finite-field setting and then to apply some analogous variants of these same techniques, suitably adapted for the more complicated setting of integers.																																	2024-12-03	PPRN:75288009		
J	Krishnamurthy, Akshay; Harris, Keegan; Foster, Dylan J.; Zhang, Cyril; Slivkins, Aleksandrs										Can large language models explore in-context?								Arxiv											3	3;2024-10-28;https://www.arxiv.org/abs/2403.15371v3| 2;2024-07-12;https://www.arxiv.org/abs/2403.15371v2| 1;2024-03-22;https://www.arxiv.org/abs/2403.15371v1	arXiv:2403.15371			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 28 2024	2024	We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration , a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context , i.e., within the LLM prompt. We experiment with GPT3.5 , GPT-4 , and LLAMA2 , using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chainof-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. Although these findings can be interpreted positively, they suggest that external summarization—which may not be possible in more complex settings—is important for obtaining desirable behavior from LLM agents. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.																																	2024-11-30	PPRN:88261837		
J	Riess, Adam G.; Scolnic, Dan; Anand, Gagandeep S.; Breuval, Louise; Casertano, Stefano; Macri, Lucas M.; Li, Siyang; Yuan, Wenlong; Huang, Caroline D.; Jha, Saurabh; Murakami, Yukei S.; Beaton, Rachael; Brout, Dillon; Wu, Tianrui; Addison, Graeme E.; Bennett, Charles; Anderson, Richard I.; Filippenko, Alexei V.; Carr, Anthony				Riess, Adam/ABF-2480-2020; Carr, Anthony/HPD-1155-2023; SCOLNIC, DANIEL/OHR-7390-2025						<italic>JWST</italic> Validates <italic>HST</italic> Distance Measurements: Selection of Supernova Subsample Explains Differences in <italic>JWST</italic> Estimates of Local H0								Arxiv											2	2;2024-10-28;https://www.arxiv.org/abs/2408.11770v2| 1;2024-08-21;https://www.arxiv.org/abs/2408.11770v1	arXiv:2408.11770			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 28 2024	2024	We cross-check the HST Cepheid/SNe Ia distance ladder, which yields the most precise local H0, against early JWST subsamples (~1/4 of the HST sample) from SH0ES and CCHP, calibrated only with NGC 4258. We find HST Cepheid distances agree well (~1 σ ) with all combinations of methods, samples, and telescopes. The comparisons explicitly include the measurement uncertainty of each method in NGC 4258, an oft-neglected but dominant term. Mean differences are ~0.03 mag, far smaller than the 0.18 mag “Hubble tension.” Combining all measures produces the strongest constraint yet on the linearity of HST Cepheid distances, 0.994 ± 0.010, ruling out distance-dependent bias or offset as the source of the tension at ~7 σ. However, current JWST subsamples produce large sampling differences in H0 whose size and direction we can directly estimate from the full HST set. We show that ∆H0~2.5 km s− 1 Mpc− 1 between the CCHP JWST program and the full HST sample is entirely consistent with differences in sample selection. We combine all JWST samples into a new distance-limited set of 16 SNe Ia at D ≤ 25 Mpc. Using JWST Cepheids, JAGB, and TRGB, we find 73.4 ± 2.1, 72.2 ± 2.2, and 72.1 ± 2.2 km s− 1 Mpc− 1, respectively. Explicitly accounting for common SNe, the three-method JWST result is H0 = 72.6 ± 2.0, similar to H0 = 72.8 expected from HST Cepheids in the same galaxies. The small JWST sample trivially lowers the Hubble tension significance due to small-sample statistics and is not yet competitive with the HST set (42 SNe Ia and 4 anchors), which yields 73.2±0.9. Still, the joint JWST sample provides important crosschecks which the HST data passes.																																	2024-12-06	PPRN:91500489		
J	Shuttleworth, Reece; Andreas, Jacob; Torralba, Antonio; Sharma, Pratyusha										LoRA vs Full Fine-tuning: An Illusion of Equivalence								Arxiv											1	1;2024-10-28;https://www.arxiv.org/abs/2410.21228v1	arXiv:2410.21228			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 28 2024	2024	Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, are their learned solutions really equivalent? We study how different fine-tuning methods change pre-trained models by analyzing the model’s weight matrices through the lens of their spectral properties. We find that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task’s distribution. More specifically, we first show that the weight matrices trained with LoRA have new, high-ranking singular vectors, which we call intruder dimensions . Intruder dimensions do not appear during full fine-tuning. Second, we show that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning access different parts of parameter space, even when they perform equally on the fine-tuned distribution. We conclude by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized.																																	2024-12-06	PPRN:118943102		
J	Pan, Leyi; Liu, Aiwei; He, Zhiwei; Gao, Zitian; Zhao, Xuandong; Lu, Yijian; Zhou, Binglin; Liu, Shuliang; Hu, Xuming; Wen, Lijie; King, Irwin; Yu, Philip S.				Zhao, Xuandong/LIG-4204-2024; Hu, Xuming/HTS-1538-2023						MarkLLM: An Open-Source Toolkit for LLM Watermarking								Arxiv											4	4;2024-10-26;https://www.arxiv.org/abs/2405.10051v6| 3;2024-10-16;https://www.arxiv.org/abs/2405.10051v5| 2;2024-07-22;https://www.arxiv.org/abs/2405.10051v3| 1;2024-05-24;https://www.arxiv.org/abs/2405.10051v2	arXiv:2405.10051			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 26 2024	2024	LLM watermarking, which embeds imperceptible yet algorithmically detectable signals in model outputs to identify LLM-generated text, has become crucial in mitigating the potential misuse of large language models. However, the abundance of LLM watermarking algorithms, their intricate mechanisms, and the complex evaluation procedures and perspectives pose challenges for researchers and the community to easily experiment with, understand, and assess the latest advancements. To address these issues, we introduce MarkLLM, an open-source toolkit for LLM watermarking. MarkLLM offers a unified and extensible framework for implementing LLM watermarking algorithms, while providing user-friendly interfaces to ensure ease of access. Furthermore, it enhances understanding by supporting automatic visualization of the underlying mechanisms of these algorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools spanning three perspectives, along with two types of automated evaluation pipelines. Through MarkLLM, we aim to support researchers while improving the comprehension and involvement of the general public in LLM watermarking technology, fostering consensus and driving further advancements in research and application.																																	2024-12-06	PPRN:89099149		
J	Lu, Weizheng; Zhang, Jing; Fan, Ju; Fu, Zihao; Chen, Yueguo; Du, Xiaoyong				Fu, Zihao/HMW-2109-2023						Large Language Model for Table Processing: A Survey								Arxiv											3	3;2024-10-24;https://www.arxiv.org/abs/2402.05121v3| 2;2024-07-26;https://www.arxiv.org/abs/2402.05121v2| 1;2024-02-04;https://www.arxiv.org/abs/2402.05121v1	arXiv:2402.05121			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 24 2024	2024	Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of tablerelated tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.																																	2024-11-27	PPRN:87572260		
J	Kawaharazuka, Kento; Matsushima, Tatsuya; Gambardella, Andrew; Guo, Jiaxian; Paxton, Chris; Zeng, Andy										Real-World Robot Applications of Foundation Models: A Review								Arxiv											2	2;2024-10-23;https://www.arxiv.org/abs/2402.05741v2| 1;2024-02-08;https://www.arxiv.org/abs/2402.05741v1	arXiv:2402.05741			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 23 2024	2024	Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.																																	2024-11-24	PPRN:87572395		
J	Wu, Junyi; Wang, Haoxuan; Shang, Yuzhang; Shah, Mubarak; Yan, Yan				Shang, Yuzhang/HTO-5198-2023; Wang, Haoxuan/U-6740-2019						PTQ4DiT: Post-training Quantization for Diffusion Transformers								Arxiv											3	3;2024-10-17;https://www.arxiv.org/abs/2405.16005v3| 2;2024-09-26;https://www.arxiv.org/abs/2405.16005v2| 1;2024-05-25;https://www.arxiv.org/abs/2405.16005v1	arXiv:2405.16005			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 17 2024	2024	The recent introduction of Diffusion Transformers (DiTs) has demonstrated exceptional capabilities in image generation by using a different backbone architecture, departing from traditional U-Nets and embracing the scalable nature of transformers. Despite their advanced capabilities, the wide deployment of DiTs, particularly for real-time applications, is currently hampered by considerable computational demands at the inference stage. Post-training Quantization (PTQ) has emerged as a fast and data-efficient solution that can significantly reduce computation and memory footprint by using low-bit weights and activations. However, its applicability to DiTs has not yet been explored and faces non-trivial difficulties due to the unique design of DiTs. In this paper, we propose PTQ4DiT, a specifically designed PTQ method for DiTs. We discover two primary quantization challenges inherent in DiTs, notably the presence of salient channels with extreme magnitudes and the temporal variability in distributions of salient activation over multiple timesteps. To tackle these challenges, we propose Channel-wise Salience Balancing (CSB) and Spearmen's $rho$-guided Salience Calibration (SSC). CSB leverages the complementarity property of channel magnitudes to redistribute the extremes, alleviating quantization errors for both activations and weights. SSC extends this approach by dynamically adjusting the balanced salience to capture the temporal variations in activation. Additionally, to eliminate extra computational costs caused by PTQ4DiT during inference, we design an offline re-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT successfully quantizes DiTs to 8-bit precision (W8A8) while preserving comparable generation ability and further enables effective quantization to 4-bit weight precision (W4A8) for the first time.																																	2024-11-09	PPRN:89071431		
J	Biran, Eden; Gottesman, Daniela; Yang, Sohee; Geva, Mor; Globerson, Amir				Geva, Mor/JJF-9095-2023						Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2406.12775v2| 1;2024-06-18;https://www.arxiv.org/abs/2406.12775v1	arXiv:2406.12775			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally. Motivated by this, we study how LLMs answer multi-hop queries such as "The spouse of the performer of Imagine is". These queries require two information extraction steps: a latent one for resolving the first hop ("the performer of Imagine") into the bridge entity (John Lennon), and another for resolving the second hop ("the spouse of John Lennon") into the target entity (Yoko Ono). Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel "back-patching" analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall, our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.																																	2024-11-07	PPRN:89358656		
J	Gao, Yeqi; Song, Zhao; Yang, Xin; Zhou, Yufa				Zhou, Yufa/MIP-8150-2025						Differentially Private Attention Computation								Arxiv											2	2;2024-10-14;https://www.arxiv.org/abs/2305.04701v2| 1;2023-05-08;https://www.arxiv.org/abs/2305.04701v1	arXiv:2305.04701			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	Large language models (LLMs), especially those based on the Transformer architecture, have had a profound impact on various aspects of daily life, such as natural language processing, content generation, research methodologies, and more. Nevertheless, a crucial concern regarding the inference results of large language models is the issue of security and privacy. Given that large language models can generate results that may leak sensitive confidential or copyright information in many scenarios, it is crucial to compute the attention matrix with provable privacy guarantees, as attention is all you need. In this work, we propose a novel and efficient algorithm for approximating the attention matrix while providing differential privacy (DP) guarantees. To achieve this, we build on recent advancements in fast attention computation and differentially private matrix publishing.																																	2024-11-05	PPRN:68519997		
J	Marro, Samuele; Malfa, Emanuele La; Wright, Jesse; Li, Guohao; Shadbolt, Nigel; Wooldridge, Michael; Torr, Philip										A Scalable Communication Protocol for Networks of Large Language Models								Arxiv											1	1;2024-10-14;https://www.arxiv.org/abs/2410.11905v1	arXiv:2410.11905			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 14 2024	2024	Communication is a prerequisite for collaboration. When scaling networks of AI-powered agents, communication must be versatile, efficient, and portable. These requisites, which we refer to as the Agent Communication Trilemma, are hard to achieve in large networks of agents. We introduce Agora, a meta protocol that leverages existing communication standards to make LLM-powered agents solve complex problems efficiently. In Agora, agents typically use standardised routines for frequent communications, natural language for rare communications, and LLM-written routines for everything in between. Agora sidesteps the Agent Communication Trilemma and robustly handles changes in interfaces and members, allowing unprecedented scalability with full decentralisation and minimal involvement of human beings. On large Agora networks, we observe the emergence of self-organising, fully automated protocols that achieve complex goals without human intervention.																																	2024-11-11	PPRN:114105309		
J	Zhang, Kechi; Li, Ge; Dong, Yihong; Xu, Jingjing; Zhang, Jun; Su, Jing; Liu, Yongfei; Jin, Zhi				Dong, Yihong/LCE-6194-2024; Xu, Jingjing/ACJ-3010-2022						CodeDPO: Aligning Code Models with Self Generated and Verified Source Code								Arxiv											1	1;2024-10-08;https://www.arxiv.org/abs/2410.05605v1	arXiv:2410.05605			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 08 2024	2024	Code generation models have shown significant potential for programming tasks. However, existing training methods like supervised fine-tuning face key limitations: they do not effectively teach models to prioritize correct over incorrect solutions in ambiguous situations, nor do they effectively optimize the runtime efficiency of the generated code. To address these challenges, we propose CodeDPO, a framework that integrates preference learning into code generation to improve two key code preference factors: code correctness and efficiency. CodeDPO employs a novel dataset construction method, utilizing a self-generationand-validation mechanism that simultaneously generates and evaluates code and test cases. The underlying assumption is that test cases executable by multiple code snippets provide more reliable validation, and code that passes more tests is more likely to be correct. Through this self-validation process, our PageRankinspired algorithm iteratively updates the ranking score of each code snippet, ultimately creating a code preference optimization dataset based on correctness and efficiency. CodeDPO is flexible and scalable, generating diverse preference optimization data without depending on external resources. Through comprehensive evaluations of five widely used benchmarks, CodeDPO demonstrates significant improvements in correctness and efficiency compared to existing methods. Our experiments prove that CodeDPO enhances the capabilities of LLMs in code generation and provides a robust foundation for conducting code preference optimization in more complex and challenging real-world scenarios.																																	2024-10-24	PPRN:105765295		
J	Nguyen, Nhat-Minh; Schmidt, Fabian; Tucci, Beatriz; Reinecke, Martin; Kostic, Andrija				Nguyen, Nhat-Minh/JFK-7438-2023						How much information can be extracted from galaxy clustering at the field level?								Arxiv											4	4;2024-10-03;https://www.arxiv.org/abs/2403.03220v3| 3;2024-04-02;https://www.arxiv.org/abs/2403.03220v2| 2;2024-03-05;https://www.arxiv.org/abs/2403.03220v1| 1;2024-03-05;https://www.arxiv.org/abs/2403.03220v1	arXiv:2403.03220			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	We present optimal Bayesian field-level cosmological constraints from nonlinear tracers of cosmic large-scale structure, specifically the amplitude σ8 of linear matter fluctuations inferred from restframe simulated dark matter halos in a comoving volume of 8 (h −1Gpc)3 . Our constraint on σ8 is entirely due to nonlinear information, and obtained by explicitly sampling the initial conditions along with tracer bias and noise parameters via a Lagrangian EFT-based forward model, LEFTfield. The comparison with a simulation-based inference of the power spectrum and bispectrum—likewise using the LEFTfield forward model—shows that, when including precisely the same modes of the same data up to kmax = 0.10 h Mpc−1 (0.12 h Mpc−1 ), the field-level approach yields a factor of 3.5 (5.2) improvement on the σ8 constraint, going from 20.0% to 5.7% (17.0% to 3.3%). This study provides direct insights into cosmological information encoded in galaxy clustering beyond low-order n-point functions.																																	2024-11-03	PPRN:88035094		
J	Yuan, Zhengqing; Liu, Yixin; Cao, Yihan; Sun, Weixiang; Jia, Haolong; Chen, Ruoxi; Li, Zhaoxu; Lin, Bin; Yuan, Li; He, Lifang; Wang, Chi; Ye, Yanfang; Sun, Lichao				Yuan, Li/AET-1324-2022; He, Lifang/D-8175-2016; Sun, Weixiang/GNW-6241-2022; Li, Zhaoxu/JTS-6041-2023; Yuan, Zhengqing/HTS-6231-2023; he, lifang/AAF-5683-2020						Mora: Enabling Generalist Video Generation via A Multi-Agent Framework								Arxiv											3	3;2024-10-03;https://www.arxiv.org/abs/2403.13248v3| 2;2024-03-22;https://www.arxiv.org/abs/2403.13248v2| 1;2024-03-20;https://www.arxiv.org/abs/2403.13248v1	arXiv:2403.13248			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 03 2024	2024	Text-to-video generation has made significant strides, but replicating the capabilities of advanced systems like OpenAI Sora remains challenging due to their closed-source nature. Existing open-source methods struggle to achieve comparable performance, often hindered by ineffective agent collaboration and inadequate training data quality. In this paper, we introduce Mora, a novel multi-agent framework that leverages existing open-source modules to replicate Sora functionalities. We address these fundamental limitations by proposing three key techniques: (1) multi-agent fine-tuning with a self-modulation factor to enhance inter-agent coordination, (2) a data-free training strategy that uses large models to synthesize training data, and (3) a human-in-the-loop mechanism combined with multimodal large language models for data filtering to ensure high-quality training datasets. Our comprehensive experiments on six video generation tasks demonstrate that Mora achieves performance comparable to Sora on VBench, outperforming existing open-source methods across various tasks. Specifically, in the text-to-video generation task, Mora achieved a Video Quality score of 0.800, surpassing Sora 0.797 and outperforming all other baseline models across six key metrics. Additionally, in the image-to-video generation task, Mora achieved a perfect Dynamic Degree score of 1.00, demonstrating exceptional capability in enhancing motion realism and achieving higher Imaging Quality than Sora. These results highlight the potential of collaborative multi-agent systems and human-in-the-loop mechanisms in advancing text-to-video generation. 																																	2024-10-25	PPRN:88221375		
J	Zhang, Bowen; Soh, Harold				Zhang, Bowen/KCR-0055-2024						Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction								Arxiv											2	2;2024-10-02;https://www.arxiv.org/abs/2404.03868v2| 1;2024-04-05;https://www.arxiv.org/abs/2404.03868v1	arXiv:2404.03868			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 02 2024	2024	In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs' context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc.																																	2024-10-16	PPRN:88430107		
J	Nomura, Masahiro; Shibata, Masashi				Nomura, Masahiro/N-5826-2018						cmaes : A Simple yet Practical Python Library for CMA-ES								Arxiv											2	2;2024-10-01;https://www.arxiv.org/abs/2402.01373v2| 1;2024-02-02;https://www.arxiv.org/abs/2402.01373v1	arXiv:2402.01373			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 01 2024	2024	The covariance matrix adaptation evolution strategy (CMA-ES) has been highly effective in black-box continuous optimization, as demonstrated by its success in both benchmark problems and various real-world applications. To address the need for an accessible yet potent tool in this domain, we developed cmaes, a simple and practical Python library for CMA-ES. cmaes is characterized by its simplicity, offering intuitive use and high code readability. This makes it suitable for quickly using CMA-ES, as well as for educational purposes and seamless integration into other libraries. Despite its simplistic design, cmaes maintains enhanced functionality. It incorporates recent advancements in CMA-ES, such as learning rate adaptation for challenging scenarios, transfer learning, and mixed-integer optimization capabilities. These advanced features are accessible through a user-friendly API, ensuring that cmaes can be easily adopted in practical applications. We regard cmaes as the first choice for a Python CMA-ES library among practitioners. 																																	2024-10-11	PPRN:87509325		
J	Buder, S.; Kos, J.; Wang, E.X.; Mckenzie, M.; Howell, M.; Martell, S.L.; Hayden, M.R.; Zucker, D.B.; Nordlander, T.; Montet, B.T.; Traven, G.; Bland-Hawthorn, J.; De Silva, G.M.; Freeman, K.C.; Lewis, G.F.; Lind, K.; Sharma, S.; Simpson, J.D.; Stello, D.; Zwitter, T.; Amarsi, A.M.; Armstrong, J.J.; Banks, K.; Beavis, M.A.; Beeson, K.; Chen, B.; Ciuca, I.; Da Costa, G.S.; de Grijs, R.; Martin, B.; Nataf, D.M.; Ness, M.K.; Rains, A.D.; Scarr, T.; Vogrinčič, R.; Wang, Z.; Wittenmyer, R.A.; Xie, Y.; Collaboration, The GALAH				McKenzie, Madeleine/NYT-2784-2025						The GALAH Survey: Data Release 4								Arxiv											1	1;2024-09-30;https://www.arxiv.org/abs/2409.19858v1	arXiv:2409.19858			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 30 2024	2024	The stars of the Milky Way carry the chemical history of our Galaxy in their atmospheres as they journey through its vast expanse. Like barcodes, we can extract the chemical fingerprints of stars from high-resolution spectroscopy. The fourth data release (DR4) of the Galactic Archaeology with HERMES (GALAH) Survey, based on a decade of observations, provides the chemical abundances of up to 32 elements for 917 588 stars that also have exquisite astrometric data from the Gaia satellite. For the first time, these elements include life-essential nitrogen to complement carbon, and oxygen as well as more measurements of rare-earth elements critical to modern-life electronics, offering unparalleled insights into the chemical composition of the Milky Way. For this release, we use neural networks to simultaneously fit stellar parameters and abundances across the full spectrum, leveraging synthetic grids computed with Spectroscopy Made Easy. These grids account for atomic line formation in non-local thermodynamic equilibrium for 14 elements. In a two-iteration process, we first fit stellar labels for all 1 085 520 spectra, then co-add repeated observations and refine these labels using astrometric data from Gaia and 2MASS photometry, improving the accuracy and precision of stellar parameters and abundances. Our validation thoroughly assesses the reliability of spectroscopic measurements and highlights key caveats for catalogue users. GALAH DR4 represents yet another milestone in Galactic archaeology, combining detailed chemical compositions from multiple nucleosynthetic channels with kinematic information and age estimates. The resulting dataset, covering nearly a million stars, opens new avenues for understanding not only the chemical and dynamical history of the Milky Way, but also the broader questions of the origin of elements and the evolution of planets, stars, and galaxies.																																	2025-01-24	PPRN:100732688		
J	Gendler, Naomi; Marsh, David J.E.; McAllister, Liam; Moritz, Jakob										Glimmers from the Axiverse								Arxiv											2	2;2024-09-27;https://www.arxiv.org/abs/2309.13145v2| 1;2023-09-22;https://www.arxiv.org/abs/2309.13145v1	arXiv:2309.13145			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 27 2024	2024	We study axion-photon couplings in compactifications of type IIB string theory. We find that these couplings are systematically suppressed compared to the inverse axion periodicity, as a result of two effects. First, couplings to the QED theta angle are suppressed for axion mass eigenstates that are light compared to the mass scale set by stringy instantons on the cycle supporting QED. Second, in compactifications with many axions the intersection matrix is sparse, making kinetic mixing weak. We study the resulting phenomenology in an ensemble of 200,000 ,000 toy models constructed from the Kreuzer-Skarke database up to the maximum Hodge number h1 ,1 = 491. We examine freeze-in production and decay of thermal axions, birefringence of the cosmic microwave background, X-ray spectrum oscillations, and constraints on the QCD axion from supernovae. We conclude that compactifications in this corner of the landscape involve many invisible axions, as well as a handful that may be detectable via photon couplings.																																	2024-10-09	PPRN:85184005		
J	Hayashi, Masahito; Yamasaki, Hayata				Hayashi, Masahito/F-2964-2012						Generalized Quantum Stein's Lemma and Second Law of Quantum Resource Theories								Arxiv											2	2;2024-09-27;https://www.arxiv.org/abs/2408.02722v2| 1;2024-08-05;https://www.arxiv.org/abs/2408.02722v1	arXiv:2408.02722			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 27 2024	2024	The second law of thermodynamics is the cornerstone of physics, characterizing the convertibility between thermodynamic states through a single function, entropy. Given the universal applicability of thermodynamics, a fundamental question in quantum information theory is whether an analogous second law can be formulated to characterize the convertibility of resources for quantum information processing by a single function. In 2008, a promising formulation was proposed, linking resource convertibility to the optimal performance of a variant of the quantum version of hypothesis testing. Central to this formulation was the generalized quantum Stein’s lemma, which aimed to characterize this optimal performance by a measure of quantum resources, the regularized relative entropy of resource. If proven valid, the generalized quantum Stein’s lemma would lead to the second law for quantum resources, with the regularized relative entropy of resource taking the role of entropy in thermodynamics. However, in 2023, a logical gap was found in the original proof of this lemma, casting doubt on the possibility of such a formulation of the second law. In this work, we address this problem by developing alternative techniques and successfully proving the generalized quantum Stein’s lemma. Based on our proof, we reestablish and extend the formulation of quantum resource theories with the second law, applicable to both static resources of quantum states and a fundamental class of dynamical resources represented by classical-quantum (CQ) channels. These results resolve the fundamental problem of bridging the analogy between thermodynamics and quantum information theory.																																	2024-10-09	PPRN:91255231		
J	Cao, Yuji; Zhao, Huan; Cheng, Yuheng; Shu, Ting; Chen, Yue; Liu, Guolong; Liang, Gaoqi; Zhao, Junhua; Yan, Jinyue; Li, Yun				Chen, Yue/AAB-2557-2022; Cao, Yuji/JLL-0659-2023; Zhao, Junhua/L-8194-2019; YAN, JINYUE/Y-3099-2019						Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods								Arxiv											2	2;2024-09-23;https://www.arxiv.org/abs/2404.00282v2| 1;2024-03-30;https://www.arxiv.org/abs/2404.00282v1	arXiv:2404.00282			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 23 2024	2024	With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, a comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications such as robotics, autonomous driving, and energy systems.																																	2024-10-03	PPRN:88366842		
J	Xue, Haoru; Pan, Chaoyi; Yi, Zeji; Qu, Guannan; Shi, Guanya				Xue, Haoru/IXN-5427-2023; Qu, Guannan/GRS-8133-2022						Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing								Arxiv											1	1;2024-09-23;https://www.arxiv.org/abs/2409.15610v1	arXiv:2409.15610			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Sep 23 2024	2024	Due to high dimensionality and non-convexity, real-time optimal control using full-order dynamics models for legged robots is challenging. Therefore, Nonlinear Model Predictive Control (NMPC) approaches are often limited to reduced-order models. Sampling-based MPC has shown potential in nonconvex even discontinuous problems, but often yields suboptimal solutions with high variance, which limits its applications in high-dimensional locomotion. This work introduces DIAL-MPC (Diffusion-Inspired Annealing for Legged MPC), a sampling-based MPC framework with a novel diffusion-style annealing process. Such an annealing process is supported by the theoretical landscape analysis of Model Predictive Path Integral Control (MPPI) and the connection between MPPI and single-step diffusion. Algorithmically, DIAL-MPC iteratively refines solutions online and achieves both global coverage and local convergence. In quadrupedal torque-level control tasks, DIAL-MPC reduces the tracking error of standard MPPI by 13.4 times and outperforms reinforcement learning (RL) policies by 50% in challenging climbing tasks without any training. In particular, DIAL-MPC enables precise real-world quadrupedal jumping with payload. To the best of our knowledge, DIAL-MPC is the first training-free method that optimizes over full-order quadruped dynamics in real-time.																																	2024-10-07	PPRN:98862812		
J	Zhou, Shuang; Xu, Zidu; Zhang, Mian; Xu, Chunpu; Guo, Yawen; Zhan, Zaifu; Ding, Sirui; Wang, Jiashuo; Xu, Kaishuai; Fang, Yi; Xia, Liqiao; Yeung, Jeremy; Zha, Daochen; Melton, Genevieve B.; Lin, Mingquan; Zhang, Rui				Xia, Liqiao/KHZ-9845-2024; zhou, shuang/HKV-5321-2023; Guo, Yawen/OHT-0128-2025; Melton-Meaux, Genevieve/HSE-3064-2023						Large Language Models for Disease Diagnosis: A Scoping Review								Arxiv											2	2;2024-09-19;https://www.arxiv.org/abs/2409.00097v2| 1;2024-08-27;https://www.arxiv.org/abs/2409.00097v1	arXiv:2409.00097			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 19 2024	2024	Automatic disease diagnosis has become increasingly valuable in clinical practice. The advent of large language models (LLMs) has catalyzed a paradigm shift in artificial intelligence, with growing evidence supporting the efficacy of LLMs in diagnostic tasks. Despite the increasing attention in this field, a holistic view is still lacking. Many critical aspects remain unclear, such as the diseases and clinical data to which LLMs have been applied, the LLM techniques employed, and the evaluation methods used. In this article, we perform a comprehensive review of LLM-based methods for disease diagnosis. Our review examines the existing literature across various dimensions, including disease types and associated clinical specialties, clinical data, LLM techniques, and evaluation methods. Additionally, we offer recommendations for applying and evaluating LLMs for diagnostic tasks. Furthermore, we assess the limitations of current research and discuss future directions. To our knowledge, this is the first comprehensive review for LLM-based disease diagnosis.																																	2024-10-02	PPRN:91722991		
J	Cheng, Jeffrey; Marone, Marc; Weller, Orion; Lawrie, Dawn; Khashabi, Daniel; Van Durme, Benjamin										Dated Data: Tracing Knowledge Cutoffs in Large Language Models								Arxiv											2	2;2024-09-17;https://www.arxiv.org/abs/2403.12958v2| 1;2024-03-19;https://www.arxiv.org/abs/2403.12958v1	arXiv:2403.12958			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 17 2024	2024	Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies: (1) temporal biases of CommonCrawl data due to non-trivial amounts of old data in new dumps and (2) complications in LLM deduplication schemes involving semantic duplicates and lexical near-duplicates. Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models.																																	2024-09-29	PPRN:88243140		
J	Lou, Haozhe; Liu, Yurong; Pan, Yike; Geng, Yiran; Chen, Jianteng; Ma, Wenlong; Li, Chenglong; Wang, Lin; Feng, Hengzhen; Shi, Lu; Luo, Liyi; Shi, Yongliang				Li, Chenglong/KCL-2003-2024; Lou, Haozhe/HGT-9928-2022						Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm with Hybrid Representation								Arxiv											2	2;2024-09-17;https://www.arxiv.org/abs/2408.14873v2| 1;2024-08-27;https://www.arxiv.org/abs/2408.14873v1	arXiv:2408.14873			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Sep 17 2024	2024	The Real2Sim2Real (R2S2R) paradigm is critical for advancing robotic learning. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with both spatial representations and their associated physics attributes in the Real2Sim stage. We propose a Real2Sim pipeline to generate digital assets enabling high-fidelity simulation. We design a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the representation of robotic arms in digital assets. This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and the Gaussian model. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm’s interaction with its environment through mesh geometry. With the digital assets, we propose a fully manipulable Real2Sim pipeline that standardizes coordinate systems and scales, ensuring the seamless integration of multiple components. To demonstrate its effectiveness, we include datasets covering various robotic manipulation tasks with their mesh reconstructions. Our model achieves state-of-the-art results in realistic rendering and mesh reconstruction quality for robotic applications. Our code and datasets will be made publicly available at robostudioapp.com.																																	2024-10-03	PPRN:91564415		
J	Fan, Xiaojing; Tao, Chunliang				Fan, XiaoJing/IQW-9451-2023						Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness								Arxiv											3	3;2024-09-14;https://www.arxiv.org/abs/2408.04585v3| 2;2024-08-09;https://www.arxiv.org/abs/2408.04585v2| 1;2024-08-08;https://www.arxiv.org/abs/2408.04585v1	arXiv:2408.04585			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 14 2024	2024	Context. All magnetic field vector measurements lead to ambiguous results. Aims. We show that observations in two different lines belonging to the same multiplet but having different absorption coefficients so that they are formed at two different depths like Fe I 6302.5 Å and 6301.5 Å, enable the resolution of the azimuth ambiguity remaining from the Zeeman signal interpretation. Methods. What is measured by interpretation of the Zeeman effect is the magnetic field H , and not the divergence-free magnetic induction B . We analyze how the anisotropy of the photosphere, which is strongly stratified due to gravity and density at the star surface, affects div H and how the ambiguity resolution has to be performed in these conditions. Results. As a consequence, two ambiguity-resolved field vector maps are obtained at two different but close altitudes, which enable the derivation of the current density full vector via curl H = J . This reveals the horizontal component of the current density, which is generally found markedly larger than the better known vertical one. We observe some systematical trends, of which we present examples in the paper, like circular currents wrapping spots clockwise about a positive polarity spot and anticlockwise about a negative polarity spot and strong horizontal current components crossing active region neutral lines. Conclusions. We finally remark that the Na I D1 and D2 lines form another such line pair. We propose them as an access to the low chromosphere where they are formed. However, a better spatial resolution for both observations and analysis would probably be necessary in such a medium.																																	2024-12-23	PPRN:91292380		
J	Vacareanu, Robert; Negru, Vlad-Andrei; Suciu, Vasile; Surdeanu, Mihai				Negru, Vlad/LFS-5095-2024; Vacareanu, Robert/ILP-3548-2023						From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples								Arxiv											3	3;2024-09-10;https://www.arxiv.org/abs/2404.07544v3| 2;2024-04-30;https://www.arxiv.org/abs/2404.07544v2| 1;2024-04-11;https://www.arxiv.org/abs/2404.07544v1	arXiv:2404.07544			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Sep 10 2024	2024	We analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. Our findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman #2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. We then investigate how well the performance of large language models scales with the number of in-context exemplars. We borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret.1																																	2024-11-29	PPRN:88502895		
J	Liu, Yuhang; Zhang, Zhen; Gong, Dong; Gong, Mingming; Huang, Biwei; van den Hengel, Anton; Zhang, Kun; Shi, Javen				Zhang, Zhen/AAV-3609-2020; Huang, Biwei/JJF-8515-2023						Identifying Weight-Variant Latent Causal Models								Arxiv											2	2;2024-09-02;https://www.arxiv.org/abs/2208.14153v6| 1;2022-08-30;https://www.arxiv.org/abs/2208.14153v1	arXiv:2208.14153			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Sep 02 2024	2024	The task of causal representation learning aims to uncover latent higher-level causal representations that affect lower-level observations. Identifying true latent causal representations from observed data, while allowing instantaneous causal relations among latent variables, remains a challenge, however. To this end, we start from the analysis of three intrinsic properties in identifying latent space from observations: transitivity, permutation indeterminacy, and scaling indeterminacy. We find that transitivity acts as a key role in impeding the identifiability of latent causal representations. To address the unidentifiable issue due to transitivity, we introduce a novel identifiability condition where the underlying latent causal model satisfies a linear-Gaussian model, in which the causal coefficients and the distribution of Gaussian noise are modulated by an additional observed variable. Under some mild assumptions, we can show that the latent causal representations can be identified up to trivial permutation and scaling. Furthermore, based on this theoretical result, we propose a novel method, termed Structural caUsAl Variational autoEncoder, which directly learns latent causal representations and causal relationships among them, together with the mapping from the latent causal variables to the observed ones. We show that the proposed method learns the true parameters asymptotically. Experimental results on synthetic and real data demonstrate the identifiability and consistency results and the efficacy of the proposed method in learning latent causal representations.																																	2024-09-11	PPRN:13167188		
J	Bhardwaj, Lakshya; Inamura, Kansei; Tiwari, Apoorv				Inamura, Kentaro/G-4229-2018; Bhardwaj, Lakshya/ISU-5186-2023						Fermionic Non-Invertible Symmetries in (1+1)d: Gapped and Gapless Phases, Transitions, and Symmetry TFTs								Arxiv											2	2;2024-08-20;https://www.arxiv.org/abs/2405.09754v2| 1;2024-05-16;https://www.arxiv.org/abs/2405.09754v1	arXiv:2405.09754			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 20 2024	2024	We study fermionic non-invertible symmetries in (1+1)d, which are generalized global symmetries that mix fermion parity symmetry with other invertible and non-invertible internal symmetries. Such symmetries are described by fermionic fusion supercategories, which are fusion π-supercategories with a choice of fermion parity. The aim of this paper is to flesh out the categorical Landau paradigm for fermionic symmetries. We use the formalism of Symmetry Topological Field Theory (SymTFT) to study possible gapped and gapless phases for such symmetries, along with possible deformations between these phases, which are organized into a Hasse phase diagram. The phases can be characterized in terms of sets of condensed, confined and deconfined generalized symmetry charges, reminiscent of notions familiar from superconductivity. Many of the gapless phases also serve as phase transitions between gapped phases. The associated fermionic conformal field theories (CFTs) can be obtained by performing generalized fermionic Kennedy-Tasaki (KT) transformations on bosonic CFTs describing simpler transitions. The fermionic non-invertible symmetries along with their charges and phases discussed here can be obtained from those of bosonic non-invertible symmetries via fermionization or Jordan-Wigner transformation, which is discussed in detail.																																	2024-08-30	PPRN:89083644		
J	Radnaev, A. G.; Chung, W. C.; Cole, D. C.; Mason, D.; Ballance, T. G.; Bedalov, M. J.; Belknap, D. A.; Berman, M. R.; Blakely, M.; Bloomfield, I. L.; Buttler, P. D.; Campbell, C.; Chopinaud, A.; Copenhaver, E.; Dawes, M. K.; Eubanks, S. Y.; Friss, A. J.; Garcia, D. M.; Gilbert, J.; Gillette, M.; Goiporia, P.; Gokhale, P.; Goldwin, J.; Goodwin, D.; Graham, T. M.; Guttormsson, CJ; Hickman, G. T.; Hurtley, L.; Iliev, M.; Jones, E. B.; Jones, R. A.; Kuper, K. W.; Lewis, T. B.; Lichtman, M. T.; Majdeteimouri, F.; Mason, J. J.; McMaster, J. K.; Miles, J. A.; Mitchell, P. T.; Murphree, J. D.; Neff-Mallon, N. A.; Oh, T.; Omole, V.; Simon, C. Parlo; Pederson, N.; Perlin, M. A.; Reiter, A.; Rines, R.; Romlow, P.; Scott, A. M.; Stiefvater, D.; Tanner, J. R.; Tucker, A. K.; Vinogradov, I. V.; Warter, M. L.; Yeo, M.; Saffman, M.; Noel, T. W.				Perlin, Michael A/JNE-4215-2023; Saffman, Mark/A-8120-2009						A universal neutral-atom quantum computer with individual optical addressing and non-destructive readout								Arxiv											2	2;2024-08-20;https://www.arxiv.org/abs/2408.08288v2| 1;2024-08-15;https://www.arxiv.org/abs/2408.08288v1	arXiv:2408.08288			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 20 2024	2024	Quantum computers must achieve large-scale, fault-tolerant operation to deliver on their promise of transformational processing power [1-4]. This will require thousands or millions of high-fidelity quantum gates and similar numbers of qubits [5]. Demonstrations using neutral-atom qubits trapped and manipulated by lasers have shown that this modality can provide high two-qubit gate (CZ) fidelities and scalable operation [6-10]. However, the gates in these demonstrations are driven by lasers that do not resolve individual qubits, with universal computation enabled by physical mid-circuit shuttling of the qubits. This relatively slow operation will greatly extend runtimes for useful, large-scale computation. Here we demonstrate a universal neutral-atom quantum computer with gate rates limited by optical switching times, rather than shuttling, by individually addressing tightly focused laser beams at an array of single atoms. We achieve CZ fidelity of 99.35(4)% and local single qubit RZ gate fidelity of 99.902(8)%. Moreover, we demonstrate non-destructive readout of alkali-atom qubits with sub-percent loss, which boosts operational speed. This technique also enables us to measure 99.73(3)% CZ fidelity with atom-loss events excluded, which is a record among long lived neutral-atom qubits and highlights the path to higher fidelity and error correction. Our results represent a critical step towards large-scale, fault-tolerant neutral-atom quantum computers that can execute computations on practical timescales.																																	2024-09-23	PPRN:91415165		
J	Zhu, Liyuan; Li, Yue; Sandstrom, Erik; Huang, Shengyu; Schindler, Konrad; Armeni, Iro				Huang, Shengyu/IUM-4031-2023						LoopSplat: Loop Closure by Registering 3D Gaussian Splats								Arxiv											1	1;2024-08-20;https://www.arxiv.org/abs/2408.10154v2	arXiv:2408.10154			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 20 2024	2024	Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats (3DGS) has recently shown promise towards more accurate, dense 3D scene maps. However, existing 3DGS-based methods fail to address the global consistency of the scene via loop closure and/or global bundle adjustment. To this end, we propose LoopSplat, which takes RGB-D images as input and performs dense mapping with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure online and computes relative loop edge constraints between submaps directly via 3DGS registration, leading to improvements in efficiency and accuracy over traditional global-to-local point cloud registration. It uses a robust pose graph optimization formulation and rigidly aligns the submaps to achieve global consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD, ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking, mapping, and rendering compared to existing methods for dense RGB-D SLAM. 																																	2024-08-31	PPRN:91499548		
J	Bick, Aviv; Li, Kevin Y.; Xing, Eric P.; Kolter, J.Zico; Gu, Albert										Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models								Arxiv											1	1;2024-08-19;https://www.arxiv.org/abs/2408.10189v1	arXiv:2408.10189			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 19 2024	2024	Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.																																	2024-08-30	PPRN:91497194		
J	Jung, Jaehun; West, Peter; Jiang, Liwei; Brahman, Faeze; Lu, Ximing; Fisher, Jillian; Sorensen, Taylor; Choi, Yejin				Jiang, Liwei/IYK-0150-2023; Lu, Ximing/LLL-7542-2024; JUNG, JAEHUN/KHE-5939-2024						Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing								Arxiv											4	4;2024-08-19;https://www.arxiv.org/abs/2305.16635v4| 3;2024-04-05;https://www.arxiv.org/abs/2305.16635v3| 2;2024-03-19;https://www.arxiv.org/abs/2305.16635v2| 1;2023-05-26;https://www.arxiv.org/abs/2305.16635v1	arXiv:2305.16635			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 19 2024	2024	We present IMPOSSIBLE DISTILLATION , a novel framework for paraphrasing and sentence summarization, that distills a high-quality dataset and model from a low-quality teacher that itself cannot perform these tasks. Unlike prior works that rely on an extreme-scale teacher model ( e.g., GPT3) or task-specific architecture, we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs ( e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution. By identifying and distilling generations from these subspaces, IMPOSSIBLE DISTILLATION produces a high-quality dataset and model even from GPT2-scale LMs. We evaluate our method on multiple benchmarks spanning unconstrained / syntax-controlled paraphrase generation and sentence summarization. Our model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher diversity and fidelity than up to 13 times larger datasets.																																	2024-08-30	PPRN:72729751		
J	Li, Tianjiao; Lan, Guanghui										A simple uniformly optimal method without line search for convex optimization								Arxiv											3	3;2024-08-17;https://www.arxiv.org/abs/2310.10082v3| 2;2023-10-27;https://www.arxiv.org/abs/2310.10082v2| 1;2023-10-16;https://www.arxiv.org/abs/2310.10082v1	arXiv:2310.10082			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 17 2024	2024	Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal O(1 /k2) rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with Holder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Finally, we report some encouraging numerical results that demonstrate the advantages of AC-FGM over the previously developed parameter-free methods for convex optimization.																																	2024-08-30	PPRN:85662347		
J	Fan, Zhou; Marmolejo-Cossio, Francisco; Moroz, Daniel J.; Neuder, Michael; Rao, Rithvik; Parkes, David C.										Strategic Liquidity Provision in Uniswap v3								Arxiv											3	3;2024-08-16;https://www.arxiv.org/abs/2106.12033v5| 2;2023-09-01;https://www.arxiv.org/abs/2106.12033v4| 1;2023-07-11;https://www.arxiv.org/abs/2106.12033v2	arXiv:2106.12033			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 16 2024	2024	Uniswap v3 is the largest decentralized exchange for digital currencies. A novelty of its design is that it allows a liquidity provider (LP) to allocate liquidity to one or more closed intervals of the price of an asset instead of the full range of possible prices. An LP earns fee rewards proportional to the amount of its liquidity allocation when prices move in this interval. This induces the problem of em strategic liquidity provision: smaller intervals result in higher concentration of liquidity and correspondingly larger fees when the price remains in the interval, but with higher risk as prices may exit the interval leaving the LP with no fee rewards. Although reallocating liquidity to new intervals can mitigate this loss, it comes at a cost, as LPs must expend gas fees to do so. We formalize the dynamic liquidity provision problem and focus on a general class of strategies for which we provide a neural network-based optimization framework for maximizing LP earnings. We model a single LP that faces an exogenous sequence of price changes that arise from arbitrage and non-arbitrage trades in the decentralized exchange. We present experimental results informed by historical price data that demonstrate large improvements in LP earnings over existing allocation strategy baselines. Moreover we provide insight into qualitative differences in optimal LP behaviour in different economic environments.																																	2024-08-25	PPRN:73869067		
J	Ihssen, Friederike; Pawlowski, Jan M.; Sattler, Franz R.; Wink, Nicolas				Pawlowski, Jan/G-9949-2011						Towards quantitative precision in functional QCD I								Arxiv											1	1;2024-08-15;https://www.arxiv.org/abs/2408.08413v1	arXiv:2408.08413			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Aug 15 2024	2024	Functional approaches are the only first principle QCD setup that allow for direct computations at finite density. Predictive power and quantitative reliability of the respective results can only be obtained within a systematic expansion scheme with controlled systematic error estimates. Here we set up such a scheme within the functional renormalisation group (fRG) approach to QCD, aiming for full apparent convergence. In the current work we test this setup, using correlation functions and observables in 2+1 flavour vacuum QCD as a natural benchmark case. While the current work includes many evolutionary improvements collected over the past two decades, we also report on three novel important developments: (i) A comprehensive systematic error analysis based on the modular nature of the fRG approach. (ii) The introduction of a fully automated computational framework, allowing for unprecedented access and improvement of the fRG approach to QCD. (iii) The inclusion of the full effective potential of the chiral order parameter. This also gives access to all-order scattering events of pions and to the full momentum dependence of correlation functions, which is a first application of the automated computational framework (ii). The results compare very well to other state-of-the-art results both from functional approaches and lattice simulations, and provide data on general multi-scattering events of pions and the sigma mode for the first time.																																	2024-08-25	PPRN:91487434		
J	Ma, Yifeng; Wang, Suzhen; Ding, Yu; Ma, Bowen; Lv, Tangjie; Fan, Changjie; Hu, Zhipeng; Deng, Zhidong; Yu, Xin				Wang, Suzhen/JHU-1442-2023; Xu, Xiangyang/N-9292-2014; ma, bowen/AAH-7064-2019; Ma, Yifeng/ITU-4239-2023						TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles								Arxiv											3	3;2024-08-11;https://www.arxiv.org/abs/2304.00334v4| 2;2024-07-15;https://www.arxiv.org/abs/2304.00334v3| 1;2023-04-01;https://www.arxiv.org/abs/2304.00334v1	arXiv:2304.00334			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 11 2024	2024	Audio-driven talking head generation has drawn growing attention. To produce talking head videos with desired facial expressions, previous methods rely on extra reference videos to provide expression information, which may be difficult to find and hence limits their usage. In this work, we propose TalkCLIP, a framework that can generate talking heads where the expressions are specified by natural language, hence allowing for specifying expressions more conveniently. To model the mapping from text to expressions, we first construct a text-video paired talking head dataset where each video has diverse text descriptions that depict both coarse-grained emotions and fine-grained facial movements. Leveraging the proposed dataset, we introduce a CLIP-based style encoder that projects natural language-based descriptions to the representations of expressions. TalkCLIP can even infer expressions for descriptions unseen during training. TalkCLIP can also use text to modulate expression intensity and edit expressions. Extensive experiments demonstrate that TalkCLIP achieves the advanced capability of generating photo-realistic talking heads with vivid facial expressions guided by text descriptions.																																	2024-08-21	PPRN:53673140		
J	Shao, Wenqi; Lei, Meng; Hu, Yutao; Gao, Peng; Zhang, Kaipeng; Meng, Fanqing; Xu, Peng; Huang, Siyuan; Li, Hongsheng; Qiao, Yu; Luo, Ping				pluo/GPG-2707-2022; Li, Hongsheng/AES-5328-2022; Meng, fanqing/AAE-7775-2022; Qiao, Yu/ABD-5787-2021						TinyLVLM-eHub: Towards Comprehensive and Efficient Evaluation for Large Vision-Language Models								Arxiv											2	2;2024-08-10;https://www.arxiv.org/abs/2308.03729v2| 1;2023-08-07;https://www.arxiv.org/abs/2308.03729v1	arXiv:2308.03729			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 10 2024	2024	Large Vision-Language Models (LVLMs) have made significant strides in various multimodal tasks. Notably, GPT4V, Claude, Gemini, and others showcase exceptional multimodal capabilities, marked by profound comprehension and reasoning skills. This study introduces a comprehensive and efficient evaluation framework, TinyLVLM-eHub, to assess LVLMs’ performance, including proprietary models. TinyLVLM-eHub covers six key multimodal capabilities, such as visual perception, knowledge acquisition, reasoning, commonsense understanding, object hallucination, and embodied intelligence. The benchmark, utilizing 2.1K image-text pairs, provides a user-friendly and accessible platform for LVLM evaluation. The evaluation employs the ChatGPT Ensemble Evaluation (CEE) method, which improves alignment with human evaluation compared to word-matching approaches. Results reveal that closed-source API models like GPT4V and GeminiPro-V excel in most capabilities compared to previous open-source LVLMs, though they show some vulnerability in object hallucination. This evaluation underscores areas for LVLM improvement in real-world applications and serves as a foundational assessment for future multimodal advancements.																																	2024-08-22	PPRN:74300956		
J	Wang, Ning; Yao, Bingkun; Zhou, Jie; Wang, Xi; Guan, Nan; Jiang, Zhe										Large Language Model for Verilog Generation with Golden Code Feedback								Arxiv											2	2;2024-08-05;https://www.arxiv.org/abs/2407.18271v2| 1;2024-07-21;https://www.arxiv.org/abs/2407.18271v1	arXiv:2407.18271			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 05 2024	2024	Recent advancements in large language models (LLMs) have catalyzed significant interest in the automatic generation of Register-Transfer Level (RTL) code, particularly Verilog, from natural language instructions. While commercial LLMs like ChatGPT have dominated this domain, open-source alternatives have lagged considerably in performance, limiting the flexibility and data privacy of this emerging technology. This study introduces a novel approach utilizing reinforcement learning with golden code feedback to enhance the performance of pre-trained models. Leveraging open-source data and base models, we have achieved state-of-the-art (SOTA) results with a substantial margin. Notably, our 6.7B parameter model VeriSeek demonstrates superior performance compared to current best-inclass 13B and 16B models. Furthermore, through a comprehensive analysis of the limitations in direct fine-tuning and the training dynamics of reinforcement learning, we posit that the development of comprehensive supervisory signals, which are align with the inherent parallel semantics of Verilog code, is critical to effective generation. 																																	2024-08-09	PPRN:91122915		
J	Kou, Tengchuan; Liu, Xiaohong; Zhang, Zicheng; Li, Chunyi; Wu, Haoning; Min, Xiongkuo; Zhai, Guangtao; Liu, Ning				Min, Xiongkuo/MHR-2244-2025; Li, Chunyi/GRY-3022-2022; Zhai, Guangtao/G-5258-2013; Liu, Xiaohong/AHB-8577-2022						Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment								Arxiv											4	4;2024-05-18;https://www.arxiv.org/abs/2403.11956v4| 3;2024-03-28;https://www.arxiv.org/abs/2403.11956v3| 2;2024-03-19;https://www.arxiv.org/abs/2403.11956v2| 1;2024-08-01;	arXiv:2403.11956			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Aug 01 2024	2024	With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives. Among them, Text-to-Video (T2V) generation has received widespread attention. Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively. To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The dataset is composed of 10,000 videos generated by 9 different T2V models. We also conduct a subjective study to obtain each video's corresponding mean opinion score. Based on T2VQA-DB, we propose a novel transformer-based model for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a large language model to give the prediction score. Experimental results show that T2VQA outperforms existing T2V metrics and SOTA video quality assessment models. Quantitative analysis indicates that T2VQA is capable of giving subjective-align predictions, validating its effectiveness. 																																	2024-11-18	PPRN:88211342		
J	Sabour, Sara; Goli, Lily; Kopanas, George; Matthews, Mark; Lagun, Dmitry; Guibas, Leonidas; Jacobson, Alec; Fleet, David J.; Tagliasacchi, Andrea										SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting								Arxiv											2	2;2024-07-29;https://www.arxiv.org/abs/2406.20055v2| 1;2024-06-28;https://www.arxiv.org/abs/2406.20055v1	arXiv:2406.20055			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 29 2024	2024	3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction, offering efficient training and rendering speeds, making it suitable for real-time applications.However, current methods require highly controlled environments (no moving people or wind-blown elements, and consistent lighting) to meet the inter-view consistency assumption of 3DGS. This makes reconstruction of real-world captures problematic. We present SpotLessSplats, an approach that leverages pre-trained and general-purpose features coupled with robust optimization to effectively ignore transient distractors. Our method achieves state-of-the-art reconstruction quality both visually and quantitatively, on casual captures. 																																	2024-08-06	PPRN:90635724		
J	Jiang, Zhongyi; Zhu, Min; Lu, Lu				Li, Qiuzi/F-6474-2011; Lu, Lu/AAG-7335-2019						Fourier-MIONet: Fourier-enhanced multiple-input neural operators for multiphase modeling of geological carbon sequestration								Arxiv											2	2;2024-07-24;https://www.arxiv.org/abs/2303.04778v2| 1;2023-03-08;https://www.arxiv.org/abs/2303.04778v1	arXiv:2303.04778			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jul 24 2024	2024	Geologic carbon sequestration (GCS) is a safety-critical technology that aims to reduce the amount of carbon dioxide in the atmosphere, which also places high demands on reliability. Multiphase flow in porous media is essential to understand CO2 migration and pressure fields in the subsurface associated with GCS. However, numerical simulation for such problems in 4D is computationally challenging and expensive, due to the multiphysics and multiscale nature of the highly nonlinear governing partial differential equations (PDEs). It prevents us from considering multiple subsurface scenarios and conducting real-time optimization. Here, we develop a Fourier-enhanced multiple-input neural operator (Fourier-MIONet) to learn the solution operator of the problem of multiphase flow in porous media. Fourier-MIONet utilizes the recently developed framework of the multiple-input deep neural operators (MIONet) and incorporates the Fourier neural operator (FNO) in the network architecture. Once Fourier-MIONet is trained, it can predict the evolution of saturation and pressure of the multiphase flow under various reservoir conditions, such as permeability and porosity heterogeneity, anisotropy, injection configurations, and multiphase flow properties. Compared to the enhanced FNO (U-FNO), the proposed Fourier-MIONet has 90% fewer unknown parameters, and it can be trained in significantly less time (about 3.5 times faster) with much lower CPU memory (< < 15%) and GPU memory (< < 35%) requirements, to achieve similar prediction accuracy. In addition to the lower computational cost, Fourier-MIONet can be trained with only 6 snapshots of time to predict the PDE solutions for 30 years. Furthermore, we observed that Fourier-MIONet can maintain good accuracy when predicting out-of-distribution (OOD) data. The excellent generalizability of Fourier-MIONet is enabled by its adherence to the physical principle that the solution to a PDE is continuous over time. Moreover, the developed Fourier-MIONet makes it possible to solve the long-time evolution of geological carbon sequestration in a large-scale three-dimensional space accurately and efficiently.																																	2024-08-01	PPRN:44409919		
J	Lu, Jinghui; Yu, Haiyang; Wang, Yanjie; Ye, Yongjie; Tang, Jingqun; Yang, Ziwei; Wu, Binghong; Liu, Qi; Feng, Hao; Wang, Han; Liu, Hao; Huang, Can				Feng, Hao/HPB-6733-2023; Tang, Jingqun/NKP-5407-2025; XIAOJUAN, HU/GLQ-6536-2022						LayTextLLM: AA Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding								Arxiv											2	2;2024-07-24;https://www.arxiv.org/abs/2407.01976v2| 1;2024-07-02;https://www.arxiv.org/abs/2407.01976v1	arXiv:2407.01976			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Jul 24 2024	2024	Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.2% increase on KIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based LLMs on KIE tasks. Furthermore, we found that the spatial layout can be decoded back into coordinates, and inference requiring output bounding box coordinates can further alleviate the hallucination problem.																																	2024-08-01	PPRN:90673408		
J	Ullah, Saad; Han, Mingji; Pujar, Saurabh; Pearce, Hammond; Coskun, Ayse; Stringhini, Gianluca				Pearce, Hammond/AAJ-1986-2020						LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks								Arxiv											3	3;2024-07-24;https://www.arxiv.org/abs/2312.12575v3| 2;2024-04-13;https://www.arxiv.org/abs/2312.12575v2| 1;2023-12-19;https://www.arxiv.org/abs/2312.12575v1	arXiv:2312.12575			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 24 2024	2024	Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like `PaLM2' and `GPT-4': by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.																																	2024-08-01	PPRN:86737770		
J	Li, Xiao; Liu, Sheng; Zhou, Jinxin; Lu, Xinyu; Fernandez-Granda, Carlos; Zhu, Zhihui; Qu, Qing				Qu, Qing/AAA-8226-2019; Lu, Xinyu/KIB-5798-2024; Zhou, Jinxin/AAE-7081-2021; Zhu, ZhiHui/LMN-7316-2024						Understanding and Improving Transfer Learning of Deep Models via Neural Collapse								Arxiv											2	2;2024-07-18;https://www.arxiv.org/abs/2212.12206v4| 1;2022-12-23;https://www.arxiv.org/abs/2212.12206v1	arXiv:2212.12206			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 18 2024	2024	With the ever-increasing complexity of large-scale pre-trained models coupled with a shortage of labeled data for downstream training, transfer learning has become the primary approach in many fields, including natural language processing, computer vision, and multi-modal learning. Despite recent progress, the fine-tuning process for large-scale pre-trained models in vision still mostly relies on trial and error. This work investigates the relationship between neural collapse (NC) and transfer learning for classification problems. NC is an intriguing while prevalent phenomenon that has been recently discovered in terms of the final-layer features and linear classifiers of trained neural networks. Specifically, during the terminal phase of training, NC implies that the variability of the features within each class diminishes to zero, while the means of features between classes are maximally and equally distanced. In this work, we examine the NC attributes of pre-trained models on both downstream and source data for transfer learning, and we find strong correlation between feature collapse and downstream performance. In particular, we discovered a systematic pattern that emerges when linear probing pre-trained models on downstream training data: the more feature collapse of pre-trained models on downstream training data, the higher the transfer accuracy. Additionally, we also studied the relationship between NC and transfer accuracy on the source data. Moreover, these findings allow us to develop a principled, parameter-efficient fine-tuning method that employs skip-connection to induce the last-layer feature collapse on downstream data. Our proposed fine-tuning methods deliver good performances while reducing fine-tuning parameters by at least 90% and mitigating overfitting in situations especially when the downstream data is scarce.																																	2024-07-27	PPRN:35837298		
J	Lu, Guanxing; Zhang, Shiyi; Wang, Ziwei; Liu, Changliu; Lu, Jiwen; Tang, Yansong				Wang, Ziwei/MFK-1337-2025						ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation								Arxiv											1	1;2024-07-18;https://www.arxiv.org/abs/2403.08321v2	arXiv:2403.08321			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 18 2024	2024	Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn a semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1% % in average success rate1. 																																	2024-07-26	PPRN:90886256		
J	Tschannen, Michael; Eastwood, Cian; Mentzer, Fabian				Eastwood, Callum/AAA-9561-2019						GIVT: Generative Infinite-Vocabulary Transformers								Arxiv											4	4;2024-07-17;https://www.arxiv.org/abs/2312.02116v4| 3;2024-03-21;https://www.arxiv.org/abs/2312.02116v3| 2;2024-01-18;https://www.arxiv.org/abs/2312.02116v2| 1;2023-12-04;https://www.arxiv.org/abs/2312.02116v1	arXiv:2312.02116			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 17 2024	2024	We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a β-VAE. In class-conditional image generation GIVT outperforms VQGAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.																																	2024-07-25	PPRN:86367605		
J	Wang, Hanqing; Chen, Jiahe; Huang, Wensi; Ben, Qingwei; Wang, Tai; Mi, Boyu; Huang, Tao; Zhao, Siheng; Chen, Yilun; Yang, Sizhe; Cao, Peizhou; Yu, Wenye; Ye, Zichao; Li, Jialun; Long, Junfeng; Wang, Zirui; Wang, Huiling; Zhao, Ying; Tu, Zhongying; Qiao, Yu; Lin, Dahua; Pang, Jiangmiao				yu, wenye/IQT-9380-2023; Lin, Dahua/W-6576-2019; Long, Junfeng/KCZ-2128-2024; Qiao, Yu/ABD-5787-2021; 米, 博宇/AAR-2894-2021; Wang, Zirui/HDO-7058-2022; Chen, Yilun/IWV-1091-2023; Wang, Tai/MVV-1100-2025						GRUtopia: Dream General Robots in a City at Scale								Arxiv											1	1;2024-07-15;https://www.arxiv.org/abs/2407.10943v1	arXiv:2407.10943			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 15 2024	2024	Recent works have been exploring the scaling laws in the field of Embodied AI. Given the prohibitive costs of collecting real-world data, we believe the Simulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the learning of embodied models. This paper introduces project GRUtopia, the first simulated interactive 3D society designed for various robots. It features several advancements: (a) The scene dataset, GRScenes, includes 100k interactive, finely annotated scenes, which can be freely combined into city-scale environments. In contrast to previous works mainly focusing on home, GRScenes covers 89 diverse scene categories, bridging the gap of service-oriented environments where general robots would be initially deployed. (b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC) system that is responsible for social interaction, task generation, and task assignment, thus simulating social scenarios for embodied AI applications. (c) The benchmark, GRBench, supports various robots but focuses on legged robots as primary agents and poses moderately challenging tasks involving Object Loco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that this work can alleviate the scarcity of high-quality data in this field and provide a more comprehensive assessment of Embodied AI research. The project is available at https://github.com/OpenRobotLab/GRUtopia.																																	2024-07-23	PPRN:90820542		
J	Niu, Muyao; Cun, Xiaodong; Wang, Xintao; Zhang, Yong; Shan, Ying; Zheng, Yinqiang				Cun, Xiaodong/AAA-4674-2022						MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model								Arxiv											3	3;2024-07-11;https://www.arxiv.org/abs/2405.20222v3| 2;2024-06-02;https://www.arxiv.org/abs/2405.20222v2| 1;2024-05-30;https://www.arxiv.org/abs/2405.20222v1	arXiv:2405.20222			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 11 2024	2024	We present MOFA-Video, an advanced controllable image animation method that generates video from the given image using various additional controllable signals (such as human landmarks reference, manual trajectories, and another even provided video) or their combinations. This is different from previous methods which only can work on a specific motion domain or show weak control abilities with diffusion prior. To achieve our goal, we design several domain-aware motion field adapters (ie, MOFA-Adapters) to control the generated motions in the video generation pipeline. For MOFA-Adapters, we consider the temporal motion consistency of the video and generate the dense motion flow from the given sparse control conditions first, and then, the multi-scale features of the given image are wrapped as a guided feature for stable video diffusion generation. We naively train two motion adapters for the manual trajectories and the human landmarks individually since they both contain sparse information about the control. After training, the MOFA-Adapters in different domains can also work together for more controllable video generation. 																																	2024-07-23	PPRN:89113027		
J	Wang, Bin; Liu, Zhengyuan; Huang, Xin; Jiao, Fangkai; Ding, Yang; Aw, Aiti; Chen, Nancy F.				Huang, Xin/ADX-4483-2022; Chen, Nancy/GSD-8813-2022; Jiao, Fangkai/HNB-4284-2023						SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning								Arxiv											5	5;2024-07-11;https://www.arxiv.org/abs/2309.04766v5| 4;2024-04-01;https://www.arxiv.org/abs/2309.04766v4| 3;1800-01-01;https://www.arxiv.org/abs/2309.04766v3| 2;2023-12-19;https://www.arxiv.org/abs/2309.04766v2| 1;2023-09-09;https://www.arxiv.org/abs/2309.04766v1	arXiv:2309.04766			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 11 2024	2024	We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingually-trained models have not attained "balanced multilingual" capabilities. Our endeavors underscore the need for more generalizable semantic representations and enhanced multilingual contextualization. SeaEval can serve as a launchpad for more thorough investigations and evaluations for multilingual and multicultural scenarios.1																																	2024-07-23	PPRN:84950479		
J	Zhang, Zhenyu; Jaiswal, Ajay; Yin, Lu; Liu, Shiwei; Zhao, Jiawei; Tian, Yuandong; Wang, Zhangyang				Zhihua, Wang/AFO-5263-2022; Zhang, Zhenyu/ISS-1688-2023; zhao, jiawei/IXD-4249-2023						Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients								Arxiv											1	1;2024-07-11;https://www.arxiv.org/abs/2407.08296v1	arXiv:2407.08296			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 11 2024	2024	Training Large Language Models (LLMs) is memory-intensive due to the large number of parameters and associated optimization states. GaLore, a recent method, reduces memory usage by projecting weight gradients into a low-rank subspace without compromising performance. However, GaLore relies on time-consuming Singular Value Decomposition (SVD) operations to identify the subspace, and the frequent subspace updates lead to significant training time overhead. Moreover, GaLore offers minimal improvements in accuracy and efficiency compared to LoRA in more accessible fine-tuning scenarios. To address these limitations, we introduce Q-Galore, a novel approach that substantially reduces memory usage by combining quantization and low-rank projection, surpassing the benefits of GaLore. Our method is based on two key observations: (i) the gradient subspace exhibits diverse properties, with some layers converging early in training while others are subject to frequent changes; (ii) the projection matrices are highly resilient to low-bit quantization. Leveraging these insights, Q-GaLore adaptively updates the gradient subspace based on its convergence statistics, achieving comparable performance while significantly reducing the number of SVD operations. We maintain the projection matrices in INT4 format and weights in INT8 format, incorporating stochastic rounding to capture accumulated gradient information. This approach enables a high-precision training trajectory using only low-precision weights. We demonstrate that Q-GaLore achieves highly competitive performance with exceptional memory efficiency. At pre-training, Q-GaLore facilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060 Ti with only 16 GB memory. At fine-tuning, it reduces memory consumption by up to 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at the same memory cost.																																	2024-07-22	PPRN:90769708		
J	Chewi, Sinho; Erdogdu, Murat A.; Li, Mufan Bill; Shen, Ruoqi; Zhang, Matthew										Analysis of Langevin Monte Carlo from Poincare&nbsp;to Log-Sobolev								Arxiv											1	1;2024-07-10;https://www.arxiv.org/abs/2112.12662v2	arXiv:2112.12662			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 10 2024	2024	Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution π under the sole assumption that π satisfies a Poincare´ inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or Renyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that π satisfies either a Lata la–Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincare´ and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions.																																	2024-07-21	PPRN:90760170		
J	Zhang, Yuji; Li, Sha; Liu, Jiateng; Yu, Pengfei; Fung, Yi R.; Li, Jing; Li, Manling; Ji, Heng				Li, Jing/IQU-9459-2023						Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models								Arxiv											1	1;2024-07-10;https://www.arxiv.org/abs/2407.08039v1	arXiv:2407.08039			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jul 10 2024	2024	Hallucination is often regarded as a major impediment for using large language models (LLMs), especially for knowledge-intensive tasks. Even when the training corpus consists solely of true statements, language models still generate hallucinations in the form of amalgamations of multiple facts. We coin this phenomenon as "knowledge overshadowing'': when we query knowledge from a language model with multiple conditions, some conditions overshadow others, leading to hallucinated outputs. This phenomenon partially stems from training data imbalance, which we verify on both pretrained models and fine-tuned models, over a wide range of LM model families and sizes.From a theoretical point of view, knowledge overshadowing can be interpreted as over-generalization of the dominant conditions (patterns). We show that the hallucination rate grows with both the imbalance ratio (between the popular and unpopular condition) and the length of dominant condition description, consistent with our derived generalization bound. Finally, we propose to utilize overshadowing conditions as a signal to catch hallucination before it is produced, along with a training-free self-contrastive decoding method to alleviate hallucination during inference. Our proposed approach showcases up to 82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control, with different models and datasets.																																	2024-07-23	PPRN:90771150		
J	Ying, Zonghao; Liu, Aishan; Liu, Xianglong; Tao, Dacheng				Tao, Dacheng/A-5449-2012; Liu, Xianglong/NTQ-2427-2025						Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak Attacks								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2406.06302v2| 1;2024-06-10;https://www.arxiv.org/abs/2406.06302v1	arXiv:2406.06302			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 03 2024	2024	The recent release of GPT-4o has garnered widespread attention due to its powerful general capabilities. While its impressive performance is widely acknowledged, its safety aspects have not been sufficiently explored. Given the potential societal impact of risky content generated by advanced generative AI such as GPT-4o, it is crucial to rigorously evaluate its safety. In response to this question, this paper for the first time conducts a rigorous evaluation of GPT-4o against jailbreak attacks. Specifically, this paper adopts a series of multi-modal and uni-modal jailbreak attacks on 4 commonly used benchmarks encompassing three modalities (ie, text, speech, and image), which involves the optimization of over 4,000 initial text queries and the analysis and statistical evaluation of nearly 8,000+ response on GPT-4o. Our extensive experiments reveal several novel observations: (1) In contrast to the previous version (such as GPT-4V), GPT-4o has enhanced safety in the context of text modality jailbreak; (2) The newly introduced audio modality opens up new attack vectors for jailbreak attacks on GPT-4o; (3) Existing black-box multimodal jailbreak attack methods are largely ineffective against GPT-4o and GPT-4V. These findings provide critical insights into the safety implications of GPT-4o and underscore the need for robust alignment guardrails in large models. 																																	2024-07-20	PPRN:89266390		
J	Sim, Gibaik; Knolle, Johannes										Pair Density Waves and Supercurrent Diode Effect in Altermagnets								Arxiv											1	1;2024-07-01;https://www.arxiv.org/abs/2407.01513v1	arXiv:2407.01513			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jul 01 2024	2024	Metallic altermagnets are unusual collinear magnets that feature zero net magnetization with momentum-dependent spin splitting. Here, we show that this spin splitting can induce pair density wave states even in the absence of external magnetic fields. Focusing on BCS-type attractive interactions, we find the stabilization of symmetrically distinct pair density wave states depending on the chemical potential. These states include Fulde-Ferrell and Fulde-Ferrell* states, both of which break inversion symmetry. We investigate the supercurrent properties and discover non-reciprocal supercurrents for both the Fulde-Ferrell and Fulde-Ferrell* states with distinct spatial dependencies. We propose that the supercurrent diode effect can serve as an experimental tool for distinguishing between different pair density waves in metallic altermagnets and discuss the relation to material candidates.																																	2024-07-19	PPRN:90659268		
J	Duarte, Andre V.; Zhao, Xuandong; Oliveira, Arlindo L.; Li, Lei				Zhao, Xuandong/LIG-4204-2024						DE-COP: Detecting Copyrighted Content in Language Models Training Data								Arxiv											2	2;2024-06-25;https://www.arxiv.org/abs/2402.09910v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.09910v1	arXiv:2402.09910			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 25 2024	2024	How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP’s core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model’s training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 4% accuracy. The code and datasets are available at https : //github . com/LeiLiLab/DE-COP .																																	2024-07-15	PPRN:87698984		
J	Geigle, Gregor; Jain, Abhay; Timofte, Radu; Glavas, Goran				Timofte, Radu/H-4438-2011						mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs								Arxiv											3	3;2024-06-20;https://www.arxiv.org/abs/2307.06930v3| 2;2023-10-02;https://www.arxiv.org/abs/2307.06930v2| 1;2023-07-13;https://www.arxiv.org/abs/2307.06930v1	arXiv:2307.06930			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 20 2024	2024	Modular vision-language models (VisionLLMs) align pretrained image encoders with (frozen) large language models (LLMs) and post-hoc condition LLMs to ‘understand’ the image input. With the abundance of readily available high-quality English image-text data as well as strong monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. We present mBLIP, the first Vision-LLM leveraging multilingual LLMs, which we obtain in a computationally efficient manner on consumer-level hardware. To this end, we re-align an image encoder previously tuned to an English LLM to a new, multilingual LLM using only a few million multilingual training examples derived from a mix of vision-and-language tasks, which we obtain by machine-translating high-quality English data to 95 languages. On the IGLUE benchmark and XM3600, mBLIP yields results competitive with state-of-the-art models and it greatly outperforms strong Englishonly Vision-LLMs like Llava 1.5. We release our model,																																	2024-07-06	PPRN:73905565		
J	Gao, Jingtong; Chen, Bo; Zhao, Xiangyu; Liu, Weiwen; Li, Xiangyang; Wang, Yichao; Zhang, Zijian; Wang, Wanyu; Ye, Yuyang; Lin, Shanru; Guo, Huifeng; Tang, Ruiming				Ye, Yuyang/LJM-2427-2024; Zhao, Xiangyu/AAO-2203-2020; WANG, Yichao/HKM-6669-2023; Liu, Weiwen/LMQ-1488-2024; chen, bo/AAC-7188-2022; Chen, Feiliang/HGU-6386-2022						LLM-enhanced Reranking in Recommender Systems								Arxiv											4	4;2025-02-03;https://www.arxiv.org/abs/2406.12433v4| 3;2025-01-31;https://www.arxiv.org/abs/2406.12433v3| 2;2024-06-20;https://www.arxiv.org/abs/2406.12433v2| 1;2024-06-18;https://www.arxiv.org/abs/2406.12433v1	arXiv:2406.12433			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Reranking is a critical component in recommender systems, playing an essential role in refining the output of recommendation algorithms. Traditional reranking models have focused predominantly on accuracy, but modern applications demand consideration of additional criteria such as diversity and fairness. Existing reranking approaches often fail to harmonize these diverse criteria effectively at the model level. Moreover, these models frequently encounter challenges with scalability and personalization due to their complexity and the varying significance of different reranking criteria in diverse scenarios. In response, we introduce a comprehensive reranking framework enhanced by LLM, designed to seamlessly integrate various reranking criteria while maintaining scalability and facilitating personalized recommendations. This framework employs a fully connected graph structure, allowing the LLM to simultaneously consider multiple aspects such as accuracy, diversity, and fairness through a coherent Chain-of-Thought (CoT) process. A customizable input mechanism is also integrated, enabling the tuning of the language model's focus to meet specific reranking needs. We validate our approach using three popular public datasets, where our framework demonstrates superior performance over existing state-of-the-art reranking models in balancing multiple criteria. The code for this implementation is publicly available.																																	2025-08-07	PPRN:89360806		
J	Gorantla, Pranay; Shao, Shu-Heng; Tantivasadakarn, Nathanan										Tensor networks for non-invertible symmetries in 3+1d and beyond								Arxiv											1	1;2024-06-18;https://www.arxiv.org/abs/2406.12978v1	arXiv:2406.12978			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Tensor networks provide a natural language for non-invertible symmetries in general Hamiltonian lattice models. We use ZX-diagrams, which are tensor network presentations of quantum circuits, to define a non-invertible operator implementing the Wegner duality in 3+1d lattice Z2 gauge theory. The non-invertible algebra, which mixes with lattice translations, can be efficiently computed using ZX-calculus. We further deform the Z2 gauge theory while preserving the duality and find a model with nine exactly degenerate ground states on a torus, consistent with the Lieb-Schultz-Mattis-type constraint imposed by the symmetry. Finally, we provide a ZX-diagram presentation of the non-invertible duality operators (including non-invertible parity/reflection symmetries) of generalized Ising models based on graphs, encompassing the 1+1d Ising model, the three-spin Ising model, the Ashkin-Teller model, and the 2+1d plaquette Ising model. The mixing (or lack thereof) with spatial symmetries is understood from a unifying perspective based on graph theory.																																	2024-07-06	PPRN:89379243		
J	Jiang, Ruili; Chen, Kehai; Bai, Xuefeng; He, Zhixuan; Li, Juntao; Yang, Muyun; Zhao, Tiejun; Nie, Liqiang; Zhang, Min				Bai, Xuefeng/KVB-1864-2024; Chen, Kehai/ABF-1874-2020; He, Zhixuan/HTS-6024-2023						A Survey on Human Preference Learning for Large Language Models								Arxiv											1	1;2024-06-18;https://www.arxiv.org/abs/2406.11191v2	arXiv:2406.11191			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.																																	2025-08-07	PPRN:123164837		
J	Wan, Zhongwei; Wu, Xinjian; Zhang, Yu; Xin, Yi; Tao, Chaofan; Zhu, Zhihong; Wang, Xin; Luo, Siqi; Xiong, Jing; Zhang, Mi				Zhu, Zhihong/OIR-6326-2025; Wang, Xin/MXL-4068-2025; Wan, Zhongwei/JDM-4369-2023						D2O:Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models								Arxiv											3	3;2025-03-13;https://www.arxiv.org/abs/2406.13035v3| 2;2024-06-23;https://www.arxiv.org/abs/2406.13035v2| 1;2024-06-18;https://www.arxiv.org/abs/2406.13035v1	arXiv:2406.13035			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 18 2024	2024	Efficient inference in Large Language Models (LLMs) is impeded by the growing memory demands of key-value (KV) caching, especially for longer sequences. Traditional KV cache eviction strategies, which prioritize less critical KV-pairs based on attention scores, often degrade generation quality, leading to issues such as context loss or hallucinations. To address this, we introduce Dynamic Discriminative Operations (D2O), a novel method that utilizes two-level discriminative strategies to optimize KV cache size without fine-tuning, while preserving essential context. Initially, by observing varying densities of attention weights between shallow and deep layers, we use this insight to determine which layers should avoid excessive eviction to minimize information loss. Subsequently, for the eviction strategy in each layer, D2O innovatively incorporates a compensation mechanism that maintains a similarity threshold to re-discriminate the importance of previously discarded tokens, determining whether they should be recalled and merged with similar tokens. Our approach not only achieves significant memory savings and enhances inference throughput by more than 3x but also maintains high-quality long-text generation. Extensive experiments across various benchmarks and LLM architectures have demonstrated that D2O significantly enhances performance with a constrained KV cache budget.																																	2025-08-07	PPRN:89375304		
J	Bogomolov, Egor; Eliseeva, Aleksandra; Galimzyanov, Timur; Glukhov, Evgeniy; Shapkin, Anton; Tigina, Maria; Golubev, Yaroslav; Kovrigin, Alexander; van Deursen, Arie; Izadi, Maliheh; Bryksin, Timofey				Bogomolov, Egor/AAA-2510-2022; Golubev, Yaroslav/AHC-5703-2022; Bryksin, Timofey/AAG-6343-2020; van Deursen, Arie/G-3084-2011						Long Code Arena: a Set of Benchmarks for Long-Context Code Models								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.11612v1	arXiv:2406.11612			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 17 2024	2024	Nowadays, the fields of code and natural language processing are evolving rapidly. In particular, models become better at processing long context windows — supported context sizes have increased by orders of magnitude over the last few years. However, there is a shortage of benchmarks for code processing that go beyond a single file of context, while the most popular ones are limited to a single method. With this work, we aim to close this gap by introducing Long Code Arena, a suite of six benchmarks for code processing tasks that require project -wide context. These tasks cover different aspects of code processing: library -based code generation, CI builds repair, project -level code completion, commit message generation, bug localization, and module summarization. For each task, we provide a manually verified dataset for testing, an evaluation suite, and open -source baseline solutions based on popular LLMs to showcase the usage of the dataset and to simplify adoption by other researchers. We publish the benchmark page on HuggingFace Spaces with the leaderboard, links to HuggingFace Hub for all the datasets, and link to the GitHub repository with baselines: https://huggingface.co/ spaces/JetBrains-Research/long-code-arena. .																																	2024-07-04	PPRN:89344007		
J	Deep, Pala Tej; Bhardwaj, Rishabh; Poria, Soujanya				PORIA, SOUJANYA/KIJ-4789-2024						DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling								Arxiv											1	1;2024-06-17;https://www.arxiv.org/abs/2406.11617v1	arXiv:2406.11617			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 17 2024	2024	With the proliferation of domain-specific models, model merging has emerged as a set of techniques that combine the capabilities of multiple models into one that can multitask without the cost of additional training. In this paper, we propose a new model merging technique, Drop and rEscaLe via sampLing with mAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE, which shows significant advantages over DARE and TIES. MAGPRUNE first ranks the parameters in order of their magnitude and assigns higher dropout probabilities (p) to parameters with lower ranks corresponding to lower magnitudes. To approximate the original embeddings, MAGPRUNE employs a rescaling operation on the parameters that survive the random dropping by 1/(1 - p). On three different expert models considered for merging (LM, Math, Code) and corresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an average improvement of 2.4 points over baseline methods employing delta parameter pruning (an improvement of 3.6 points over TIES, 1.2 points over DARE), and 11.1 points over the no-pruning baseline (TA). 																																	2024-07-10	PPRN:89349199		
J	Hoof, Sebastian; Marsh, David J.E.; Sisk-Reynes, Julia; Matthews, James H.; Reynolds, Christopher										Getting More Out of Black Hole Superradiance: a Statistically Rigorous Approach to Ultralight Boson Constraints								Arxiv											1	1;2024-06-14;https://www.arxiv.org/abs/2406.10337v1	arXiv:2406.10337			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 14 2024	2024	Black hole (BH) superradiance can provide strong constraints on the properties of ultralight bosons (ULBs). Since most of the previous work has focused on the theoretical predictions, here we investigate the most suitable statistical framework to constrain ULB masses and self-interactions. We argue that a Bayesian approach provides a clear statistical interpretation, deals with limitations regarding the reproducibility of existing BH analyses, incorporates the full information from BH data, and allows us to include additional nuisance parameters or to perform hierarchical modelling with BH populations in the future. We demonstrate the feasibility of our approach using mass and spin posterior samples for the X-ray binary BH M33 X-7 and, for the first time in this context, the supermassive BH IRAS 09149-6206. We explain the differences to existing ULB constraints in the literature and illustrate the effects of various assumptions about the superradiance process (equilibrium regime vs cloud collapse, higher occupation levels). As a result, our procedure yields the most rigorous ULB constraints available in the literature, with important implications for the QCD axion and axion-like particles. We encourage all groups analysing BH data to publish likelihood functions or posterior samples as supplementary material to facilitate this type of analysis.																																	2024-07-04	PPRN:89352762		
J	Pan, Jing; Wu, Jian; Gaur, Yashesh; Sivasankaran, Sunit; Chen, Zhuo; Liu, Shujie; Li, Jinyu				Sivasankaran, Sunit/GQR-0582-2022						COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning								Arxiv											2	2;2024-06-14;https://www.arxiv.org/abs/2311.02248v2| 1;2023-11-03;https://www.arxiv.org/abs/2311.02248v1	arXiv:2311.02248			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Jun 14 2024	2024	We present a cost-effective method to integrate speech into a large language model (LLM), resulting in a Contextual Speech Model with Instruction-following/in-context-learning Capabilities (COSMIC) multi-modal LLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA) pairs from speech transcriptions for supervised instruction tuning. With under 30 million trainable parameters and only 450 hours of English speech data, COSMIC demonstrates emerging capabilities in instruction-following and in-context learning. Equipped with such capabilities, COSMIC achieves a maximum 33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and a significant boost in the 1-shot setting. Additionally, there is an average 25.8% relative Word Error Rate (WER) reduction for 1-shot cross-domain adaptation. COSMIC exhibits a significant automatic speech recognition (ASR) accuracy gain in contextual biasing tasks due to its instruction-following capability.																																	2024-07-02	PPRN:86054249		
J	Zhao, Wei; Li, Zhe; Li, Yige; Zhang, Ye; Sun, Jun										Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing								Arxiv											2	2;2024-06-14;https://www.arxiv.org/abs/2405.18166v2| 1;2024-05-28;https://www.arxiv.org/abs/2405.18166v1	arXiv:2405.18166			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 14 2024	2024	Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs response to harmful prompts and propose a novel defense method termed textbf{L}ayer-specific textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical textit{safety layers} exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from selected target layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. 																																	2024-07-02	PPRN:89090954		
J	Qi, Jiaxing; Luan, Zhongzhi; Huang, Shaohan; Fung, Carol; Yang, Hailong; Qian, Depei				Huang, Shaohan/LDF-3300-2024; Yang, Hailong/HMP-0616-2023						FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning								Arxiv											1	1;2024-06-12;https://www.arxiv.org/abs/2406.07925v1	arXiv:2406.07925			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 12 2024	2024	Large language models (LLMs) have emerged as important components across various fields, yet their training requires substantial computation resources and abundant labeled data. It poses a challenge to robustly training LLMs for individual users (clients). To tackle this challenge, the intuitive idea is to introduce federated learning (FL), which can collaboratively train models on distributed private data. However, existing methods suffer from the challenges of data heterogeneity, system heterogeneity, and model size, resulting in suboptimal performance and high costs. In this work, we proposed a variant of personalized federated learning (PFL) framework, namely FDLoRA, which allows the client to be a single device or a cluster and adopts low-rank adaptation (LoRA) tuning. FDLoRA sets dual LoRA modules on each client to capture personalized and global knowledge, respectively, and only the global LoRA module uploads parameters to the central server to aggregate cross-client knowledge. Finally, an adaptive fusion approach is employed to combine the of the dual LoRAs. This enables FDLoRA to make effective use of private data distributed across different clients, thereby improving performance on the client without incurring high communication and computing costs. We conducted extensive experiments in two practice scenarios. The results demonstrate that FDLoRA outperforms six baselines in terms of performance, stability, robustness, computation cost, and communication cost.																																	2024-07-04	PPRN:89289472		
J	De Smet, Maxim; Matsumoto, Yuta; Zwerver, Anne-Marije J.; Tryputen, Larysa; de Snoo, Sander L.; Amitonov, Sergey V.; Sammak, Amir; Samkharadze, Nodar; Gul, Oender; Wasserman, Rick N.M.; Rimbach-Russ, Maximilian; Scappucci, Giordano; Vandersypen, Lieven M.K.				Vandersypen, Lieven/LZE-7734-2025; Russ, Maximilian/AHD-4565-2022						High-fidelity single-spin shuttling in silicon								Arxiv											1	1;2024-06-11;https://www.arxiv.org/abs/2406.07267v1	arXiv:2406.07267			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 11 2024	2024	The computational power and fault-tolerance of future large-scale quantum processors derive in large part from the connectivity between the qubits. One approach to increase connectivity is to engineer qubit-qubit interactions at a distance. Alternatively, the connectivity can be increased by physically displacing the qubits. This has been explored in trapped-ion experiments and using neutral atoms trapped with optical tweezers. For semiconductor spin qubits, several studies have investigated spin coherent shuttling of individual electrons, but high-fidelity transport over extended distances remains to be demonstrated. Here we report shuttling of an electron inside an isotopically purified Si/SiGe heterostructure using electric gate potentials. First, we form static quantum dots, and study how spin coherence decays as we repeatedly move a single electron between up to five dots. Next, we create a traveling wave potential to transport an electron in a moving quantum dot. This second method shows substantially better spin coherence than the first. It allows us to displace an electron over an effective distance of 10 {mu}m in under 200 ns with an average fidelity of 99%. These results will guide future efforts to realize large-scale semiconductor quantum processors, making use of electron shuttling both within and between qubit arrays [17-22].																																	2024-07-12	PPRN:89279123		
J	Cosentino, Justin; Belyaeva, Anastasiya; Liu, Xin; Furlotte, Nicholas A.; Yang, Zhun; Lee, Chace; Schenck, Erik; Patel, Yojan; Cui, Jian; Schneider, Logan Douglas; Bryant, Robby; Gomes, Ryan G.; Jiang, Allen; Lee, Roy; Liu, Yun; Perez, Javier; Rogers, Jameson K.; Speed, Cathy; Tailor, Shyam; Walker, Megan; Yu, Jeffrey; Althoff, Tim; Heneghan, Conor; Hernandez, John; Malhotra, Mark; Stern, Leor; Matias, Yossi; Corrado, Greg S.; Patel, Shwetak; Shetty, Shravya; Zhan, Jiening; Prabhakara, Shruthi; McDuff, Daniel; McLean, Cory Y.				Schneider, Logan/D-3372-2015						Towards a Personal Health Large Language Model								Arxiv											1	1;2024-06-10;https://www.arxiv.org/abs/2406.06474v1	arXiv:2406.06474			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 10 2024	2024	Large language models (LLMs) can retrieve, reason over, and make inferences about a wide range of information. In health, most LLM efforts to date have focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into clinical tasks, provide a rich, continuous, and longitudinal source of data relevant for personal health monitoring. Here we present a new model, Personal Health Large Language Model (PH-LLM), a version of Gemini fine-tuned for text understanding and reasoning over numerical time -series personal health data for applications in sleep and fitness. To systematically evaluate PH-LLM, we created and curated three novel benchmark datasets that test 1) production of personalized insights and recommendations from measured sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self -reported sleep quality outcomes. For the insights and recommendations tasks we created 857 case studies in sleep and fitness. These case studies, designed in collaboration with domain experts, represent real -world scenarios and highlight the model’s capabilities in understanding and coaching. Through comprehensive human and automatic evaluation of domain -specific rubrics, we observed that both Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights. To further assess expert domain knowledge, we evaluated PH-LLM performance on multiple choice question examinations in sleep medicine and fitness. PH-LLM achieved 79% on sleep (N=629 questions) and 88% on fitness (N=99 questions), both of which exceed average scores from a sample of human experts as well as benchmarks for receiving continuing credit in those domains. To enable PH-LLM to predict self -reported assessments of sleep quality, we trained the model to predict self -reported sleep disruption and sleep impairment outcomes from textual and multimodal encoding representations of wearable sensor data. We demonstrate that multimodal encoding is both necessary and sufficient to match performance of a suite of discriminative models to predict these outcomes. Although further development and evaluation are necessary in the safety -critical personal health domain, these results demonstrate both the broad knowledge base and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM.																																	2024-07-04	PPRN:89264705		
J	Dutt, Raman; Ericsson, Linus; Sanchez, Pedro; Tsaftaris, Sotirios A.; Hospedales, Timothy				Tsaftaris, Sotirios/E-3725-2010						Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity								Arxiv											2	2;2024-06-10;https://www.arxiv.org/abs/2305.08252v4| 1;2023-05-14;https://www.arxiv.org/abs/2305.08252v1	arXiv:2305.08252			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 10 2024	2024	Foundation models have significantly advanced medical image analysis through the pre-train fine-tune paradigm. Among various fine-tuning algorithms, Parameter-Efficient Fine-Tuning (PEFT) is increasingly utilized for knowledge transfer across diverse tasks, including visionlanguage and text-to-image generation. However, its application in medical image analysis is relatively unexplored due to the lack of a structured benchmark for evaluating PEFT methods. This study fills this gap by evaluating 17 distinct PEFT algorithms across convolutional and transformer-based networks on image classification and text-to-image generation tasks using six medical datasets of varying size, modality, and complexity. Through a battery of over 700 controlled experiments, our findings demonstrate PEFT’s effectiveness, particularly in low data regimes common in medical imaging, with performance gains of up to 22% in discriminative and generative tasks. These recommendations can assist the community in incorporating PEFT into their workflows and facilitate fair comparisons of future PEFT methods, ensuring alignment with advancements in other areas of machine learning and AI.																																	2024-06-22	PPRN:69577243		
J	Liu, Yepeng; Bu, Yuheng				Liu, Yepeng/ONJ-2942-2025; Bu, Yuheng/JJF-3354-2023						Adaptive Text Watermark for Large Language Models								Arxiv											2	2;2024-06-09;https://www.arxiv.org/abs/2401.13927v2| 1;2024-01-25;https://www.arxiv.org/abs/2401.13927v1	arXiv:2401.13927			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 09 2024	2024	The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining robustness, security, and the ability to detect watermarks without prior knowledge of the prompt and model. This paper proposes an adaptive text watermarking strategy to address such a challenge. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured by an auxiliary model and keep the low-entropy token distributions untouched. For the sake of security and to further minimize the watermark’s impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of un-watermarked LLMs while maintaining sufficient security.1																																	2024-07-04	PPRN:87334780		
J	An, Bang; Ding, Mucong; Rabbani, Tahseen; Agrawal, Aakriti; Xu, Yuancheng; Deng, Chenghao; Zhu, Sicheng; Mohamed, Abdirisak; Wen, Yuxin; Goldstein, Tom; Huang, Furong				Deng, Chenghao/JFL-1456-2023; Wen, Yuxin/AAA-4882-2019						WAVES: Benchmarking the Robustness of Image Watermarks								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2401.08573v3| 2;2024-01-22;https://www.arxiv.org/abs/2401.08573v2| 1;2024-01-16;https://www.arxiv.org/abs/2401.08573v1	arXiv:2401.08573			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 07 2024	2024	In the burgeoning age of generative AI, watermarks act as identifiers of provenance and artificial content. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a benchmark for assessing image watermark robustness, overcoming the limitations of current evaluation methods. WAVES integrates detection and identification tasks and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced, novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. Our novel, comprehensive evaluation reveals previously undetected vulnerabilities of several modern watermarking algorithms. We envision WAVES as a toolkit for the future development of robust watermarks.																																	2024-07-04	PPRN:87186315		
J	Aoki, Toshitaka; Higashitani, Akihiro; Iyama, Osamu; Kase, Ryoichi; Mizuno, Yuya										Fans and polytopes in tilting theory I: Foundations								Arxiv											3	3;2024-06-07;https://www.arxiv.org/abs/2203.15213v4| 2;2024-06-03;https://www.arxiv.org/abs/2203.15213v3| 1;2022-03-29;https://www.arxiv.org/abs/2203.15213v2	arXiv:2203.15213			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jun 07 2024	2024	For a finite dimensional algebra A over a field k, the 2-term silting complexes of A gives a simplicial complex Δ(A) called the g-simplicial complex. We give tilting theoretic interpretations of the h-vectors and Dehn-Sommerville equations of Δ(A). Using g-vectors of 2-term silting complexes, Δ(A) gives a nonsingular fan Σ(A) in the real Grothendieck group K0(proj A)R called the g-fan. We give several basic properties of Σ(A) including sign-coherence, sign decomposition, idempotent reductions, Jasso reductions, pairwise positivity and a connection with Newton polytopes of A-modules. Moreover, Σ(A) gives a (possibly infinite and non-convex) polytope P(A) in K0(proj A)R called the g-polytope of A. We call A g-convex if P(A) is convex. In this case, we show that it is a reflexive polytope, and that the dual polytope is given by the 2-term simple minded collections of A. There are precisely 7 convex g-polyogons up to isomorphism. We give a classification of algebras whose g-polytopes are smooth Fano.<br /> We study g-fans and g-polytopes of two important classes of algebras. We show that the g-fan of a classical or generalized preprojective algebra is given by the Coxeter fan. It is g-convex if and only if it is of type A or B, and in this case, its g-polytope is the dual polytope of the short root polytope. Moreover we classify Brauer graph algebras which are g-convex, and describe their g-polytopes as the root polytopes of type A or C.																																	2024-11-09	PPRN:19426037		
J	Schlarmann, Christian; Singh, Naman Deep; Croce, Francesco; Hein, Matthias										Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models								Arxiv											2	2;2024-06-05;https://www.arxiv.org/abs/2402.12336v2| 1;2024-02-19;https://www.arxiv.org/abs/2402.12336v1	arXiv:2402.12336			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jun 05 2024	2024	Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. 																																	2024-07-04	PPRN:87798644		
J	Yu, Jiahao; Luo, Haozheng; Yao-Chieh, Jerry; Guo, Wenbo; Liu, Han; Xing, Xinyu				Luo, Robin/OGN-7952-2025; Yu, Jiahao/HIR-5202-2022						Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens								Arxiv											1	1;2024-05-31;https://www.arxiv.org/abs/2405.20653v1	arXiv:2405.20653			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 31 2024	2024	Along with the remarkable successes of Language language models, recent research also started to explore the security threats of LLMs, including jailbreaking attacks. Attackers carefully craft jailbreaking prompts such that a target LLM will respond to the harmful question. Existing jailbreaking attacks require either human experts or leveraging complicated algorithms to craft jailbreaking prompts. In this paper, we introduce BOOST, a simple attack that leverages only the eos tokens. We demonstrate that rather than constructing complicated jailbreaking prompts, the attacker can simply append a few eos tokens to the end of a harmful question. It will bypass the safety alignment of LLMs and lead to successful jailbreaking attacks. We further apply BOOST to four representative jailbreak methods and show that the attack success rates of these methods can be significantly enhanced by simply adding eos tokens to the prompt. To understand this simple but novel phenomenon, we conduct empirical analyses. Our analysis reveals that adding eos tokens makes the target LLM believe the input is much less harmful, and eos tokens have low attention values and do not affect LLM's understanding of the harmful questions, leading the model to actually respond to the questions. Our findings uncover how fragile an LLM is against jailbreak attacks, motivating the development of strong safety alignment approaches.																																	2024-06-19	PPRN:89125311		
J	Zhuang, Shengyao; Zhuang, Honglei; Koopman, Bevan; Zuccon, Guido				Koopman, Bevan/AAF-3038-2021						A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models								Arxiv											2	2;2024-05-30;https://www.arxiv.org/abs/2310.09497v2| 1;2023-10-14;https://www.arxiv.org/abs/2310.09497v1	arXiv:2310.09497			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 30 2024	2024	We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at https://github.com/ielab/llm-rankers.																																	2024-06-16	PPRN:85660832		
J	Li, Jun-Peng; Wang, Sai; Zhao, Zhi-Chao; Kohri, Kazunori				LI, JUNPENG/ACI-1314-2022; Kohri, Kazunori/KLE-1606-2024						Complete analysis of the background and anisotropies of scalar-induced gravitational waves: primordial non-Gaussianity fNL and gNL considered								Arxiv											2	2;2024-05-29;https://www.arxiv.org/abs/2309.07792v2| 1;2023-09-14;https://www.arxiv.org/abs/2309.07792v1	arXiv:2309.07792			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	May 29 2024	2024	Investigation of primordial non-Gaussianity holds immense importance in testing the inflation paradigm and shedding light on the physics of the early Universe. In this study, we conduct the complete analysis of scalar-induced gravitational waves (SIGWs) by incorporating the local-type non-Gaussianity fNL and gNL. We develop Feynman-like diagrammatic technique and derive semi-analytic formulas for both the energy-density fraction spectrum and the angular power spectrum. For the energy-density fraction spectrum, we analyze all the relevant Feynman-like diagrams, determining their contributions to the spectrum in an order-by-order fashion. As for the angular power spectrum, our focus lies on the initial inhomogeneities, giving rise to anisotropies in SIGWs, that arise from the coupling between shortand long-wavelength modes due to primordial non-Gaussianity. Our analysis reveals that this spectrum exhibits a typical multipole dependence, characterized by C˜ℓ ∝ [ℓ(ℓ + 1)]−1 , which plays a crucial role in distinguishing between different sources of gravitational waves. Depending on model parameters, significant anisotropies can be achieved. We also show that the degeneracies in model parameters can be broken. The findings of our study underscore the angular power spectrum as a robust probe for investigating primordial non-Gaussianity and the physics of the early Universe. Moreover, our theoretical predictions can be tested using space-borne gravitational-wave detectors and pulsar timing arrays.																																	2024-07-04	PPRN:85309633		
J	Shen, Xinyue; Wu, Yixin; Backes, Michael; Zhang, Yang				Wu, Yixin/GVT-1788-2022; Shen, Xinyue/KLZ-1606-2024						Voice Jailbreak Attacks Against GPT-4o								Arxiv											1	1;2024-05-29;https://www.arxiv.org/abs/2405.19103v1	arXiv:2405.19103			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 29 2024	2024	Recently, the concept of artificial assistants has evolved from science fiction into real-world applications. GPT-4o, the newest multimodal large language model (MLLM) across audio, vision, and text, has further blurred the line between fiction and reality by enabling more natural human-computer interactions. However, the advent of GPT-4o's voice mode may also introduce a new attack surface. In this paper, we present the first systematic measurement of jailbreak attacks against the voice mode of GPT-4o. We show that GPT-4o demonstrates good resistance to forbidden questions and text jailbreak prompts when directly transferring them to voice mode. This resistance is primarily due to GPT-4o's internal safeguards and the difficulty of adapting text jailbreak prompts to voice mode. Inspired by GPT-4o's human-like behaviors, we propose VoiceJailbreak, a novel voice jailbreak attack that humanizes GPT-4o and attempts to persuade it through fictional storytelling (setting, character, and plot). VoiceJailbreak is capable of generating simple, audible, yet effective jailbreak prompts, which significantly increases the average attack success rate (ASR) from 0.033 to 0.778 in six forbidden scenarios. We also conduct extensive experiments to explore the impacts of interaction steps, key elements of fictional writing, and different languages on VoiceJailbreak's effectiveness and further enhance the attack performance with advanced fictional writing techniques. We hope our study can assist the research community in building more secure and well-regulated MLLMs.																																	2024-08-24	PPRN:91461006		
J	Boyeau, Pierre; Angelopoulos, Anastasios N.; Yosef, Nir; Malik, Jitendra; Jordan, Michael I.				Angelopoulos, Anastasios/AAT-5355-2020						AutoEval Done Right: Using Synthetic Data for Model Evaluation								Arxiv											2	2;2024-05-28;https://www.arxiv.org/abs/2403.07008v2| 1;2024-03-09;https://www.arxiv.org/abs/2403.07008v1	arXiv:2403.07008			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 28 2024	2024	The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.																																	2024-06-13	PPRN:88119193		
J	Robertson, Brant; Johnson, Benjamin D; Tacchella, Sandro; Eisenstein, Daniel J; Hainline, Kevin; Arribas, Santiago; Baker, William M; Bunker, Andrew J; Carniani, Stefano; Carreira, Courtney; Cargile, Phillip A; Charlot, Stephane; Chevallard, Jacopo; Curti, Mirko; Curtis-Lake, Emma; D'Eugenio, Francesco; Egami, Eiichi; Hausen, Ryan; Helton, Jakon M; Jakobsen, Peter; Ji, Zhiyuan; Jones, Gareth C; Maiolino, Roberto; Maseda, Michael V; Nelson, Erica; Perez-Gonzalez, Pablo G; Puskas, David; Rieke, Marcia; Smit, Renske; Sun, Fengwu; Ubler, Hannah; Whitler, Lily; Williams, Christina C; Willmer, Christopher N A; Willott, Chris; Witstok, Joris				Jones, Gareth/AAD-7663-2022; Robertson, Brant/AAA-6124-2022; BOEKER, TORSTEN/KVC-3022-2024; Witstok, Joris/GQA-8643-2022; D'Eugenio, Francesco/H-2606-2019; Tacchella, Sandro/AAT-1602-2021; Baker, William/KUD-6412-2024; Puskás, Dávid/NZO-4934-2025; Nelson, Erica/OUI-1817-2025; Smit, Renske/MIK-8564-2025; Perez-Gonzalez, Pablo G./IVH-0781-2023						Earliest Galaxies in the JADES Origins Field: Luminosity Function and Cosmic Star-Formation Rate Density 300 Myr after the Big Bang								Arxiv											1	1;2024-05-28;https://www.arxiv.org/abs/2312.10033v2	arXiv:2312.10033			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 28 2024	2024	We characterize the earliest galaxy population in the JADES Origins Field (JOF), the deepest imaging field observed with JWST. We make use of the ancillary Hubble optical images (5 filters spanning 0.4 −0.9µm) and novel JWST images with 14 filters spanning 0.8 −5µm, including 7 mediumband filters, and reaching total exposure times of up to 46 hours per filter. We combine all our data at > 2.3µm to construct an ultradeep image, reaching as deep as ≈ 31.4 AB mag in the stack and 30.3-31.0 AB mag (5σ, r = 0.1” circular aperture) in individual filters. We measure photometric redshifts and use robust selection criteria to identify a sample of eight galaxy candidates at redshifts z = 11.5 − 15. These objects show compact half-light radii of R 1/2 ∼ 50 − 200pc, stellar masses of M⋆ ∼ 107 −108 M⊙ , and star-formation rates of SFR ∼ 0.1 −1 M⊙ yr − 1 . Our search finds no candidates at 15 < z < 20, placing upper limits at these redshifts. We develop a forward modeling approach to infer the properties of the evolving luminosity function without binning in redshift or luminosity that marginalizes over the photometric redshift uncertainty of our candidate galaxies and incorporates the impact of non-detections. We find a z = 12 luminosity function in good agreement with prior results, and that the luminosity function normalization and UV luminosity density decline by a factor of ∼ 2.5 from z = 12 to z = 14. We discuss the possible implications of our results in the context of theoretical models for evolution of the dark matter halo mass function.																																	2024-08-04	PPRN:86651776		
J	Rout, Litu; Chen, Yujia; Ruiz, Nataniel; Kumar, Abhishek; Caramanis, Constantine; Shakkottai, Sanjay; Chu, Wen-Sheng				Rout, Litu/LCD-6775-2024; yujia, chen/JLM-5613-2023; Chu, Wen-Sheng/AAF-6871-2019						RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control								Arxiv											1	1;2024-05-27;https://www.arxiv.org/abs/2405.17401v1	arXiv:2405.17401			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 27 2024	2024	We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing trainingfree approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a crossattention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.																																	2024-06-09	PPRN:89060824		
J	Monsalve, R.A.; Altamirano, C.; Bidula, V.; Bustos, R.; Bye, C.H.; Chiang, H.C.; Diaz, M.; Fernandez, B.; Guo, X.; Hendricksen, I.; Hornecker, E.; Lucero, F.; Mani, H.; McGee, F.; Mena, F.P.; Pessoa, M.; Prabhakar, G.; Restrepo, O.; Sievers, J.L.; Thyagarajan, N.				Bustos, Ricardo/H-3286-2014; Pessoa-Filho, Marco/A-8880-2013; Mena, Fausto/H-8193-2013; Restrepo Gaitán, Oscar/IXN-5185-2023						Mapper of the IGM spin temperature: instrument overview								Arxiv											2	2;2024-05-24;https://www.arxiv.org/abs/2309.02996v2| 1;2023-09-06;https://www.arxiv.org/abs/2309.02996v1	arXiv:2309.02996			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 24 2024	2024	The observation of the global 21 cm signal produced by neutral hydrogen gas in the intergalactic medium (IGM) during the Dark Ages, Cosmic Dawn, and Epoch of Reionization requires measurements with extremely well-calibrated wideband radiometers. We describe the design and characterization of the Mapper of the IGM Spin Temperature (MIST), which is a new ground-based, single-antenna, global 21 cm experiment. The design of MIST was guided by the objectives of avoiding systematics from an antenna ground plane and cables around the antenna, as well as maximizing the instrument's on-sky efficiency and portability for operations at remote sites. We have built two MIST instruments, which observe in the range 25-105 MHz. For the 21 cm signal, this frequency range approximately corresponds to redshifts 55.5 > z > 12.5, encompassing the Dark Ages and Cosmic Dawn. The MIST antenna is a horizontal blade dipole of 2.42 m in length, 60 cm in width, and 52 cm in height above the ground. This antenna operates without a metal ground plane. The instruments run on 12 V batteries and have a maximum power consumption of 17 W. The batteries and electronics are contained in a single receiver box located under the antenna. We present the characterization of the instruments using electromagnetic simulations and lab measurements. We also show sample sky measurements from recent observations at remote sites in California, Nevada, and the Canadian High Arctic. These measurements indicate that the instruments perform as expected. Detailed analyses of the sky measurements are left for future work.																																	2024-07-02	PPRN:84846043		
J	Huang, Wei; Qin, Haotong; Liu, Yangdong; Li, Yawei; Liu, Xianglong; Benini, Luca; Magno, Michele; Qi, Xiaojuan				Liu, Xianglong/NTQ-2427-2025; Qin, Haotong/AGP-1834-2022; Qi, Xiaojuan/MVV-7776-2025; Bian, Sizhen/AAI-8450-2021; LI, YAWEI/Q-2207-2017						SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14917v1	arXiv:2405.14917			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 23 2024	2024	Large language models (LLMs) achieve remarkable performance in natural language understanding but require substantial computation and memory resources. Post-training quantization (PTQ) is a powerful compression technique extensively investigated in LLMs. However, existing PTQ methods are still not ideal in terms of accuracy and efficiency, especially with below 4 bit-widths. Standard PTQ methods using group-wise quantization suffer difficulties in quantizing LLMs accurately to such low-bit, but advanced methods remaining high-precision weights element-wisely are hard to realize their theoretical hardware efficiency. This paper presents a Salience-Driven Mixed-Precision Quantization scheme for LLMs, namely SliM-LLM. The scheme exploits the salience distribution of weights to determine optimal bit-width and quantizers for accurate LLM quantization, while aligning bit-width partition to groups for compact memory usage and fast integer inference. Specifically, the proposed SliM-LLM mainly relies on two novel techniques: (1) Salience-Determined Bit Allocation utilizes the clustering characteristics of salience distribution to allocate the bit-widths of each group, increasing the accuracy of quantized LLMs and maintaining the inference efficiency; (2) Salience-Weighted Quantizer Calibration optimizes the parameters of the quantizer by considering the element-wise salience within the group, balancing the maintenance of salient information and minimization of errors. Comprehensive experiments show that SliM-LLM significantly improves the accuracy of LLMs at ultra-low bits, e.g., 2-bit LLaMA-7B achieves a 5.5-times memory-saving than original model on NVIDIA A800 GPUs, and 48% decrease of perplexity compared to the state-of-the-art gradient-free PTQ method. Moreover, SliM-LLM+, which is integrated from the extension of SliM-LLM with gradient-based quantizers, further reduces perplexity by 35.1%.																																	2024-06-08	PPRN:89010054		
J	Li, Wangyue; Li, Liangzhi; Xiang, Tong; Liu, Xiao; Deng, Wei; Garcia, Noa				Li, Liangzhi/D-1524-2018						Can multiple-choice questions really be useful in detecting the abilities of LLMs?								Arxiv											3	3;2024-05-23;https://www.arxiv.org/abs/2403.17752v3| 2;2024-03-28;https://www.arxiv.org/abs/2403.17752v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17752v1	arXiv:2403.17752			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM’s capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ’s efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs’ output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs .																																	2024-06-05	PPRN:88295807		
J	Mazet, Laurent										STABLE MINIMAL HYPERSURFACES IN R6								Arxiv											1	1;2024-05-23;https://www.arxiv.org/abs/2405.14676v1	arXiv:2405.14676			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 23 2024	2024	Following the strategy developed by Chodosh, Li, Minter and Stryker, and using the volume estimate of Antonelli and Xu, we prove that, in R6 , a complete, two-sided, stable minimal hypersurfaces is flat.																																	2024-06-05	PPRN:88989306		
J	Zhang, Shun; Chen, Zhenfang; Chen, Sunli; Shen, Yikang; Sun, Zhiqing; Gan, Chuang				Chen, Zhenfang/HTS-8543-2023						Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble								Arxiv											2	2;2024-05-21;https://www.arxiv.org/abs/2401.16635v2| 1;2024-01-30;https://www.arxiv.org/abs/2401.16635v1	arXiv:2401.16635			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 21 2024	2024	Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of- n and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.																																	2024-06-05	PPRN:87420492		
J	Lin, Zhiqiu; Chen, Xinyue; Pathak, Deepak; Zhang, Pengchuan; Ramanan, Deva				chen, xinyue/HHR-9308-2022						Revisiting the Role of Language Priors in Vision-Language Models								Arxiv											4	4;2024-05-15;https://www.arxiv.org/abs/2306.01879v4| 3;2024-02-01;https://www.arxiv.org/abs/2306.01879v3| 2;2023-10-05;https://www.arxiv.org/abs/2306.01879v2| 1;2023-06-02;https://www.arxiv.org/abs/2306.01879v1	arXiv:2306.01879			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 15 2024	2024	Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, without any fine-tuning. We study generative VLMs that are trained for next-word generation given an image. We explore their zeroshot performance on the illustrative task of imagetext retrieval across nine popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the Visual Generative Pre-Training Score (VisualGPTScore). While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions. In fact, we demonstrate that even a “blind” language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.																																	2024-06-01	PPRN:72853865		
J	Lu, Ning; Liu, Shengcai; He, Rui; Ong, Yew-Soon; Wang, Qi; Tang, Ke				Liu, Shengcai/ABA-9832-2020; Wang, Qi/AAM-6775-2020						Large Language Models can be Guided to Evade AI-Generated Text Detection								Arxiv											3	3;2024-05-15;https://www.arxiv.org/abs/2305.10847v6| 2;2023-12-14;https://www.arxiv.org/abs/2305.10847v5| 1;2023-05-18;https://www.arxiv.org/abs/2305.10847v1	arXiv:2305.10847			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 15 2024	2024	Large language models (LLMs) have shown remarkable performance in various tasks and have been extensively utilized by the public. However, the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods. In this study, we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of these detectors. We propose a novel S ubstitution-based I n-Context C ontext example O ptimization method (SICO) to automatically construct prompts for evading the detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has been constructed, it can be universally used against a wide range of detectors. Extensive experiments across three real-world tasks demonstrate that SICO significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation show that the SICO-generated text achieves human-level readability and task completion rates, while preserving high imperceptibility. Finally, we propose an ensemble approach to enhance the robustness of detectors against SICO attack. 1																																	2024-06-01	PPRN:70533332		
J	Bellemare-Pepin, Antoine; Lespinasse, Francois; Tholke, Philipp; Harel, Yann; Mathewson, Kory; Olson, Jay A.; Bengio, Yoshua; Jerbi, Karim				HAREL, Yann/JUF-6165-2023; Jerbi, Karim/NDT-2402-2025						Divergent Creativity in Humans and Large Language Models								Arxiv											1	1;2024-05-13;https://www.arxiv.org/abs/2405.13012v1	arXiv:2405.13012			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	May 13 2024	2024	The recent surge in the capabilities of Large Language Models (LLMs) has led to claims that they are approaching a level of creativity akin to human capabilities. This idea has sparked a blend of excitement and apprehension. However, a critical piece that has been missing in this discourse is a systematic evaluation of LLM creativity, particularly in comparison to human divergent thinking. To bridge this gap, we leverage recent advances in creativity science to build a framework for in-depth analysis of divergent creativity in both state-of-the-art LLMs and a substantial dataset of 100,000 humans. We found evidence suggesting that LLMs can indeed surpass human capabilities in specific creative tasks such as divergent association and creative writing. Our quantitative benchmarking framework opens up new paths for the development of more creative LLMs, but it also encourages more granular inquiries into the distinctive elements that constitute human inventive thought processes, compared to those that can be artificially generated.																																	2024-06-04	PPRN:88982748		
J	Gopalkrishnan, Akshay; Greer, Ross; Trivedi, Mohan				Gopalkrishnan, Akshay/LDE-5435-2024; Greer, Ross/AAD-1234-2022						Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving								Arxiv											2	2;2024-05-09;https://www.arxiv.org/abs/2403.19838v2| 1;2024-03-28;https://www.arxiv.org/abs/2403.19838v1	arXiv:2403.19838			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 09 2024	2024	Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher CIDEr and ROUGE-L scores than the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits the ability to extract relevant information from traffic views related to prompts and can answer questions for various autonomous driving subtasks. 																																	2024-05-27	PPRN:88342508		
J	Wang, Hongyi; Polo, Felipe Maia; Sun, Yuekai; Kundu, Souvik; Xing, Eric P; Yurochkin, Mikhail										Fusing Models with Complementary Expertise								Arxiv											2	2;2024-05-09;https://www.arxiv.org/abs/2310.01542v2| 1;2023-10-02;https://www.arxiv.org/abs/2310.01542v1	arXiv:2310.01542			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 09 2024	2024	Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the “frugal” setting where it is desired to reduce the number of expert model evaluations at test time. Our implementation is publicly available at https: //github.com/hwang595/FoE-ICLR2024. .																																	2024-05-28	PPRN:85378792		
J	Kohler, Jonas; Pumarola, Albert; Schonfeld, Edgar; Sanakoyeu, Artsiom; Sumbaly, Roshan; Vajda, Peter; Thabet, Ali										Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation								Arxiv											1	1;2024-05-08;https://www.arxiv.org/abs/2405.05224v1	arXiv:2405.05224			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 08 2024	2024	Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation.																																	2024-06-04	PPRN:88976593		
J	Golovneva, Olga; Allen-Zhu, Zeyuan; Weston, Jason; Sukhbaatar, Sainbayar										Reverse Training to Nurse the Reversal Curse								Arxiv											3	3;2024-05-07;https://www.arxiv.org/abs/2403.13799v3| 2;2024-05-01;https://www.arxiv.org/abs/2403.13799v2| 1;2024-03-20;https://www.arxiv.org/abs/2403.13799v1	arXiv:2403.13799			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 07 2024	2024	Large language models (LLMs) have a surprising failure: when trained on "A has a feature B", they do not generalize to "B is a feature of A", which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.																																	2024-06-04	PPRN:88244866		
J	Birrer, S.; Millon, M.; Sluse, D.; Shajib, A.J.; Courbin, F.; Erickson, S; Koopmans, L.V.E.; Suyu, S.H.; Treu, T.				Millon, Martin/AAB-1940-2021; Treu, Tommaso/KYP-7127-2024; Shajib, Anowar/AAD-2510-2020; Sluse, Dominique/ABF-3723-2020						Time-Delay Cosmography: Measuring the Hubble Constant and other cosmological parameters with strong gravitational lensing								Arxiv											2	2;2024-05-06;https://www.arxiv.org/abs/2210.10833v4| 1;2022-10-19;https://www.arxiv.org/abs/2210.10833v1	arXiv:2210.10833			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	Multiply lensed images of a same source experience a relative time delay in the arrival of photons due to the path length difference and the different gravitational potentials the photons travel through. This effect can be used to measure absolute distances and the Hubble constant ( H 0 ) and is known as time-delay cosmography. The method is independent of the local distance ladder and early-universe physics and provides a precise and competitive measurement of H0 . With upcoming observatories, time-delay cosmography can provide a 1% precision measurement of H0 and can decisively shed light on the current reported ’Hubble tension’. This manuscript details the general methodology developed over the past decades in time-delay cosmography, discusses recent advances and results, and, foremost, provides a foundation and outlook for the next decade in providing accurate and ever more precise measurements with increased sample size and improved observational techniques.																																	2024-05-24	PPRN:22123205		
J	Chen, Yu-Hsueh; Grover, Tarun				Grover, Tarun/AAX-6598-2021						Symmetry-enforced many-body separability transitions								Arxiv											1	1;2024-05-06;https://www.arxiv.org/abs/2310.07286v2	arXiv:2310.07286			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	May 06 2024	2024	We study quantum many-body mixed states with a symmetry from the perspective of separability, i.e., whether a mixed state can be expressed as an ensemble of short-range entangled (SRE) symmetric pure states. We provide evidence for 'symmetry-enforced separability transitions' in a variety of states, where in one regime the mixed state is expressible as a convex sum of symmetric SRE pure states, while in the other regime, such a representation is not feasible. We first discuss Gibbs state of Hamiltonians that exhibit spontaneous breaking of a discrete symmetry, and argue that the associated thermal phase transition can be thought of as a symmetry-enforced separability transition. Next, we study cluster states in various dimensions subjected to local decoherence, and identify several distinct mixed-state phases and associated separability phase transitions, which also provides an alternate perspective on recently discussed 'average SPT order'. We also study decohered p+ip superconductors, and find that if the decoherence breaks the fermion parity explicitly, then the resulting mixed state can be expressed as a convex sum of non-chiral states, while a fermion-parity preserving decoherence results in a phase transition at a non-zero threshold that corresponds to spontaneous breaking of fermion parity. Finally, we briefly discuss systems that satisfy NLTS (no low-energy trivial state) property, such as the recently discovered good LDPC codes, and argue that the Gibbs state of such systems exhibits a temperature-tuned separability transition.																																	2024-05-24	PPRN:88790367		
J	Wang, Qianning; Hu, He; Zhou, Yucheng										MemoryMamba: Memory-Augmented State Space Model for Defect Recognition								Arxiv											1	1;2024-05-06;https://www.arxiv.org/abs/2405.03673v1	arXiv:2405.03673			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	May 06 2024	2024	As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows. Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings. These models especially struggle in scenarios involving limited or imbalanced defect data. In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models. MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training. Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection. In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities. The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios.																																	2024-05-28	PPRN:88839902		
J	Liu, Weiyang; Qiu, Zeju; Feng, Yao; Xiu, Yuliang; Xue, Yuxuan; Yu, Longhui; Feng, Haiwen; Liu, Zhen; Heo, Juyeon; Peng, Songyou; Wen, Yandong; Black, Michael J.; Weller, Adrian; Schoelkopf, Bernhard				lh, yu/GVT-4141-2022; Wen, Yandong/MTF-6285-2025; XUE, YUXUAN/HLG-9947-2023; Schölkopf, Bernhard/A-7570-2013; Xiu, Yuliang/AAT-8542-2021						Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization								Arxiv											2	2;2024-04-28;https://www.arxiv.org/abs/2311.06243v2| 1;2023-11-10;https://www.arxiv.org/abs/2311.06243v1	arXiv:2311.06243			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 28 2024	2024	Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm - Orthogonal Finetuning (OFT) - for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language.																																	2024-06-22	PPRN:86125137		
J	Pfoertner, Marvin; Steinwart, Ingo; Hennig, Philipp; Wenger, Jonathan				Steinwart, Ingo/KBD-2936-2024						Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers								Arxiv											4	4;2024-04-28;https://www.arxiv.org/abs/2212.12474v6| 3;2023-12-02;https://www.arxiv.org/abs/2212.12474v5| 2;2023-10-28;https://www.arxiv.org/abs/2212.12474v4| 1;2022-12-23;https://www.arxiv.org/abs/2212.12474v1	arXiv:2212.12474			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 28 2024	2024	Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bounded linear operator. Crucially, this probabilistic viewpoint allows to (1) quantify the inherent discretization error; (2) propagate uncertainty about the model parameters to the solution; and (3) condition on noisy measurements. Demonstrating the strength of this formulation, we prove that it strictly generalizes methods of weighted residuals, a central class of PDE solvers including collocation, finite volume, pseudospectral, and (generalized) Galerkin methods such as finite element and spectral methods. This class can thus be directly equipped with a structured error estimate. In summary, our results enable the seamless integration of mechanistic models as modular building blocks into probabilistic models by blurring the boundaries between numerical analysis and Bayesian inference.																																	2024-05-05	PPRN:35872088		
J	Liu, Hao; Shen, Yi; Zhou, Wenjing; Zou, Yuelin; Zhou, Chang; He, Shuyao				Liu, Hao/MVU-2719-2025						Adaptive speed planning for Unmanned Vehicle Based on Deep Reinforcement Learning								Arxiv											1	1;2024-04-26;https://www.arxiv.org/abs/2404.17379v1	arXiv:2404.17379			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 26 2024	2024	In order to solve the problem of frequent deceleration of unmanned vehicles when approaching obstacles, this article uses a Deep Q -Network (DQN) and its extension, the Double Deep Q -Network (DDQN), to develop a local navigation system that adapts to obstacles while maintaining optimal speed planning. By integrating improved reward functions and obstacle angle determination methods, the system demonstrates significant enhancements in maneuvering capabilities without frequent decelerations. Experiments conducted in simulated environments with varying obstacle densities confirm the effectiveness of the proposed method in achieving more stable and efficient path planning.																																	2024-05-06	PPRN:88664373		
J	Siegel, Kyler										Higher symplectic capacities								Arxiv											2	2;2024-04-23;https://www.arxiv.org/abs/1902.01490v4| 1;2020-02-03;https://www.arxiv.org/abs/1902.01490v3	arXiv:1902.01490			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 23 2024	2024	We construct new families of symplectic capacities indexed by certain symmetric polynomials, defined using rational symplectic field theory. In particular, we introduce a sequence of capacities based on an L∞ structure on linearized contact homology and rational curve counts with local tangency constraints. We prove various structural properties of these capacities and give some preliminary computations which show that they give state of the art symplectic embedding obstructions in basic examples.																																	2024-05-02	PPRN:13086550		
J	Merboldt, Marco; Schueler, Michael; Schmitt, David; Bange, Jan Philipp; Bennecke, Wiebke; Gadge, Karun; Pierz, Klaus; Schumacher, Hans Werner; Momeni, Davood; Steil, Daniel; Manmana, Salvatore R.; Sentef, Michael; Reutzel, Marcel; Mathias, Stefan				Schumacher, Hans/C-7403-2009; Mathias, Stefan/I-4679-2012; Manmana, Salvatore/C-9822-2011; Steil, Daniel/J-8981-2015; Bennecke, Wiebke/KIB-8096-2024; Sentef, Michael/L-5717-2013; Momeni, Davood/AAP-5696-2020; Reutzel, Marcel/AAK-7322-2021						Observation of Floquet states in graphene								Arxiv											1	1;2024-04-19;https://www.arxiv.org/abs/2404.12791v1	arXiv:2404.12791			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 19 2024	2024	Recent advances in the field of condensed-matter physics have unlocked the potential to realize and control emergent material phases that do not exist in thermal equilibrium. One of the most promising concepts in this regard is Floquet engineering, the coherent dressing of matter via time-periodic perturbations. However, the broad applicability of Floquet engineering to quantum materials is still unclear. For the paradigmatic case of monolayer graphene, the theoretically predicted Floquet-induced effects, despite a seminal report of the light-induced anomalous Hall effect, have been put into question. Here, we overcome this problem by using electronic structure measurements to provide direct experimental evidence of Floquet engineering in graphene. We report light-matter-dressed Dirac bands by measuring the contribution of Floquet sidebands, Volkov sidebands, and their quantum path interference to graphene's photoemission spectral function. Our results finally demonstrate that Floquet engineering in graphene is possible, paving the way for the experimental realization of the many theoretical proposals on Floquet-engineered band structures and topological phases.																																	2024-04-29	PPRN:88590013		
J	Chang, Jonathan D.; Zhan, Wenhao; Oertell, Owen; Brantley, Kiante; Misra, Dipendra; Lee, Jason D.; Sun, Wen				Zhan, Wenhao/MCX-3749-2025						Dataset Reset Policy Optimization for RLHF								Arxiv											1	1;2024-04-16;https://www.arxiv.org/abs/2404.08495v3	arXiv:2404.08495			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 16 2024	2024	Reinforcement Learning (RL) from Human Preference -based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR -PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR -PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR -PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win -rate.																																	2024-04-26	PPRN:88544732		
J	Maiolino, Roberto; Ubler, Hannah; Perna, Michele; Scholtz, Jan; D'Eugenio, Francesco; Witten, Callum; Laporte, Nicolas; Witstok, Joris; Carniani, Stefano; Tacchella, Sandro; Baker, William M; Arribas, Santiago; Nakajima, Kimihiko; Eisenstein, Daniel J; Bunker, Andrew J; Charlot, Stephane; Cresci, Giovanni; Curti, Mirko; Curtis-Lake, Emma; de Graaff, Anna; Egami, Eiichi; Ji, Zhiyuan; Johnson, Benjamin D.; Kumari, Nimisha; Looser, Tobias J.; Maseda, Michael; Nelson, Erica; Robertson, Brant; Del Pino, Bruno Rodriguez; Sandles, Lester; Simmonds, Charlotte; Smit, Renske; Sun, Fengwu; Venturi, Giacomo; Williams, Christina C; Willmer, Christopher N. A				Smit, Renske/MIK-8564-2025; Tacchella, Sandro/AAT-1602-2021; Baker, William/KUD-6412-2024; Nakajima, Kimihiko/JRW-3889-2023; Witstok, Joris/GQA-8643-2022; Venturi, Giacomo/AAB-4352-2021; D'Eugenio, Francesco/H-2606-2019; Kumari, Nimisha/AFS-1631-2022; Arribas, Santiago/F-9277-2015; Nelson, Erica/OUI-1817-2025; Perna, Michele/GNP-0399-2022; Robertson, Brant/AAA-6124-2022; Del Pino, Bruno/C-3326-2017						JWST-JADES. Possible Population III signatures at z=10.6 in the halo of GN-z11								Arxiv											2	2;2024-04-16;https://www.arxiv.org/abs/2306.00953v3| 1;2023-06-03;https://www.arxiv.org/abs/2306.00953v2	arXiv:2306.00953			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 16 2024	2024	Apr Finding the first generation of stars formed out of pristine gas in the early Universe, known as Population III (PopIII) stars, is one of the most important goals of modern astrophysics. Recent models have suggested that PopIII stars may form in pockets of pristine gas in the halo of more evolved galaxies. We present NIRSpec integral field spectroscopy and micro-shutter array spectroscopic observations of the region around GNz11, an exceptionally luminous galaxy at z = 10.6, that reveal a greater than 5σ detection of a feature consistent with being HeIIλ1640 emission at the redshift of GN-z11. The very high equivalent width of the putative HeII emission in this clump (log (EWrest(HeII)/Å) = 1.79 −0.25+0.15) and a lack of metal lines can be explained in terms of photoionisation by PopIII stars, while photoionisation by PopII stars is inconsistent with the data. The high equivalent width would also indicate that the putative PopIII stars likely have an initial mass function with an upper cutoff reaching at least 500 M⊙. The PopIII bolometric luminosity inferred from the HeII line would be ~ 7 x 109 L⊙, which would imply a total stellar mass formed in the burst of ~ 2 x 105 M⊙. We find that photoionisation by the active galactic nucleus (AGN) in GN-z11 cannot account for the HeII luminosity observed in the clump but can potentially be responsible for an additional HeII emission observed closer to GN-z11. We also consider the possibility of in situ photoionisation by an accreting direct collapse black hole hosted by the HeII clump. We find that this scenario is less favoured, but it remains a possible alternative interpretation. We also report the detection of a Lyα halo stemming out of GN-z11 and extending out to ~ 2 kpc as well as resolved funnel-shaped CIII emission likely tracing the ionisation cone of the AGN.																																	2024-05-04	PPRN:72811185		
J	Hartsock, Iryna; Rasool, Ghulam				Rasool, Ghulam/T-7960-2019						Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review								Arxiv											2	2;2024-04-15;https://www.arxiv.org/abs/2403.02469v2| 1;2024-03-04;https://www.arxiv.org/abs/2403.02469v1	arXiv:2403.02469			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 15 2024	2024	Medical vision-language models (VLMs) combine computer vision (CV) and natural language processing (NLP) to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on models designed for medical report generation and visual question answering (VQA). We provide background on NLP and CV, explaining how techniques from both fields are integrated into VLMs to enable learning from multimodal data. Key areas we address include the exploration of medical visionlanguage datasets, in-depth analyses of architectures and pre-training strategies employed in recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs’ performance in medical report generation and VQA. We also highlight current challenges and propose future directions, including enhancing clinical validity and addressing patient privacy concerns. Overall, our review summarizes recent progress in developing VLMs to harness multimodal medical data for improved healthcare applications.																																	2024-05-03	PPRN:88033020		
J	Nasir, Muhammad U.; Earle, Sam; Cleghorn, Christopher; James, Steven; Togelius, Julian				Cleghorn, Christopher/N-2887-2017						LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization								Arxiv											6	6;2024-04-12;https://www.arxiv.org/abs/2306.01102v8| 5;2024-04-10;https://www.arxiv.org/abs/2306.01102v7| 4;2023-10-04;https://www.arxiv.org/abs/2306.01102v6| 3;2023-09-17;https://www.arxiv.org/abs/2306.01102v4| 2;2023-09-09;https://www.arxiv.org/abs/2306.01102v3| 1;2023-06-01;https://www.arxiv.org/abs/2306.01102v1	arXiv:2306.01102			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 12 2024	2024	Large language models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. Here, we propose using the coding abilities of LLMs to introduce meaningful variations to code defining neural networks. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and high-performing networks. We test LLMatic on the CIFAR-10 and NAS-bench-201 benchmarks, demonstrating that it can produce competitive networks while evaluating just 2, 000 candidates, even without prior knowledge of the benchmark domain or exposure to any previous top-performing models for the benchmark. 																																	2024-04-27	PPRN:72842377		
J	Cheng, Ho Kei; Oh, Seoung Wug; Price, Brian; Lee, Joon-Young; Schwing, Alexander				Cheng, Rex/GWZ-5023-2022						Putting the Object Back into Video Object Segmentation								Arxiv											2	2;2024-04-11;https://www.arxiv.org/abs/2310.12982v2| 1;2023-10-19;https://www.arxiv.org/abs/2310.12982v1	arXiv:2310.12982			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Apr 11 2024	2024	We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries. Via those, it interacts with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challenging MOSE dataset, Cutie improves by 8.7 J&F over XMem with a similar running time and improves by 4.2 J&F over DeAOT while being three times faster.																																	2024-04-26	PPRN:85722845		
J	Yan, Xu; Wang, Weimin; Xiao, Mingxuan; Li, Yufeng; Gao, Min				FENG, YING/KWU-9364-2024						Survival Prediction Across Diverse Cancer Types Using Neural Networks								Arxiv											1	1;2024-04-11;https://www.arxiv.org/abs/2404.08713v1	arXiv:2404.08713			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 11 2024	2024	Gastric cancer and Colon adenocarcinoma represent widespread and challenging malignancies with high mortality rates and complex treatment landscapes. In response to the critical need for accurate prognosis in cancer patients, the medical community has embraced the 5 -year survival rate as a vital metric for estimating patient outcomes. This study introduces a pioneering approach to enhance survival prediction models for gastric and Colon adenocarcinoma patients. Leveraging advanced image analysis techniques, we sliced whole slide images (WSI) of these cancers, extracting comprehensive features to capture nuanced tumor characteristics. Subsequently, we constructed patient -level graphs, encapsulating intricate spatial relationships within tumor tissues. These graphs served as inputs for a sophisticated 4 -layer graph convolutional neural network (GCN), designed to exploit the inherent connectivity of the data for comprehensive analysis and prediction. By integrating patients’ total survival time and survival status, we computed C -index values for gastric cancer and Colon adenocarcinoma, yielding 0.57 and 0.64, respectively. Significantly surpassing previous convolutional neural network models, these results underscore the efficacy of our approach in accurately predicting patient survival outcomes. This research holds profound implications for both the medical and AI communities, offering insights into cancer biology and progression while advancing personalized treatment strategies. Ultimately, our study represents a significant stride in leveraging AI -driven methodologies to revolutionize cancer prognosis and improve patient outcomes on a global scale.																																	2024-04-25	PPRN:88529322		
J	Wu, Renkai; Liu, Yinghao; Liang, Pengchen; Chang, Qing				Chang, Qing/OIT-3014-2025						UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation								Arxiv											2	2;2024-04-09;https://www.arxiv.org/abs/2403.20035v2| 1;2024-03-29;https://www.arxiv.org/abs/2403.20035v1	arXiv:2403.20035			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 09 2024	2024	Traditionally for improving the segmentation performance of models, most approaches prefer to use adding more complex modules. And this is not suitable for the medical field, especially for mobile medical devices, where computationally loaded models are not suitable for real clinical environments due to computational resource constraints. Recently, state-space models (SSMs), represented by Mamba, have become a strong competitor to traditional CNNs and Transformers. In this paper, we deeply explore the key elements of parameter influence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet) based on this. Specifically, we propose a method for processing features in parallel Vision Mamba, named PVM Layer, which achieves excellent performance with the lowest computational load while keeping the overall number of processing channels constant. We conducted comparisons and ablation experiments with several state-of-the-art lightweight models on three skin lesion public datasets and demonstrated that the UltraLight VM-UNet exhibits the same strong performance competitiveness with parameters of only 0.049M and GFLOPs of 0.060. In addition, this study deeply explores the key elements of parameter influence in Mamba, which will lay a theoretical foundation for Mamba to possibly become a new mainstream module for lightweighting in the future. 																																	2024-04-22	PPRN:88346316		
J	Jiao, Rui; Huang, Wenbing; Liu, Yu; Zhao, Deli; Liu, Yang				Huang, Wenbing/AHB-1846-2022; Jiao, Rui/L-6955-2019						Space Group Constrained Crystal Generation								Arxiv											2	2;2024-04-08;https://www.arxiv.org/abs/2402.03992v2| 1;2024-02-06;https://www.arxiv.org/abs/2402.03992v1	arXiv:2402.03992			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 08 2024	2024	Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates. Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP (Jiao et al., 2023) by further taking space group constraint into account. Experiments on several popular datasets verify the benefit of the involvement of the space group constraint, and show that our DiffCSP++ achieves promising performance on crystal structure prediction, ab initio crystal generation and controllable generation with customized space groups.																																	2024-04-21	PPRN:87533572		
J	Yu, Qiying; Sun, Quan; Zhang, Xiaosong; Cui, Yufeng; Zhang, Fan; Cao, Yue; Wang, Xinlong; Liu, Jingjing				Wang, Xinlong/AFI-8800-2022; zhang, charles/HHS-7805-2022						CapsFusion: Rethinking Image-Text Data at Scale								Arxiv											3	3;2024-04-05;https://www.arxiv.org/abs/2310.20550v3| 2;2023-11-02;https://www.arxiv.org/abs/2310.20550v2| 1;2023-10-31;https://www.arxiv.org/abs/2310.20550v1	arXiv:2310.20550			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 05 2024	2024	Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zeroshot manner. Large-scale web -based image -text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly -simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher -quality and more scalable multimodal pretraining data, we propose CAPSFUSION, an advanced framework that leverages large language models to consolidate and refine information from both web -based image -text pairs and synthetic captions. Extensive experiments show that CAPSFUSION captions exhibit remarkable all-round superiority over existing captions in terms of model performance (e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample efficiency (requiring 11-16 times less computation than baselines), world knowledge depth, and scalability. These effectiveness, efficiency and scalability advantages position CAPSFUSION as a promising candidate for future scaling of LMM training.																																	2024-04-20	PPRN:85912804		
J	Lin, Ji; Chen, Wei-Ming; Cai, Han; Gan, Chuang; Han, Song				Chen, Wei-Ming/ACA-7706-2022; Cai, Han/NKP-2131-2025						MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning								Arxiv											1	1;2024-04-03;https://www.arxiv.org/abs/2110.15352v2	arXiv:2110.15352			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 03 2024	2024	Tiny deep learning on microcontroller units (MCUs) is challenging due to the limited memory size. We find that the memory bottleneck is due to the imbalanced memory distribution in convolutional neural network (CNN) designs: the first several blocks have an order of magnitude larger memory usage than the rest of the network. To alleviate this issue, we propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. However, naive implementation brings overlapping patches and computation overhead. We further propose network redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. Manually redistributing the receptive field is difficult. We automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling, leading to MCUNetV2. Patch-based inference effectively reduces the peak memory usage of existing networks by 4-8x. Co-designed with neural networks, MCUNetV2 sets a record ImageNet accuracy on MCU (71.8%), and achieves >90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2 also unblocks object detection on tiny devices, achieving 16.9% higher mAP on Pascal VOC compared to the state-of-the-art result. Our study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification. Avideo demo can be found here.																																	2024-04-18	PPRN:88389338		
J	Lin, Zheng; Chen, Zhe; Fang, Zihan; Chen, Xianhao; Wang, Xiong; Gao, Yue				Gao, Yue/AAJ-9469-2020; Chen, Zhe/JNR-1848-2023; CHEN, XIANHAO/AAX-6311-2021						FedSN: A Novel Federated Learning Framework over LEO Satellite Networks								Arxiv											4	4;2024-04-02;https://www.arxiv.org/abs/2311.01483v4| 3;2024-03-27;https://www.arxiv.org/abs/2311.01483v3| 2;2023-11-22;https://www.arxiv.org/abs/2311.01483v2| 1;2023-11-02;https://www.arxiv.org/abs/2311.01483v1	arXiv:2311.01483			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 02 2024	2024	Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, and fully explore data diversity on LEO satellites. Specifically, we first present a novel sub-structure scheme to enable heterogeneous local model training considering different computing, memory, and communication constraints on LEO satellites. Additionally, we propose a pseudo-synchronous model aggregation strategy to dynamically schedule model aggregation for compensating model staleness. To further demonstrate the effectiveness of the FedSN, we evaluate it using space modulation recognition and remote sensing image classification tasks by leveraging the data from real-world satellite networks. Extensive experimental results demonstrate that FedSN framework achieves higher accuracy, lower computing, and communication overhead than the state-of-the-art benchmarks and the effectiveness of each components in FedSN.																																	2024-04-18	PPRN:86035442		
J	Chen, Yihang; Wu, Qianyi; Cai, Jianfei; Harandi, Mehrtash; Lin, Weiyao				Wu, Qianyi/JEO-6949-2023; lin, yuxi/HKF-6212-2023; Harandi, Mehrtash/D-6586-2018						HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression								Arxiv											3	3;2024-07-12;https://www.arxiv.org/abs/2403.14530v3| 2;2024-03-21;https://www.arxiv.org/abs/2403.14530v1| 1;2024-04-01;	arXiv:2403.14530			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Apr 01 2024	2024	3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over 75× compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over 11×  size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here.																																	2024-11-09	PPRN:88261286		
J	Hatamizadeh, Ali; Heinrich, Greg; Yin, Hongxu; Tao, Andrew; Alvarez, Jose M.; Kautz, Jan; Molchanov, Pavlo				Yin, Hongxu/AAZ-3328-2020; Serrano, Jose/J-6865-2016						FASTERVIT: FAST VISION TRANSFORMERS WITH HIERARCHICAL ATTENTION								Arxiv											2	2;2024-04-01;https://www.arxiv.org/abs/2306.06189v2| 1;2023-06-09;https://www.arxiv.org/abs/2306.06189v1	arXiv:2306.06189			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Apr 01 2024	2024	We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution.																																	2024-04-18	PPRN:73298644		
J	Choudhary, Nurendra; Reddy, Chandan K.				Reddy, Challa Kishore/HSE-5935-2023; Choudhary, Nurendra/ABG-8152-2020						Complex Logical Reasoning over Knowledge Graphs using Large Language Models								Arxiv											2	2;2024-03-31;https://www.arxiv.org/abs/2305.01157v3| 1;2023-05-02;https://www.arxiv.org/abs/2305.01157v1	arXiv:2305.01157			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Mar 31 2024	2024	Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs. Our work presents a new direction for addressing the challenges of complex KG reasoning and paves the way for future research in this area.																																	2024-04-18	PPRN:66782033		
J	Singh, Jaisidh; Shrivastava, Ishaan; Vatsa, Mayank; Singh, Richa; Bharati, Aparna				Vatsa, Mayank/I-5050-2013; Vatsa, Mayank/AAR-7199-2020						Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations								Arxiv											2	2;2024-03-29;https://www.arxiv.org/abs/2403.20312v1| 1;2024-03-29;https://www.arxiv.org/abs/2403.20312v1	arXiv:2403.20312			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 29 2024	2024	Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word "not" in a given prompt. To enable evaluation of VLMs on fluent prompts with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions. Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations. This training paradigm improves CoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for zero-shot image classification across 8 datasets. Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text. Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale foundation models with significantly reduced computational cost, promoting efficiency and accessibility.																																	2025-08-07	PPRN:88342967		
J	Liu, Jiaming; Yang, Senqiao; Jia, Peidong; Zhang, Renrui; Lu, Ming; Guo, Yandong; Xue, Wei; Zhang, Shanghang				Zhang, Zhuosheng/AAF-4919-2020; liu, jiaming/KVA-6603-2024; Zhang, Qian/LKK-3896-2024						ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation								Arxiv											3	3;2024-03-27;https://www.arxiv.org/abs/2306.04344v3| 2;2023-09-30;https://www.arxiv.org/abs/2306.04344v2| 1;2023-06-07;https://www.arxiv.org/abs/2306.04344v1	arXiv:2306.04344			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 27 2024	2024	Since real-world machine systems are running in non-stationary environments, Continual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained model to continually changing target domains. Recently, existing methods mainly focus on model-based adaptation, which aims to leverage a self-training manner to extract the target domain knowledge. However, pseudo labels can be noisy and the updated model parameters are unreliable under dynamic data distributions, leading to error accumulation and catastrophic forgetting in the continual adaptation process. To tackle these challenges and maintain the model plasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly handling both domain-specific and domain-shared knowledge. Specifically, we first comprehensively explore the different domain representations of the adapters with trainable high-rank or low-rank embedding spaces. Then we inject ViDAs into the pre-trained model, which leverages high-rank and low-rank features to adapt the current domain distribution and maintain the continual domain-shared knowledge, respectively. To exploit the low-rank and high-rank ViDAs more effectively, we further propose a Homeostatic Knowledge Allotment (HKA) strategy, which adaptively combines different knowledge from each ViDA. Extensive experiments conducted on four widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance in both classification and segmentation CTTA tasks. Note that, our method can be regarded as a novel transfer paradigm for large-scale models, delivering promising results in adaptation to continually changing distributions. 																																	2024-04-15	PPRN:73045478		
J	Otto, Felix; Sauer, Jonas; Smith, Scott; Weber, Hendrik				Otto, Felix/NKP-5547-2025						A priori bounds for quasi-linear SPDEs in the full sub-critical regime								Arxiv											2	2;2024-03-26;https://www.arxiv.org/abs/2103.11039v4| 1;2021-03-19;https://www.arxiv.org/abs/2103.11039v2	arXiv:2103.11039			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 26 2024	2024	This paper is concerned with quasi-linear parabolic equations driven by an additive forcing ξ ∈ Cα−2, in the full sub-critical regime α ∈ (0, 1). We are inspired by Hairer’s regularity structures, however we work with a more parsimonious model indexed by multi-indices rather than trees. This allows us to capture additional symmetries which play a crucial role in our analysis. Assuming bounds on this model, which is modified in agreement with the concept of algebraic renormalization, we prove local a priori estimates on solutions to the quasi-linear equations modified by the corresponding counter terms.																																	2024-04-15	PPRN:11501503		
J	Pan, Zhenyu; Luo, Haozheng; Li, Manling; Liu, Han				Luo, Robin/OGN-7952-2025						Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models								Arxiv											2	2;2025-02-21;https://www.arxiv.org/abs/2403.17359v2| 1;2024-03-26;https://www.arxiv.org/abs/2403.17359v1	arXiv:2403.17359			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 26 2024	2024	We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.																																	2025-08-07	PPRN:88291539		
J	Chen, Junkai; Pan, Zhiyuan; Hu, Xing; Li, Zhenhao; Li, Ge; Xia, Xin				CHEN, Juan/O-6169-2019						Evaluating Large Language Models with Runtime Behavior of Program Execution								Arxiv											2	2;2024-07-03;https://www.arxiv.org/abs/2403.16437v2| 1;2024-03-25;https://www.arxiv.org/abs/2403.16437v1	arXiv:2403.16437			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs.																																	2025-08-07	PPRN:88280656		
J	Wang, Tianqi; Xie, Enze; Chu, Ruihang; Li, Zhenguo; Luo, Ping				WANG, TIANQI/HOA-5441-2023						DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving								Arxiv											1	1;2024-03-25;https://www.arxiv.org/abs/2403.16996v1	arXiv:2403.16996			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 25 2024	2024	End-to-end driving has made significant progress in recent years, demonstrating benefits such as system simplicity and competitive driving performance under both open-loop and closed-loop settings. Nevertheless, the lack of interpretability and controllability in its driving decisions hinders real-world deployment for end-to-end driving systems. In this paper, we collect a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA simulator. It contains sensor data, control decisions, and chain-of-thought labels to indicate the reasoning process. We utilize the challenging driving scenarios from the CARLA leaderboard 2.0, which involve high-speed driving and lane-changing, and propose a rule-based expert policy to control the vehicle and generate ground truth labels for its reasoning process across different driving aspects and the final decisions. This dataset can serve as an open-loop end-to-end driving benchmark, enabling the evaluation of accuracy in various chain-of-thought aspects and the final decision. In addition, we propose a baseline model called DriveCoT-Agent, trained on our dataset, to generate chain-of-thought predictions and final decisions. The trained model exhibits strong performance in both open-loop and closed-loop evaluations, demonstrating the effectiveness of our proposed dataset.																																	2025-08-07	PPRN:123158180		
J	Yu, Qifan; Li, Juncheng; Wei, Longhui; Pang, Liang; Ye, Wentao; Qin, Bosheng; Tang, Siliang; Tian, Qi; Zhuang, Yueting				Qin, Bosheng/AAD-6679-2021						HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data								Arxiv											1	1;2024-03-25;https://www.arxiv.org/abs/2311.13614v2	arXiv:2311.13614			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 25 2024	2024	Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored. This work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations. Based on that, we execute counterfactual visual instruction expansion to balance data distribution, thereby enhancing MLLMs' resistance to hallucinations. Comprehensive experiments on hallucination evaluation benchmarks show that our method successfully mitigates 44.6% hallucinations relatively and maintains competitive performance compared to LLaVA. The data and code for this paper are publicly available. url{https://github.com/Yuqifan1117/HalluciDoctor}.																																	2025-08-07	PPRN:123158629		
J	Tang, Yunlong; Shimada, Daiki; Bi, Jing; Xu, Chenliang				Tang, Yunlong/JZE-2341-2024						AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue								Arxiv											2	2;2024-08-21;https://www.arxiv.org/abs/2403.16276v2| 1;2024-03-24;https://www.arxiv.org/abs/2403.16276v1	arXiv:2403.16276			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 24 2024	2024	In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment of audio-visual information. Additionally, we develop the A5-222K dataset, encompassing more than 200,000 audio-text pairings, to facilitate the audio and text alignments. Our experiments demonstrate that AVicuna can effectively handle TRD in audio-visual videos and achieve state-of-the-art performance on various audio-visual video understanding tasks, particularly in untrimmed videos. We further investigate the optimal audio-interleaving rate for interleaved audio-visual inputs, which maximizes performance on the Audio-Visual Event Dense Localization task.																																	2025-08-07	PPRN:88279913		
J	Zheng, Shunyuan; Zhou, Boyao; Shao, Ruizhi; Liu, Boning; Zhang, Shengping; Nie, Liqiang; Liu, Yebin				Liu, Yebin/L-7393-2019; Liu, Boning/AAK-7643-2021; 邵, 睿智/JAC-0742-2023						GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis								Arxiv											1	1;2024-03-24;https://www.arxiv.org/abs/2312.02155v2	arXiv:2312.02155			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 24 2024	2024	We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.																																	2025-08-07	PPRN:123157219		
J	Fornasier, Massimo; Klock, Timo; Riedl, Konstantin				Riedl, Konstantin/HKW-8739-2023; Fornasier, Massimo/AAB-6671-2020						Consensus-Based Optimization Methods Converge Globally								Arxiv											3	3;2024-08-02;https://www.arxiv.org/abs/2103.15130v6| 2;2024-03-23;https://www.arxiv.org/abs/2103.15130v5| 1;2021-03-28;https://www.arxiv.org/abs/2103.15130v4	arXiv:2103.15130			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 23 2024	2024	In this paper we study consensus-based optimization (CBO), which is a multi-agent metaheuristic derivative-free optimization method that can globally minimize nonconvex nonsmooth functions and is amenable to theoretical analysis. Based on an experimentally supported intuition that, on average, CBO performs a gradient descent of the squared Euclidean distance to the global minimizer, we devise a novel technique for proving the convergence to the global minimizer in mean-field law for a rich class of objective functions. The result unveils internal mechanisms of CBO that are responsible for the success of the method. In particular, we prove that CBO performs a convexification of a large class of optimization problems as the number of optimizing agents goes to infinity. Furthermore, we improve prior analyses by requiring mild assumptions about the initialization of the method and by covering objectives that are merely locally Lipschitz continuous. As a core component of this analysis, we establish a quantitative nonasymptotic Laplace principle, which may be of independent interest. From the result of CBO convergence in mean-field law, it becomes apparent that the hardness of any global optimization problem is necessarily encoded in the rate of the mean-field approximation, for which we provide a novel probabilistic quantitative estimate. The combination of these results allows to obtain probabilistic global convergence guarantees of the numerical CBO method.																																	2025-08-07	PPRN:13212823		
J	Frenkel, Yarden; Vinker, Yael; Shamir, Ariel; Cohen-Or, Daniel										Implicit Style-Content Separation using B-LoRA								Arxiv											1	1;2024-03-21;https://www.arxiv.org/abs/2403.14572v1	arXiv:2403.14572			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 21 2024	2024	Image stylization involves manipulating the visual appearance and texture (style) of an image while preserving its underlying objects, structures, and concepts (content). The separation of style and content is essential for manipulating the image's style independently from its content, ensuring a harmonious and visually pleasing result. Achieving this separation requires a deep understanding of both the visual and semantic characteristics of images, often necessitating the training of specialized models or employing heavy optimization. In this paper, we introduce B-LoRA, a method that leverages LoRA (Low-Rank Adaptation) to implicitly separate the style and content components of a single image, facilitating various image stylization tasks. By analyzing the architecture of SDXL combined with LoRA, we find that jointly learning the LoRA weights of two specific blocks (referred to as B-LoRAs) achieves style-content separation that cannot be achieved by training each B-LoRA independently. Consolidating the training into only two blocks and separating style and content allows for significantly improving style manipulation and overcoming overfitting issues often associated with model fine-tuning. Once trained, the two B-LoRAs can be used as independent components to allow various image stylization tasks, including image style transfer, text-based image stylization, consistent style generation, and style-content mixing.																																	2024-04-13	PPRN:88261461		
J	Kong, Xiangtao; Dong, Chao; Zhang, Lei				Zhang, Lei/P-8881-2014						Towards Effective Multiple-in-One Image Restoration: A Sequential and Prompt Learning Strategy								Arxiv											3	3;2024-03-20;https://www.arxiv.org/abs/2401.03379v3| 2;2024-03-17;https://www.arxiv.org/abs/2401.03379v2| 1;2024-01-07;https://www.arxiv.org/abs/2401.03379v1	arXiv:2401.03379			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 20 2024	2024	While single task image restoration (IR) has achieved significant successes, it remains a challenging issue to train a single model which can tackle multiple IR tasks. In this work, we investigate in-depth the multiple-in-one (MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO IR faces two pivotal challenges: the optimization of diverse objectives and the adaptation to multiple tasks. To tackle these challenges, we present two simple yet effective strategies. The first strategy, referred to as sequential learning, attempts to address how to optimize the diverse objectives, which guides the network to incrementally learn individual IR tasks in a sequential manner rather than mixing them together. The second strategy, i.e., prompt learning, attempts to address how to adapt to the different IR tasks, which assists the network to understand the specific task and improves the generalization ability. By evaluating on 19 test sets, we demonstrate that the sequential and prompt learning strategies can significantly enhance the MiO performance of commonly used CNN and Transformer backbones. Our experiments also reveal that the two strategies can supplement each other to learn better degradation representations and enhance the model robustness. It is expected that our proposed MiO IR formulation and strategies could facilitate the research on how to train IR models with higher generalization capabilities.																																	2024-04-12	PPRN:87069607		
J	Fei, Hao; Wu, Shengqiong; Ji, Wei; Zhang, Hanwang; Chua, Tat-Seng				Wang, Meng/AEZ-9059-2022; Fei, Hao/IZD-5292-2023; JI, WEI/KEI-2602-2024						Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs								Arxiv											2	2;2024-03-19;https://www.arxiv.org/abs/2308.13812v2| 1;2023-08-26;https://www.arxiv.org/abs/2308.13812v1	arXiv:2308.13812			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Mar 19 2024	2024	Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via in-context learning, Dysen realizes (nearly) human-level temporal dynamics understanding. Finally, the resulting video DSG with rich action scene details is encoded as fine-grained spatio-temporal features, integrated into the backbone T2V DM for video generating. Experiments on popular T2V datasets suggest that our Dysen-VDM consistently outperforms prior arts with significant margins, especially in scenarios with complex actions. 																																	2024-04-12	PPRN:84284228		
J	Shi, Baifeng; Wu, Ziyang; Mao, Maolin; Wang, Xin; Darrell, Trevor										When Do We Not Need Larger Vision Models?								Arxiv											1	1;2024-03-19;https://www.arxiv.org/abs/2403.13043v1	arXiv:2403.13043			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 19 2024	2024	Scaling up the size of vision models has been the de facto standard to obtain more powerful visual representations. In this work, we discuss the point beyond which larger vision models are not necessary. First, we demonstrate the power of Scaling on Scales (S2), whereby a pre -trained and frozen smaller vision model (e.g., ViT-B or ViT-L), run over multiple image scales, can outperform larger models (e.g., ViT-H or ViT-G) on classification, segmentation, depth estimation, Multimodal LLM (MLLM) benchmarks, and robotic manipulation. Notably, S2 achieves state-of-the-art performance in detailed understanding of MLLM on the V∗ benchmark, surpassing models such as GPT-4V. We examine the conditions under which S2 is a preferred scaling approach compared to scaling on model size. While larger models have the advantage of better generalization on hard examples, we show that features of larger vision models can be well approximated by those of multi -scale smaller models. This suggests most, if not all, of the representations learned by current large pre -trained models can also be obtained from multi -scale smaller models. Our results show that a multi -scale smaller model has comparable learning capacity to a larger model, and pre -training smaller models with S2 can match or even exceed the advantage of larger models. 																																	2024-04-12	PPRN:88221435		
J	Mark, Daniel K.; Surace, Federica; Elben, Andreas; Shaw, Adam L.; Choi, Joonhee; Refael, Gil; Endres, Manuel; Choi, Soonwon										A Maximum Entropy Principle in Deep Thermalization and in Hilbert-Space Ergodicity								Arxiv											2	2;2024-03-18;https://www.arxiv.org/abs/2403.11970v1| 1;2024-03-18;https://www.arxiv.org/abs/2403.11970v1	arXiv:2403.11970			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 18 2024	2024	We report universal statistical properties displayed by ensembles of pure states that naturally emerge in quantum many-body systems. Specifically, two classes of state ensembles are considered: those formed by i) the temporal trajectory of a quantum state under unitary evolution or ii) the quantum states of small subsystems obtained by partial, local projective measurements performed on their complements. These cases respectively exemplify the phenomena of "Hilbert-space ergodicity" and "deep thermalization." In both cases, the resultant ensembles are defined by a simple principle: the distributions of pure states have maximum entropy, subject to constraints such as energy conservation, and effective constraints imposed by thermalization. We present and numerically verify quantifiable signatures of this principle by deriving explicit formulae for all statistical moments of the ensembles; proving the necessary and sufficient conditions for such universality under widely-accepted assumptions; and describing their measurable consequences in experiments. We further discuss information-theoretic implications of the universality: our ensembles have maximal information content while being maximally difficult to interrogate, establishing that generic quantum state ensembles that occur in nature hide (scramble) information as strongly as possible. Our results generalize the notions of Hilbert-space ergodicity to time-independent Hamiltonian dynamics and deep thermalization from infinite to finite effective temperature. Our work presents new perspectives to characterize and understand universal behaviors of quantum dynamics using statistical and information theoretic tools.																																	2025-01-08	PPRN:88199085		
J	Zuo, Qi; Gu, Xiaodong; Qiu, Lingteng; Dong, Yuan; Zhao, Zhengyi; Yuan, Weihao; Peng, Rui; Zhu, Siyu; Dong, Zilong; Bo, Liefeng; Huang, Qixing				袁伟皓/AAF-9458-2020; Peng, Rui/AAL-7506-2020; Gu, Xiaodong/GQH-8261-2022						VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model								Arxiv											1	1;2024-03-18;https://www.arxiv.org/abs/2403.12010v1	arXiv:2403.12010			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 18 2024	2024	Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. 																																	2024-04-11	PPRN:88199527		
J	Jiang, Wenqi; Zhang, Shuai; Han, Boran; Wang, Jie; Wang, Bernie; Kraska, Tim										PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design								Arxiv											1	1;2024-03-08;https://www.arxiv.org/abs/2403.05676v1	arXiv:2403.05676			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Mar 08 2024	2024	Retrieval -augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6× speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.																																	2024-04-07	PPRN:88099381		
J	Adelani, David Ifeoluwa; Liu, Hannah; Shen, Xiaoyu; Vassilyev, Nikita; Alabi, Jesujoba O.; Mao, Yanke; Gao, Haonan; Lee, En-Shiun Annie										SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects								Arxiv											3	3;2024-03-07;https://www.arxiv.org/abs/2309.07445v3| 2;2024-02-05;https://www.arxiv.org/abs/2309.07445v2| 1;2023-09-14;https://www.arxiv.org/abs/2309.07445v1	arXiv:2309.07445			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 07 2024	2024	Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200— a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pretraining of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset encourage a more inclusive evaluation of multilingual language models on a more diverse set of languages. 1																																	2024-04-01	PPRN:85018121		
J	Kong, Jiaolong; Cheng, Mingfei; Xie, Xiaofei; Liu, Shangqing; Du, Xiaoning; Guo, Qi				Xie, Xiaofei/ABE-4095-2020; Du, Xiaoning/AAE-5334-2019; Cheng, Mingfei/HCH-3725-2022						<italic>ContrastRepair:</italic> Enhancing Conversation-Based Automated via Contrastive Test Case Pairs								Arxiv											2	2;2024-03-07;https://www.arxiv.org/abs/2403.01971v2| 1;2024-03-04;https://www.arxiv.org/abs/2403.01971v1	arXiv:2403.01971			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 07 2024	2024	Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this paper, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM. Our key insight isto minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing informative and specific feedback, ContrastRepair enables the LLM to produce effective bug fixes. The implementation of ContrastRepair is based on the stateof-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated. We evaluate ContrastRepair on multiple benchmark datasets, including Defects4j, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new stateof-the-art in program repair. For instance, among Defects4j 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.																																	2024-04-04	PPRN:88024352		
J	Ma, Huan; Zhang, Changqing; Fu, Huazhu; Zhao, Peilin; Wu, Bingzhe				changqing, zhang/JJE-9538-2023; 赵, 陪嶙/GYD-6972-2022; 吴, 秉哲/KZV-0893-2024; Fu, Huazhu/A-1411-2014						Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning								Arxiv											3	3;2024-03-07;https://www.arxiv.org/abs/2310.03400v2| 2;2023-10-05;https://www.arxiv.org/abs/2310.03400v1| 1;2023-10-05;https://www.arxiv.org/abs/2310.03400v1	arXiv:2310.03400			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 07 2024	2024	Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. A common approach is to use a discriminative model to classify the content, but this method often requires strict data engineering, otherwise it will face unacceptable overfitting. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. Thanks to the knowledge of the foundation models, we can develop more robust privately deployed models with limited data via fine-tuning these foundation models. Moreover, as a generative model, it can provide detailed analysis of the review process, enhancing interpretability. In this paper, we introduce how to fine-tune a LLM model that can be privately deployed for content moderation. Specifically, we discuss the differences between discriminative and generative models using content moderation as an example. Additionally, we reveal that incorporating reasoning processes during the fine-tuning of LLMs can effectively alleviate overfitting, even if the model is not allowed to directly output reasoning processes during deployment. We present a complete process, from data collection and construction to model training and overfitting elimination, for fine-tuning LLMs in vertical domain deployments. We report the entire research process and the key findings in this paper, hoping to provide valuable experience for researchers who are fine-tuning privately deployed models in their domain-specific research.																																	2024-04-05	PPRN:85430704		
J	Chernozhukov, Victor; Hansen, Christian; Kallus, Nathan; Spindler, Martin; Syrgkanis, Vasilis										Applied Causal Inference Powered by ML and AI								Arxiv											1	1;2024-03-04;https://www.arxiv.org/abs/2403.02467v1	arXiv:2403.02467			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 04 2024	2024																																		2024-04-04	PPRN:88034525		
J	Sclocchi, Antonio; Favero, Alessandro; Wyart, Matthieu				wyart, matthieu/L-3640-2013						A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data								Arxiv											1	1;2024-03-04;https://www.arxiv.org/abs/2402.16991v2	arXiv:2402.16991			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 04 2024	2024	Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the generated sample may still be composed of low-level elements of the initial image. We validate these theoretical insights through numerical experiments on class-unconditional ImageNet diffusion models. Our analysis characterises the relationship between time and scale in diffusion models and puts forward generative models as powerful tools to model combinatorial data properties.																																	2024-03-30	PPRN:88020040		
J	Ishibashi, Yoichi; Shimodaira, Hidetoshi				Shimodaira, Hidetoshi/B-9127-2008						Knowledge Sanitization of Large Language Models								Arxiv											2	2;2024-03-02;https://www.arxiv.org/abs/2309.11852v2| 1;2023-09-21;https://www.arxiv.org/abs/2309.11852v1	arXiv:2309.11852			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Mar 02 2024	2024	We explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (LLMs). LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. Our technique efficiently fine-tunes these models using the Low -Rank Adaptation (LoRA) method, prompting them to generate harmless responses such as “I don’t know” when queried about specific information. Experimental results in a closed -book question -answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of LLMs. These two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations.1																																	2024-03-31	PPRN:85081663		
J	Wang, Yixuan; Jiao, Ruochen; Zhan, Sinong Simon; Lang, Chengtian; Huang, Chao; Wang, Zhaoran; Yang, Zhuoran; Zhu, Qi				Wang, Yixuan/JWO-5957-2024; Yang, Zhuoran/IYJ-4459-2023; Zhu, Qi/KYR-6238-2024; Huang, Chao/AFN-4962-2022						Empowering Autonomous Driving with Large Language Models: A Safety Perspective								Arxiv											4	4;2023-12-18;https://www.arxiv.org/abs/2312.00812v3| 3;2023-12-13;https://www.arxiv.org/abs/2312.00812v2| 2;2023-11-28;https://www.arxiv.org/abs/2312.00812v1| 1;2024-03-01;	arXiv:2312.00812			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Mar 01 2024	2024	Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approaches, our approach shows the promising potential for using LLMs for autonomous vehicles.																																	2025-11-07	PPRN:86377949		
J	Coda-Forno, Julian; Binz, Marcel; Wang, Jane X.; Schulz, Eric										CogBench: a large language model walks into a psychology lab								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18225v1	arXiv:2402.18225			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 28 2024	2024	Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs’ behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open -source models are less risk -prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs’ behavior. Finally, we explore the effects of prompt -engineering techniques. We discover that chain -of -thought prompting improves probabilistic reasoning, while take -a -step -back prompting fosters model -based behaviors.																																	2024-03-28	PPRN:87989722		
J	Koide, Takashi; Fukushi, Naoki; Nakano, Hiroki; Chiba, Daiki				Nakano, Hiroki/AAX-8677-2021						ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection								Arxiv											1	1;2024-02-28;https://www.arxiv.org/abs/2402.18093v1	arXiv:2402.18093			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 28 2024	2024	The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in spam filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as spam, risking the possibility of missing important communications or mistakenly trusting phishing emails. This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails. By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems. We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.																																	2024-03-28	PPRN:88005988		
J	Boyett, Kristan; Trenti, Michele; Leethochawalit, Nicha; Calabro, Antonello; Metha, Benjamin; Roberts-Borsani, Guido; Dalmasso, Nicolo; Yang, Lilan; Santini, Paola; Treu, Tommaso; Jones, Tucker; Henry, Alaina; Mason, Charlotte A.; Morishita, Takahiro; Nanayakkara, Themiya; Roy, Namrata; Wang, Xin; Fontana, Adriano; Merlin, Emiliano; Castellano, Marco; Paris, Diego; Bradac, Marusa; Marchesini, Danilo; Mascia, Sara; Pentericci, Laura; Vanzella, Eros; Vulcani, Benedetta				Mason, Charlotte/IYJ-2820-2023; Vulcani, Benedetta/ABB-4847-2020; Metha, Benjamin/NHP-4095-2025; Calabrò, Antonello/AAX-1028-2020; Nanayakkara, Themiya/OAJ-0439-2025; Treu, Tommaso/KYP-7127-2024						A massive interacting galaxy 510 million years after the Big Bang								Arxiv											2	2;2024-02-27;https://www.arxiv.org/abs/2303.00306v2| 1;2023-03-01;https://www.arxiv.org/abs/2303.00306v1	arXiv:2303.00306			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 27 2024	2024	JWST observations spectroscopically confirmed the existence of galaxies as early as 300 million years after the Big Bang and with a higher number density than what was expected based on galaxy formation models and Hubble Space Telescope observations. Yet, the majority of sources confirmed spectroscopically so far in the first 500 million years have rest-frame UV-luminosities below the characteristic luminosity (M∗UV), limiting the signal to noise ratio for investigating substructure. Here, we present a high-resolution spectroscopic and spatially resolved study of a bright (MUV = −21.66 ± 0.03, ∼ 2M∗UV) galaxy at a redshift z = 9.3127 ± 0.0002 (510 million years after the Big Bang) with an estimated stellar mass of (1.6+0.5−0.4) × 109 M⊙, forming 19+5 −6 Solar masses per year and with a metallicity of about one tenth of Solar. The system has a morphology typically associated to two interacting galaxies, with a two-component main clump of very young stars (age less than 10 million years) surrounded by an extended stellar population (120 ± 20 million years old, identified from modeling of the NIRSpec spectrum) and an elongated clumpy tidal tail. The observations acquired at high spectral resolution identify oxygen, neon and hydrogen emission lines, as well as the Lyman break, where there is evidence of substantial absorption of Lyα. The [O ii] doublet is resolved spectrally, enabling an estimate of the electron number density and ionization parameter of the interstellar medium and showing higher densities and ionization than in analogs at lower redshifts. We identify evidence of absorption lines (silicon, carbon and iron), with low confidence individual detections but signal-to-noise ratio larger than 6 when stacked. These absorption features suggest that Lyα is damped by the interstellar and circumgalactic medium. Our observations provide evidence of rapid and efficient build up of mass and metals in the immediate aftermath of the Big Bang through mergers, demonstrating that massive galaxies with several billion stars are in place at early times.																																	2024-11-09	PPRN:41242090		
J	Mireshghallah, Niloofar; Mattern, Justus; Gao, Sicun; Shokri, Reza; Berg-Kirkpatrick, Taylor										Smaller Language Models are Better Black-box Machine-Generated Text Detectors								Arxiv											4	4;2024-02-24;https://www.arxiv.org/abs/2305.09859v4| 3;2024-02-12;https://www.arxiv.org/abs/2305.09859v3| 2;2024-02-04;https://www.arxiv.org/abs/2305.09859v2| 1;2023-05-17;https://www.arxiv.org/abs/2305.09859v1	arXiv:2305.09859			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 24 2024	2024	As large language models are becoming more ubiquitous and embedded in different user-facing services, it is important to be able to distinguish between human-written and machine-generated text, to verify the authenticity of news articles, product reviews, etc. Thus, in this paper we set out to explore whether it is possible to use one language model to identify machine-generated text produced by another language model, even if the two have different architectures and are trained on different data. Further, if this is possible, which language models make the best general-purpose detectors? We find that overall, smaller and partially-trained models are better universal machine-generated text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether or not the detector and generator models were trained on the same data or have similar parameter counts is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.																																	2024-03-25	PPRN:70025043		
J	Ke, Zixuan; Kong, Weize; Li, Cheng; Zhang, Mingyang; Mei, Qiaozhu; Bendersky, Michael				Ke, Zixuan/KHT-6176-2024						Bridging the Preference Gap between Retrievers and LLMs								Arxiv											2	2;2024-02-20;https://www.arxiv.org/abs/2401.06954v2| 1;2024-01-13;https://www.arxiv.org/abs/2401.06954v1	arXiv:2401.06954			http://creativecommons.org/publicdomain/zero/1.0/	http://creativecommons.org/publicdomain/zero/1.0/			preprint	Feb 20 2024	2024	Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-"friendly" information and assembling a LLM-"friendly" context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.																																	2024-11-09	PPRN:87188810		
J	Saxena, Aayush; Bunker, Andrew J.; Jones, Gareth C.; Stark, Daniel P.; Cameron, Alex J.; Witstok, Joris; Arribas, Santiago; Baker, William M.; Baum, Stefi; Bhatawdekar, Rachana; Bowler, Rebecca; Boyett, Kristan; Carniani, Stefano; Charlot, Stephane; Chevallard, Jacopo; Curti, Mirko; Curtis-Lake, Emma; Eisenstein, Daniel J.; Endsley, Ryan; Hainline, Kevin; Helton, Jakob M.; Johnson, Benjamin D.; Kumari, Nimisha; Looser, Tobias J.; Maiolino, Roberto; Rieke, Marcia; Rix, Hans-Walter; Robertson, Brant E.; Sandles, Lester; Simmonds, Charlotte; Smit, Renske; Tacchella, Sandro; Williams, Christina C.; Willmer, Christopher N.A.; Willott, Chris				Witstok, Joris/GQA-8643-2022; Tacchella, Sandro/AAT-1602-2021; Baker, William/KUD-6412-2024; BOEKER, TORSTEN/KVC-3022-2024; Robertson, Brant/AAA-6124-2022; Arribas, Santiago/F-9277-2015; Jones, Gareth/AAD-7663-2022; Kumari, Nimisha/AFS-1631-2022; Endsley, Ryan/AAJ-5103-2021; Helton, Jakob/KXS-1907-2024; Smit, Renske/MIK-8564-2025						JADES: The production and escape of ionizing photons from faint Lyman-alpha emitters in the epoch of reionization								Arxiv											2	2;2024-02-20;https://www.arxiv.org/abs/2306.04536v2| 1;2023-06-07;https://www.arxiv.org/abs/2306.04536v1	arXiv:2306.04536			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 20 2024	2024	We present the properties of 17 faint Lyman-α emitting galaxies (LAEs) at z > 5.8 from the JWST Advanced Deep Extragalactic Survey (JADES) spectroscopic data in the Hubble Ultra Deep Field/GOODS-S. These LAEs span a redshift range z ≈ 5.8 − 8.0 and a UV magnitude range MUV ≈ −17 to −20.6, with the Lyα equivalent width (EW) in the range ≈ 25 − 350 Å. The detection of other rest-optical emission lines in the spectra of these LAEs enables the determination of accurate systemic redshifts and Lyα velocity offsets, as well as the physical and chemical composition of their stars and interstellar media. These faint LAEs are consistent with metal-poor systems with high ionization parameters, similar to the general galaxy population at z > 6. We measured an average ionizing photon production efficiency, log(ξion/erg−1 Hz) ≈ 25.57 across our LAEs, which does not evolve strongly with redshift. We report an anti-correlation between the Lyα escape fraction and the velocity offset from systemic redshift, consistent with model expectations. We further find that the strength and velocity offset of Lyα are neither correlated with galaxy spectroscopic properties nor with ξion. We find a decrease in Lyα escape fractions with redshift, indicative of decreasing sizes of ionized bubbles around LAEs at high redshifts. We used a range of galaxy properties to predict Lyman continuum escape fractions for our LAEs, finding that the ionizing photon output into the intergalactic medium from our LAEs remains roughly constant across the observed Lyα EW, showing a mild increase at fainter UV magnitudes and at higher redshifts. We derived correlations between the ionizing photon output from LAEs and their UV magnitudes, Lyα strengths and redshifts, which can be used to constrain the ionizing photon contribution of LAEs at z > 6 towards cosmic reionization.																																	2024-03-20	PPRN:73069946		
J	Zhao, Ziyu; Gan, Leilei; Wang, Guoyin; Zhou, Wangchunshu; Yang, Hongxia; Kuang, Kun; Wu, Fei				zhao, ziyu/MIQ-5218-2025; Wu, Fan/AID-2699-2022; Bai, Feng-Yang/J-8158-2017; Wang, Benyou/Y-5146-2019						LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild								Arxiv											2	2;2024-02-15;https://www.arxiv.org/abs/2402.09997v1| 1;2024-02-15;https://www.arxiv.org/abs/2402.09997v1	arXiv:2402.09997			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 15 2024	2024	Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing efficient batch inference to accommodate heterogeneous requests. Experimental results indicate that LoraRetriever consistently outperforms the baselines, highlighting its practical effectiveness and versatility.																																	2024-03-19	PPRN:87703680		
J	Saberi, Mehrdad; Sadasivan, Vinu Sankar; Rezaei, Keivan; Kumar, Aounon; Chegini, Atoosa; Wang, Wenxiao; Feizi, Soheil				Wang, Wenxiao/KVY-3617-2024						Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks								Arxiv											2	2;2024-02-14;https://www.arxiv.org/abs/2310.00076v2| 1;2023-09-29;https://www.arxiv.org/abs/2310.00076v1	arXiv:2310.00076			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 14 2024	2024	In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa. Various techniques have been introduced for identifying AI-generated images, with watermarking emerging as a promising approach. In this paper, we analyze the robustness of various AI-image detectors including watermarking and classifier-based deepfake detectors. For watermarking methods that introduce subtle image perturbations (i.e., low perturbation budget methods), we reveal a fundamental trade-off between the evasion error rate (i.e., the fraction of watermarked images detected as non-watermarked ones) and the spoofing error rate (i.e., the fraction of nonwatermarked images detected as watermarked ones) upon an application of diffusion purification attack. To validate our theoretical findings, we also provide empirical evidence demonstrating that diffusion purification effectively removes low perturbation budget watermarks by applying minimal changes to images. For high perturbation watermarking methods where notable changes are applied to images, the diffusion purification attack is not effective. In this case, we develop a model substitution adversarial attack that can successfully remove watermarks. Moreover, we show that watermarking methods are vulnerable to spoofing attacks where the attacker aims to have real images (potentially obscene) identified as watermarked ones, damaging the reputation of the developers. In particular, by just having black-box access to the watermarking method, we show that one can generate a watermarked noise image, which can be added to the real images, leading to their incorrect classification as watermarked. Finally, we extend our theory to characterize a fundamental trade-off between the robustness and reliability of classifier-based deep fake detectors and demonstrate it through experiments. Code is available at https://github.com/mehrdadsaberi/watermark robustness.																																	2024-05-25	PPRN:85356286		
J	Wang, Jianing; Wu, Junda; Hou, Yupeng; Liu, Yao; Gao, Ming; McAuley, Julian				Hou, Yupeng/GNP-3072-2022; Wu, Junda/KIB-5820-2024; yao, liu/KHV-3062-2024; Wang, Jianing/I-2038-2019						InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment								Arxiv											1	1;2024-02-13;https://www.arxiv.org/abs/2402.08785v1	arXiv:2402.08785			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Feb 13 2024	2024	Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output’s reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13% and 38%, respectively.																																	2024-05-25	PPRN:87687877		
J	Kang, Kaifei; Shen, Bowen; Qiu, Yichen; Watanabe, Kenji; Taniguchi, Takashi; Shan, Jie; Mak, Kin Fai				TANIGUCHI, Takashi/H-2718-2011; Watanabe, Kenji/H-2825-2011; Mak, Kin/E-9004-2015						Observation of the fractional quantum spin Hall effect in moiré MoTe2								Arxiv											2	2;2024-02-12;https://www.arxiv.org/abs/2402.03294v2| 1;2024-02-05;https://www.arxiv.org/abs/2402.03294v1	arXiv:2402.03294			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Feb 12 2024	2024	Quantum spin Hall (QSH) insulators are two-dimensional electronic materials that have a bulk band gap like an ordinary insulator but have topologically protected pairs of edge modes of opposite chiralities 1-6. To date, experimental studies have found only integer QSH insulators with counter -propagating up -spins and downspins at each edge leading to a quantized conductance G0 = e2/h (with e and h denoting the electron charge and Planck’s constant, respectively) 7-14. Here we report transport evidence of a fractional QSH insulator in 2.1 -degree -twisted bilayer MoTe2, which supports spin -Si conservation and flat spin -contrasting Chern bands 15,16. At filling factor v = 3 of the moire valence bands, each edge contributes a conductance z G0with zero anomalous Hall conductivity. The state is likely a timereversal pair of the even -denominator 3/2 -fractional Chern insulators. Further, at v = 2, 4 and 6, we observe a single, double and triple QSH insulator with each edge contributing a conductance G0, 2G0 and 3G0, respectively. Our results open up the possibility of realizing time reversal symmetric non-abelian anyons and other unexpected topological phases in highly tunable moiré materials 17-19.																																	2024-05-25	PPRN:87521734		
J	Zhang, Bohang; Luo, Shengjie; Wang, Liwei; He, Di				zhang, bohang/AHA-6273-2022						Rethinking the Expressive Power of GNNs via Graph Biconnectivity								Arxiv											2	2;2024-02-11;https://www.arxiv.org/abs/2301.09505v3| 1;2023-01-23;https://www.arxiv.org/abs/2301.09505v2	arXiv:2301.09505			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 11 2024	2024	Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.																																	2024-05-25	PPRN:40737132		
J	Zhu, Yinghao; Ren, Changyu; Xie, Shiyun; Liu, Shukai; Ji, Hangyuan; Wang, Zixiang; Sun, Tao; He, Long; Li, Zhoujun; Zhu, Xi; Pan, Chengwei				pan, chengwei/HDM-8386-2022; Zhu, Yinghao/JNT-3876-2023; He, Longtao/OXC-6599-2025; Sun, Tao/ORK-0166-2025; 任, 昶宇/GZG-8961-2022						REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models								Arxiv											1	1;2024-02-10;https://www.arxiv.org/abs/2402.07016v1	arXiv:2402.07016			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 10 2024	2024	The integration of multimodal Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time -series EHR, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph (KG). Previous approaches with KG knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge. In response, we propose REALM, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR representations that address these limitations. Firstly, we apply Large Language Model (LLM) to encode long context clinical notes and GRU model to encode time -series EHR data. Secondly, we prompt LLM to extract task -relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding medical knowledge. By matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency. Lastly, we propose an adaptive multimodal fusion network to integrate extracted knowledge with multimodal EHR data. Our extensive experiments on MIMICIII mortality and readmission tasks showcase the superior performance of our REALM framework over baselines, emphasizing the effectiveness of each module. REALM framework contributes to refining the use of multimodal EHR data in healthcare and bridging the gap with nuanced medical context essential for informed clinical predictions.																																	2024-02-27	PPRN:87638343		
J	Silva, Joao Daniel; Magalhaes, Joao; Tuia, Devis; Martins, Bruno				Tuia, Devis/AAE-9339-2019; Magalhaes, Joao/A-2054-2010; Martins, Bruno/J-9735-2015						Large Language Models for Captioning and Retrieving Remote Sensing Images								Arxiv											1	1;2024-02-09;https://www.arxiv.org/abs/2402.06475v1	arXiv:2402.06475			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 09 2024	2024	Image captioning and cross-modal retrieval are examples of tasks that involve the joint analysis of visual and linguistic information. In connection to remote sensing imagery, these tasks can help non-expert users in extracting relevant Earth observation information for a variety of applications. Still, despite some previous efforts, the development and application of vision and language models to the remote sensing domain have been hindered by the relatively small size of the available datasets and models used in previous studies. In this work, we propose RS-CapRet, a Vision and Language method for remote sensing tasks, in particular image captioning and text-image retrieval. We specifically propose to use a highly capable large decoder language model together with image encoders adapted to remote sensing imagery through contrastive language-image pre-training. To bridge together the image encoder and language decoder, we propose training simple linear layers with examples from combining different remote sensing image captioning datasets, keeping the other parameters frozen. RS-CapRet can then generate descriptions for remote sensing images and retrieve images from textual descriptions, achieving SOTA or competitive performance with existing methods. Qualitative results illustrate that RS-CapRet can effectively leverage the pre-trained large language model to describe remote sensing images, retrieve them based on different types of queries, and also show the ability to process interleaved sequences of images and text in a dialogue manner.																																	2024-02-26	PPRN:87608801		
J	Yang, Brian; Su, Huangyuan; Gkanatsios, Nikolaos; Ke, Tsung-Wei; Ayush, Jain; Schneider, Jeff; Fragkiadaki, Katerina										Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following								Arxiv											1	1;2024-02-09;https://www.arxiv.org/abs/2402.06559v1	arXiv:2402.06559			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 09 2024	2024	Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much more efficient exploration of the solution space. We show that DiffusionES achieves state-of-the-art performance on nuPlan, an established closed-loop planning benchmark for autonomous driving. Diffusion-ES outperforms existing sampling-based planners, reactive deterministic or diffusion-based policies, and reward-gradient guidance. Additionally, we show that unlike prior guidance methods, our method can optimize non-differentiable language-shaped reward functions generated by few-shot LLM prompting. When guided by a human teacher that issues instructions to follow, our method can generate novel, highly complex behaviors, such as aggressive lane weaving, which are not present in the training data. This allows us to solve the hardest nuPlan scenarios which are beyond the capabilities of existing trajectory optimization methods and driving policies.																																	2024-05-25	PPRN:87604393		
J	Zhang, Xingxuan; Li, Jiansheng; Chu, Wenjing; Hai, Junjia; Xu, Renzhe; Yang, Yuqing; Guan, Shikai; Xu, Jiazheng; Cui, Peng				Xu, Renzhe/KZU-9046-2024; Yang, Yuqing/LSJ-1291-2024; Zhang, Xuan/MVW-3285-2025						On the Out-Of-Distribution Generalization of Multimodal Large Language Models								Arxiv											1	1;2024-02-09;https://www.arxiv.org/abs/2402.06599v1	arXiv:2402.06599			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 09 2024	2024	We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples and test data.																																	2024-05-25	PPRN:87603569		
J	Tornberg, Petter										Best Practices for Text Annotation with Large Language Models								Arxiv											1	1;2024-02-05;https://www.arxiv.org/abs/2402.05129v1	arXiv:2402.05129			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 05 2024	2024	Large Language Models (LLMs) have ushered in a new era of text annotation, as their ease-of-use, high accuracy, and relatively low costs have meant that their use has exploded in recent months. However, the rapid growth of the field has meant that LLM-based annotation has become something of an academic Wild West: the lack of established practices and standards has led to concerns about the quality and validity of research. Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results. Recognizing the transformative potential of LLMs, this paper proposes a comprehensive set of standards and best practices for their reliable, reproducible, and ethical use. These guidelines span critical areas such as model selection, prompt engineering, structured prompting, prompt stability analysis, rigorous model validation, and the consideration of ethical and legal implications. The paper emphasizes the need for a structured, directed, and formalized approach to using LLMs, aiming to ensure the integrity and robustness of text annotation practices, and advocates for a nuanced and critical engagement with LLMs in social scientific research.																																	2024-05-25	PPRN:87572325		
J	Gallegos, Isabel O.; Rossi, Ryan A.; Barrow, Joe; Tanjim, Md Mehrab; Yu, Tong; Deilamsalehy, Hanieh; Zhang, Ruiyi; Kim, Sungchul; Dernoncourt, Franck				Zhang, Ruiyi/AAB-8402-2021						Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes								Arxiv											1	1;2024-02-03;https://www.arxiv.org/abs/2402.01981v1	arXiv:2402.01981			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 03 2024	2024	Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.																																	2024-05-25	PPRN:87523183		
J	Liu, Yihao; Zhang, Jiaming; She, Zhangcong; Kheradmand, Amir; Armand, Mehran				Liu, Yihao/ITW-1350-2023						SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM								Arxiv											2	2;2024-02-03;https://www.arxiv.org/abs/2304.05622v4| 1;2023-04-12;https://www.arxiv.org/abs/2304.05622v1	arXiv:2304.05622			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Feb 03 2024	2024	The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest available segmentation dataset. The model has demonstrated that, with prompts, it can create high-quality masks for general images. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and application of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer - an image processing and visualization software extensively used by the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.																																	2024-02-19	PPRN:58892325		
J	Rasheed, Zeeshan; Waseem, Muhammad; Saari, Mika; Systa, Kari; Abrahamsson, Pekka				Saari, M./AAK-7235-2020; Systä, Kari/G-4320-2014; Rasheed, Zeeshan/LWK-2207-2024; Abrahamsson, Pekka/A-5559-2018						CodePori: Large Scale Model for Autonomous Software Development by Using Multi-Agents								Arxiv											1	1;2024-02-02;https://www.arxiv.org/abs/2402.01411v1	arXiv:2402.01411			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs) are reshaping the field of Software Engineering (SE). Existing LLM-based multi-agent systems have successfully resolved simple dialogue tasks. However, the potential of LLMs for more complex tasks, such as automated code generation for large and complex projects, have been explored in only a few existing works. This paper introduces CodePori, a novel model designed to automate code generation for extensive and complex software projects based on natural language prompts. We employ LLM-based multi-AI agents to handle creative and challenging tasks in autonomous software development. Each agent engages with a specific task, including system design, code development, code review, code verification, and test engineering. We show in the paper that CodePori is able to generate running code for large-scale projects, completing the entire software development process in minutes rather than hours, and at a cost of a few dollars. It identifies and mitigates potential security vulnerabilities and corrects errors while maintaining a solid code performance level. We also conducted an evaluation of CodePori against existing solutions using HumanEval and the Massively Multitask Benchmark for Python (MBPP) benchmark. The results indicate that CodePori improves upon the benchmarks in terms of code accuracy, efficiency, and overall performance. For example, CodePori improves the pass@1 metric on HumanEval to 87.5% and on MBPP to 86.5%, representing a clear improvement over the existing models. We also assessed CodePori's performance through practitioner evaluations, with 91% expressing satisfaction with the model's performance.																																	2024-05-25	PPRN:87507783		
J	Sadana, Utsav; Chenreddy, Abhilash; Delage, Erick; Forel, Alexandre; Frejinger, Emma; Vidal, Thibaut				Sadana, Utsav/GRJ-0617-2022; Vidal, Thibaut/M-3271-2013; Delage, Erick/E-9396-2012						A Survey of Contextual Optimization Methods for Decision Making under Uncertainty								Arxiv											2	2;2024-02-02;https://www.arxiv.org/abs/2306.10374v2| 1;2023-06-17;https://www.arxiv.org/abs/2306.10374v1	arXiv:2306.10374			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 02 2024	2024	Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. Focusing on single and two-stage stochastic programming problems, this review article identifies three main frameworks for learning policies from data and discusses their strengths and limitations. We present the existing models and methods under a uniform notation and terminology and classify them according to the three main frameworks identified. Our objective with this survey is to both strengthen the general understanding of this active field of research and stimulate further theoretical and algorithmic advancements in integrating ML and stochastic programming.																																	2024-05-25	PPRN:73441176		
J	Morley, Caroline V.; Mukherjee, Sagnick; Marley, Mark S.; Fortney, Jonathan J.; Visscher, Channon; Lupu, Roxana; Gharib-Nezhad, Ehsan; Thorngren, Daniel; Freedman, Richard				Lupu, Roxana/P-9060-2014; Marley, Mark/I-4704-2013; Fortney, Jonathan/F-8813-2014						The Sonora Substellar Atmosphere Models. III. Diamondback: Atmospheric Properties, Spectra, and Evolution for Warm Cloudy Substellar Objects								Arxiv											1	1;2024-02-01;https://www.arxiv.org/abs/2402.00758v1	arXiv:2402.00758			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Feb 01 2024	2024	We present a new grid of cloudy atmosphere and evolution models for substellar objects. These models include the effect of refractory cloud species, including silicate clouds, on the spectra and evolution. We include effective temperatures from 900 to 2400 K and surface gravities from log g=3.5– 5.5, appropriate for a broad range of objects with masses between 1 and 84 MJ. Model pressure– temperature structures are calculated assuming radiative–convective and chemical equilibrium. We consider the effect of both clouds and metallicity on the atmospheric structure, resulting spectra, and thermal evolution of substellar worlds. We parameterize clouds using the Ackerman & Marley (2001) cloud model, including cloud parameter fsed values from 1–8; we include three metallicities (−0.5, 0.0, and +0.5). Refractory clouds and metallicity both alter the evolution of substellar objects, changing the inferred temperature at a given age by up to 100–200 K. We compare to the observed photometry of brown dwarfs, finding broad agreement with the measured photometry. We publish the spectra, evolution, and other data products online with open access.																																	2024-05-25	PPRN:87455070		
J	Zhong, Zihan; Tang, Zhiqiang; He, Tong; Fang, Haoyang; Yuan, Chun				Yuan, Chun/JMD-0079-2023; Tang, Zhiqiang/AGN-6546-2022						Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model								Arxiv											1	1;2024-01-31;https://www.arxiv.org/abs/2401.17868v1	arXiv:2401.17868			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 31 2024	2024	The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.																																	2024-05-25	PPRN:87436061		
J	Hassid, Michael; Remez, Tal; Nguyen, Tu Anh; Gat, Itai; Conneau, Alexis; Kreuk, Felix; Copet, Jade; Defossez, Alexandre; Synnaeve, Gabriel; Dupoux, Emmanuel; Schwartz, Roy; Adi, Yossi				Adi, Yossi/HLG-5748-2023						Textually Pretrained Speech Language Models								Arxiv											3	3;2024-01-30;https://www.arxiv.org/abs/2305.13009v3| 2;2023-11-02;https://www.arxiv.org/abs/2305.13009v2| 1;2023-05-22;https://www.arxiv.org/abs/2305.13009v1	arXiv:2305.13009			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Jan 30 2024	2024	Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm -start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold -start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better -performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available.																																	2024-05-25	PPRN:70809321		
J	Shekhar, Shivanshu; Dubey, Tanishq; Mukherjee, Koyel; Saxena, Apoorv; Tyagi, Atharv; Kotla, Nishanth				MUKHERJEE, KOYEL/LDF-1944-2024						Towards Optimizing the Costs of LLM Usage								Arxiv											1	1;2024-01-29;https://www.arxiv.org/abs/2402.01742v1	arXiv:2402.01742			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sentence simplification model for reducing the number of tokens in a controlled manner. Additionally, we propose several deterministic heuristics for reducing tokens in a quality aware manner, and study the related optimization problem of applying the heuristics optimizing the quality and cost trade-off. We perform extensive empirical validation of our methods on not only enterprise datasets but also on open-source datasets, annotated by us, and show that we perform much better compared to closest baselines. Our methods reduce costs by 40%- 90% while improving quality by 4%-7%. We will release the annotated open source datasets to the community for further research and exploration.																																	2024-05-25	PPRN:87517676		
J	Zhang, Dingyuan; Liang, Dingkang; Yang, Hongcheng; Zou, Zhikang; Ye, Xiaoqing; Liu, Zhe; Bai, Xiang										SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model								Arxiv											2	2;2024-01-29;https://www.arxiv.org/abs/2306.02245v2| 1;2023-06-04;https://www.arxiv.org/abs/2306.02245v1	arXiv:2306.02245			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 29 2024	2024	With the development of large language models, many remarkable linguistic systems like ChatGPT have thrived and achieved astonishing success on many tasks, showing the incredible power of foundation models. In the spirit of unleashing the capability of foundation models on vision tasks, the Segment Anything Model (SAM), a vision foundation model for image segmentation, has been proposed recently and presents strong zero-shot ability on many downstream 2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be explored, especially 3D object detection. With this inspiration, we explore adapting the zero-shot ability of SAM to 3D object detection in this paper. We propose a SAM-powered BEV processing pipeline to detect objects and get promising results on the large-scale Waymo open dataset. As an early attempt, our method takes a step toward 3D object detection with vision foundation models and presents the opportunity to unleash their power on 3D vision tasks. 																																	2024-05-25	PPRN:72853641		
J	Blanchet, Jose; Li, Jiajin; Lin, Sirui; Zhang, Xuhui										Distributionally Robust Optimization and Robust Statistics								Arxiv											1	1;2024-01-26;https://www.arxiv.org/abs/2401.14655v1	arXiv:2401.14655			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 26 2024	2024	We review distributionally robust optimization (DRO), a principled approach for constructing statistical estimators that hedge against the impact of deviations in the expected loss between the training and deployment environments. Many well-known estimators in statistics and machine learning (e.g. AdaBoost, LASSO, ridge regression, dropout training, etc.) are distributionally robust in a precise sense. We hope that by discussing the DRO interpretation of well-known estimators, statisticians who may not be too familiar with DRO may find a way to access the DRO literature through the bridge between classical results and their DRO equivalent formulation. On the other hand, the topic of robustness in statistics has a rich tradition associated with removing the impact of contamination. Thus, another objective of this paper is to clarify the difference between DRO and classical statistical robustness. As we will see, these are two fundamentally different philosophies leading to completely different types of estimators. In DRO, the statistician hedges against an environment shift that occurs after the decision is made; thus DRO estimators tend to be pessimistic in an adversarial setting, leading to a min-max type formulation. In classical robust statistics, the statistician seeks to correct contamination that occurred before a decision is made; thus robust statistical estimators tend to be optimistic leading to a min-min type formulation.																																	2024-02-14	PPRN:87372842		
J	Jiang, Hanwen; Jiang, Zhenyu; Grauman, Kristen; Zhu, Yuke				zy, j/JYP-2387-2024						Few-View Object Reconstruction with Unknown Categories and Camera Poses								Arxiv											2	2;2024-01-25;https://www.arxiv.org/abs/2212.04492v3| 1;2023-09-12;https://www.arxiv.org/abs/2212.04492v2	arXiv:2212.04492			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	While object reconstruction has made great strides in recent years, current methods typically require densely captured images and/or known camera poses, and generalize poorly to novel object categories. To step toward object reconstruction in the wild, this work explores reconstructing general real-world objects from a few images without known camera poses or object categories. The crux of our work is solving two fundamental 3D vision problems — shape reconstruction and pose estimation — in a unified approach. Our approach captures the synergies of these two problems: reliable camera pose estimation gives rise to accurate shape reconstruction, and the accurate reconstruction, in turn, induces robust correlations between different views and facilitates pose estimation. Our method FORGE predicts 3D features from each view and leverages them in conjunction with the input images to establish cross-view correlations for estimating relative camera poses. The 3D features are then transformed by the estimated poses into a shared space and are fused into a neural radiance field. The reconstruction results are rendered by volume rendering techniques, enabling us to train the model without 3D shape ground-truth. Our experiments on both real and synthetic datasets, as well as in the wild images, show that FORGE reliably reconstructs objects from five views. Our pose estimation method outperforms existing ones by a large margin. The reconstruction results under predicted poses are comparable to the ones using ground-truth poses. And the performance on novel testing categories matches the results on categories seen during training.																																	2024-02-14	PPRN:85008397		
J	Liu, Lei; Yu, Shuo; Wang, Runze; Ma, Zhenxun; Shen, Yanming				Wang, Runze/OUJ-3750-2025						How Can Large Language Models Understand Spatial-Temporal Data?								Arxiv											1	1;2024-01-25;https://www.arxiv.org/abs/2401.14192v1	arXiv:2401.14192			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 25 2024	2024	While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diverse spatial-temporal benchmark datasets show that STG-LLM successfully unlocks LLM potential for spatial-temporal forecasting. Remarkably, our approach achieves competitive performance on par with dedicated SOTA methods.																																	2024-05-25	PPRN:87335158		
J	Jeong, Cheonsu				Jeong, Cheonsu/JQX-2553-2023						Fine-tuning and Utilization Methods of Domain-specific LLMs								Arxiv											2	2;2024-01-24;https://www.arxiv.org/abs/2401.02981v2| 1;2024-01-01;https://www.arxiv.org/abs/2401.02981v1	arXiv:2401.02981			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jan 24 2024	2024	Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance.   In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extraction, and customer service enhancement, are exemplified. The study explores the potential of LLMs in the financial domain, identifies limitations, and proposes directions for improvement, contributing valuable insights for future research. Ultimately, it advances natural language processing technology in business, suggesting proactive LLM utilization in financial services across industries.																																	2024-05-25	PPRN:87015322		
J	Gao, Jie; Guo, Yuchen; Lim, Gionnieve; Zhang, Tianqin; Zhang, Zheng; Li, Toby Jia-Jun; Perrault, Simon Tangi				Guo, Yuchen/GWN-1482-2022						CollabCoder: A Lower-barrier, Rigorous Workflow for Inductive Collaborative Qualitative Analysis with Large Language Models								Arxiv											2	2;2024-01-22;https://www.arxiv.org/abs/2304.07366v4| 1;2023-04-14;https://www.arxiv.org/abs/2304.07366v2	arXiv:2304.07366			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jan 22 2024	2024	Collaborative Qualitative Analysis (CQA) can enhance qualitative analysis rigor and depth by incorporating varied viewpoints. Nevertheless, ensuring a rigorous CQA procedure itself can be both demanding and costly. To lower this bar, we take a theoretical perspective to design the CollabCoder workflow, that integrates Large Language Models (LLMs) into key inductive CQA stages: independent open coding, iterative discussions, and final codebook creation. In the open coding phase, CollabCoder offers AI-generated code suggestions and records decision-making data. During discussions, it promotes mutual understanding by sharing this data within the coding team and using quantitative metrics to identify coding (dis)agreements, aiding in consensus-building. In the code grouping stage, CollabCoder provides primary code group suggestions, lightening the cognitive load of finalizing the codebook. A 16-user evaluation confirmed the effectiveness of CollabCoder, demonstrating its advantages over existing software and providing empirical insights into the role of LLMs in the CQA practice.																																	2024-05-25	PPRN:64389504		
J	Niedermayr, Simon; Stumpfegger, Josef; Westermann, Ruediger				Niedermayr, Simon/OGP-3281-2025						Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis								Arxiv											2	2;2024-01-22;https://www.arxiv.org/abs/2401.02436v2| 1;2023-11-17;https://www.arxiv.org/abs/2401.02436v1	arXiv:2401.02436			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 22 2024	2024	Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets. Making such representations suitable for applications like network streaming and rendering on low-power devices requires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to 31× on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to 4× higher framerates than reported via an optimized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach.																																	2024-05-25	PPRN:86996605		
J	Geng, Saibo; Josifoski, Martin; Peyrard, Maxime; West, Robert				West, Robert/B-5414-2009						Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning								Arxiv											5	5;2024-01-18;https://www.arxiv.org/abs/2305.13971v6| 4;2023-11-10;https://www.arxiv.org/abs/2305.13971v5| 3;2023-10-31;https://www.arxiv.org/abs/2305.13971v4| 2;2023-10-23;https://www.arxiv.org/abs/2305.13971v3| 1;2023-05-23;https://www.arxiv.org/abs/2305.13971v2	arXiv:2305.13971			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 18 2024	2024	Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. 																																	2024-02-03	PPRN:72716693		
J	Avara, Mark J.; Krolik, Julian H.; Campanelli, Manuela; Noble, Scott C.; Bowen, Dennis; Ryu, Taeho										Accretion onto a Supermassive Black Hole Binary Before Merger								Arxiv											2	2;2024-01-16;https://www.arxiv.org/abs/2305.18538v2| 1;2023-05-29;https://www.arxiv.org/abs/2305.18538v1	arXiv:2305.18538			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 16 2024	2024	While supermassive binary black holes (SMBBHs) inspiral toward merger they may also experience significant accretion of matter from a surrounding disk. To study the dynamics of this system requires simultaneously describing the evolving spacetime and the magnetized plasma. We present the first relativistic calculation simulating two equal-mass, non-spinning black holes as they inspiral from a 20M (G = c = 1) initial separation almost to merger, ≃ 9M (M =total binary mass). Our dynamical results imply important observational consequences: for instance, the accretion rate M˙ onto the black holes first decreases and then reaches a plateau, dropping by only a factor of ∼ 3 despite the rapid inspiral. An estimated bolometric light curve follows the same profile, suggesting some merging SMBBHs may be significantly luminous past the predicted decoupling from the circumbinary disk. The minidisks through which the accretion reaches the black holes are very non-standard: Reynolds, not Maxwell, stresses dominate, and they oscillate between two distinct states. In one part of the cycle, “sloshing” streams transfer mass from one minidisk to the other through the L1 point at a rate ∼ 0.1× the accretion rate, carrying kinetic energy at a rate that can be as large as the peak minidisk bolometric luminosity. We also discover that episodic accretion drives minidisks with time-varying tilts with respect to the orbital plane. The accretion cycles, energy dissipated by sloshing material, and variable inclination to the observer all contribute to unique cyclical behavior in the light curves of late-time inspiraling SMBBHs. The unsigned poloidal magnetic flux on the black hole event horizon is roughly constant at a dimensionless level ϕ ∼ 2 − 3, but doubles just before merger; if the black holes had significant spin, this flux indicates the potential for powerful jets with variability driven by binary dynamics, another prediction of potentially unique EM signatures. This simulation is the first to employ our multipatch infrastructure PATCHWORKMHD, decreasing computational expense to ∼ 3% of conventional single-grid methods’ cost.																																	2024-02-02	PPRN:72764083		
J	Barnett, Scott; Kurniawan, Stefanus; Thudumu, Srikanth; Brannelly, Zach; Abdelrazek, Mohamed				Thudumu, Srikanth/AAE-8746-2021						Seven Failure Points When Engineering a Retrieval Augmented Generation System								Arxiv											2	2;2024-01-11;https://www.arxiv.org/abs/2401.05856v1| 1;2024-01-11;https://www.arxiv.org/abs/2401.05856v1	arXiv:2401.05856			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 11 2024	2024	Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.																																	2024-02-19	PPRN:87124490		
J	Fei, Zhengcong; Fan, Mingyuan; Huang, Junshi										A-JEPA: Joint-Embedding Predictive Architecture Can Listen								Arxiv											2	2;2024-01-11;https://www.arxiv.org/abs/2311.15830v3| 1;2023-11-28;https://www.arxiv.org/abs/2311.15830v2	arXiv:2311.15830			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Jan 11 2024	2024	This paper presents that the masked-modeling principle driving the success of large foundational vision models can be effectively applied to audio by making predictions in a latent space. We introduce Audio-based Joint-Embedding Predictive Architecture (A-JEPA), a simple extension method for self-supervised learning from the audio spectrum. Following the design of I-JEPA, our A-JEPA encodes visible audio spectrogram patches with a curriculum masking strategy via context encoder, and predicts the representations of regions sampled at well-designed locations. The target representations of those regions are extracted by the exponential moving average of context encoder, emph{i.e.}, target encoder, on the whole spectrogram. We find it beneficial to transfer random block masking into time-frequency aware masking in a curriculum manner, considering the complexity of highly correlated in local time and frequency in audio spectrograms. To enhance contextual semantic understanding and robustness, we fine-tune the encoder with a regularized masking on target datasets, instead of input dropping or zero. Empirically, when built with Vision Transformers structure, we find A-JEPA to be highly scalable and sets new state-of-the-art performance on multiple audio and speech classification tasks, outperforming other recent models that use externally supervised pre-training.																																	2024-05-25	PPRN:86311029		
J	Fan, Chao; Hou, Saihui; Huang, Yongzhen; Yu, Shiqi				Yu, Shiqi/GZM-1998-2022						Exploring Deep Models for Practical Gait Recognition								Arxiv											2	2;2024-01-10;https://www.arxiv.org/abs/2303.03301v3| 1;2023-03-06;https://www.arxiv.org/abs/2303.03301v2	arXiv:2303.03301			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Jan 10 2024	2024	Gait recognition is a rapidly advancing vision technique for person identification from a distance. Prior studies predominantly employed relatively shallow networks to extract subtle gait features, achieving impressive successes in constrained settings. Nevertheless, experiments revealed that existing methods mostly produce unsatisfactory results when applied to newly released real-world gait datasets. This paper presents a unified perspective to explore how to construct deep models for state-of-the-art outdoor gait recognition, including the classical CNN-based and emerging Transformer-based architectures. Specifically, we challenge the stereotype of shallow gait models and demonstrate the superiority of explicit temporal modeling and deep transformer structure for discriminative gait representation learning. Consequently, the proposed CNN-based DeepGaitV2 series and Transformer-based SwinGait series exhibit significant performance improvements on Gait3D and GREW. As for the constrained gait datasets, the DeepGaitV2 series also reaches a new state-of-the-art in most cases, convincingly showing its practicality and generality. The source code is available at https://github.com/ShiqiYu/OpenGait.																																	2024-05-25	PPRN:43522974		
J	Jiang, Ruichen; Mokhtari, Aryan				Mokhtari, Aryan/LBI-3300-2024; Jiang, Ruichen/LDF-4465-2024						Generalized Optimistic Methods for Convex-Concave Saddle Point Problems								Arxiv											2	2;2024-01-10;https://www.arxiv.org/abs/2202.09674v2| 1;2022-02-19;https://www.arxiv.org/abs/2202.09674v1	arXiv:2202.09674			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 10 2024	2024	The optimistic gradient method has seen increasing popularity as an efficient first-order method for solving convex-concave saddle point problems. To analyze its iteration complexity, a recent work [MOP20b] proposed an interesting perspective that interprets the optimistic gradient method as an approximation to the proximal point method. In this paper, we follow this approach and distill the underlying idea of optimism to propose a generalized optimistic method, which encompasses the optimistic gradient method as a special case. Our general framework can handle constrained saddle point problems with composite objective functions and can work with arbitrary norms with compatible Bregman distances. Moreover, we also develop a backtracking line search scheme to select the step sizes without knowledge of the smoothness coefficients. We instantiate our method with first-order, second-order and higher-order oracles and give best-known global iteration complexity bounds. For our first-order method, we show that the averaged iterates converge at a rate of O(1/N) when the objective function is convex-concave, and it achieves linear convergence when the objective is further strongly-convex-strongly-concave. For our second-order and higher-order methods, under the additional assumption that the distance-generating function 2 has Lipschitz gradient, we prove a complexity bound of O(1/ϵ2/p+1) in the convex-concave setting and a complexity bound of O((LpDp-1/2 /µ)2/p+1 + log log 1/ϵ) in the strongly-convex-strongly-concave setting, where Lp (p ≥ 2) is the Lipschitz constant of the p-th-order derivative, µ is the strong convexity parameter, and D is the initial Bregman distance to the saddle point. Moreover, our line search scheme provably only requires a constant number of calls to a subproblem solver per iteration on average, making our first-order and second-order methods particularly amenable to implementation.																																	2024-01-31	PPRN:12070028		
J	Achtibat, Reduan; Dreyer, Maximilian; Eisenbraun, Ilona; Bosse, Sebastian; Wiegand, Thomas; Samek, Wojciech; Lapuschkin, Sebastian				Bosse, Sebastian/HNR-3792-2023; Samek, Wojciech/AAZ-2156-2021						From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation								Arxiv											1	1;2024-01-06;https://www.arxiv.org/abs/2206.03208v2	arXiv:2206.03208			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 06 2024	2024	The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the "where" and "what" questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, concept composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision making.																																	2024-01-22	PPRN:87030322		
J	Chen, Chang; Deng, Fei; Kawaguchi, Kenji; Gulcehre, Caglar; Ahn, Sungjin				Kawaguchi, Kenji/AAR-8297-2020; Ahn, Sungjin/LCD-8968-2024						Simple Hierarchical Planning with Diffusion								Arxiv											1	1;2024-01-05;https://www.arxiv.org/abs/2401.02644v1	arXiv:2401.02644			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 05 2024	2024	Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a "jumpy" planning strategy at the higher level, which allows it to have a larger receptive field but at a lower computational cost -- a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of training and planning speed compared to the non-hierarchical Diffuser as well as other hierarchical planning methods. Moreover, we explore our model's generalization capability, particularly on how our method improves generalization capabilities on compositional out-of-distribution tasks.																																	2024-05-25	PPRN:86997661		
J	Xue, Siqiao; Jiang, Caigao; Shi, Wenhui; Cheng, Fangyin; Chen, Keting; Yang, Hongjun; Zhang, Zhiping; He, Jianshan; Zhang, Hongyang; Wei, Ganglin; Zhao, Wang; Zhou, Fan; Qi, Danrui; Yi, Hong; Liu, Shaodong; Chen, Faqiang				Yi, honghu/KOZ-8248-2024; Shi, Wenhui/AAW-8090-2021						DB-GPT: Empowering Database Interactions with Private Large Language Models								Arxiv											1	1;2024-01-03;https://www.arxiv.org/abs/2312.17449v2	arXiv:2312.17449			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Jan 03 2024	2024	The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. Database technologies particularly have an important entanglement with LLMs as efficient and intuitive database interactions are paramount. In this paper, we present DB-GPT, a revolutionary and production-ready project that integrates LLMs with traditional database systems to enhance user experience and accessibility. DB-GPT is designed to understand natural language queries, provide context-aware responses, and generate complex SQL queries with high accuracy, making it an indispensable tool for users ranging from novice to expert. The core innovation in DB-GPT lies in its private LLM technology, which is fine-tuned on domain-specific corpora to maintain user privacy and ensure data security while offering the benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which includes a novel retrieval augmented generation (RAG) knowledge system, an adaptive learning mechanism to continuously improve performance based on user feedback and a service-oriented multi-model framework (SMMF) with powerful data-driven agents. Our extensive experiments and user studies confirm that DB-GPT represents a paradigm shift in database interactions, offering a more natural, efficient, and secure way to engage with data repositories. The paper concludes with a discussion of the implications of DB-GPT framework on the future of human-database interaction and outlines potential avenues for further enhancements and applications in the field. The project code is available at https://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by installing it with the instructions https://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute video at https://www.youtube.com/watch?v=KYs4nTDzEhk.																																	2024-01-11	PPRN:86943562		
J	Sun, Guangyu; Khalid, Umar; Mendieta, Matias; Wang, Pu; Chen, Chen				Yang, Taojiannan/HDN-5944-2022; Sun, Guangyu/HSH-4924-2023						Exploring Parameter-Efficient Fine-Tuning to Enable Foundation Models in Federated Learning								Arxiv											4	4;2024-12-24;https://www.arxiv.org/abs/2210.01708v5| 3;2024-10-23;https://www.arxiv.org/abs/2210.01708v4| 2;2024-04-03;https://www.arxiv.org/abs/2210.01708v3| 1;2022-11-23;https://www.arxiv.org/abs/2210.01708v2	arXiv:2210.01708			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 24 2024	2024	Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm ( e.g. , FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown to be effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters, known as the “Foundation Models.” In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fine-tuning in federated learning and thus introduce a new framework: FedPEFT. Specifically, we systemically evaluate the performance of FedPEFT across a variety of client stability, data distribution, and differential privacy settings. By only locally tuning and globally sharing a small portion of the model weights, significant reductions in the total communication overhead can be achieved while maintaining competitive or even better performance in a wide range of federated learning scenarios, providing insight into a new paradigm for practical and effective federated systems.																																	2025-02-05	PPRN:23119570		
J	Liu, Yuhan; Chen, Xiuying; Zhang, Xiaoqing; Gao, Xing; Zhang, Ji; Yan, Rui				Chen, Xiuying/ISU-7033-2023						From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News								Arxiv											2	2;2024-12-23;https://www.arxiv.org/abs/2403.09498v2| 1;2024-03-14;https://www.arxiv.org/abs/2403.09498v1	arXiv:2403.09498			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Dec 23 2024	2024	In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions. Our simulation results uncover patterns in fake news propagation related to topic relevance, and individual traits, aligning with real-world observations. Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications. Our study underscores the significant utility and potential of LLMs in combating fake news.																																	2025-02-02	PPRN:88140432		
J	Cheng, Ho Kei; Ishii, Masato; Hayakawa, Akio; Shibuya, Takashi; Schwing, Alexander; Mitsufuji, Yuki				Cheng, Rex/GWZ-5023-2022; Shibuya, Takashi/AAP-7115-2021						Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis								Arxiv											1	1;2024-12-19;https://www.arxiv.org/abs/2412.15322v1	arXiv:2412.15322			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 19 2024	2024	We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. 																																	2025-01-24	PPRN:120117895		
J	Ren, Richard; Basart, Steven; Khoja, Adam; Gatti, Alice; Phan, Long; Yin, Xuwang; Mazeika, Mantas; Pan, Alexander; Mukobi, Gabriel; Kim, Ryan H.; Fitz, Stephen; Hendrycks, Dan				Kim, Hyunjae/J-5811-2015						Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?								Arxiv											2	2;2024-12-19;https://www.arxiv.org/abs/2407.21792v2| 1;2024-07-31;https://www.arxiv.org/abs/2407.21792v1	arXiv:2407.21792			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 19 2024	2024	As artificial intelligence systems grow more powerful, there has been increasing interest in “AI safety” research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with both upstream model capabilities and training compute, potentially enabling “safetywashing”—where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.																																	2025-01-26	PPRN:91174498		
J	Lacroix, Nathan; Bourassa, Alexandre; Heras, Francisco J.H.; Zhang, Lei M.; Bausch, Johannes; Senior, Andrew W.; Edlich, Thomas; Shutty, Noah; Sivak, Volodymyr; Bengtsson, Andreas; Mcewen, Matt; Higgott, Oscar; Kafri, Dvir; Claes, Jahan; Morvan, Alexis; Chen, Zijun; Zalcman, Adam; Madhuk, Sid; Acharya, Rajeev; Beni, Laleh Aghababaie; Aigeldinger, Georg; Alcaraz, Ross; Andersen, Trond I.; Ansmann, Markus; Arute, Frank; Arya, Kunal; Asfaw, Abraham; Atalaya, Juan; Babbush, Ryan; Ballard, Brian; Bardin, Joseph C.; Bilmes, Alexander; Blackwell, Sam; Bovaird, Jenna; Bowers, Dylan; Brill, Leon; Broughton, Michael; Browne, David A.; Buchea, Brett; Buckley, Bob B.; Burger, Tim; Burkett, Brian; Bushnell, Nicholas; Cabrera, Anthony; Campero, Juan; Chang, Hung-Shen; Chiaro, Ben; Chih, Liang-Ying; Cleland, Agnetta Y.; Cogan, Josh; Collins, Roberto; Conner, Paul; Courtney, William; Crook, Alexander L.; Curtin, Ben; Das, Sayan; Demura, Sean; Lorenzo, Laura De; Paolo, Agustin Di; Donohoe, Paul; Drozdov, Ilya; Dunsworth, Andrew; Eickbusch, Alec; Elbag, Aviv Moshe; Elzouka, Mahmoud; Erickson, Catherine; Ferreira, Vinicius S.; Burgos, Leslie Flores; Forati, Ebrahim; Fowler, Austin G.; Foxen, Brooks; Ganjam, Suhas; Garcia, Gonzalo; Gasca, Robert; Genois, Elie; Giang, William; Gilboa, Dar; Gosula, Raja; Dau, Alejandro Grajales; Graumann, Dietrich; Greene, Alex; Gross, Jonathan A.; Ha, Tan; Habegger, Steve; Hansen, Monica; Harrigan, Matthew P.; Harrington, Sean D.; Heslin, Stephen; Heu, Paula; Hiltermann, Reno; Hilton, Jeremy; Hong, Sabrina; Huang, Hsin-Yuan; Huff, Ashley; Huggins, William J.; Jeffrey, Evan; Jiang, Zhang; Jin, Xiaoxuan; Joshi, Chaitali; Juhas, Pavol; Kabel, Andreas; Kang, Hui; Karamlou, Amir H.; Kechedzhi, Kostyantyn; Khaire, Trupti; Khattar, Tanuj; Khezri, Mostafa; Kim, Seon; Klimov, Paul V.; Kobrin, Bryce; Korotkov, Alexander N.; Kostritsa, Fedor; Kreikebaum, John Mark; Kurilovich, Vladislav D.; Landhuis, David; Lange-Dei, Tiano; Langley, Brandon W.; Laptev, Pavel; Lau, Kim-Ming; Ledford, Justin; Lee, Kenny; Lester, Brian J.; Guevel, Loick Le; Li, Wing Yan; Li, Yin; Lill, Alexander T.; Livingston, William P.; Locharla, Aditya; Lucero, Erik; Lundahl, Daniel; Lunt, Aaron; Maloney, Ashley; Mandra, Salvatore; Martin, Leigh S.; Martin, Orion; Maxfield, Cameron; Mcclean, Jarrod R.; Meeks, Seneca; Megrant, Anthony; Miao, Kevin C.; Molavi, Reza; Molina, Sebastian; Montazeri, Shirin; Movassagh, Ramis; Neill, Charles; Newman, Michael; Nguyen, Anthony; Nguyen, Murray; Ni, Chia-Hung; Niu, Murphy Y.; Oas, Logan; Oliver, William D.; Orosco, Raymond; Ottosson, Kristoffer; Pizzuto, Alex; Potter, Rebecca; Pritchard, Orion; Quintana, Chris; Ramachandran, Ganesh; Reagor, Matthew J.; Resnick, Rachel; Rhodes, David M.; Roberts, Gabrielle; Rosenberg, Eliott; Rosenfeld, Emma; Rossi, Elizabeth; Roushan, Pedram; Sankaragomathi, Kannan; Schurkus, Henry F.; Shearn, Michael J.; Shorter, Aaron; Shvarts, Vladimir; Small, Spencer; Clarke Smith, W.; Springer, Sofia; Sterling, George; Suchard, Jordan; Szasz, Aaron; Sztein, Alex; Thor, Douglas; Tomita, Eifu; Torres, Alfredo; Mert Torunbalci, M.; Vaishnav, Abeer; Vargas, Justin; Vdovichev, Sergey; Vidal, Guifre; Heidweiller, Catherine Vollgraff; Waltman, Steven; Waltz, Jonathan; Wang, Shannon X.; Ware, Brayden; Weidel, Travis; White, Theodore; Wong, Kristi; Woo, Bryan W.K.; Woodson, Maddy; Xing, Cheng; Jamie Yao, Z.; Yeh, Ping; Ying, Bicheng; Yoo, Juhwan; Yosri, Noureldin; Young, Grayson; Zhang, Yaxing; Zhu, Ningfeng; Zobrist, Nicholas; Neven, Hartmut; Kohli, Pushmeet; Davies, Alex; Boixo, Sergio; Kelly, Julian; Jones, Cody; Gidney, Craig; Satzinger, Kevin J.				Harrigan, Matthew/JCE-2375-2023; Roushan, Pedram/ABT-9302-2022; Bengtsson, Andreas/AAX-3421-2020; Gross, Jonathan/M-5661-2014; Heras, Francisco/I-3895-2016; Browne, David/B-4831-2013; Juhas, Pavol/Q-7225-2019; Khaire, Trupti/G-6342-2016; Higgott, Oscar/AAM-2892-2020; Szasz, Aaron/JPK-7383-2023; Fowler, Austin/J-4961-2012; Chang, Hung-Shen/AGZ-8235-2022; Laptev, Pavel/AGH-9668-2022; Vdovichev, Sergei/U-5446-2019; Rosenberg, Eliott/MIU-3161-2025						Scaling and logic in the color code on a superconducting quantum processor								Arxiv											1	1;2024-12-18;https://www.arxiv.org/abs/2412.14256v1	arXiv:2412.14256			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 18 2024	2024	Quantum error correction [1–6] is essential for bridging the gap between the error rates of physical devices and the extremely low logical error rates required for quantum algorithms. Recent error-correction demonstrations on superconducting processors [7–13] have focused primarily on the surface code [14], which offers a high error threshold but poses limitations for logical operations. In contrast, the color code [15] enables much more efficient logic, although it requires more complex stabilizer measurements and decoding techniques. Measuring these stabilizers in planar architectures such as superconducting qubits is challenging, and so far, realizations of color codes [16–24] have not addressed performance scaling with code size on any platform. Here, we present a comprehensive demonstration of the color code on a superconducting processor [12], achieving logical error suppression and performing logical operations. Scaling the code distance from three to five suppresses logical errors by a factor of Λ3/5 = 1.56(4). Simulations indicate this performance is below the threshold of the color code, and furthermore that the color code may be more efficient than the surface code with modest device improvements. Using logical randomized benchmarking [25], we find that transversal Clifford gates add an error of only 0.0027(3), which is substantially less than the error of an idling error correction cycle. We inject magic states [26], a key resource for universal computation, achieving fidelities exceeding 99 % with post-selection (retaining about 75 % of the data). Finally, we successfully teleport logical states between distance-three color codes using lattice surgery [27], with teleported state fidelities between 86.5(1) % and 90.7(1) %. This work establishes the color code as a compelling research direction to realize fault-tolerant quantum computation on superconducting processors in the near future.																																	2025-01-24	PPRN:120064478		
J	Sun, Qi; Pickett, Marc; Nain, Aakash Kumar; Jones, Llion										Transformer Layers as Painters								Arxiv											1	1;2024-12-18;https://www.arxiv.org/abs/2407.09298v3	arXiv:2407.09298			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 18 2024	2024	Despite their nearly universal adoption for large language models, the internal workings of transformers are not well understood. We aim to better understand the impact of removing or reorganizing information throughout the layers of a pretrained transformer. Such an understanding could both yield better usage of existing models as well as to make architectural improvements to produce new variants. We present a series of empirical studies on frozen models that show that the lower and final layers of pretrained transformers differ from middle layers, but that middle layers have a surprising amount of uniformity. We further show that some classes of problems have robustness to skipping layers, running the layers in an order different from how they were trained, or running the layers in parallel. Our observations suggest that even frozen pretrained models may gracefully trade accuracy for latency by skipping layers or running layers in parallel.																																	2025-08-07	PPRN:123174639		
J	Katz, Michael L.; Karnesis, Nikolaos; Korsakova, Natalia; Gair, Jonathan R.; Stergioulas, Nikolaos				Karnesis, Nikolaos/GYJ-6909-2022						An efficient GPU-accelerated multi-source global fit pipeline for LISA data analysis								Arxiv											2	2;2024-12-11;https://www.arxiv.org/abs/2405.04690v2| 1;2024-05-07;https://www.arxiv.org/abs/2405.04690v1	arXiv:2405.04690			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 11 2024	2024	The large-scale analysis task of deciphering gravitational wave signals in the LISA data stream will be difficult, requiring a large amount of computational resources and extensive development of computational methods. Its high dimensionality, multiple model types, and complicated noise profile require a global fit to all parameters and input models simultaneously. In this work, we detail our global fit algorithm, called “Erebor,” designed to accomplish this challenging task. It is capable of analysing current state-of-the-art datasets and then growing into the future as more pieces of the pipeline are completed and added. We describe our pipeline strategy, the algorithmic setup, and the results from our analysis of the LDC2A Sangria dataset, which contains Massive Black Hole Binaries, compact Galactic Binaries, and a parameterized noise spectrum whose parameters are unknown to the user. The Ereb or algorithm includes three unique and very useful contributions: GPU acceleration for enhanced computational efficiency; ensemble MCMC sampling with multiple MCMC walkers per temperature for better mixing and parallelized sample creation; and special online updates to reversible-jump (or trans-dimensional) sampling distributions to ensure sampler mixing and accurate initial estimates for detectable sources in the data. We recover posterior distributions for all 15 (6) of the injected MBHBs in the LDC2A training (hidden) dataset. We catalog ∼ 12000 Galactic Binaries (∼ 8000 as high confidence detections) for both the training and hidden datasets. All of the sources and their posterior distributions are provided in publicly available catalogs.																																	2025-01-19	PPRN:88976882		
J	Decoppet, Thibault D.; Yu, Matthew										Fiber 2-Functors and Tambara-Yamagami Fusion 2-Categories								Arxiv											2	2;2024-12-10;https://www.arxiv.org/abs/2306.08117v2| 1;2023-06-13;https://www.arxiv.org/abs/2306.08117v1	arXiv:2306.08117			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Dec 10 2024	2024	We introduce group-theoretical fusion 2-categories, a categorification of the notion of a group-theoretical fusion 1-category. Physically speaking, such fusion 2-categories arise by gauging subgroups of a global symmetry. We show that group-theoretical fusion 2-categories are completely characterized by the property that the braided fusion 1-category of endomorphisms of the monoidal unit is Tannakian. Then, we describe the underlying finite semisimple 2-category of group-theoretical fusion 2-categories, and, more generally, of certain 2-categories of bimodules. We also partially describe the fusion rules of group-theoretical fusion 2-categories. Using our previous results, we classify fusion 2-categories admitting a fiber 2functor. Next, we study fusion 2-categories with a Tambara-Yamagami defect, that is Z/2-graded fusion 2-categories whose non-trivially graded factor is 2Vect. We classify these fusion 2-categories, and examine more closely the more restrictive notion of Tambara-Yamagami fusion 2-categories. Throughout, we give many examples to illustrate our various results.																																	2025-01-18	PPRN:73384533		
J	Gao, Zhaolin; Chang, Jonathan D.; Zhan, Wenhao; Oertell, Owen; Swamy, Gokul; Brantley, Kiante; Joachims, Thorsten; Bagnell, J.Andrew; Lee, Jason D.; Sun, Wen				Zhan, Wenhao/MCX-3749-2025						REBEL: Reinforcement Learning via Regressing Relative Rewards								Arxiv											4	4;2024-12-10;https://www.arxiv.org/abs/2404.16767v4| 3;2024-09-01;https://www.arxiv.org/abs/2404.16767v3| 2;2024-05-29;https://www.arxiv.org/abs/2404.16767v2| 1;2024-04-25;https://www.arxiv.org/abs/2404.16767v1	arXiv:2404.16767			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 10 2024	2024	While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative reward between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.																																	2025-08-07	PPRN:88651074		
J	Wu, Taiqiang; Tao, Chaofan; Wang, Jiahao; Yang, Runming; Zhao, Zhe; Wong, Ngai				Wang, Jiahao/ABA-3280-2021						Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models								Arxiv											4	4;2024-12-08;https://www.arxiv.org/abs/2404.02657v4| 3;2024-09-18;https://www.arxiv.org/abs/2404.02657v3| 2;2024-06-16;https://www.arxiv.org/abs/2404.02657v2| 1;2024-04-03;https://www.arxiv.org/abs/2404.02657v1	arXiv:2404.02657			http://creativecommons.org/licenses/by-nc-nd/4.0/	http://creativecommons.org/licenses/by-nc-nd/4.0/			preprint	Dec 08 2024	2024	Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL. Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses.																																	2025-01-17	PPRN:88389618		
J	Collura, Mario; Nardis, Jacopo De; Alba, Vincenzo; Lami, Guglielmo				Lami, Guglielmo/MBW-1156-2025; Alba, Vincenzo/ABA-4538-2020						The quantum magic of fermionic Gaussian states								Arxiv											1	1;2024-12-06;https://www.arxiv.org/abs/2412.05367v1	arXiv:2412.05367			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 06 2024	2024	We introduce an efficient method to quantify nonstabilizerness in fermionic Gaussian states, overcoming the long-standing challenge posed by their extensive entanglement. Using a perfect sampling scheme based on an underlying determinantal point process, we compute the Stabilizer Rényi Entropies (SREs) for systems with hundreds of qubits. Benchmarking on random Gaussian states with and without particle conservation, we reveal an extensive leading behavior equal to that of Haar random states, with logarithmic subleading corrections. We support these findings with analytical calculations for a set of related quantities, the participation entropies in the computational (or Fock) basis, for which we derive an exact formula. Applying the sampling algorithm to a two-dimensional free-fermionic topological model, we uncover a sharp transition in magic at the topological phase boundary, highlighting the power of our approach in exploring different phases of quantum many-body systems, even in higher dimensions.																																	2025-01-17	PPRN:119792356		
J	Xia, Renqiu; Peng, Haoyang; Ye, Hancheng; Li, Mingsheng; Yan, Xiangchao; Ye, Peng; Shi, Botian; Qiao, Yu; Yan, Junchi; Zhang, Bo				Shi, Botian/HTT-0363-2023; Qiao, Yu/ABD-5787-2021; Yan, Junchi/ADK-0658-2022; Zhang, Bo/ABF-8476-2021						StructChart: On the Schema, Metric, and Augmentation for Visual Chart Understanding								Arxiv											5	5;2024-12-04;https://www.arxiv.org/abs/2309.11268v5| 4;2024-02-19;https://www.arxiv.org/abs/2309.11268v4| 3;2024-02-01;https://www.arxiv.org/abs/2309.11268v3| 2;2023-09-25;https://www.arxiv.org/abs/2309.11268v2| 1;2023-09-20;https://www.arxiv.org/abs/2309.11268v1	arXiv:2309.11268			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 04 2024	2024	Charts are common in literature across various scientific fields, conveying rich information easily accessible to readers. Current chart-related tasks focus on either chart perception that extracts information from the visual charts, or chart reasoning given the extracted data, e.g. in a tabular form. In this paper, we introduce StructChart, a novel framework that leverages Structured Triplet Representations (STR) to achieve a unified and label-efficient approach to chart perception and reasoning tasks, which is generally applicable to different downstream tasks, beyond the question-answering task as specifically studied in peer works. Specifically, StructChart first reformulates the chart data from the tubular form (linearized CSV) to STR, which can friendlily reduce the task gap between chart perception and reasoning. We then propose a Structuring Chart-oriented Representation Metric (SCRM) to quantitatively evaluate the chart perception task performance. To augment the training, we further explore the potential of Large Language Models (LLMs) to enhance the diversity in both chart visual style and statistical information. Extensive experiments on various chart-related tasks demonstrate the effectiveness and potential of a unified chart perception-reasoning paradigm to push the frontier of chart understanding.																																	2025-01-15	PPRN:85076734		
J	Fu, Yao; Kim, Dong-Ki; Kim, Jaekyeom; Sohn, Sungryull; Logeswaran, Lajanugen; Bae, Kyunghoon; Lee, Honglak				Kim, Jaekyeom/LIC-8900-2024; Sohn, Sungryull/GQI-4606-2022						AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents								Arxiv											2	2;2024-12-03;https://www.arxiv.org/abs/2403.08978v2| 1;2024-03-13;https://www.arxiv.org/abs/2403.08978v1	arXiv:2403.08978			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 03 2024	2024	Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AUTO GUIDE , which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent’s current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AU-TOGUIDE significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.																																	2025-01-15	PPRN:88140947		
J	Zhang, Cong; Lewandowski, Jerzy; Ma, Yongge; Yang, Jinsong				Zhang, Cong/ONJ-4240-2025						Black holes and covariance in effective quantum gravity: A solution without Cauchy horizons								Arxiv											1	1;2024-12-03;https://www.arxiv.org/abs/2412.02487v1	arXiv:2412.02487			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Dec 03 2024	2024	As a continuation of our previous work addressing general covariance in effective quantum gravity models within the Hamiltonian framework, this study presents explicit derivations of the covariance equation proposed earlier. By solving this equation, a new Hamiltonian constraint is obtained, incorporating free functions that can account for quantum gravity effects. Specifying these functions allows for an analysis of the resulting spacetime structure. Remarkably, in this model, the classical singularity is replaced by a region where the metric asymptotically approaches a Schwarzschild-de Sitter one with negative mass. Unlike previously studied spacetime structures, this new quantum-corrected model avoids the presence of Cauchy horizons, potentially suggesting its stability under perturbations. Furthermore, this work establishes a foundation for exploring matter coupling and lays the groundwork for investigating the formation of quantum black holes in covariant effective models.																																	2025-01-15	PPRN:119685568		
J	Yu, Weihao; Yang, Zhengyuan; Ren, Lingfeng; Li, Linjie; Wang, Jianfeng; Lin, Kevin; Lin, Chung-Ching; Liu, Zicheng; Wang, Lijuan; Wang, Xinchao				李, 李林洁/JAD-1884-2023; Yang, Zhengyuan/AGQ-1232-2022; Lin, Kevin/JFS-1634-2023; Yu, Weihao/HJH-1824-2023						MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities								Arxiv											3	3;2024-12-01;https://www.arxiv.org/abs/2408.00765v2| 2;2024-08-01;https://www.arxiv.org/abs/2408.00765v1| 1;2024-08-01;https://www.arxiv.org/abs/2408.00765v1	arXiv:2408.00765			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Dec 01 2024	2024	MM-Vet, with open-ended vision-language questions targeting at evaluating integrated capabilities, has become one of the most popular benchmarks for large multimodal model evaluation. MM-Vet assesses six core vision-language (VL) capabilities: recognition, knowledge, spatial awareness, language generation, OCR, and math. However, its question format is restricted to single image-text pairs, lacking the interleaved image and text sequences prevalent in real-world scenarios. To address this limitation, we introduce MM-Vet v2, which includes a new VL capability called "image-text sequence understanding", evaluating models' ability to process VL sequences. Furthermore, we maintain the high quality of evaluation samples while further expanding the evaluation set size. Using MM-Vet v2 to benchmark large multimodal models, we found that Claude 3.5 Sonnet is the best model with a score of 71.8, slightly outperforming GPT-4o which scored 71.0. Among open-weight models, InternVL2-Llama3-76B leads with a score of 68.4. 																																	2025-01-11	PPRN:91240485		
J	Romanou, Angelika; Foroutan, Negar; Sotnikova, Anna; Chen, Zeming; Nelaturu, Sree Harsha; Singh, Shivalika; Maheshwary, Rishabh; Altomare, Micol; Haggag, Mohamed A.; Schlag, Imanol; Fadaee, Marzieh; Hooker, Sara; Bosselut, Antoine				Chen, ZM/GRY-3011-2022						INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge								Arxiv											1	1;2024-11-29;https://www.arxiv.org/abs/2411.19799v1	arXiv:2411.19799			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 29 2024	2024	The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.																																	2025-01-10	PPRN:119581226		
J	Gao, Jiaxuan; Xu, Shusheng; Ye, Wenjie; Liu, Weilin; He, Chuyi; Fu, Wei; Mei, Zhiyu; Wang, Guangju; Wu, Yi										ON DESIGNING EFFECTIVE RL REWARD AT TRAINING TIME FOR LLM REASONING								Arxiv											3	3;2024-11-27;https://www.arxiv.org/abs/2410.15115v3| 2;2024-10-25;https://www.arxiv.org/abs/2410.15115v2| 1;2024-10-19;https://www.arxiv.org/abs/2410.15115v1	arXiv:2410.15115			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 27 2024	2024	Reward models have been increasingly critical for improving the reasoning capability of LLMs. Existing research has shown that a well-trained reward model can substantially improve model performances at inference time via search or best-of-N votes. However, the potential of reward models during RL training time still remains largely under-explored. It is currently unclear whether these reward models can provide additional training signals to RL training that uses sparse success rewards, which verify the correctness of solutions. In this work, we evaluate popular reward models for RL training, including the Outcome-supervised Reward Model (ORM) and the Process-supervised Reward Model (PRM), and train a collection of LLMs for math problems using RL by combining these learned rewards with success rewards. Surprisingly, even though these learned reward models have strong inference-time performances, they may NOT help or even hurt RL training, producing worse performances than LLMs trained with the success reward only. Our analysis reveals that an LLM can receive high rewards from some of these reward models by repeating correct but unnecessary reasoning steps, leading to a severe reward hacking issue for RL training. Therefore, we introduce two novel reward refinement techniques, including Clipping and Delta. The key idea is to ensure the accumulative reward of any reasoning trajectory is upper-bounded to keep a learned reward model effective without being exploited. We evaluate our techniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH and GSM8K benchmarks, where both Clipping and Delta consistently stabilize RL training. Finally, we also demonstrate that with a carefully designed reward function, pure RL training without any additional supervised tuning can further improve all the evaluated LLMs, including the state-of-the-art 7B LLM Qwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.																																	2025-01-08	PPRN:118757429		
J	Ye, Zhen; Sun, Peiwen; Lei, Jiahe; Lin, Hongzhan; Tan, Xu; Dai, Zheqi; Kong, Qiuqiang; Chen, Jianyi; Pan, Jiahao; Liu, Qifeng; Guo, Yike; Xue, Wei				ye, zhen/JXL-5088-2024						Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model								Arxiv											3	3;2024-11-27;https://www.arxiv.org/abs/2408.17175v3| 2;2024-09-19;https://www.arxiv.org/abs/2408.17175v2| 1;2024-08-30;https://www.arxiv.org/abs/2408.17175v1	arXiv:2408.17175			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 27 2024	2024	Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. 																																	2025-01-10	PPRN:91633150		
J	Cao, Zhenbiao; Zheng, Yuanlei; Fan, Zhihao; Zhang, Xiaojin; Chen, Wei; Bai, Xiang				zhang, xiaojin/E-9639-2015						RSL-SQL: Robust Schema Linking in Text-to-SQL Generation								Arxiv											2	2;2024-11-26;https://www.arxiv.org/abs/2411.00073v2| 1;2024-10-31;https://www.arxiv.org/abs/2411.00073v1	arXiv:2411.00073			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 26 2024	2024	Text-to-SQL generation aims to translate natural language questions into SQL statements. In Text-to-SQL based on large language models, schema linking is a widely adopted strategy to streamline the input for LLMs by selecting only relevant schema elements, therefore reducing noise and computational overhead. However, schema linking faces risks that require caution, including the potential omission of necessary elements and disruption of database structural integrity. To address these challenges, we propose a novel framework called RSL-SQL that combines bidirectional schema linking, contextual information augmentation, binary selection strategy, and multi-turn self-correction. We improve the recall of pattern linking using forward and backward pruning methods, achieving a strict recall of 94% while reducing the number of input columns by 83%. Furthermore, it hedges the risk by voting between a full mode and a simplified mode enhanced with contextual information. Experiments on the BIRD and Spider benchmarks demonstrate that our approach achieves SOTA execution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on Spider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4 based Text-to-SQL systems when adopting DeepSeek (much cheaper) with same intact prompts. Extensive analysis and ablation studies confirm the effectiveness of each component in our framework. 																																	2025-01-08	PPRN:119007146		
J	Brahman, Faeze; Kumar, Sachin; Balachandran, Vidhisha; Dasigi, Pradeep; Pyatkin, Valentina; Ravichander, Abhilasha; Wiegreffe, Sarah; Dziri, Nouha; Chandu, Khyathi; Hessel, Jack; Tsvetkov, Yulia; Smith, Noah A.; Choi, Yejin; Hajishirzi, Hannaneh										The Art of Saying No: Contextual Noncompliance in Language Models								Arxiv											2	2;2024-11-22;https://www.arxiv.org/abs/2407.12043v2| 1;2024-07-02;https://www.arxiv.org/abs/2407.12043v1	arXiv:2407.12043			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 22 2024	2024	Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of "unsafe" queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.																																	2025-01-03	PPRN:90872539		
J	Liao, Zhanfeng; Xu, Yuelang; Li, Zhe; Li, Qijing; Zhou, Boyao; Bai, Ruifeng; Xu, Di; Zhang, Hongwen; Liu, Yebin				Liu, Yebin/L-7393-2019; Li, Qijing/GQY-7164-2022; Wang, Lizhen/JXL-5347-2024; Li, Zhefan/MIQ-7700-2025; Bai, Ruifeng/MGW-1405-2025						HHAvatar: Gaussian Head Avatar with Dynamic Hairs								Arxiv											3	3;2024-11-20;https://www.arxiv.org/abs/2312.03029v3| 2;2024-03-30;https://www.arxiv.org/abs/2312.03029v2| 1;2023-12-05;https://www.arxiv.org/abs/2312.03029v1	arXiv:2312.03029			http://creativecommons.org/licenses/by-sa/4.0/	http://creativecommons.org/licenses/by-sa/4.0/			preprint	Nov 20 2024	2024	Creating high-fidelity 3D head avatars has always been a research hotspot, but it remains a great challenge under lightweight sparse view setups. In this paper, we propose HHAvatar represented by controllable 3D Gaussians for high-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians to represent the appearance of the head, and then jointly optimize neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. To address the problem of dynamic hair modeling, we introduce a hybrid head model into our avatar representation based Gaussian Head Avatar and a training method that considers timing information and an occlusion perception module to model the non-rigid motion of hair. Experiments show that our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions and driving hairs reasonably with the motion of the head																																	2024-12-31	PPRN:86418258		
J	Canonne, Clement L.; Kamath, Gautam; Steinke, Thomas										The Discrete Gaussian for Differential Privacy								Arxiv											2	2;2024-11-18;https://www.arxiv.org/abs/2004.00010v6| 1;2020-03-31;https://www.arxiv.org/abs/2004.00010v5	arXiv:2004.00010			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 18 2024	2024	A key tool for building differentially private systems is adding Gaussian noise to the output of a function evaluated on a sensitive dataset. Unfortunately, using a continuous distribution presents several practical challenges. First and foremost, finite computers cannot exactly represent samples from continuous distributions, and previous work has demonstrated that seemingly innocuous numerical errors can entirely destroy privacy. Moreover, when the underlying data is itself discrete (e.g., population counts), adding continuous noise makes the result less interpretable. With these shortcomings in mind, we introduce and analyze the discrete Gaussian in the context of differential privacy. Specifically, we theoretically and experimentally show that adding discrete Gaussian noise provides essentially the same privacy and accuracy guarantees as the addition of continuous Gaussian noise. We also present an simple and efficient algorithm for exact sampling from this distribution. This demonstrates its applicability for privately answering counting queries, or more generally, low-sensitivity integer-valued queries.																																	2024-12-28	PPRN:10586202		
J	Yu, Zichun; Das, Spandan; Xiong, Chenyan				Yu, ZiChun/MGU-9969-2025						MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models								Arxiv											2	2;2024-11-16;https://www.arxiv.org/abs/2406.06046v2| 1;2024-06-10;https://www.arxiv.org/abs/2406.06046v1	arXiv:2406.06046			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 16 2024	2024	Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora. Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining. In this paper, we introduce model-aware data selection with data influence models (MATES), where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress. Specifically, we collect oracle data influence by locally probing the pretraining model and fine-tune a small data influence model to approximate it accurately. The data influence model then predicts data influence over the whole pretraining corpus and selects the most influential data for the next pretraining stage. Experiments of pretraining 410M and 1B models on the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks. It doubles the gains achieved by the state-of-the-art data selection approach that leverages larger reference models and reduces the total FLOPs required to reach certain performances by half. Further analyses validate the effectiveness of the locally probed oracle data influence and the approximation with data influence models. 																																	2024-12-27	PPRN:89267977		
J	Fu, Yu; Cai, Zefan; Asi, Abedelkadir; Xiong, Wayne; Dong, Yue; Xiao, Wen										Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning								Arxiv											3	3;2024-11-14;https://www.arxiv.org/abs/2410.19258v3| 2;2024-10-28;https://www.arxiv.org/abs/2410.19258v2| 1;2024-10-25;https://www.arxiv.org/abs/2410.19258v1	arXiv:2410.19258			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 14 2024	2024	Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64 & 128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark.																																	2024-12-22	PPRN:118851878		
J	Yang, Chen; Li, Sikuang; Fang, Jiemin; Liang, Ruofan; Xie, Lingxi; Zhang, Xiaopeng; Shen, Wei; Tian, Qi				Xie, Lingxi/ABF-6996-2020; Yang, Chen/KIC-6398-2024						GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting								Arxiv											4	4;2024-11-13;https://www.arxiv.org/abs/2402.10259v4| 3;2024-09-17;https://www.arxiv.org/abs/2402.10259v3| 2;2024-02-20;https://www.arxiv.org/abs/2402.10259v2| 1;2024-02-15;https://www.arxiv.org/abs/2402.10259v1	arXiv:2402.10259			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 13 2024	2024	Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination, which explicitly inject structure priors into the initial optimization process to help build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. We further design a COLMAP-free variant, where pre-given accurate camera poses are not required, which achieves competitive quality and facilitates wider applications. GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, OpenIllumination, and our-collected unposed images, achieving superior performance from only four views and significantly outperforming previous SOTA methods. 																																	2024-12-23	PPRN:87730104		
J	Yang, Shu; Liu, Siyi; Zeng, Donglin; Wang, Xiaofei				Liu, Siyi/HNR-3743-2023; Wang, Dandan/V-1897-2017						Data fusion methods for the heterogeneity of treatment effect and confounding function								Arxiv											2	2;2024-11-13;https://www.arxiv.org/abs/2007.12922v3| 1;2020-07-25;https://www.arxiv.org/abs/2007.12922v2	arXiv:2007.12922			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 13 2024	2024	The heterogeneity of treatment effect (HTE) lies at the heart of precision medicine. Randomized controlled trials are gold-standard for treatment effect estimation but are typically underpowered for heterogeneous effects. In contrast, large observational studies have high predictive power but are often confounded due to the lack of randomization of treatment. We show that an observational study, even subject to hidden confounding, may be used to empower trials in estimating the HTE using the notion of confounding function. The confounding function summarizes the impact of unmeasured confounders on the difference between the observed treatment effect and the causal treatment effect, given the observed covariates, which is unidentifiable based only on the observational study. Coupling the trial and observational study, we show that the HTE and confounding function are identifiable. We then derive the semiparametric efficient scores and the integrative estimators of the HTE and confounding function. We clarify the conditions under which the integrative estimator of the HTE is strictly more efficient than the trial estimator. Finally, we illustrate the integrative estimators via simulation and an application.																																	2024-12-22	PPRN:12031116		
J	Kahatapitiya, Kumara; Liu, Haozhe; He, Sen; Liu, Ding; Jia, Menglin; Zhang, Chenyang; Ryoo, Michael S.; Xie, Tian				Kahatapitiya, Kumara/AAU-3656-2021						Adaptive Caching for Faster Video Generation with Diffusion Transformers								Arxiv											1	1;2024-11-07;https://www.arxiv.org/abs/2411.02397v2	arXiv:2411.02397			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 07 2024	2024	Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) - despite making significant headway in this context - have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.																																	2024-12-16	PPRN:119074179		
J	Davoudi, Zohreh; Hsieh, Chung-Chun; Kadam, Saurabh V.										Scattering wave packets of hadrons in gauge theories: Preparation on a quantum computer								Arxiv											2	2;2024-11-05;https://www.arxiv.org/abs/2402.00840v3| 1;2024-02-01;https://www.arxiv.org/abs/2402.00840v1	arXiv:2402.00840			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 05 2024	2024	Quantum simulation holds promise of enabling a complete description of high-energy scattering processes rooted in gauge theories of the Standard Model. A first step in such simulations is preparation of interacting hadronic wave packets. To create the wave packets, one typically resorts to adiabatic evolution to bridge between wave packets in the free theory and those in the interacting theory, rendering the simulation resource intensive. In this work, we construct a wave-packet creation operator directly in the interacting theory to circumvent adiabatic evolution, taking advantage of resource-efficient schemes for ground-state preparation, such as variational quantum eigensolvers. By means of an ansatz for bound mesonic excitations in confining gauge theories, which is subsequently optimized using classical or quantum methods, we show that interacting mesonic wave packets can be created efficiently and accurately using digital quantum algorithms that we develop. Specifically, we obtain highfidelity mesonic wave packets in the Z2 and U(1) lattice gauge theories coupled to fermionic matter in 1+1 dimensions. Our method is applicable to both perturbative and non-perturbative regimes of couplings. The wave-packet creation circuit for the case of the Z2 lattice gauge theory is built and implemented on the Quantinuum H1-1 trapped-ion quantum computer using 13 qubits and up to 308 entangling gates. The fidelities agree well with classical benchmark calculations after employing a simple symmetry-based noise-mitigation technique. This work serves as a step toward quantum computing scattering processes in quantum chromodynamics.																																	2025-04-19	PPRN:87455692		
J	Su, Aofeng; Wang, Aowen; Ye, Chao; Zhou, Chen; Zhang, Ga; Zhu, Guangcheng; Wang, Haobo; Xu, Haokai; Chen, Hao; Li, Haoze; Lan, Haoxuan; Tian, Jiaming; Yuan, Jing; Zhao, Junbo; Zhou, Junlin; Shou, Kaizhe; Zha, Liangyu; Long, Lin; Li, Liyao; Wu, Pengzuo; Zhang, Qi; Huang, Qingyi; Yang, Saisai; Zhang, Tao; Ye, Wentao; Zhu, Wufang; Hu, Xiaomeng; Gu, Xijun; Sun, Xinjie; Li, Xiang; Yang, Yuhang; Xiao, Zhiqing				立尧, 李/AAE-4071-2022; Yang, Yuhang/KHV-9297-2024; Xu, Haokai/HIQ-1723-2022; huang, qingyi/GLQ-6980-2022; zhang, tao/GYA-5878-2022; Li, Haoze/GLT-9913-2022; Wang, Haobo/JCF-1064-2023; Sun, Xinjie/IZQ-3533-2023; Ye, Chao/HGD-3564-2022						TableGPT2: A Large Multimodal Model with Tabular Data Integration								Arxiv											2	2;2024-11-07;https://www.arxiv.org/abs/2411.02059v3| 1;2024-11-04;https://www.arxiv.org/abs/2411.02059v1	arXiv:2411.02059			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Nov 04 2024	2024	The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI applications, presenting vast new opportunities across industries. Yet, the integration of tabular data remains notably underdeveloped, despite its foundational role in numerous real-world domains. This gap is critical for three main reasons. First, database or data warehouse data integration is essential for advanced applications; second, the vast and largely untapped resource of tabular data offers immense potential for analysis; and third, the business intelligence domain specifically demands adaptable, precise solutions that many current LLMs may struggle to provide. In response, we introduce TableGPT2, a model rigorously pre-trained and finetuned with over 593.8K tables and 2.36M high quality query-table-output tuples, a scale of table-related data unprecedented in prior research. This extensive training enables TableGPT2 to excel in table-centric tasks while maintaining strong general language and coding abilities. One of TableGPT2’s key innovations is its novel table encoder, specifically designed to capture schema-level and cell-level information. This encoder strengthens the model’s ability to handle ambiguous queries, missing column names, and irregular tables commonly encountered in real-world applications. Similar to the VLMs, this pioneering approach integrates with the decoder to form a robust large multimodal model. We believe the results are compelling: over 23 benchmarking metrics, TableGPT2 achieves an average performance improvement of 35.20% in the 7B model and 49.32% in the 72B model over prior benchmark-neutral2 LLMs, with robust general-purpose capabilities intact. We release an open-source repository (Section 2) that includes both the model and a comprehensive agent workflow, along with a subset of RealTabBench. This release aims to foster further exploration and application in real-world data-driven and BI production environments.																																	2024-12-16	PPRN:119070312		
J	Xu, Yifan; Liu, Xiao; Sun, Xueqiao; Cheng, Siyi; Yu, Hao; Lai, Hanyu; Zhang, Shudan; Zhang, Dan; Tang, Jie; Dong, Yuxiao				Cheng, Siyi/IUQ-6798-2023; Xu, Yifan/KVC-2581-2024						AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents								Arxiv											2	2;2024-11-04;https://www.arxiv.org/abs/2410.24024v2| 1;2024-10-31;https://www.arxiv.org/abs/2410.24024v1	arXiv:2410.24024			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 04 2024	2024	Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose A NDROID L AB as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. A NDROID L AB benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the A NDROID L AB environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. 																																	2024-12-16	PPRN:118937476		
J	Wei, Yuxiang; Cassano, Federico; Liu, Jiawei; Ding, Yifeng; Jain, Naman; Mueller, Zachary; de Vries, Harm; von Werra, Leandro; Guha, Arjun; Zhang, Lingming				Liu, Jiawei/JVZ-3421-2024; Wei, Yuxiang/KYQ-5192-2024; Mueller, Zachary/U-1583-2019						SelfCodeAlign: Self-Alignment for Code Generation								Arxiv											2	2;2024-11-01;https://www.arxiv.org/abs/2410.24198v2| 1;2024-10-31;https://www.arxiv.org/abs/2410.24198v1	arXiv:2410.24198			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Nov 01 2024	2024	Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. For programming tasks, most models are finetuned with costly human-annotated instruction-response pairs or those generated by large, proprietary LLMs, which may not be permitted. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70BInstruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component’s effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and selfaligned code LLM that achieves state-of-the-art coding performance. Overall, SelfCodeAlign shows for the first time that a strong instruction-tuned code LLM can result from self-alignment rather than distillation.																																	2024-12-06	PPRN:118937222		
J	Chang, Li-Wen; Bao, Wenlei; Hou, Qi; Jiang, Chengquan; Zheng, Ningxin; Zhong, Yinmin; Zhang, Xuanrun; Song, Zuquan; Yao, Chengji; Jiang, Ziheng; Lin, Haibin; Jin, Xin; Liu, Xin				Jiang, Ziheng/GON-6875-2022; zhong, yinmin/OHU-3633-2025; Lin, Haibin/ABI-8047-2020						FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion								Arxiv											4	4;2024-10-23;https://www.arxiv.org/abs/2406.06858v5| 3;2024-06-18;https://www.arxiv.org/abs/2406.06858v4| 2;2024-06-14;https://www.arxiv.org/abs/2406.06858v3| 1;2024-06-12;https://www.arxiv.org/abs/2406.06858v2	arXiv:2406.06858			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 23 2024	2024	Large deep learning models have demonstrated strong ability to solve many tasks across a wide range of applications. Those large models typically require training and inference to be distributed. Tensor parallelism is a common technique partitioning computation of an operation or layer across devices to overcome the memory capacity limitation of a single processor, and/or to accelerate computation to meet a certain latency requirement. However, this kind of parallelism introduces additional communication that might contribute a significant portion of overall runtime. Thus limits scalability of this technique within a group of devices with high speed interconnects, such as GPUs with NVLinks in a node. This paper proposes a novel method, Flux, to significantly hide communication latencies with dependent computations for GPUs. Flux overdecomposes communication and computation operations into much finer-grained operations and further fuses them into a larger kernel to effectively hide communication without compromising kernel efficiency. Flux can potentially overlap up to 96% of communication given a fused kernel. Overall, it can achieve up to 1.24x speedups for training over Megatron-LM on a cluster of 128 GPUs with various GPU generations and interconnects, and up to 1.66x and 1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8 GPUs with various GPU generations and interconnects.																																	2024-11-27	PPRN:89291036		
J	Wu, Wenshan; Mao, Shaoguang; Zhang, Yadong; Xia, Yan; Dong, Li; Cui, Lei; Wei, Furu				Zhang, Yadong/JDV-6631-2023						Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models								Arxiv											3	3;2024-10-23;https://www.arxiv.org/abs/2404.03622v3| 2;2024-05-24;https://www.arxiv.org/abs/2404.03622v2| 1;2024-04-04;https://www.arxiv.org/abs/2404.03622v1	arXiv:2404.03622			http://arxiv.org/licenses/nonexclusive-distrib/1.0/	http://arxiv.org/licenses/nonexclusive-distrib/1.0/			preprint	Oct 23 2024	2024	Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind’s Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind’s eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes in our project page. [GRAPHICS]																																	2024-11-24	PPRN:88405803		
J	Zhang, Haiyu; Chen, Xinyuan; Wang, Yaohui; Liu, Xihui; Wang, Yunhong; Qiao, Yu				yaohui, Wang/HJA-6302-2022; Liu, Xihui/LHA-5141-2024; Wang, Yunhong/MZS-0685-2025; Qiao, Yu/ABD-5787-2021; Zhang, Jie/ABB-1353-2021						4Diffusion: Multi-view Video Diffusion Model for 4D Generation								Arxiv											2	2;2024-10-22;https://www.arxiv.org/abs/2405.20674v2| 1;2024-05-31;https://www.arxiv.org/abs/2405.20674v1	arXiv:2405.20674			http://creativecommons.org/licenses/by-nc-sa/4.0/	http://creativecommons.org/licenses/by-nc-sa/4.0/			preprint	Oct 22 2024	2024	Current 4D generation methods have achieved noteworthy efficacy with the aid of advanced diffusion generative models. However, these methods lack multiview spatial-temporal modeling and encounter challenges in integrating diverse prior knowledge from multiple diffusion models, resulting in inconsistent temporal appearance and flickers. In this paper, we propose a novel 4D generation pipeline, namely 4Diffusion, aimed at generating spatial-temporally consistent 4D content from a monocular video. We first design a unified diffusion model tailored for multiview video generation by incorporating a learnable motion module into a frozen 3Daware diffusion model to capture multi-view spatial-temporal correlations. After training on a curated dataset, our diffusion model acquires reasonable temporal consistency and inherently preserves the generalizability and spatial consistency of the 3D-aware diffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling loss, which is based on our multi-view video diffusion model, to optimize 4D representation parameterized by dynamic NeRF. This aims to eliminate discrepancies arising from multiple diffusion models, allowing for generating spatial-temporally consistent 4D content. Moreover, we devise an anchor loss to enhance the appearance details and facilitate the learning of dynamic NeRF. Extensive qualitative and quantitative experiments demonstrate that our method achieves superior performance compared to previous methods.																																	2024-11-22	PPRN:89131545		
J	Cao, Maosong; Lam, Alexander; Duan, Haodong; Liu, Hongwei; Zhang, Songyang; Chen, Kai				duan, haodong/JEP-4396-2023; Zhang, Songyang/GPX-5621-2022						CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution								Arxiv											1	1;2024-10-21;https://www.arxiv.org/abs/2410.16256v1	arXiv:2410.16256			http://creativecommons.org/licenses/by/4.0/	http://creativecommons.org/licenses/by/4.0/			preprint	Oct 21 2024	2024	Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce CompassJudger-1, the first open-source all-in-one judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established JudgerBench, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community at CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.																																	2024-11-20	PPRN:118748272		
